[{"authors":null,"categories":null,"content":"Sobre m√≠ Soy climat√≥logo y actualmente investigador posdoctoral y profesor en la Universidad de Santiago de Compostela. Soy originario de Grevenbroich, cerca de Colonia, en Alemania. Me gradu√© en Geograf√≠a y Filolog√≠a Hisp√°nica en la Universidad de Colonia y la Universidad RWTH-Aachen en 2010. Despu√©s de haber conocido mi esposa en Galicia, me vine a Santiago de Compostela, en el noroeste de Espa√±a, e hice mi doctorado en 2015 sobre la relaci√≥n entre salud y clima en la misma universidad.\nLas principales l√≠neas de investigaci√≥n son, por un lado, la biometeorolog√≠a y la geograf√≠a de salud, la relaci√≥n entre la salud humana y el ambiente atmosf√©rico, y por otro lado, la geograf√≠a f√≠sica aplicada enfocando las variables atmosf√©ricas y sus comportamientos espacio-temporales.\nSoy un entusiasta usuario R, con mucha curiosidad por el an√°lisis espacial, la visualizaciones, gesti√≥n y la manipulaci√≥n de datos, y GIS.\nSoy miembro del grupo Salud P√∫blica en la Universidad de Santiago de Compostela. Adem√°s, soy colaborador de otros dos grupos de investigaci√≥n, Geobiomet en la Universidad de Cantabria y Climatology Group en la Universidad de Barcelona. Desde 2019 soy miembro del MCC Collaborative Research Network, un programa de investigaci√≥n internacional sobre las asociaciones entre los factores ambientales, el clima y la salud.\nNo dudes en ponerte en contacto conmigo.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"es","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dominicroye.github.io/es/author/dr.-dominic-roye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/es/author/dr.-dominic-roye/","section":"authors","summary":"Sobre m√≠ Soy climat√≥logo y actualmente investigador posdoctoral y profesor en la Universidad de Santiago de Compostela. Soy originario de Grevenbroich, cerca de Colonia, en Alemania. Me gradu√© en Geograf√≠a y Filolog√≠a Hisp√°nica en la Universidad de Colonia y la Universidad RWTH-Aachen en 2010.","tags":null,"title":"Dr. Dominic Roy√©","type":"authors"},{"authors":null,"categories":null,"content":"   √çndice  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Dr. Dominic Roy√© FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"es","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://dominicroye.github.io/es/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/es/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://dominicroye.github.io/es/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/es/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://dominicroye.github.io/es/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/es/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://dominicroye.github.io/es/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/es/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://dominicroye.github.io/es/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/es/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["L Nottmeyer","B Armstrong","R Lowe","et al"],"categories":null,"content":"","date":1662508800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1662508800,"objectID":"8e945f1ae3dfa57e3795327b7379a321","permalink":"https://dominicroye.github.io/es/publication/2022-covid19-meteo-global-multicity-analysis-stoten/","publishdate":"2022-09-07T00:00:00Z","relpermalink":"/es/publication/2022-covid19-meteo-global-multicity-analysis-stoten/","section":"publication","summary":"Background and aim: The associations between COVID-19 transmission and meteorological factors are scientifically debated. Several studies have been conducted worldwide, with inconsistent findings. However, often these studies had methodological issues, e.g., did not exclude important confounding factors, or had limited geographic or temporal resolution. Our aim was to quantify associations between temporal variations in COVID-19 incidence and meteorological variables globally. Methods: We analysed data from 455 cities across 20 countries from 3 February to 31 October 2020. We used a time-series analysis that assumes a quasi-Poisson distribution of the cases and incorporates distributed lag non-linear modelling for the exposure associations at the city-level while considering effects of autocorrelation, long-term trends, and day of the week. The confounding by governmental measures was accounted for by incorporating the Oxford Governmental Stringency Index. The effects of daily mean air temperature, relative and absolute humidity, and UV radiation were estimated by applying a meta-regression of local estimates with multi-level random effects for location, country, and climatic zone. Results: We found that air temperature and absolute humidity influenced the spread of COVID-19 over a lag period of 15 days. Pooling the estimates globally showed that overall low temperatures (7.5 ¬∞C compared to 17.0 ¬∞C) and low absolute humidity (6.0 g/m¬≥ compared to 11.0 g/m¬≥) were associated with higher COVID-19 incidence (RR temp =1.33 with 95%CI: 1.08; 1.64 and RR AH =1.33 with 95%CI: 1.12; 1.57). RH revealed no significant trend and for UV some evidence of a positive association was found. These results were robust to sensitivity analysis. However, the study results also emphasise the heterogeneity of these associations in different countries. Conclusion: Globally, our results suggest that comparatively low temperatures and low absolute humidity were associated with increased risks of COVID-19 incidence. However, this study underlines regional heterogeneity of weather-related effects on COVID-19 transmission.","tags":["temperatura","Humedad","Radiaci√≥n UV","COVID-19","Distributed lag non-linear model","An√°lisis global"],"title":"The association of COVID-19 incidence with temperature, humidity, and UV radiation ‚Äì A global multi-city analysis","type":"publication"},{"authors":["HM Choi","W Lee","D Roy√©","et al"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1661990400,"objectID":"adb5c6b08a74b35c686a8cf73342e39f","permalink":"https://dominicroye.github.io/es/publication/2022-greeness-mortality-heat-ebiomedicine/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/es/publication/2022-greeness-mortality-heat-ebiomedicine/","section":"publication","summary":"Background: Identifying how greenspace impacts the temperature-mortality relationship in urban environments is crucial, especially given climate change and rapid urbanization. However, the effect modification of greenspace on heat-related mortality has been typically focused on a localized area or single country. This study examined the heat-mortality relationship among different greenspace levels in a global setting. Methods: We collected daily ambient temperature and mortality data for 452 locations in 24 countries and used Enhanced Vegetation Index (EVI) as the greenspace measurement. We used distributed lag non-linear model to estimate the heat-mortality relationship in each city and the estimates were pooled adjusting for city-specific average temperature, city-specific temperature range, city-specific population density, and gross domestic product (GDP). The effect modification of greenspace was evaluated by comparing the heat-related mortality risk for different greenspace groups (low, medium, and high), which were divided into terciles among 452 locations. Findings: Cities with high greenspace value had the lowest heat-mortality relative risk of 1¬∑19 (95% CI: 1¬∑13, 1¬∑25), while the heat-related relative risk was 1¬∑46 (95% CI: 1¬∑31, 1¬∑62) for cities with low greenspace when comparing the 99th temperature and the minimum mortality temperature. A 20% increase of greenspace is associated with a 9¬∑02% (95% CI: 8¬∑88, 9¬∑16) decrease in the heat-related attributable fraction, and if this association is causal (which is not within the scope of this study to assess), such a reduction could save approximately 933 excess deaths per year in 24 countries.","tags":["espacio verde","calor","mortalidad","modificaci√≥n del efecto"],"title":"Effect modification of greenness on the association between heat and mortality: A multi-city multi-country study","type":"publication"},{"authors":null,"categories":["sig","R","R:intermedio","visualizaci√≥n"],"content":"\rEs muy habitual ver mapas de relieve con efectos de sombras, tambi√©n conocidos como ‚Äòhillshade‚Äô lo que genera una profundidad visual. ¬øC√≥mo podemos crear estos efectos en R y visualizarlos en ggplot2?\nPaquetes\r\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\relevatr\rAcceso a datos de elevaci√≥n desde varias API\r\rterra\rImportar, exportar y manipular raster (paquete sucesor de raster)\r\rwhitebox\rUna interfaz R para la biblioteca ‚ÄòWhiteboxTools‚Äô, que es una plataforma avanzada de an√°lisis de datos geoespaciales\r\rtidyterra\rFunciones auxilares para trabajar con {terra}\r\rgiscoR\rL√≠mites administrativos del mundo\r\rggnewscale\rExtensi√≥n para ggplot2 de m√∫ltiples ‚Äòscales‚Äô\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;elevatr\u0026quot;)) install.packages(\u0026quot;elevatr\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;whitebox\u0026quot;)) install.packages(\u0026quot;whitebox\u0026quot;)\rif(!require(\u0026quot;tidyterra\u0026quot;)) install.packages(\u0026quot;tidyterra\u0026quot;)\rif(!require(\u0026quot;giscoR\u0026quot;)) install.packages(\u0026quot;giscoR\u0026quot;)\rif(!require(\u0026quot;ggnewscale\u0026quot;)) install.packages(\u0026quot;ggnewscale\u0026quot;)\r# paquetes\rlibrary(sf)\rlibrary(elevatr)\rlibrary(tidyverse)\rlibrary(terra)\rlibrary(whitebox)\rlibrary(ggnewscale)\rlibrary(tidyterra)\rlibrary(giscoR)\rlibrary(units)\r\rDatos\rComo √°rea de inter√©s usamos Suiza en este ejemplo. Con excepci√≥n de los l√≠mites de lagos descarga, los datos necesarios los obtenemos a trav√©s de APIs usando diferentes paquetes. El paquete giscoR permite obtener los l√≠mites de pa√≠ses con diferentes resoluciones.\nsuiz \u0026lt;- gisco_get_countries(country = \u0026quot;Switzerland\u0026quot;, resolution = \u0026quot;03\u0026quot;)\rplot(suiz)\rLos l√≠mites de los lagos corresponden a una capa de modelos cartogr√°ficos digitales (DKM500) que ofrece swisstopo. El objetivo es quedar s√≥lo con los grandes lagos, por tanto excluimos todos aquellos con menos de 50 km2 y tambi√©n aquellos situados completamente en territorio italiano. Recuerda que con el paquete units podemos indicar unidades y as√≠ hacer c√°lculos.\n# importamos los lagos\rsuiz_lakes \u0026lt;- st_read(\u0026quot;22_DKM500_GEWAESSER_PLY.shp\u0026quot;)\r## Reading layer `22_DKM500_GEWAESSER_PLY\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\es\\post\\2022-07-19-hillshade-effect\\22_DKM500_GEWAESSER_PLY.shp\u0026#39; ## using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 596 features and 14 fields\r## Geometry type: POLYGON\r## Dimension: XY\r## Bounding box: xmin: 2480000 ymin: 1062000 xmax: 2865000 ymax: 1302000\r## Projected CRS: CH1903+ / LV95\r# filtramos a grandes lagos\rsuiz_lakes \u0026lt;- mutate(suiz_lakes, areakm = set_units(SHP_AREA, \u0026quot;m2\u0026quot;) %\u0026gt;% set_units(\u0026quot;km2\u0026quot;)) %\u0026gt;% filter(areakm \u0026gt; set_units(50, \u0026quot;km2\u0026quot;),\r!NAMN1 %in% c(\u0026quot;Lago di Como / Lario\u0026quot;,\r\u0026quot;Lago d\u0026#39;Iseo\u0026quot;,\r\u0026quot;Lago di Garda\u0026quot;))\rplot(suiz_lakes)\r## Warning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\r## all\r\rModelo digital de terreno (MDT)\rLa funci√≥n get_elev_raster() nos permite descargar un MDT de cualquier regi√≥n del mundo a trav√©s de diferentes proveedores en formato de r√°ster. Por defecto usa AWS. Un argumento esencial es la resoluci√≥n que depende de la latitud, la que se puede indicar como nivel de zoom (v√©ase la ayuda). Por ejemplo, aqu√≠ usamos nivel 10 que a una latitud de 45¬∫ corresponder√≠a a aproximadamente 100m.\nDespu√©s de obtener el MDT de Suiza debemos enmascarar los l√≠mites del pa√≠s. La clase del objeto es RasterLayer del paquete raster, no obstante, el nuevo est√°ndar es terra con la clase SpatRaster. Por eso lo convertimos y despu√©s aplicamos la m√°scara. Finalmente reproyectamos al sistema de coordenadas de Suiza obtenido de los datos vectoriales.\n# obtenemos el mdt con mdt \u0026lt;- get_elev_raster(suiz, z = 10)\r## Mosaicing \u0026amp; Projecting\r## Note: Elevation units are in meters.\rmdt # clase antigua de RasterLayer\r## class : RasterLayer ## dimensions : 3869, 7913, 30615397 (nrow, ncol, ncell)\r## resolution : 0.0006219649, 0.0006219649 (x, y)\r## extent : 5.625, 10.54661, 45.58354, 47.98992 (xmin, xmax, ymin, ymax)\r## crs : +proj=longlat +datum=WGS84 +no_defs ## source : file3f7c489b49.tif ## names : file3f7c489b49 ## values : -32768, 32767 (min, max)\rplot(mdt)\r# convertir a terra y enmascarar la zona de inter√©s\rmdt \u0026lt;- rast(mdt) %\u0026gt;% mask(vect(suiz)) # reproyectamos\rmdt \u0026lt;- project(mdt, crs(suiz_lakes))\r# reproyectamos suiz \u0026lt;- st_transform(suiz, st_crs(suiz_lakes))\rAntes de calcular el efecto de sombra, creamos un simple mapa de relieve. En ggplot2 empleamos la geometr√≠a geom_raster() indicando la longitud, latitud y la variable para definir el color. A√±adimos los l√≠mites de los lagos usando geom_sf() dado que se trata de un objeto sf. Aqu√≠ s√≥lo indicamos el color de relleno con un azul claro. Con ayuda de scale_fill_hypso_tint_c() aplicamos una gama de colores correspondientes a un relieve, tambi√©n llamado tintas hipsom√©tricas, y definimos los cortes en la leyenda. En el resto de funciones hacemos ajustes de aspecto en la leyenda y en el estilo del gr√°fico.\n# convertimos nuestro r√°ster en data.frame de xyz\rmdtdf \u0026lt;- as.data.frame(mdt, xy = TRUE)\rnames(mdtdf)[3] \u0026lt;- \u0026quot;alt\u0026quot;\r# mapa\rggplot() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt)) +\rgeom_sf(data = suiz_lakes,\rfill = \u0026quot;#c6dbef\u0026quot;, colour = NA) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\r## Warning: Removed 1 rows containing missing values (geom_raster).\r\rCalcular el hillshade\rRecordemos que el efecto hillshade es nada m√°s que a√±adir una iluminaci√≥n hipot√©tica con respecto a una posici√≥n de una fuente de luz para as√≠ ganar profundidad. Las sombras dependen de dos variables, el acimut, el √°ngulo de la orientaci√≥n sobra la superficie de una esfera, y la elevaci√≥n, el √°ngulo de la altura de la fuente.\nLa informaci√≥n requerida para simular la iluminaci√≥n es el modelo digital de terreno. La pendiente y la orientaci√≥n podemos derivar del MDT usando la funci√≥n terrain() del paquete terra. La unidad debe ser radianes. Una vez que tenemos todos los datos podemos hacer uso de la funci√≥n shade() indicando el √°ngulo (elevaci√≥n) y la direcci√≥n (acimut). El resultado es un r√°ster con valores entre 0 y 255, lo que nos indica sombras con bajos valores, siendo 0 negro y 255 blanco.\n# estimamos la pendiente\rsl \u0026lt;- terrain(mdt, \u0026quot;slope\u0026quot;, unit = \u0026quot;radians\u0026quot;)\rplot(sl)\r# estimamos la orientaci√≥n\rasp \u0026lt;- terrain(mdt, \u0026quot;aspect\u0026quot;, unit = \u0026quot;radians\u0026quot;)\rplot(asp)\r# calculamos el efecto hillshade con 45¬∫ de elevaci√≥n hill_single \u0026lt;- shade(sl, asp, angle = 45, direction = 300,\rnormalize= TRUE)\r# resultado final hillshade plot(hill_single, col = grey(1:100/100))\r\rCombinar el relieve y el efecto de sombra\rEl problema para a√±adir al mismo tiempo el relieve con su tinta hipsom√©trica y el efecto hillshade dentro de ggplot2 es que tenemos dos diferentes rellenos para cada capa.\rLa soluci√≥n consiste en usar la extensi√≥n ggnewscale que permite a√±adir m√∫ltiples scales. Primero a√±adimos con geom_raster() el hillshade, despu√©s definimos los tonos grises y antes de a√±adir la altitud incluimos la funci√≥n new_scale_fill() para marcar otro relleno diferente. Para lograr el efecto es necesario dar un grado de transparencia a la capa del relieve, en este caso es del 70%. La elecci√≥n de la direcci√≥n es importante, de ah√≠ que debemos tener en cuenta siempre el lugar y el recorrido aparente del sol. (sunearthtools).\n# convertimos el hillshade a xyz\rhilldf_single \u0026lt;- as.data.frame(hill_single, xy = TRUE)\r# mapa ggplot() +\rgeom_raster(data = hilldf_single,\raes(x, y, fill = lyr1),\rshow.legend = FALSE) +\rscale_fill_distiller(palette = \u0026quot;Greys\u0026quot;) +\rnew_scale_fill() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt),\ralpha = .7) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rgeom_sf(data = suiz_lakes,\rfill = \u0026quot;#c6dbef\u0026quot;, colour = NA) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\r\rSombras multidireccionales\rLo que hemos visto es un efecto unidireccional, aunque es lo m√°s habitual, podemos crear un efecto m√°s suave e incluso m√°s realista combinando varias direcciones.\nSimplemente mapeamos sobre un vector de varias direcciones al que se aplica la funci√≥n shade() con una elevaci√≥n fija. Despu√©s convertimos la lista de r√°ster en un objeto multidimensional de varias capas para reducirlas sumando todas las capas.\n# pasamos varias direcciones a shade()\rhillmulti \u0026lt;- map(c(270, 15, 60, 330), function(dir){ shade(sl, asp, angle = 45, direction = dir,\rnormalize= TRUE)}\r)\r# creamos un raster multidimensional y lo reducimos sumando\rhillmulti \u0026lt;- rast(hillmulti) %\u0026gt;% sum()\r# multidireccional\rplot(hillmulti, col = grey(1:100/100))\r# unidireccional\rplot(hill_single, col = grey(1:100/100))\rHacemos lo mismo como antes para visualizar el relieve con sombras multidireccionales.\n# convertimos el hillshade a xyz\rhillmultidf \u0026lt;- as.data.frame(hillmulti, xy = TRUE)\r# mapa\rggplot() +\rgeom_raster(data = hillmultidf,\raes(x, y, fill = sum),\rshow.legend = FALSE) +\rscale_fill_distiller(palette = \u0026quot;Greys\u0026quot;) +\rnew_scale_fill() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt),\ralpha = .7) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rgeom_sf(data = suiz_lakes,\rfill = \u0026quot;#c6dbef\u0026quot;, colour = NA) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;)\r## Warning: Removed 1 rows containing missing values (geom_raster).\r\rOtra alternativa para direcciones multidireccionales\rCon menos control sobre las direcciones tambi√©n ser√≠a posible aplicar la funci√≥n wbt_multidirectional_hillshade() del paquete whitebox. WhiteboxTool contiene muchas herramientas como plataforma avanzada de an√°lisis de datos geoespaciales. La desventaja es que perdemos control sobre las direcciones y que tambi√©n es necesario exportar el MDT a geotiff para obtener otro r√°ster con las sombras.\nPrimero instalamos la librer√≠a con la funci√≥n install_whitebox().\n# instalar whitebox\rinstall_whitebox()\r# exportamos el mdt\rwriteRaster(mdt, \u0026quot;mdt.tiff\u0026quot;, overwrite = TRUE)\r# iniciamos whitebox\rwbt_init()\r# creamos el hillshade\rwbt_multidirectional_hillshade(\u0026quot;mdt.tiff\u0026quot;,\r\u0026quot;hillshade.tiff\u0026quot;)\r# volvemos a importar el hillshade\rhillwb \u0026lt;- rast(\u0026quot;hillshade.tiff\u0026quot;)\rplot(hillwb)\r# volver a enmascarar\rhillwb \u0026lt;- mask(hillwb, vect(suiz))\rplot(hillwb)\r# convertimos el hillshade a xyz\rhillwbdf \u0026lt;- as.data.frame(hillwb, xy = TRUE)\r# mapa\rggplot() +\rgeom_raster(data = hillwbdf,\raes(x, y, fill = hillshade),\rshow.legend = FALSE) +\rscale_fill_distiller(palette = \u0026quot;Greys\u0026quot;) +\rnew_scale_fill() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt),\ralpha = .7) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;)\r## Warning: Removed 1 rows containing missing values (geom_raster).\r\n\r","date":1658275200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1658275200,"objectID":"fadf940e3fd7cdca9741b1f072313588","permalink":"https://dominicroye.github.io/es/2022/efecto-hillshade-o-sombras-de-laderas/","publishdate":"2022-07-20T00:00:00Z","relpermalink":"/es/2022/efecto-hillshade-o-sombras-de-laderas/","section":"post","summary":"Es muy habitual ver mapas de relieve con efectos de sombras, tambi√©n conocidos como 'hillshade' lo que genera una profundidad visual. ¬øC√≥mo podemos crear estos efectos en R y visualizarlos en ggplot2?","tags":["raster","hillshade","mdt","sombras","altitud"],"title":"Efecto hillshade o sombras de laderas","type":"post"},{"authors":["Yao Wu","Shanshan Li","et al"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1651363200,"objectID":"eec262e7bde3c91585c338b442a56200","permalink":"https://dominicroye.github.io/es/publication/2022-temperature-variability-lancet-planetary-health/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/es/publication/2022-temperature-variability-lancet-planetary-health/","section":"publication","summary":"Background Increased mortality risk is associated with short-term temperature variability. However, to our knowledge, there has been no comprehensive assessment of the temperature variability-related mortality burden worldwide. In this study, using data from the MCC Collaborative Research Network, we first explored the association between temperature variability and mortality across 43 countries or regions. Then, to provide a more comprehensive picture of the global burden of mortality associated with temperature variability, global gridded temperature data with a resolution of 0¬∑5¬∞ √ó 0¬∑5¬∞ were used to assess the temperature variability-related mortality burden at the global, regional, and national levels. Furthermore, temporal trends in temperature variability-related mortality burden were also explored from 2000‚Äì19. Methods In this modelling study, we applied a three-stage meta-analytical approach to assess the global temperature variability-related mortality burden at a spatial resolution of 0¬∑5¬∞ √ó 0¬∑5¬∞ from 2000‚Äì19. Temperature variability was calculated as the SD of the average of the same and previous days‚Äô minimum and maximum temperatures. We first obtained location-specific temperature variability related-mortality associations based on a daily time series of 750 locations from the Multi-country Multi-city Collaborative Research Network. We subsequently constructed a multivariable meta-regression model with five predictors to estimate grid-specific temperature variability related-mortality associations across the globe. Finally, percentage excess in mortality and excess mortality rate were calculated to quantify the temperature variability-related mortality burden and to further explore its temporal trend over two decades. Findings An increasing trend in temperature variability was identified at the global level from 2000 to 2019. Globally, 1 753 392 deaths (95% CI 1 159 901‚Äì2 357 718) were associated with temperature variability per year, accounting for 3¬∑4% (2¬∑2‚Äì4¬∑6) of all deaths. Most of Asia, Australia, and New Zealand were observed to have a higher percentage excess in mortality than the global mean. Globally, the percentage excess in mortality increased by about 4¬∑6% (3¬∑7‚Äì5¬∑3) per decade. The largest increase occurred in Australia and New Zealand (7¬∑3%, 95% CI 4¬∑3‚Äì10¬∑4), followed by Europe (4¬∑4%, 2¬∑2‚Äì5¬∑6) and Africa (3¬∑3, 1¬∑9‚Äì4¬∑6). Interpretation Globally, a substantial mortality burden was associated with temperature variability, showing geographical heterogeneity and a slightly increasing temporal trend. Our findings could assist in raising public awareness and improving the understanding of the health impacts of temperature variability.","tags":["variabilidad de la temperatura","calor","efecto de modificaci√≥n","mundial, regional, nacional","mortalidad"],"title":"Global, regional, and national burden of mortality associated with short-term temperature variability from 2000-19: a three-stage modelling study","type":"publication"},{"authors":["Fantina Tedim","Vittorio Leone","et al"],"categories":null,"content":"","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1647302400,"objectID":"6c16604b09cb7b54b26e562e0406fc92","permalink":"https://dominicroye.github.io/es/publication/2022-forest-fire-causes-forests/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/es/publication/2022-forest-fire-causes-forests/","section":"publication","summary":"Forest fires causes and motivations are poorly understood in southern and south-eastern Europe. This research aims to identify how experts perceive the different causes of forest fires as defined in the classification proposed by the European Commission in 2013. A panel of experts (N = 271) was gathered from the EU Southern Member States (France, Greece, Italy, Portugal, and Spain) and from Central (Switzerland) and south-eastern Europe (Croatia, Serbia, Bosnia and Herzegovina, Republic of North Macedonia, and Turkey). Experts were asked to answer a questionnaire to score the importance of the 29 fire causes using a five point (1‚Äì5) Likert Scale. Agricultural burnings received the highest score, followed by Deliberate fire for profit, and Vegetation management. Most of the events stem from negligence, whereas malicious fire setting is arguably overestimated although there are differences among the countries. This research demonstrates the importance of different techniques to enhance the knowledge of the causes of the complex anthropogenic phenomenon of forest fire occurrence.","tags":["incendios forestales","incendios forestales","causas","motivaciones","Europa","percepci√≥n","expertos"],"title":"Forest Fire Causes and Motivations in Southern and South-Eastern Europe through the Perception of Experts Contribution to Enhance the Current Policies","type":"publication"},{"authors":["Malcolm Mistry","Rochelle Schneider","Pierre Masselot","Dominic Roy√©","et al."],"categories":null,"content":"","date":1646870400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1646870400,"objectID":"1a2e9f34223fb351a08a69b76fa38d40","permalink":"https://dominicroye.github.io/es/publication/2022-era5land-global-scientific-reports/","publishdate":"2022-03-10T00:00:00Z","relpermalink":"/es/publication/2022-era5land-global-scientific-reports/","section":"publication","summary":"Epidemiological analyses of health risks associated with non-optimal temperature are traditionally based on ground observations from weather stations that offer limited spatial and temporal coverage. Climate reanalysis represents an alternative option that provide complete spatio-temporal exposure coverage, and yet are to be systematically explored for their suitability in assessing temperature-related health risks at a global scale. Here we provide the first comprehensive analysis over multiple regions to assess the suitability of the most recent generation of reanalysis datasets for health impact assessments and evaluate their comparative performance against traditional station-based data. Our findings show that reanalysis temperature from the last ERA5 products generally compare well to station observations, with similar non-optimal temperature-related risk estimates. However, the analysis offers some indication of lower performance in tropical regions, with a likely underestimation of heat-related excess mortality. Reanalysis data represent a valid alternative source of exposure variables in epidemiological analyses of temperature-related risk.","tags":["temperatura","Rean√°lisis ERA5 Land","mortalidad"],"title":"Comparison of weather station and climate reanalysis data for modelling temperature-related mortality","type":"publication"},{"authors":null,"categories":["sig","R","R:avanzado","visualizaci√≥n"],"content":"\r\rConsideraciones iniciales\rLa informaci√≥n espacio-temporal es clave en muchas disciplinas, especialmente en la climatolog√≠a o la meteorolog√≠a, y ello hace necesario disponer de un formato que permita una estructura multidimensional. Adem√°s es importante que ese formato tenga un alto grado de compatibilidad de intercambio y pueda almacenar un elevado n√∫mero de datos. Estas caracter√≠sticas llevaron al desarrollo del est√°ndar abierto netCDF (NetworkCommon Data Form). El formato netCDF es un est√°ndar abierto de intercambio de datos cient√≠ficos multidimensionales que se utiliza con datos de observaciones o modelos, principalmente en disciplinas como la climatolog√≠a, la meteorolog√≠a y la oceanograf√≠a. La convenci√≥n netCDF es gestionada por Unidata (unidata.ucar.edu/software/netcdf). Se trata de un formato espacio-temporal con una cuadr√≠cula regular o irregular. La estructura multidimensional en forma de matriz (array) permite usar no s√≥lo datos espacio-temporales, sino tambi√©n multivariables. Las caracter√≠sticas generales del netCDF se refieren al uso de un sistema de coordenadas n-dimensional, de m√∫ltiples variables y de una rejilla regular o irregular. Adem√°s se incluyen metadatos que describen los contenidos. La extensi√≥n del formato netCDF es ‚Äúnc‚Äù.\nRecientemente hice uso de datos de sequ√≠a de Espa√±a en formato netCDF con una resoluci√≥n de 1 km para representar el estado de sequ√≠a de cada a√±o desde 1960 (https://monitordesequia.csic.es/historico/). El √≠ndice SPEI (Standardized Precipitation-Evapotranspiration Index) es ampliamente usado para describir la situaci√≥n de sequ√≠a con referencia a diferentes intervalos temporales (3, 6, 12 meses etc).\n{{\u0026lt; tweet 1490260694851362821 \u0026gt;}}\nHe sido preguntado en varias ocaciones sobre el manejo del formato netCDF, por esta raz√≥n, en este post hacemos uso de un subconjunto, el a√±o 2017 del SPEI 12 meses, de estos mismos datos.\n\rPaquetes\rEl manejo de datos en formato netCDF es posible a trav√©s de varios paquetes de forma directa o indirecta. Destaca el paquete {ncdf4} espec√≠ficamente dise√±ado, del que hacen uso tambi√©n otros paquetes aunque no lo veamos. El manejo con {ncdf4} es algo complejo, particularmente por la necesidad de gestionar la memoria RAM cuando tratamos grandes conjuntos de datos o tambi√©n por la forma de manejar la clase array. Otro paquete muy potente es {terra}, que conocemos cuando trabajamos con datos raster y permite usar sus funciones tambi√©n para el manejo del formato netCDF.\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rterra\rImportar, exportar y manipular raster (paquete sucesor de raster)\r\rmapSpain\rL√≠mites administrativos de Espa√±a\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;mapSpain\u0026quot;)) install.packages(\u0026quot;mapSpain\u0026quot;)\r# paquetes\rlibrary(tidyverse)\rlibrary(sf)\rlibrary(terra)\rlibrary(lubridate)\rlibrary(mapSpain)\rPara aquellos con menos experiencia con tidyverse, recomiendo una breve introducci√≥n en este blog post.\n\rDatos\rPrimero descargamos los datos aqu√≠. Importamos los datos del √≠ndice SPEI-12 del a√±o 2017 usando la funci√≥n rast(). En realidad en este paso s√≥lo hemos creado una referencia al archivo sin importar todos los datos a la memoria. Vemos en los metadatos el n√∫mero de capas (layers) disponibles. El √≠ndice SPEI-12 est√° calculado semanalmente con 4 semanas por mes. Si nos fijamos en los metadatos, falta la definic√≥n del sistema de coordenadas, por ello la definimos asignando el c√≥digo EPSG:25830 (ETRS89/UTM 30N).\n# importamos\rspei \u0026lt;- rast(\u0026quot;spei12_2017.nc\u0026quot;)\r# metadatos\rspei\r## class : SpatRaster ## dimensions : 834, 1115, 48 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ## source : spei12_2017.nc ## names : spei1~017_1, spei1~017_2, spei1~017_3, spei1~017_4, spei1~017_5, spei1~017_6, ... ## time : 2017-01-01 to 2017-12-23\r# definimos el sistema de coordenadas\rcrs(spei) \u0026lt;- \u0026quot;EPSG:25830\u0026quot;\r# mapeamos las primeras semanas\rplot(spei)\r\rExtraer metadatos\rExisten diferentes funciones para acceder a metadatos como las fechas, los nombres de las capas o de las variables. Recordemos que los archivos netCDF tambi√©n pueden contener varias variables.\n# fechas\rt \u0026lt;- time(spei)\rhead(t)\r## [1] \u0026quot;2017-01-01 UTC\u0026quot; \u0026quot;2017-01-09 UTC\u0026quot; \u0026quot;2017-01-16 UTC\u0026quot; \u0026quot;2017-01-23 UTC\u0026quot;\r## [5] \u0026quot;2017-02-01 UTC\u0026quot; \u0026quot;2017-02-09 UTC\u0026quot;\r# nombres de capas\rnames(spei) %\u0026gt;% head()\r## [1] \u0026quot;spei12_2017_1\u0026quot; \u0026quot;spei12_2017_2\u0026quot; \u0026quot;spei12_2017_3\u0026quot; \u0026quot;spei12_2017_4\u0026quot;\r## [5] \u0026quot;spei12_2017_5\u0026quot; \u0026quot;spei12_2017_6\u0026quot;\r# nombres de variables\rvarnames(spei)\r## [1] \u0026quot;spei12_2017\u0026quot;\r\rExtracci√≥n de series temporales\rUna posibiliad que permiten los datos netCDF es la extracci√≥n de series temporales, bien a partir de puntos o √°reas. Creamos la series temporales del SPEI-12 para la ciudad de Zaragoza y el promedio de toda la comunidad aut√≥noma de Arag√≥n.\n# coordenadas de Zaragoza\rzar \u0026lt;- st_point(c(-0.883333, 41.65)) %\u0026gt;% st_sfc(crs = 4326) %\u0026gt;% st_as_sf() %\u0026gt;% st_transform(25830)\rEl paquete {terra} s√≥lo acepta su propia clase vectorial SpatVector, por eso es necesario convertir el punto de clase sf con la funci√≥n vect(). Para extraer la serie temporal empleamos la funci√≥n extract(). Los datos extra√≠dos los encontramos en forma de una tabla, cada fila es un elemento de los datos vectoriales y cada columna una capa. En nuestro caso s√≥lo es una fila correspondiente a la ciudad de Zaragoza.\n Algunas funciones pueden tener conflictos con nombres de otros paquetes, para evitarlo podemos escribir el nombre del paquete delante de la funci√≥n que queremos usar, separados por el s√≠mbolo de dos puntos escrito dos veces (package_name::function_name).   # extraer la serie temporal\rspei_zar \u0026lt;- terra::extract(spei, vect(zar))\r# dimensiones\rdim(spei_zar)\r## [1] 1 49\r# creamos un data.frame\rspei_zar \u0026lt;- tibble(date = t, zar = unlist(spei_zar)[-1])\rhead(spei_zar)\r## # A tibble: 6 x 2\r## date zar\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2017-01-01 00:00:00 0.280\r## 2 2017-01-09 00:00:00 0.25 ## 3 2017-01-16 00:00:00 0.220\r## 4 2017-01-23 00:00:00 0.210\r## 5 2017-02-01 00:00:00 0.350\r## 6 2017-02-09 00:00:00 0.220\rEl promedio de la comunidad aut√≥noma de Arag√≥n lo obtenemos usando la geometr√≠a de pol√≠gono e indicando el tipo de funci√≥n con la que queremos resumir el √°rea. La funci√≥n esp_get_ccaa() del paquete mapSpain() es muy √∫til a la hora de importar l√≠mites administrativos espa√±oles de diferentes niveles. En la extracci√≥n es importante que pasemos el argumento na.rm = TRUE de la funci√≥n mean() para excluir p√≠xeles sin valor.\n# l√≠mites de Arag√≥n\raragon \u0026lt;- esp_get_ccaa(\u0026quot;Arag√≥n\u0026quot;) %\u0026gt;% st_transform(25830)\r# extraemos los valores medios del SPEI-12\rspei_arag \u0026lt;- terra::extract(spei, vect(aragon), fun = \u0026quot;mean\u0026quot;, na.rm = TRUE)\r# a√±adimos los nuevos valores a nuestro data.frame\rspei_zar \u0026lt;- mutate(spei_zar, arag = unlist(spei_arag)[-1])\rEn el seguiente paso transformamos la tabla al formato largo con pivot_longer(), fusionando el valor del √≠ndice SPEI de Zaragoza y Arag√≥n. Adem√°s a√±adiremos una columna con la interpretaci√≥n del √≠ndice y cambiaremos las etiquetas.\nspei_zar \u0026lt;- pivot_longer(spei_zar, 2:3, names_to = \u0026quot;reg\u0026quot;, values_to = \u0026quot;spei\u0026quot;) %\u0026gt;%\rmutate(sign = case_when(spei \u0026lt; -0.5 ~ \u0026quot;sequ√≠a\u0026quot;, spei \u0026gt; 0.5 ~ \u0026quot;h√∫medo\u0026quot;,\rTRUE ~ \u0026quot;normal\u0026quot;),\rdate = as_date(date),\rreg = factor(reg, c(\u0026quot;zar\u0026quot;, \u0026quot;arag\u0026quot;), c(\u0026quot;Zaragoza\u0026quot;, \u0026quot;Arag√≥n\u0026quot;)))\rAhora falta por construir el gr√°fico en el que comparamos el SPEI-12 de Zaragoza con el promedio de Arag√≥n. La funci√≥n geom_rect() nos ayuda a dibujar diferentes rect√°ngulos de fondo para marcar la sequ√≠a, episodio normal o h√∫medo.\n# gr√°fico de serie temporal\rggplot(spei_zar) +\rgeom_rect(aes(xmin = min(date), xmax = max(date), ymin = -0.5, ymax = 0.5), fill = \u0026quot;#41ab5d\u0026quot;) +\rgeom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1, ymax = -0.5), fill = \u0026quot;#ffffcc\u0026quot;) +\rgeom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1.5, ymax = -1), fill = \u0026quot;#F3641D\u0026quot;) +\rgeom_hline(yintercept = 0, size = 1, colour = \u0026quot;white\u0026quot;) +\rgeom_line(aes(date, spei, linetype = reg), size = 1, alpha = .7) +\rscale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rlabs(linetype = \u0026quot;\u0026quot;, y = \u0026quot;SPEI-12\u0026quot;, x = \u0026quot;\u0026quot;) +\rcoord_cartesian(expand = FALSE) +\rtheme_minimal() +\rtheme(legend.position = c(.25, .9),\rpanel.grid.minor = element_blank(),\rpanel.ontop = TRUE)\r\rMapa de sequ√≠a\rEspa√±a\rCon el objetivo de crear un mapa de la severidad de sequ√≠a en 2017, primero debemos hacer algunas modificaciones. Con la funci√≥n subset() obtenemos una capa o varias como subconjunto, aqu√≠ seleccionamos la √∫ltima para poder ver el estado de sequ√≠a de todo el a√±o.\nEn el siguiente paso reemplazamos todos los valores mayores de -0,5 con NA. Se considera sequ√≠a cuando el √≠ndice SPEI est√° debajo de -0,5 y, en cambio, si est√° encima de 0,5 hablar√≠amos de un per√≠odo h√∫medo.\nLa clase del raster no es directamente compatible con ggplot, por eso, lo convertimos en una tabla xyz con longitud, latitud y la variable. Cuando hacemos la misma conversi√≥n de varias capas cada columna representar√≠a una capa. Finalmente renombramos nuestra columna del √≠ndice y a√±adimos una nueva columna con distintos grados de severidad de sequ√≠a.\n# extraemos capa(s) con su √≠ndice spei_anual \u0026lt;- subset(spei, 48) # sustituimos valores de no-sequ√≠a con NA\rspei_anual[spei_anual \u0026gt; -0.5] \u0026lt;- NA\r# convertimos nuestro raster en una tabla de xyz\rspei_df \u0026lt;- as.data.frame(spei_anual, xy = TRUE)\rhead(spei_df)\r## x y spei12_2017_48\r## 38096 123100 4858900 -1.48\r## 39195 105500 4857800 -1.59\r## 39197 107700 4857800 -1.40\r## 39211 123100 4857800 -1.47\r## 39212 124200 4857800 -1.50\r## 40310 105500 4856700 -1.63\r# cambiamos el nombre de la variable\rnames(spei_df)[3] \u0026lt;- \u0026quot;spei\u0026quot;\r# categorizamos el √≠ndice y fijamos el orden del factor\rspei_df \u0026lt;- mutate(spei_df, spei_cat = case_when(spei \u0026gt; -0.9 ~ \u0026quot;leve\u0026quot;,\rspei \u0026gt; -1.5 \u0026amp; spei \u0026lt; -0.9 ~ \u0026quot;moderada\u0026quot;,\rspei \u0026gt; -2 \u0026amp; spei \u0026lt;= -1.5 ~ \u0026quot;severa\u0026quot;,\rTRUE ~ \u0026quot;extrema\u0026quot;) %\u0026gt;% fct_relevel(c(\u0026quot;leve\u0026quot;, \u0026quot;moderada\u0026quot;, \u0026quot;severa\u0026quot;, \u0026quot;extrema\u0026quot;)))\rUn mapa de raster lo creamos con la geometr√≠a geom_tile() indicando longitud, latitud y el color de los p√≠xeles con nuestra variable categorizada.\nccaa \u0026lt;- esp_get_ccaa() %\u0026gt;% filter(!ine.ccaa.name %in% c(\u0026quot;Canarias\u0026quot;, \u0026quot;Ceuta\u0026quot;, \u0026quot;Melilla\u0026quot;)) %\u0026gt;% st_transform(25830)\r# mapa\rggplot(spei_df) +\rgeom_tile(aes(x , y, fill = spei_cat)) +\rgeom_sf(data = ccaa, fill = NA, size = .1, colour = \u0026quot;white\u0026quot;, alpha = .4) +\rscale_fill_manual(values = c(\u0026quot;#ffffcc\u0026quot;, \u0026quot;#F3641D\u0026quot;, \u0026quot;#DE2929\u0026quot;, \u0026quot;#8B1A1A\u0026quot;),\rna.value = NA) +\rguides(fill = guide_legend(keywidth = 2, keyheight = .3, label.position = \u0026quot;bottom\u0026quot;,\rtitle.position = \u0026quot;top\u0026quot;)) +\rcoord_sf() +\rlabs(fill = \u0026quot;SEQUIA\u0026quot;) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 0.2,\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;, colour = NA),\rlegend.title = element_text(colour = \u0026quot;white\u0026quot;, size = 20, hjust = .5),\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;),\rplot.margin = margin(t = 10))\r\rArag√≥n\rEn este √∫ltimo ejemplo, seleccionamos la situaci√≥n de sequ√≠a a 12 meses vista, a principios y final de a√±o. La funci√≥n principal es crop() que recorta a la extensi√≥n de un objeto espacial, en nuestro caso es Arag√≥n, despu√©s aplicamos la funci√≥n mask() que enmascara todos aquellos p√≠xeles dentro de los l√≠mites dejando en NA los dem√°s.\n# subconjunto primera y ultima semana 2017\rspei_sub \u0026lt;- subset(spei, c(1, 48)) # recortamos y enmascaramos Arag√≥n\rspei_arag \u0026lt;- crop(spei_sub, aragon) %\u0026gt;% mask(vect(aragon)) # convertimos los datos a xyz\rspei_df_arag \u0026lt;- as.data.frame(spei_arag, xy = TRUE)\r# renombramos las dos capas\rnames(spei_df_arag)[3:4] \u0026lt;- c(\u0026quot;Enero\u0026quot;, \u0026quot;Diciembre\u0026quot;)\r# pasamos al formato de tabla larga fusionando ambos meses\rspei_df_arag \u0026lt;- pivot_longer(spei_df_arag, 3:4, names_to = \u0026quot;mes\u0026quot;, values_to = \u0026quot;spei\u0026quot;) %\u0026gt;% mutate(mes = fct_relevel(mes, c(\u0026quot;Enero\u0026quot;, \u0026quot;Diciembre\u0026quot;)))\rLos dos mapas los hacemos de la misma forma como el de Espa√±a. La diferencia principal es que usamos el √≠ndice SPEI directamente como variable continua. Adem√°s, para crear dos mapas con una fila a√±adimos la funci√≥n facet_grid(). Por √∫ltimo, el √≠ndice muestra valores negativos y positivos, por tanto, es necesario una gama divergente de colores. Con el objetivo de centrar el punto medio en 0 debemos reescalar con ayuda de la funci√≥n rescale() del paquete scales.\n# mapa de Arag√≥n\rggplot(spei_df_arag) +\rgeom_tile(aes(x , y, fill = spei)) +\rgeom_sf(data = aragon, fill = NA, size = .1, colour = \u0026quot;white\u0026quot;, alpha = .4) +\rscale_fill_distiller(palette = \u0026quot;RdYlGn\u0026quot;, direction = 1, values = scales::rescale(c(-2.1, 0, 0.9)),\rbreaks = seq(-2, 1, .5)) +\rguides(fill = guide_colorbar(barwidth = 8, barheight = .3, label.position = \u0026quot;bottom\u0026quot;)) +\rfacet_grid(. ~ mes) +\rcoord_sf() +\rlabs(fill = \u0026quot;SPEI-12\u0026quot;, title = \u0026quot;Arag√≥n\u0026quot;) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 0.5,\rlegend.title = element_text(colour = \u0026quot;white\u0026quot;, vjust = 1.1),\rstrip.text = element_text(colour = \u0026quot;white\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;, colour = NA),\rplot.title = element_text(colour = \u0026quot;white\u0026quot;, size = 20, hjust = .5, vjust = 2.5,\rmargin = margin(b = 10, t = 10)),\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;),\rplot.margin = margin(10, 10, 10, 10))\r\r\rM√°s posibilidades\rEs posible agrupar las diferentes capas aplicando una funci√≥n. Usando los meses de cada semana del SPEI-12 podemos calcular el promedio mensual en 2017. Para ello hacemos uso de la funci√≥n tapp() que a su vez aplica sobre √≠ndices otra funci√≥n. Es imporante que el grupo o bien sea un factor o el √≠ndice de cada capa. Las funci√≥nes tapp() y app() tienen un argumento para procesar en paralelo usando m√°s de un n√∫cleo.\n# meses como factor\rmo \u0026lt;- month(t, label = TRUE)\rmo\r## [1] ene ene ene ene feb feb feb feb mar mar mar mar abr abr abr abr may may may\r## [20] may jun jun jun jun jul jul jul jul ago ago ago ago sep sep sep sep oct oct\r## [39] oct oct nov nov nov nov dic dic dic dic\r## 12 Levels: ene \u0026lt; feb \u0026lt; mar \u0026lt; abr \u0026lt; may \u0026lt; jun \u0026lt; jul \u0026lt; ago \u0026lt; sep \u0026lt; ... \u0026lt; dic\r# promedio por mes\rspei_mo \u0026lt;- tapp(spei, mo, mean)\rspei_mo\r## class : SpatRaster ## dimensions : 834, 1115, 12 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## names : ene, feb, mar, abr, may, jun, ... ## min values : -1.2800, -1.4675, -2.2400, -2.6500, -2.5775, -2.4675, ... ## max values : 1.3875, 1.9175, 1.7475, 1.8375, 1.7500, 1.7000, ...\r# mapas\rplot(spei_mo)\rLa funci√≥n mean() directamente usado sobre un objeto de clase SpatRaster multidimensional devuelve el promedio por celda. El mismo resultado lo podemos obtener con la funci√≥n app() que aplica cualquier funci√≥n. El n√∫mero de capas resultante depende de la funci√≥n, por ejemplo, al aplicar range() el resultado son dos capas, una del valor m√≠nimo y otra del m√°ximo. Por √∫ltimo, la funci√≥n global() resume con la funci√≥n indicada cada capa en forma de una tabla.\n# promedio sobre capas\rspei_mean \u0026lt;- mean(spei)\rspei_mean\r## class : SpatRaster ## dimensions : 834, 1115, 1 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## name : mean ## min value : -2.127083 ## max value : 1.568542\r# mapa\rplot(spei_mean)\r# alternativa\rspei_min \u0026lt;- app(spei, min)\rspei_min\r## class : SpatRaster ## dimensions : 834, 1115, 1 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## name : min ## min value : -3.33 ## max value : 0.29\rspei_range \u0026lt;- app(spei, range)\rnames(spei_range) \u0026lt;- c(\u0026quot;min\u0026quot;, \u0026quot;max\u0026quot;)\rspei_range\r## class : SpatRaster ## dimensions : 834, 1115, 2 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## names : min, max ## min values : -3.33, -1.06 ## max values : 0.29, 2.02\r# mapa\rplot(spei_range)\r# resumen estad√≠stico por capa\rglobal(spei, \u0026quot;mean\u0026quot;, na.rm = TRUE) %\u0026gt;% head()\r## mean\r## spei12_2017_1 -0.03389126\r## spei12_2017_2 -0.17395742\r## spei12_2017_3 -0.13228593\r## spei12_2017_4 -0.07536089\r## spei12_2017_5 0.06718260\r## spei12_2017_6 -0.03461822\r\n\r","date":1646697600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1646697600,"objectID":"892a033a948b7d367c08bce310fb11c2","permalink":"https://dominicroye.github.io/es/2022/uso-de-datos-multidimensionales-espaciales/","publishdate":"2022-03-08T00:00:00Z","relpermalink":"/es/2022/uso-de-datos-multidimensionales-espaciales/","section":"post","summary":"La informaci√≥n espacio-temporal es clave en muchas disciplinas, especialmente en la climatolog√≠a o la meteorolog√≠a, y ello hace necesario disponer de un formato que permita una estructura multidimensional. Adem√°s es importante que ese formato tenga un alto grado de compatibilidad de intercambio y pueda almacenar un elevado n√∫mero de datos.","tags":["ncdf","cubo","sequia","espa√±a","raster"],"title":"Uso de datos multidimensionales espaciales","type":"post"},{"authors":["Yao Wu","Bo Wen","et al"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1646092800,"objectID":"3fbb3b84e73313b56657d0a5d4a39016","permalink":"https://dominicroye.github.io/es/publication/2022-fluctuating-temperature-the-innovation/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/es/publication/2022-fluctuating-temperature-the-innovation/","section":"publication","summary":"Studies have investigated the effects of heat and temperature variability (TV) on mortality. However, few assessed whether TV modifies the heat-mortality association. Data on daily temperature and mortality in the warm season were collected from 717 locations across 36 countries. TV was calculated as the standard deviation of the average of the same and previous days‚Äô minimum and maximum temperatures. We first used location-specific quasi-Poisson regression models with an interaction term between the cross-basis term for mean temperature and quartiles of TV to obtain heat-mortality associations under each quartile of TV, then pooled estimates at the country, regional, and global levels. Results show the increased risk in heat-related mortality with increments in TV, accounting for 0.70% (95% confidence interval [CI], -0.33‚Äì1.69), 1.34% (95% CI: -0.14‚Äì2.73), 1.99% (95% CI: 0.29‚Äì3.57), and 2.73% (95% CI: 0.76‚Äì4.50) of total deaths for Q1‚ÄìQ4 (1st quartile‚Äì4th quartile) of TV. The modification effects of TV varied geographically. Central Europe had the highest attributable fractions (AFs), corresponding to 7.68% (95% CI: 5.25‚Äì9.89) of total deaths for Q4 of TV, while the lowest AFs were observed in North America, with the values for Q4 of 1.74% (95% CI: -0.09‚Äì3.39). TV had a significant modification effect on the heat-mortality association, causing a higher heat-related mortality burden with increments of TV. Implementing targeted strategies against heat exposure and fluctuant temperatures simultaneously would benefit public health.","tags":["variabilidad de la temperatura","calor","efecto de modificaci√≥n","mortalidad"],"title":"Fluctuating temperature modifies heat-mortality association in the globe","type":"publication"},{"authors":null,"categories":["sig","R","R:intermedio","visualizaci√≥n"],"content":"\r\rEn abril de este a√±o he hecho una animaci√≥n de la temperatura media de 24 horas de enero 2020 mostrando tambi√©n el ciclo d√≠a-noche.\nThe average temperature of 24 hours in January 2020 with the day/night cycle. You can see a lot of geographic patterns. I love this kind of hypnotic temperature gifs. #rstats #rspatial #dataviz #climate pic.twitter.com/NA5haUlnie\n\u0026mdash; Dr. Dominic Roy√© (@dr_xeo) April 17, 2021  Mi mayor problema consist√≠a en encontrar una forma para proyectar correctamente el √°rea de noche sin que rompa la geometr√≠a. La soluci√≥n m√°s f√°cil que encontr√© fue rasterizar el pol√≠gono de noche y posteriormente, reproyectarlo. Seguramente se podr√≠a usar un enfoque vectorial, pero aqu√≠ he preferido el uso de datos raster.\nPaquetes\rUsaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rhms\rProporciona una clase simple para almacenar duraciones o valores de hora del d√≠a y mostrarlos en el formato hh:mm:ss\r\rterra\rImportar, exportar y manipular raster (paquete sucesor de raster)\r\rlwgeom\rAcceso a la librer√≠a liblwgeom con funciones vectoriales adicionales para sf\r\rrnaturalearth\rMapas vectoriales del mundo ‚ÄòNatural Earth‚Äô\r\rgifski\rCreaci√≥n de animaciones en formato gif\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;hms\u0026quot;)) install.packages(\u0026quot;hms\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;lwgeom\u0026quot;)) install.packages(\u0026quot;lwgeom\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\rif(!require(\u0026quot;gifski\u0026quot;)) install.packages(\u0026quot;gifski\u0026quot;)\r# paquetes\rlibrary(rnaturalearth)\rlibrary(tidyverse)\rlibrary(lwgeom)\rlibrary(sf)\rlibrary(terra)\rlibrary(lubridate)\rlibrary(hms)\rlibrary(gifski)\r Para usar la resoluci√≥n de 50 y 10 m del paquete {rnaturalearth} es necesario instalar los siguientes paquetes adicionales. Debe estar instalado el paquete {devtools}. devtools::install_github(\u0026ldquo;ropensci/rnaturalearthdata\u0026rdquo;) devtools::install_github(\u0026ldquo;ropensci/rnaturalearthhires\u0026rdquo;)   \rPreparaci√≥n\rFunciones externas\rLas funciones para estimar la l√≠nea separador entre d√≠a y noche se basan en un javascript L.Terminator.js del paquete {Leaflet} que encontr√© en stackoverflow. El script con las funciones lo pod√©is descargar aqu√≠ o acceder en github.\nsource(\u0026quot;terminator.R\u0026quot;) # importamos las funciones\r\rFunciones personalizadas\rLa funci√≥n principal terminator() basada en el javascript de {Leaflet} necesita como argumentos: la fecha con la hora, la extensi√≥n m√≠nima y m√°xima de longitud as√≠ como la resoluci√≥n o el intervalo de longitud.\nt0 \u0026lt;- Sys.time() # fecha y hora de nuestro sistema operativo\rt0\r## [1] \u0026quot;2022-03-27 13:10:39 CEST\u0026quot;\rcoord_nightday \u0026lt;- terminator(t0, -180, 180, 0.2) # estimamos la l√≠nea d√≠a-noche\r# lo convertimos en un objecto espacial de clase sf\rline_nightday \u0026lt;- st_linestring(as.matrix(coord_nightday)) %\u0026gt;% st_sfc(crs = 4326) # ploteamos\rplot(line_nightday)\rEn el sigiente paso obtenemos los pol√≠gonos que corresponden al d√≠a y la noche que separa la l√≠nea estimada anteriormente. Para ello creamos un rect√°ngulo cubriendo todo el planeta y empleamos la funci√≥n st_split() del paquete {lwgeom} que divide el rect√°ngulo.\n# rect√°ngulo wld_bbx \u0026lt;- st_bbox(c(xmin = -180, xmax = 180,\rymin = -90, ymax = 90), crs = 4326) %\u0026gt;% st_as_sfc()\r# divis√≥n con la l√≠nea d√≠a-noche\rpoly_nightday \u0026lt;- st_split(wld_bbx, line_nightday) %\u0026gt;% st_collection_extract(c(\u0026quot;POLYGON\u0026quot;)) %\u0026gt;% st_sf() # ploteamos plot(poly_nightday)\rLa pregunta que ahora surge es cu√°l de los dos poligonos corresponde a la noche y c√∫al al d√≠a. Eso depender√° en qu√© d√≠a del a√±o estamos, dado los cambios de posici√≥n de la Tierra con respecto al Sol. Entre el equinoccio de primeravera y el equinoccio de oto√±o corresponde con el primer pol√≠gono, cu√°ndo tambi√©n podemos observar el d√≠a polar en el polo norte, y en el caso contrario ser√≠a el segundo. El paquete {terra} s√≥lo acepta la clase vectorial propia llamada SpatVector, por eso convertimos el objeto vectorial sf con la funci√≥n vect().\n# selecionamos el segundo pol√≠gono\rpoly_nightday \u0026lt;- slice(poly_nightday, 2) %\u0026gt;% mutate(daynight = 1)\r# creamos el raster con una resoluci√≥n de 0,5¬∫ y la extensi√≥n del mundo\rr \u0026lt;- rast(vect(wld_bbx), resolution = .5)\r# rasterizamos el pol√≠gono de noche night_rast \u0026lt;- rasterize(vect(poly_nightday), r) # resultado en formato raster\rplot(night_rast)\rEn el √∫ltimo paso reproyectamos el raster a Mollweide.\n# definimos la proyecci√≥n del raster (WGS84)\rcrs(night_rast) \u0026lt;- \u0026quot;EPSG:4326\u0026quot;\r# reproyectamos\rnight_rast_prj \u0026lt;- project(night_rast, \u0026quot;ESRI:54009\u0026quot;, mask = TRUE, method = \u0026quot;near\u0026quot;)\r# mapa\rplot(night_rast_prj)\rFinalmente incluimos los pasos individuales que hemos hecho en una funci√≥n personalizada.\nrast_determiner \u0026lt;- function(x_min, date, res) {\r# crea fecha con hora a√±adiendo el n√∫mero de minutos\rt0 \u0026lt;- as_date(date) + minutes(x_min) # estimamos las coordenadas de la l√≠nea que separa d√≠a y noche night_step \u0026lt;- terminator(t0, -180, 180, 0.2) %\u0026gt;% as.matrix()\r# pasamos los puntos a l√≠nea\rnight_line \u0026lt;- st_linestring(night_step) %\u0026gt;% st_sfc(crs = 4326)\r# definimos el rect√°ngulo del planeta\rwld_bbx \u0026lt;- st_bbox(c(xmin = -180, xmax = 180,\rymin = -90, ymax = 90), crs = 4326) %\u0026gt;% st_as_sfc()\r# dividimos el pol√≠gono con la l√≠nea de d√≠a-noche\rpoly_nightday \u0026lt;- st_split(wld_bbx, night_line) %\u0026gt;% st_collection_extract(c(\u0026quot;POLYGON\u0026quot;)) %\u0026gt;% st_sf() # seleccionamos el pol√≠gono seg√∫n la fecha\rif(date \u0026lt;= make_date(year(date), 3, 20) | date \u0026gt;= make_date(year(date), 9, 23)) {\rpoly_nightday \u0026lt;- slice(poly_nightday, 2) %\u0026gt;% mutate(daynight = 1)\r} else {\rpoly_nightday \u0026lt;- slice(poly_nightday, 1) %\u0026gt;% mutate(daynight = 1)\r}\r# creamos el raster con la resoluci√≥n del argumento res\rr \u0026lt;- rast(vect(wld_bbx), resolution = res)\r# rasterizamos el pol√≠gono de noche\rnight_rast \u0026lt;- rasterize(vect(poly_nightday), r) return(night_rast)\r}\rDado que queremos obtener el √°rea de noche para diferentes horas del d√≠a construimos una segunda funci√≥n para aplicar la primera sobre diferentes intervalos del d√≠a (en minutos).\nnight_determinator \u0026lt;- function(time_seq, # minutos date = Sys.Date(), # fecha (por defecto del sistema)\rres = .5) { # resoluci√≥n del raster 0.5¬∫\r# aplicamos la primera funci√≥n sobre un vector de minutos\rnight_raster \u0026lt;- map(time_seq, rast_determiner,\rdate = date, res = res)\r# convertimos el raster en un objeto de tantas capas como unidades de minutos\rnight_raster \u0026lt;- rast(night_raster)\r# definimos la proyecci√≥n WGS84\rcrs(night_raster) \u0026lt;- \u0026quot;EPSG:4326\u0026quot;\rreturn(night_raster)\r}\r\r\rCear un ciclo d√≠a-noche\rPrimero creamos el √°rea de noches para el d√≠a de nuestro sistema operativo con intervalos de 30 minutos. Despu√©s lo reproyectamos a Winkel II.\n# aplicamos nuestra funci√≥n para un d√≠a de 24 horas en intervalos de 30 minutos\rnight_rast \u0026lt;- night_determinator(seq(0, 1410, 30), Sys.Date(), res = .5)\r# proyectamos a Winkel II\rnight_raster_winkel \u0026lt;- project(night_rast, \u0026quot;ESRI:54019\u0026quot;, mask = TRUE,\rmethod = \u0026quot;near\u0026quot;)\r# mapa de los primeros 5\rplot(night_raster_winkel, maxnl = 5)\r\rAnimaci√≥n del ciclo d√≠a-noche\rPreparaci√≥n\rPara crear una animaci√≥n de 24 horas mostrando el movimiento de la noche sobre la Tierra debemos hacer unos pasos previos. Primero obtenemos los l√≠mites del mundo con la funci√≥n ne_countries() y los reproyectamos a la nueva proyecci√≥n Winkel II. Despu√©s convertimos los datos raster en un data.frame indicando que mantenga valores ausentes. Podemos observar que cada capa del raster (de cada intervalo de 30 minutos) es una columna en el data.frame. Renombramos las columnas y convertimos la tabla en un formato largo empleando la funci√≥n pivot_longer(). Lo que hacemos es fusionar todas las columnas de las capas en una √∫nica. Como √∫ltimo paso excluimos los valores ausentes con la funci√≥n filter().\n# l√≠mites de pa√≠ses\rwld \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;) %\u0026gt;% st_transform(\u0026quot;ESRI:54019\u0026quot;)\r# convertimos el raster a un data.frame con xyz\rdf_winkel \u0026lt;- as.data.frame(night_raster_winkel, xy = TRUE, na.rm = FALSE)\r# renombramos todas las columnas correspondientes a los intervalos del d√≠a\rnames(df_winkel)[3:length(df_winkel)] \u0026lt;- str_c(\u0026quot;H\u0026quot;, as_hms(seq(0, 1410, 30)*60))\r# cambiamos a un formato largo de tabla\rdf_winkel \u0026lt;- pivot_longer(df_winkel, 3:length(df_winkel), names_to = \u0026quot;hour\u0026quot;, values_to = \u0026quot;night\u0026quot;) # excluimos los valores ausentes para reducir el tama√±o de la tabla\rdf_winkel \u0026lt;- filter(df_winkel, !is.na(night))\rS√≥lo resta crear una ret√≠cula y obtener la extensi√≥n del mapamundi.\n# ret√≠cula\rgrid \u0026lt;- st_graticule() %\u0026gt;% st_transform(\u0026quot;ESRI:54019\u0026quot;)\r# obtenemos la extensi√≥n del mundo\rbbx \u0026lt;- st_bbox(wld)\rEl mapa de cualquier hora lo construimos con ggplot2 a√±adiendo la geometr√≠a vectorial con geom_sf() (los l√≠mites y la ret√≠cula) y con geom_raster() los datos raster. En el t√≠tulo estamos usando un s√≠mbolo unicode como reloj. Adem√°s definimos la extensi√≥n del mapa en coord_sf() para mantenerlo constante sobre todos los mapas en la animaci√≥n. Por √∫ltimo, hacemos uso de {{ }} de {rlang} dentro de la funci√≥n filter() para poder filtrar nuestros datos raster en forma de tabla. Con el objetivo de que nuestra funci√≥n pueda evaluar correctamente los valores que pasamos en x (los intervalos del d√≠a) es necesario usar esta gramatica de tidy evaluation por data masking de tidyverse. Es un tema para otro post.\n# ejemplo 5 UTC\rx \u0026lt;- \u0026quot;H05:00:00\u0026quot;\r# mapa\rggplot() +\r# l√≠mites\rgeom_sf(data = wld,\rfill = \u0026quot;#74a9cf\u0026quot;, colour = \u0026quot;white\u0026quot;,\rsize = .1) +\r# ret√≠cula\rgeom_sf(data = grid, size = .1) +\r# datos raster filtrados geom_raster(data = filter(df_winkel, hour == {{x}}), aes(x, y), fill = \u0026quot;grey90\u0026quot;,\ralpha = .6) +\r# t√≠tulo\rlabs(title = str_c(\u0026quot;\\U1F551\u0026quot;, str_remove(x, \u0026quot;H\u0026quot;), \u0026quot; UTC\u0026quot;)) + # l√≠mites de extensi√≥n\rcoord_sf(xlim = bbx[c(1, 3)], ylim = bbx[c(2, 4)]) +\r# estilo del mapa\rtheme_void() +\rtheme(plot.title = element_text(hjust = .1, vjust = .9))\r## Warning: Raster pixels are placed at uneven horizontal intervals and will be\r## shifted. Consider using geom_tile() instead.\r\rAnimaci√≥n\rCreamos la animaci√≥n aplicando la funci√≥n walk(), que a su vez pasar√° por el vector de intervalos para filtrar nuestros datos y mapear cada paso haciendo uso de ggplot.\nwalk(str_c(\u0026quot;H\u0026quot;, as_hms(seq(0, 1410, 30)*60)), function(step){\rg \u0026lt;- ggplot() +\rgeom_sf(data = wld,\rfill = \u0026quot;#74a9cf\u0026quot;, colour = \u0026quot;white\u0026quot;,\rsize = .1) +\rgeom_sf(data = grid,\rsize = .1) +\rgeom_raster(data = filter(df_winkel, hour == {{step}}), aes(x, y), fill = \u0026quot;grey90\u0026quot;,\ralpha = .6) +\rlabs(title = str_c(\u0026quot;\\U1F551\u0026quot;, str_remove(x, \u0026quot;H\u0026quot;), \u0026quot; UTC\u0026quot;)) + coord_sf(xlim = bbx[c(1, 3)], ylim = bbx[c(2, 4)]) +\rtheme_void() +\rtheme(plot.title = element_text(hjust = .1, vjust = .9))\rggsave(str_c(\u0026quot;wld_night_\u0026quot;, str_remove_all(step, \u0026quot;:\u0026quot;), \u0026quot;.png\u0026quot;), g,\rheight = 4.3, width = 8.4, bg = \u0026quot;white\u0026quot;, dpi = 300, units = \u0026quot;in\u0026quot;)\r})\rLa creaci√≥n del gif final lo hacemos con gifski() pas√°ndole los nombres de las imagenes en el orden como deben aparecer en la animaci√≥n.\nfiles \u0026lt;- str_c(\u0026quot;wld_night_H\u0026quot;, str_remove_all(as_hms(seq(0, 710, 30)*60), \u0026quot;:\u0026quot;), \u0026quot;.png\u0026quot;)\rgifski(files, \u0026quot;night_day.gif\u0026quot;, width = 807, height = 409, loop = TRUE, delay = 0.1)\r\n\r\r","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1639958400,"objectID":"e36ffde72e50520315f5cab806aa260a","permalink":"https://dominicroye.github.io/es/2021/visualizar-el-ciclo-de-dia-noche-en-un-mapamundi/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/es/2021/visualizar-el-ciclo-de-dia-noche-en-un-mapamundi/","section":"post","summary":"En abril de este a√±o he hecho una animaci√≥n de la temperatura media de 24 horas de enero 2020 mostrando tambi√©n el ciclo d√≠a-noche. Mi mayor problema consist√≠a en encontrar una forma para proyectar correctamente el √°rea de noche sin que rompa la geometr√≠a. La soluci√≥n m√°s f√°cil que encontr√© fue rasterizar el pol√≠gono de noche y posteriormente, reproyectarlo. Seguramente se podr√≠a usar un enfoque vectorial, pero aqu√≠ he preferido el uso de datos raster.","tags":["mapamundi","noche-dia","animaci√≥n"],"title":"Visualizar el ciclo de d√≠a-noche en un mapamundi","type":"post"},{"authors":["F Sera","B Armstrong","S Abbott","S Meakin","K O'Keilly","R von Borries","R Schneider","D Roy√©","M Hashizume","M Pascal","A Tob√≠as","A Vicedo-Cabrera","MCC Collaborative Research Network","CMMID COVID-19 Working Group","A Gasparrini","R Lowe"],"categories":null,"content":"","date":1634169600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1634169600,"objectID":"d2bf4c0817addb47a5d983e8161edce6","permalink":"https://dominicroye.github.io/es/publication/2021-covid-nature-comm/","publishdate":"2021-10-14T00:00:00Z","relpermalink":"/es/publication/2021-covid-nature-comm/","section":"publication","summary":"There is conflicting evidence on the influence of weather on COVID-19 transmission. Our aim is to estimate weather-dependent signatures in the early phase of the pandemic, while controlling for socio-economic factors and non-pharmaceutical interventions. We identify a modest non-linear association between mean temperature and the effective reproduction number (Re) in 409 cities in 26 countries, with a decrease of 0.087 (95% CI: 0.025; 0.148) for a 10 ¬∞C increase. Early interventions have a greater effect on Re with a decrease of 0.285 (95% CI 0.223; 0.347) for a 5th - 95th percentile increase in the government response index. The variation in the effective reproduction number explained by government interventions is 6 times greater than for mean temperature. We find little evidence of meteorological conditions having influenced the early stages of local epidemics and conclude that population behaviour and government interventions are more important drivers of transmission.","tags":["COVID-19","salud humana","temperatura","pandemia","MCC Study"],"title":"A cross-sectional analysis of meteorological factors and SARS-CoV-2 transmission in 409 cities across 26 countries","type":"publication"},{"authors":["G Chen","et al"],"categories":null,"content":"","date":1631232000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1631232000,"objectID":"387a305ab367b7af99ba32d3233f90b0","permalink":"https://dominicroye.github.io/es/publication/2021-forest-fire-mortality-lancet-planetary-health/","publishdate":"2021-09-10T00:00:00Z","relpermalink":"/es/publication/2021-forest-fire-mortality-lancet-planetary-health/","section":"publication","summary":"Background: Many regions of the world are now facing more frequent and unprecedentedly large wildfires. However, the association between wildfire-related PM2¬∑5 and mortality has not been well characterised. We aimed to comprehensively assess the association between short-term exposure to wildfire-related PM2¬∑5 and mortality across various regions of the world. Methods: For this time series study, data on daily counts of deaths for all causes, cardiovascular causes, and respiratory causes were collected from 749 cities in 43 countries and regions during 2000‚Äì16. Daily concentrations of wildfire-related PM2¬∑5 were estimated using the three-dimensional chemical transport model GEOS-Chem at a 0¬∑25¬∞ √ó 0¬∑25¬∞ resolution. The association between wildfire-related PM2¬∑5 exposure and mortality was examined using a quasi-Poisson time series model in each city considering both the current-day and lag effects, and the effect estimates were then pooled using a random-effects meta-analysis. Based on these pooled effect estimates, the population attributable fraction and relative risk (RR) of annual mortality due to acute wildfire-related PM2¬∑5 exposure was calculated. Findings: 65¬∑6 million all-cause deaths, 15¬∑1 million cardiovascular deaths, and 6¬∑8 million respiratory deaths were included in our analyses. The pooled RRs of mortality associated with each 10 Œºg/m3 increase in the 3-day moving average (lag 0‚Äì2 days) of wildfire-related PM2¬∑5 exposure were 1¬∑019 (95% CI 1¬∑016‚Äì1¬∑022) for all-cause mortality, 1¬∑017 (1¬∑012‚Äì1¬∑021) for cardiovascular mortality, and 1¬∑019 (1¬∑013‚Äì1¬∑025) for respiratory mortality. Overall, 0¬∑62% (95% CI 0¬∑48‚Äì0¬∑75) of all-cause deaths, 0¬∑55% (0¬∑43‚Äì0¬∑67) of cardiovascular deaths, and 0¬∑64% (0¬∑50‚Äì0¬∑78) of respiratory deaths were annually attributable to the acute impacts of wildfire-related PM2¬∑5 exposure during the study period. Interpretation: Short-term exposure to wildfire-related PM2¬∑5 was associated with increased risk of mortality. Urgent action is needed to reduce health risks from the increasing wildfires.","tags":["cambio clim√°tico","incendios forestales","poluci√≥n","mortalidad","MCC Study","global","riesgo"],"title":"Mortality risk attributable to wildfire-related PM 2.5 pollution: a global time series study in 749 locations","type":"publication"},{"authors":null,"categories":["R","R:principante","visualizaci√≥n"],"content":"\r\rEl clima de un lugar se suele presentar a trav√©s de climogramas que combinan la precipitaci√≥n y temperatura mensual en un √∫nico gr√°fico. No obstante, tambi√©n es interesante visualizar el clima a nivel diario mostrando la amplitud t√©rmica y la temperatura media diaria. Para ello, se calculan las medias de cada d√≠a del a√±o de las m√≠nimas, m√°ximas y medias diarias.\nEl ciclo anual del clima presenta una buena oportunidad para usar un gr√°fico radial, polar o circular lo que nos permite visualizar de forma clara los patrones estacionales.\nPaquetes\rUsaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rfs\rProporciona una interfaz uniforme y multiplataforma para las operaciones del sistema de archivos\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rjanitor\rFunciones sencillas para examinar y limpiar datos\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r# paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(fs)\rlibrary(janitor)\r\rPreparaci√≥n\rDatos\rDescargamos los datos de temperatura de una selecci√≥n de ciudades estadounidenses descarga. Pod√©is descargar otras ciudades del mundo a trav√©s de los datasets WMO o GHCN en NCDC/NOAA.\n\rImportar\rPara importar las series temporales de temperatura de cada ciudad, que encontramos en varios archivos, aplicamos la funci√≥n read_csv() usando map_df(). La funci√≥n dir_ls() del paquete fs nos devuelve el listado de archivos con la extensi√≥n csv. El suffijo de map() indica que queremos unir todas las tablas importadas en una √∫nica. Para aquellos con menos experiencia con tidyverse, recomiendo una breve introducci√≥n en este blog post.\nDespu√©s obtenemos los nombres de las estaciones meteorologicas y definimos un nuevo vector con los nuevos nombres.\n# importamos los datos\rmeteo \u0026lt;- dir_ls(regexp = \u0026quot;.csv$\u0026quot;) %\u0026gt;% map_df(read_csv)\r# nombres de las estaciones\rstats_names \u0026lt;- unique(meteo$NAME)\rstats_names\r## [1] \u0026quot;CHICAGO OHARE INTERNATIONAL AIRPORT, IL US\u0026quot; ## [2] \u0026quot;LAGUARDIA AIRPORT, NY US\u0026quot; ## [3] \u0026quot;MIAMI INTERNATIONAL AIRPORT, FL US\u0026quot; ## [4] \u0026quot;HOUSTON INTERCONTINENTAL AIRPORT, TX US\u0026quot; ## [5] \u0026quot;ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPORT, GA US\u0026quot;\r## [6] \u0026quot;SAN FRANCISCO INTERNATIONAL AIRPORT, CA US\u0026quot; ## [7] \u0026quot;SEATTLE TACOMA AIRPORT, WA US\u0026quot; ## [8] \u0026quot;DENVER INTERNATIONAL AIRPORT, CO US\u0026quot; ## [9] \u0026quot;MCCARRAN INTERNATIONAL AIRPORT, NV US\u0026quot;\r# nuevos nombres de las ciudades\rcities \u0026lt;- c(\u0026quot;CHICAGO\u0026quot;, \u0026quot;NEW YORK\u0026quot;, \u0026quot;MIAMI\u0026quot;, \u0026quot;HOUSTON\u0026quot;, \u0026quot;ATLANTA\u0026quot;, \u0026quot;SAN FRANCISCO\u0026quot;, \u0026quot;SEATTLE\u0026quot;, \u0026quot;DENVER\u0026quot;, \u0026quot;LAS VEGAS\u0026quot;)\r\rModificar\rEn el primer paso modificaremos los datos originales, 1) seleccionando √∫nicamente las columnas de inter√©s, 2) filtrando al per√≠odo 1991-2020, 3) definiendo los nuevos nombres de las estaciones, 4) calculando la temperatura media all√≠ donde est√© ausente, 5) limpiando los nombres de las columnas, y 6) creando una nueva variable con los d√≠as del a√±o. La funci√≥n clean_names() de janitor es muy √∫til para obtener nombres de columnas limpios.\nmeteo \u0026lt;- select(meteo, NAME, DATE, TAVG:TMIN) %\u0026gt;% filter(DATE \u0026gt;= \u0026quot;1991-01-01\u0026quot;, DATE \u0026lt;= \u0026quot;2020-12-31\u0026quot;) %\u0026gt;% mutate(NAME = factor(NAME, stats_names, cities),\rTAVG = ifelse(is.na(TAVG), (TMAX+TMIN)/2, TAVG),\ryd = yday(DATE)) %\u0026gt;% clean_names()\rEn el siguiente paso calculamos el promedio de las m√°ximas, m√≠nimas y media diarias para cada d√≠a del a√±o. Despu√©s √∫nicamente falta por convertir los d√≠as del a√±o en una fecha dummy. Aqu√≠ usamos el a√±o 2000 dado que es bisiesto y tenemos en total 366 d√≠as.\n# estimamos los promedios diarios\rmeteo_yday \u0026lt;- group_by(meteo, name, yd) %\u0026gt;% summarise(ta = mean(tavg, na.rm = TRUE),\rtmx = mean(tmax, na.rm = TRUE),\rtmin = mean(tmin, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;name\u0026#39;. You can override using the\r## `.groups` argument.\rmeteo_yday\r## # A tibble: 3,294 x 5\r## # Groups: name [9]\r## name yd ta tmx tmin\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 CHICAGO 1 -3.77 0.537 -7.86\r## 2 CHICAGO 2 -2.64 1.03 -6.68\r## 3 CHICAGO 3 -2.88 0.78 -6.93\r## 4 CHICAGO 4 -2.86 0.753 -7.10\r## 5 CHICAGO 5 -4.13 -0.137 -8.33\r## 6 CHICAGO 6 -4.50 -1.15 -8.05\r## 7 CHICAGO 7 -4.70 -0.493 -8.57\r## 8 CHICAGO 8 -3.97 0.147 -8.02\r## 9 CHICAGO 9 -3.47 0.547 -7.49\r## 10 CHICAGO 10 -3.41 1.09 -7.64\r## # ... with 3,284 more rows\r# convertimos los d√≠as del a√±o en una fecha dummy\rmeteo_yday \u0026lt;- mutate(meteo_yday, yd = as_date(yd, origin = \u0026quot;1999-12-31\u0026quot;))\r\r\rCear los c√≠rculos clim√°ticos\rPredefiniciones\rDefinimos un vector divergente de varios colores.\ncol_temp \u0026lt;- c(\u0026quot;#cbebf6\u0026quot;,\u0026quot;#a7bfd9\u0026quot;,\u0026quot;#8c99bc\u0026quot;,\u0026quot;#974ea8\u0026quot;,\u0026quot;#830f74\u0026quot;,\r\u0026quot;#0b144f\u0026quot;,\u0026quot;#0e2680\u0026quot;,\u0026quot;#223b97\u0026quot;,\u0026quot;#1c499a\u0026quot;,\u0026quot;#2859a5\u0026quot;,\r\u0026quot;#1b6aa3\u0026quot;,\u0026quot;#1d9bc4\u0026quot;,\u0026quot;#1ca4bc\u0026quot;,\u0026quot;#64c6c7\u0026quot;,\u0026quot;#86cabb\u0026quot;,\r\u0026quot;#91e0a7\u0026quot;,\u0026quot;#c7eebf\u0026quot;,\u0026quot;#ebf8da\u0026quot;,\u0026quot;#f6fdd1\u0026quot;,\u0026quot;#fdeca7\u0026quot;,\r\u0026quot;#f8da77\u0026quot;,\u0026quot;#fcb34d\u0026quot;,\u0026quot;#fc8c44\u0026quot;,\u0026quot;#f85127\u0026quot;,\u0026quot;#f52f26\u0026quot;,\r\u0026quot;#d10b26\u0026quot;,\u0026quot;#9c042a\u0026quot;,\u0026quot;#760324\u0026quot;,\u0026quot;#18000c\u0026quot;)\rCreamos una tabla con las l√≠neas de la rejilla en sentido eje x.\ngrid_x \u0026lt;- tibble(x = seq(ymd(\u0026quot;2000-01-01\u0026quot;), ymd(\u0026quot;2000-12-31\u0026quot;), \u0026quot;month\u0026quot;), y = rep(-10, 12), xend = seq(ymd(\u0026quot;2000-01-01\u0026quot;), ymd(\u0026quot;2000-12-31\u0026quot;), \u0026quot;month\u0026quot;), yend = rep(41, 12))\rDefinimos todos los elementos de estilo del gr√°fico en nuestro propio tema theme_cc().\ntheme_cc \u0026lt;- function(){ theme_minimal(base_family = \u0026quot;Montserrat\u0026quot;) %+replace%\rtheme(plot.title = element_text(hjust = 0.5, colour = \u0026quot;white\u0026quot;, size = 30, margin = margin(b = 20)),\rplot.caption = element_text(colour = \u0026quot;white\u0026quot;, size = 9, hjust = .5, vjust = -30),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.margin = margin(1, 1, 2, 1, unit = \u0026quot;cm\u0026quot;),\raxis.text.x = element_text(face = \u0026quot;italic\u0026quot;, colour = \u0026quot;white\u0026quot;),\raxis.title.y = element_blank(),\raxis.text.y = element_blank(),\rlegend.title = element_text(colour = \u0026quot;white\u0026quot;),\rlegend.position = \u0026quot;bottom\u0026quot;,\rlegend.justification = 0.5,\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;),\rstrip.text = element_text(colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 14),\rpanel.spacing.y = unit(1, \u0026quot;lines\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rpanel.grid = element_blank()\r) }\r\rGr√°fico\rEmpezamos por crear un gr√°fico √∫nicamente para la ciudad de Nueva York. Usaremos geom_linerange() definiendo el rango con las m√°ximas y m√≠nimas. Adem√°s, dibujaremos las l√≠neas de rango en funci√≥n de la temperatura media. Podemos ajustar alpha y size para obtener un aspecto m√°s bonito.\n# filtramos Nueva York\rny_city \u0026lt;- filter(meteo_yday, name == \u0026quot;NEW YORK\u0026quot;) # gr√°fico\rggplot(ny_city) + geom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion()) +\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;,\rdate_labels = \u0026quot;%b\u0026quot;) +\rlabs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) \rPara conseguir el gr√°fico polar √∫nicamente har√≠a falta a√±adir la funci√≥n coord_polar().\n# gr√°fico polar\rggplot(ny_city) + geom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion()) +\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;,\rdate_labels = \u0026quot;%b\u0026quot;) +\rcoord_polar() +\rlabs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) \rCambiamos la lengua del sistema operativo para obtener los nombres de meses en ingl√©s. Para volver a la lengua de origen podemos usar simplemente la funci√≥n Sys.setlocale(\"LC_TIME\", old_lc).\nold_lc \u0026lt;- Sys.getlocale(\u0026quot;LC_TIME\u0026quot;)\rSys.setlocale(\u0026quot;LC_TIME\u0026quot;, \u0026quot;English\u0026quot;)\r## [1] \u0026quot;English_United States.1252\u0026quot;\rEn el gr√°fico final a√±adimos la rejilla definiendo las l√≠neas en el eje y con geom_hline(), y aquellas del eje x con geom_segement(). Lo m√°s importante aqu√≠ es la funci√≥n facet_wrap(), la que permite la creaci√≥n de m√∫ltiples facetas de gr√°ficos. Se usa el formato de f√≥rmula para especificar de que forma se crean las facetas: fila ~ columna. En caso de que no dispongamos de una segunda variable, se indica en la f√≥rmula un punto .. Adem√°s, hacemos cambios de aspecto en la barra de color con guides() y guide_colourbar(), e incluimos los estilos theme_cc().\nggplot(meteo_yday) + geom_hline(yintercept = c(-10, 0, 10, 20, 30, 40), colour = \u0026quot;white\u0026quot;, size = .4) +\rgeom_segment(data = grid_x , aes(x = x, y = y, xend = xend, yend = yend), linetype = \u0026quot;dashed\u0026quot;, colour = \u0026quot;white\u0026quot;, size = .2) +\rgeom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion())+\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rguides(colour = guide_colourbar(barwidth = 15,\rbarheight = 0.5, title.position = \u0026quot;top\u0026quot;)\r) +\rfacet_wrap(~name, nrow = 3) +\rcoord_polar() + labs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) +\rtheme_cc()\r\n\r\r","date":1630713600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1630713600,"objectID":"f79728d9ed7475e2e0892261f2807799","permalink":"https://dominicroye.github.io/es/2021/circulos-climaticos/","publishdate":"2021-09-04T00:00:00Z","relpermalink":"/es/2021/circulos-climaticos/","section":"post","summary":"El clima de un lugar se suele presentar a trav√©s de climogramas que combinan la precipitaci√≥n y temperatura mensual en un √∫nico gr√°fico. No obstante, tambi√©n es interesante visualizar el clima a nivel diario mostrando la amplitud t√©rmica y la temperatura media diaria. Para ello, se calculan las medias de cada d√≠a del a√±o de las m√≠nimas, m√°ximas y medias diarias. El ciclo anual del clima presenta una buena oportunidad para usar un gr√°fico radial, polar o circular lo que nos permite visualizar de forma clara los patrones estacionales.","tags":["clima","radial","temperaturas"],"title":"C√≠rculos clim√°ticos","type":"post"},{"authors":["A Tob√≠as","et al"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1630454400,"objectID":"52683671c3ed79a24a56c67b0f13c303","permalink":"https://dominicroye.github.io/es/publication/2021-minimum-mortality-temperature-environmental-epidemiology/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/es/publication/2021-minimum-mortality-temperature-environmental-epidemiology/","section":"publication","summary":"Background: Minimum mortality temperature (MMT) is an important indicator to assess the temperature-mortality association, indicating long-term adaptation to local climate. Limited evidence about the geographical variability of the MMT is available at a global scale. Methods: We collected data from 658 communities in 43 countries under different climates. We estimated temperature-mortality associations to derive the MMT for each community using Poisson regression with distributed lag nonlinear models. We investigated the variation in MMT by climatic zone using a mixed-effects meta-analysis and explored the association with climatic and socioeconomic indicators. Results: The geographical distribution of MMTs varied considerably by country between 14.2 and 31.1 ¬∫C decreasing by latitude. For climatic zones, the MMTs increased from alpine (13.0 ¬∫C) to continental (19.3 ¬∫C), temperate (21.7 ¬∫C), arid (24.5 ¬∫C), and tropical (26.5 ¬∫C). The MMT percentiles (MMTPs) corresponding to the MMTs decreased from temperate (79.5th) to continental (75.4th), arid (68.0th), tropical (58.5th), and alpine (41.4th). The MMTs indreased by 0.8 ¬∫C for a 1 ¬∫C rise in a communitys annual mean temperature, and by 1¬∫C for a 1¬∫C rise in its SD. While the MMTP decreased by 0.3 centile points for a 1 ¬∫C rise in a communitys annual mean temperature and by 1.3 for a 1 ¬∫C rise in its SD. Conclusions: The geographical distribution of the MMTs and MMTPs is driven mainly by the mean annual temperature, which seems to be a valuable indicator of overall adaptation across populations. Our results suggest that populations have adapted to the average temperature, although there is still more room for adaptation.","tags":["temperatura m√≠nima de mortalidad","mortalidad","global","MCC Study","temperatura","clima","salud humana"],"title":"Geographical Variations of the Minimum Mortality Temperature at a Global Scale","type":"publication"},{"authors":["S Mathbout","JA Lopez-Bustins","D Roy√©","J Martin-Vide"],"categories":null,"content":"","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1626912000,"objectID":"281bad00093611848241c8aa3afad58c","permalink":"https://dominicroye.github.io/es/publication/2021-mediterranean-droughts-atmosphere/","publishdate":"2021-07-22T00:00:00Z","relpermalink":"/es/publication/2021-mediterranean-droughts-atmosphere/","section":"publication","summary":"Drought is one of the most complex climate-related phenomena and is expected to progressively affect our lives by causing very serious environmental and socioeconomic damage by the end of the 21st century. In this study, we have extracted a dataset of exceptional meteorological drought events between 1975 and 2019 at the country and subregional scales. Each drought event was described by its start and end date, intensity, severity, duration, areal extent, peak month and peak area. To define such drought events and their characteristics, separate analyses based on three drought indices were performed at 12-month timescale: the Standardized Precipitation Index (SPI), the Standardized Precipitation Evapotranspiration Index (SPEI), and the Reconnaissance Drought Index (RDI). A multivariate combined drought index (DXI) was developed by merging the previous three indices for more understanding of droughts‚Äô features at the country and subregional levels. Principal component analysis (PCA) was used to identify five different drought subregions based on DXI-12 values for 312 Mediterranean stations and a new special score was defined to classify the multi-subregional exceptional drought events across the Mediterranean Basin (MED). The results indicated that extensive drought events occurred more frequently since the late 1990s, showing several drought hotspots in the last decades in the southeastern Mediterranean and northwest Africa. In addition, the results showed that the most severe events were more detected when more than single drought index was used. The highest percentage area under drought was also observed through combining the variations of three drought indices. Furthermore, the drought area in both dry and humid areas in the MED has also experienced a remarkable increase since the late 1990s. Based on a comparison of the drought events during the two periods‚Äî1975‚Äì1996 and 1997‚Äì2019‚Äîwe find that the current dry conditions in the MED are more severe, intense, and frequent than the earlier period; moreover, the strongest dry conditions occurred in last two decades. The SPEI-12 and RDI-12 have a higher capacity in providing a more comprehensive description of the dry conditions because of the inclusion of temperature or atmospheric evaporative demand in their scheme. A complex range of atmospheric circulation patterns, particularly the Western Mediterranean Oscillation (WeMO) and East Atlantic/West Russia (EATL/WRUS), appear to play an important role in severe, intense and region-wide droughts, including the two most severe droughts, 1999‚Äì2001 and 2007‚Äì2012, with lesser influence of the NAO, ULMO and SCAND.","tags":["cambio clim√°tico","evento de sequ√≠a","cuenca mediterranea","sequ√≠a meteorol√≥gica","SPEI"],"title":"Mediterranean-Scale Drought: Regional Datasets for Exceptional Meteorological Drought Events during 1975-2019","type":"publication"},{"authors":["D Roy√©","A Tobias","A Figueiras","S Gestal","A Santurtun","C I√±iguez"],"categories":null,"content":"","date":1626480000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1626480000,"objectID":"6a7c8555cac8ad123801a79619ac00d8","permalink":"https://dominicroye.github.io/es/publication/2021-medical-prescriptions-environmental-research/","publishdate":"2021-07-17T00:00:00Z","relpermalink":"/es/publication/2021-medical-prescriptions-environmental-research/","section":"publication","summary":"Background: The increased risk of mortality during periods of high and low temperatures has been well established. However, most of the studies used daily counts of deaths or hospitalisations as health outcomes, although they are the ones at the top of the health impact pyramid reflecting only a limited proportion of patients with the most severe cases. Objectives: This study evaluates the relationship between short-term exposure to the daily mean temperature and medication prescribed for the respiratory system in five Spanish cities. Methods: We fitted time series regression models to cause-specific medical prescriptions, including different respiratory subgroups and age groups. We included a distributed lag non-linear model with lags up to 14 days for daily mean temperature. City-specific associations were summarised as overall-cumulative exposure-response curves. Results: We found a positive association between cause-specific medical prescriptions and daily mean temperature with a non-linear inverted J- or V-shaped relationship in most cities. Between 0.3% and 0.6% of all respiratory prescriptions were attributed to cold for Madrid, Zaragoza and Pamplona, while in cities with only cold effects the attributable fractions were estimated as 19.2% for Murcia and 13.5% for Santander. Heat effects in Madrid, Zaragoza and Pamplona showed higher fractions between 8.7% and 17.2%. The estimated costs are in general higher for heat effects, showing annual values ranging between ‚Ç¨191,905 and ‚Ç¨311,076 for heat per 100,000 persons. Conclusions: This study provides novel evidence of the effects of the thermal environment on the prescription of medication for respiratory disorders in Spain, showing that low and high temperatures lead to an increase in the number of such prescriptions. The consumption of medication can reflect exposure to the environment with a lesser degree of severity in terms of morbidity.","tags":["Espa√±a","medicamentos","recetas m√©dicas","respiratorio","temperatura"],"title":"Temperature-related effects on respiratory medical prescriptions in Spain","type":"publication"},{"authors":["A Mart√≠","D Roy√©"],"categories":null,"content":"","date":1626307200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1626307200,"objectID":"9bc813f85710711b29872462c9195db4","permalink":"https://dominicroye.github.io/es/publication/2021-hotnights-madrid-urbclim-geographicalia/","publishdate":"2021-07-15T00:00:00Z","relpermalink":"/es/publication/2021-hotnights-madrid-urbclim-geographicalia/","section":"publication","summary":"In this work, a new methodology is applied to study hot nights, also called tropical nights, in the metropolitan area of Madrid. To evaluate hot nights from a temporal and spatial perspective in which the population may be affected by thermal stress, high resolution gridded hourly temperature data were used. The use of two indicators obtained through hourly data, together with the climatic information provided by the UrbClim model, allowed to evaluate the thermal night characteristics of July between 2008 and 2017 at a detailed scale. Hence we were able to estimate precisely the risk to the well-being and health of the population. The results show great interurban variability in terms of intensity and duration of heat stress and a significant correlation between heat island intensities and excess heat. Likewise, a close relationship between the typologies of land uses and urban structures defined in the Urban Atlas and the indices of night heat excess has been established.","tags":["Madrid","noches calurosas","clima urbano","estr√©s por calor","temperatura nocturna"],"title":"Intensity and duration of heat stress in summer in the urban area of Madrid","type":"publication"},{"authors":["Q Zhao","et al"],"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1625961600,"objectID":"c50b219bb70615ee0dffebce44592b69","permalink":"https://dominicroye.github.io/es/publication/2021-gridded-mortality-head-cold-lancet-planetary-health/","publishdate":"2021-07-11T00:00:00Z","relpermalink":"/es/publication/2021-gridded-mortality-head-cold-lancet-planetary-health/","section":"publication","summary":"Background Exposure to cold or hot temperatures is associated with premature deaths. We aimed to evaluate the global, regional, and national mortality burden associated with non-optimal ambient temperatures. Methods In this modelling study, we collected time-series data on mortality and ambient temperatures from 750 locations in 43 countries and five meta-predictors at a grid size of 0¬∑5¬∞ √ó 0¬∑5¬∞ across the globe. A three-stage analysis strategy was used. First, the temperature‚Äìmortality association was fitted for each location by use of a time-series regression. Second, a multivariate meta-regression model was built between location-specific estimates and meta-predictors. Finally, the grid-specific temperature‚Äìmortality association between 2000 and 2019 was predicted by use of the fitted meta-regression and the grid-specific meta-predictors. Excess deaths due to non-optimal temperatures, the ratio between annual excess deaths and all deaths of a year (the excess death ratio), and the death rate per 100 000 residents were then calculated for each grid across the world. Grids were divided according to regional groupings of the UN Statistics Division. Findings Globally, 5 083 173 deaths (95% empirical CI [eCI] 4 087 967‚Äì5 965 520) were associated with non-optimal temperatures per year, accounting for 9¬∑43% (95% eCI 7¬∑58‚Äì11¬∑07) of all deaths (8¬∑52% [6¬∑19‚Äì10¬∑47] were cold-related and 0¬∑91% [0¬∑56‚Äì1¬∑36] were heat-related). There were 74 temperature-related excess deaths per 100 000 residents (95% eCI 60‚Äì87). The mortality burden varied geographically. Of all excess deaths, 2 617 322 (51¬∑49%) occurred in Asia. Eastern Europe had the highest heat-related excess death rate and Sub-Saharan Africa had the highest cold-related excess death rate. From 2000‚Äì03 to 2016‚Äì19, the global cold-related excess death ratio changed by ‚àí0¬∑51 percentage points (95% eCI ‚àí0¬∑61 to ‚àí0¬∑42) and the global heat-related excess death ratio increased by 0¬∑21 percentage points (0¬∑13‚Äì0¬∑31), leading to a net reduction in the overall ratio. The largest decline in overall excess death ratio occurred in South-eastern Asia, whereas excess death ratio fluctuated in Southern Asia and Europe. Interpretation Non-optimal temperatures are associated with a substantial mortality burden, which varies spatiotemporally. Our findings will benefit international, national, and local communities in developing preparedness and prevention strategies to reduce weather-related impacts immediately and under climate change scenarios.","tags":["mortalidad","global","regional","temperaturas no √≥ptimas","MCC Study"],"title":"Global, regional, and national burden of mortality associated with non-optimal ambient temperatures from 2000 to 2019: a three-stage modelling study","type":"publication"},{"authors":null,"categories":["sig","R","R:intermedio","visualizaci√≥n"],"content":"\r\rCartograf√≠a firefly\rLos mapas firefly (lamp√≠ridos ingl.) son promocionados y descritos por\rJohn Nelson quien public√≥ un post en 2016 sobre sus caracter√≠sticas. No obstante, este tipo de mapas est√°n vinculados a ArcGIS, lo que me ha llevado a intentar recrearlos en R. La reciente extensi√≥n de ggplot2 con el paquete ggshadow nos facilitar√° la creaci√≥n de este estilo cartogr√°fico. Se caracteriza por tres elementos 1) un mapa base oscuro y desaturado (p.j. im√°genes satelitales) 2) una vi√±eta y √°rea resaltada enmascarada y 3) una √∫nica capa tem√°tica brillante. Lo esencial son los colores y el brillo que se logra con colores fr√≠os, habitualmente colores ne√≥n. John Nelson explica m√°s detalles en este post.\n¬øPara qu√© sirve el estilo firefly? En palabras de John Nelson: ‚Äúthe map style that captures our attention and dutifully honors the First Law of Geography‚Äù. John hace referencia a lo dicho por Waldo Tobler\r‚Äúeverything is related to everything else, but near things are more related than distant things‚Äù (Tobler 1970).\nEn este post visualizaremos todos los terremotos registrados en el suroeste de Europa con una magnitud mayor de 3.\n\rPaquetes\rUsaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rplotwidgets\rContiene funciones para la conversi√≥n de colores (RGB, HSL)\r\rterra\rImportar, exportar y manipular raster (paquete sucesor de raster)\r\rraster\rImportar, exportar y manipular raster\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rggshadow\rExtensi√≥n de ggplot2 para geometr√≠as con sombreado\r\rggspatial\rExtensi√≥n de ggplot2 para objetos espaciales\r\rggnewscale\rExtensi√≥n de ggplot2 para crear multiples scalas\r\rjanitor\rFunciones sencillas para examinar y limpiar datos\r\rrnaturalearth\rMapas vectoriales del mundo ‚ÄòNatural Earth‚Äô\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;plotwidgets\u0026quot;)) install.packages(\u0026quot;plotwidgets\u0026quot;)\rif(!require(\u0026quot;ggshadow\u0026quot;)) install.packages(\u0026quot;ggshadow\u0026quot;)\rif(!require(\u0026quot;ggspatial\u0026quot;)) install.packages(\u0026quot;ggspatial\u0026quot;)\rif(!require(\u0026quot;ggnewscale\u0026quot;)) install.packages(\u0026quot;ggnewscale\u0026quot;)\rif(!require(\u0026quot;janitor\u0026quot;)) install.packages(\u0026quot;janitor\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r# paquetes\rlibrary(raster)\rlibrary(terra)\rlibrary(sf)\rlibrary(tidyverse)\rlibrary(plotwidgets)\rlibrary(ggshadow)\rlibrary(ggspatial)\rlibrary(ggnewscale)\rlibrary(janitor)\rlibrary(rnaturalearth)\r\rPreparaci√≥n\rDatos\rPrimero descargamos todos los datos necesarios. Para el mapa de base usaremos la Blue Marble v√≠a el acceso a worldview.earthdata.nasa.gov donde me he descargado una selecci√≥n del √°rea de inter√©s en formato geoTiff con una resoluci√≥n de 1 km. Es importante ajustar la resoluci√≥n al detalle necesario del mapa.\n\rSelecci√≥n de Blue Marble via worldview.earthdata.nasa.gov (~66 MB) descarga\rRegistros de terremotos hist√≥ricos en el suroeste de Europa del IGN descarga\r\r\rImportar\rLo primero que hacemos es importar el raster RGB Blue Marble y los datos de los terremotos. Para importar el raster hago uso del nuevo paquete terra que es el sucesor del paquete raster. Pod√©is encontrar una comparaci√≥n reciente aqu√≠. No todos los paquetes son compatibles todav√≠a con la nueva clase de SpatRaster, por eso, nos hace falta tambi√©n el paquete raster.\n# terremotos\rterremotos \u0026lt;- read.csv2(\u0026quot;catalogoComunSV_1621713848556.csv\u0026quot;)\rstr(terremotos)\r## \u0026#39;data.frame\u0026#39;: 149724 obs. of 10 variables:\r## $ Evento : chr \u0026quot; 33\u0026quot; \u0026quot; 34\u0026quot; \u0026quot; 35\u0026quot; \u0026quot; 36\u0026quot; ...\r## $ Fecha : chr \u0026quot; 02/03/1373\u0026quot; \u0026quot; 03/03/1373\u0026quot; \u0026quot; 08/03/1373\u0026quot; \u0026quot; 19/03/1373\u0026quot; ...\r## $ Hora : chr \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; ...\r## $ Latitud : chr \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; ...\r## $ Longitud : chr \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; ...\r## $ Prof...Km. : int NA NA NA NA NA NA NA NA NA NA ...\r## $ Inten. : chr \u0026quot; VIII-IX\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ...\r## $ Mag. : chr \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ...\r## $ Tipo.Mag. : int NA NA NA NA NA NA NA NA NA NA ...\r## $ Localizaci√É¬≥n: chr \u0026quot;Ribagor√É¬ßa.L\u0026quot; \u0026quot;Ribagor√É¬ßa.L\u0026quot; \u0026quot;Ribagor√É¬ßa.L\u0026quot; \u0026quot;Ribagor√É¬ßa.L\u0026quot; ...\r# Blue Marble RGB raster\rbm \u0026lt;- rast(\u0026quot;snapshot-2017-11-30T00_00_00Z.tiff\u0026quot;)\rbm # contiene tres capas (red, green, blue)\r## class : SpatRaster ## dimensions : 7156, 7156, 3 (nrow, ncol, nlyr)\r## resolution : 0.008789272, 0.008789272 (x, y)\r## extent : -33.49823, 29.39781, 15.77547, 78.67151 (xmin, xmax, ymin, ymax)\r## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : snapshot-2017-11-30T00_00_00Z.tiff ## colors RGB : 1, 2, 3 ## names : snapshot-2~0_00_00Z_1, snapshot-2~0_00_00Z_2, snapshot-2~0_00_00Z_3\r# plot\rplotRGB(bm)\r# l√≠mites pa√≠ses\rlimits \u0026lt;- ne_countries(scale = 50, returnclass = \u0026quot;sf\u0026quot;)\r\rTerremotos\rEn este paso limpiamos los datos importados de los terremotos. 1) Convertimos en n√∫merico longitud, latitud y magnitud usando la funci√≥n parse_number() y limpiampos los nombres de las columnas con la funci√≥n clean_names(), 2) Creamos un objeto espacial sf y lo proyectamos usando el EPSG:3035 correspondiendo a ETRS89-extended/LAEA Europe.\n# limpiamos los datos y creamos un objeto sf\rterremotos \u0026lt;- terremotos %\u0026gt;% clean_names() %\u0026gt;%\rmutate(across(c(mag, latitud, longitud), parse_number)) %\u0026gt;%\rst_as_sf(coords = c(\u0026quot;longitud\u0026quot;, \u0026quot;latitud\u0026quot;), crs = 4326) %\u0026gt;% st_transform(3035) # proyectamos a Laea \r\rMapa de fondo Blue Marble\rRecortamos el mapa de fondo a una extensi√≥n menor, pero todav√≠a no limitamos el √°rea final.\n# recortamos al √°rea deseada\rbm \u0026lt;- crop(bm, extent(-20, 10, 30, 50))\rPara obtener una versi√≥n desaturada del raster RGB de Blue Marble, debemos aplicar una funci√≥n creada con este fin. En √©sta usamos la funci√≥n rgb2hsl() del paquete plotwidgets, que nos ayuda en convertir RGB a HSL y viceversa. El modelo HSL se define con Hue (tono), Saturation (saturaci√≥n), Lightness (luminosidad). Los √∫ltimos dos parametros se expresan en ratio o porcentaje. El tono est√° definido en una rueda de color de 0 a 360¬∫. 0 es rojo, 120 es verde, 240 es azul. Para cambiar la saturaci√≥n √∫nicamente debemos bajar el valor de S.\n# funci√≥n para cambiar la saturaci√≥n desde RGB\rsaturation \u0026lt;- function(rgb, s = .5){\rhsl \u0026lt;- rgb2hsl(as.matrix(rgb))\rhsl[2, ] \u0026lt;- s\rrgb_new \u0026lt;- as.vector(t(hsl2rgb(hsl)))\rreturn(rgb_new)\r}\rEmpleamos nuestra funci√≥n saturation() usando otra funci√≥n app() que la aplica a cada p√≠xel con las tres capas de RGB. A√±adimos el argumento s, el que define el nivel de saturaci√≥n deseada. Este paso puede tardar varios minutos. Despu√©s proyectamos nuestra imagen RGB.\n# aplicamos la funci√≥n para desaturar con un 5%\rbm_desat \u0026lt;- app(bm, saturation, s = .05)\r# plot nuevo imagen RGB\rplotRGB(bm_desat)\r# proyectamos bm_desat \u0026lt;- terra::project(bm_desat, \u0026quot;epsg:3035\u0026quot;)\r\r\rConstrucci√≥n del mapa firefly\rL√≠mites y grad√≠cula\rAntes de empezar a construir el mapa, creamos una grad√≠cula y estabelecemos la extensi√≥n final del mapa.\n# definimos los l√≠mites finales del mapa\rbx \u0026lt;- tibble(x = c(-13, 6.7), y = c(31, 47)) %\u0026gt;% st_as_sf(coords = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), crs = 4326) %\u0026gt;%\rst_transform(3035) %\u0026gt;% st_bbox()\r# crearmos una grad√≠cula del mapa\rgrid \u0026lt;- st_graticule(terremotos) \r\rMapa con fondo de imagen\rLa funci√≥n layer_spatial() de ggspatial nos permite a√±adir un raster RGB sin grandes problemas, no obstante, todav√≠a no apoya la nueva clase SpatRaster. Por eso, debemos convertirlo en la clase stack con la funci√≥n stack(). Tambi√©n es posible usar en lugar de geom_sf(), la funci√≥n layer_spatial() para objetos vectoriales de clase sf o sp.\nggplot() +\rlayer_spatial(data = stack(bm_desat)) + # mapa de fondo Blue Marble\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) + # l√≠mites pa√≠ses\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void()\r\rMapa con fondo y los terremotos\rPara crear el efecto del brillo en los mapas firefly, hacemos uso de la funci√≥n geom_glowpoint() del paquete ggshadow. Tambi√©n existe la misma funci√≥n para l√≠neas. Dado que nuestros datos son espaciales de clase sf y la geometr√≠a no apoya directamente el uso de sf, debemos indicar como argumento stats = \"sf_coordinates\" y dentro de aes() indicar geometry = geometry. Mapearemos el tama√±o de los puntos en funci√≥n de la magnitud. Adem√°s, filtramos aquellos terremotos con una magnitud mayor del 3.\nDentro de la funci√≥n geom_glowpoint(), 1) definimos el color deseado para el punto y el brillo, 2) el grado de transparencia con alpha bien para el punto o bien para el brillo. Por √∫ltimo, en la funci√≥n scale_size() estabelecemos el rango (m√≠nimo, m√°ximo) del tama√±o que tendr√°n los puntos.\nggplot() +\rlayer_spatial(data = stack(bm_desat)) +\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = grid, colour = \u0026quot;white\u0026quot;, size = .1, alpha = .5) +\rgeom_glowpoint(data = filter(terremotos, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .8,\rcolor = \u0026quot;#6bb857\u0026quot;,\rshadowcolour = \u0026quot;#6bb857\u0026quot;,\rshadowalpha = .1,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.1, 1.5)) +\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void()\r\rMapa final\rEl brillo de los mapas firefly se caracteriza por tener un tono blanco o un tono m√°s claro en el centro de los puntos. Para lograrlo, debemos duplicar la capa anterior creada, cambiando √∫nicamente el color y hacer los puntos con su brillo m√°s peque√±os.\nPor defecto, ggplot2 no permite emplear diferentes escalas para la misma caracteristica (tama√±o, color, etc) de distintas capas. Pero el paquete ggnewscale nos da la posibilidad de incorporar m√∫ltiples escalas de una caracter√≠stica de distintas capas. Lo √∫nico importante para lograrlo es el orden en el que se a√±ade cada capa y escala. Primero debemos poner la geometr√≠a y desp√∫es su escala correspondiente. Indicamos con new_scale('size') que la siguiente capa y escala es una nueva independiente de la anterior. Si usaramos colour o fill ser√≠a con new_scale_*().\nggplot() +\rlayer_spatial(data = stack(bm_desat)) +\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = grid, colour = \u0026quot;white\u0026quot;, size = .1, alpha = .5) +\rgeom_glowpoint(data = filter(terremotos, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .8,\rcolor = \u0026quot;#6bb857\u0026quot;,\rshadowcolour = \u0026quot;#6bb857\u0026quot;,\rshadowalpha = .1,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.1, 1.5)) +\rnew_scale(\u0026quot;size\u0026quot;) +\rgeom_glowpoint(data = filter(terremotos, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .6,\rshadowalpha = .05,\rcolor = \u0026quot;#ffffff\u0026quot;,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.01, .7)) +\rlabs(title = \u0026quot;TERREMOTOS\u0026quot;) +\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void() +\rtheme(plot.title = element_text(size = 50, vjust = -5, colour = \u0026quot;white\u0026quot;, hjust = .95))\rggsave(\u0026quot;firefly_map.png\u0026quot;, width = 15, height = 15, units = \u0026quot;in\u0026quot;, dpi = 300)\r\n\r\r","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1622505600,"objectID":"be14f024f7c71452cbc85eae86956e96","permalink":"https://dominicroye.github.io/es/2021/cartografia-firefly/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/es/2021/cartografia-firefly/","section":"post","summary":"Los mapas *firefly* (lamp√≠ridos ingl.) son promocionados y descritos por [John Nelson](https://twitter.com/John_M_Nelson) quien public√≥ un [post](https://adventuresinmapping.com/2016/10/17/firefly-cartography/) en 2016 sobre sus caracter√≠sticas. No obstante, este tipo de mapas est√°n vinculados a ArcGIS, lo que me ha llevado a intentar recrearlos en R.","tags":["firefly","mapa","terremotos","cartograf√≠a"],"title":"Cartograf√≠a firefly","type":"post"},{"authors":["D Roy√©","F Sera","A Tob√≠as","R Lowe","A Gasparrini","M Pascal","F de'Donato","B Nunes","JP Teixeira"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1622505600,"objectID":"7ba7d1ca7996cbb04df8057cf7f5f489","permalink":"https://dominicroye.github.io/es/publication/2021-hotnights-europe-epidemiology/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/es/publication/2021-hotnights-europe-epidemiology/","section":"publication","summary":"Background: There is strong evidence concerning the impact of heat stress on mortality, particularly from high temperatures. However, few studies to our knowledge emphasize the importance of hot nights, which may prevent necessary nocturnal rest. Objectives: In this study, we use hot-night duration and excess to predict daily cause-specific mortality in summer, using multiple cities across Southern Europe. Methods: We fitted time series regression models to summer cause-specific mortality, including natural, respiratory, and cardiovascular causes, in 11 cities across four countries. We included a distributed lag non-linear model with lags up to 7 days for hot night duration and excess adjusted by daily mean temperature. We summarized city-specific associations as overall-cumulative exposure‚Äìresponse curves at the country level using meta-analysis. Results: We found positive but generally non-linear associations between relative risk of cause-specific mortality and duration and excess of hot nights. Relative risk of duration associated with nonaccidental mortality in Portugal was 1.29 (95% CI, 1.07 to 1.54); other associations were imprecise, but we also found positive city-specific estimates for Rome and Madrid. Risk of hot-night excess ranged from 1.12 (95% CI, 1.05 to 1.20) for France to 1.37 (95% CI, 1.26 to 1.48) for Portugal. Risk estimates for excess were consistently higher than for duration. Conclusions: This study provides new evidence that, over a wider range of locations, hot night indices are strongly associated with cause-specific deaths. Modeling the impact of thermal characteristics during summer nights on mortality could improve decisionmaking for preventive public health strategies.","tags":["mortalidad","noches calurosas","salud humana","cambio clim√°tico","noches tropicales","MCC Study"],"title":"Effects of hot nights on mortality in Southern Europe","type":"publication"},{"authors":["A Vicedo-Cabrera","N Scovronick","F Sera","D Roy√©","et al"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1622505600,"objectID":"f1ad4e7e93503a88f012c2248be8f453","permalink":"https://dominicroye.github.io/es/publication/2021-attribution-mortality-nature-climatechange/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/es/publication/2021-attribution-mortality-nature-climatechange/","section":"publication","summary":"Climate change affects human health; however, there have been no large-scale, systematic efforts to quantify the heat-related human health impacts that have already occurred due to climate change. Here, we use empirical data from 732 locations in 43 countries to estimate the mortality burdens associated with the additional heat exposure that has resulted from recent human-induced warming, during the period 1991‚Äì2018. Across all study countries, we find that 37.0% (range 20.5‚Äì76.3%) of warm-season heat-related deaths can be attributed to anthropogenic climate change and that increased mortality is evident on every continent. Burdens varied geographically but were of the order of dozens to hundreds of deaths per year in many locations. Our findings support the urgent need for more ambitious mitigation and adaptation strategies to minimize the public health impacts of climate change.","tags":["mortalidad","salud humana","cambio clim√°tico","atribuci√≥n","MCC Study"],"title":"The burden of heat-related mortality attributable to recent human-induced climate change","type":"publication"},{"authors":["E de Schrijver","CL Folly","R Schneider","D Roy√©","OH Franco","A Gasparrini","AM Vicedo-Cabrera"],"categories":null,"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1619827200,"objectID":"8908a93adb7726c9f15b6513a326eb41","permalink":"https://dominicroye.github.io/es/publication/2021-mortality-gridded-datasets-switzerland-geohealth/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/es/publication/2021-mortality-gridded-datasets-switzerland-geohealth/","section":"publication","summary":"New gridded climate datasets (GCDs) on spatially‚Äêresolved modelled weather data have recently been released to explore the impacts of climate change. GCDs have been suggested as potential alternatives to weather station data in epidemiological assessments on health impacts of temperature and climate change. These can be particularly useful for assessment in regions that have remained understudied due to limited or low quality weather station data. However to date, no study has critically evaluated the application of GCDs of variable spatial resolution in temperature‚Äêmortality assessments across regions of different orography, climate and size. Here we explored the performance of population‚Äêweighted daily mean temperature data from the global ERA5 reanalysis dataset in the 10 regions in the United Kingdom and the 26 cantons in Switzerland, combined with two local high‚Äêresolution GCDs (Haduk‚Äêgrid UKPOC‚Äê9 and MeteoSwiss‚Äêgrid‚Äêproduct, respectively) and compared these to weather station data and unweighted homologous series. We applied quasi‚ÄêPoisson time series regression with distributed lag non‚Äêlinear models to obtain the GCD‚Äê and region‚Äêspecific temperature‚Äêmortality associations and calculated the corresponding cold‚Äê and heat‚Äêrelated excess mortality. Although the five exposure datasets yielded different average area‚Äêlevel temperature estimates, these deviations did not result in substantial variations in the temperature‚Äêmortality association or impacts. Moreover, local population‚Äêweighted GCDs showed better overall performance, suggesting that they could be excellent alternatives to help advance knowledge on climate change impacts in remote regions with large climate and population distribution variability, which has remained largely unexplored in present literature due to the lack of reliable exposure data.","tags":["conjunto de datos clim√°ticos en cuadr√≠cula","an√°lisis espacio-temporal","rean√°lisis","calor","fr√≠o","mortalidad","cambio clim√°tico"],"title":"A Comparative Analysis of the Temperature‚ÄêMortality Risks Using Different Weather Datasets Across Heterogeneous Regions","type":"publication"},{"authors":["N Lorenzo","A D√≠az-Poso","D Roy√©"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1617235200,"objectID":"69f0f0f60f7732338b3e84373a13bbfb","permalink":"https://dominicroye.github.io/es/publication/2021-heat-wave-ehf-peninsula-atmospheric-research/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/es/publication/2021-heat-wave-ehf-peninsula-atmospheric-research/","section":"publication","summary":"Heatwaves are the most relevant extreme climatic events, particularly in the context of global warming and the related increasing impacts on society and the natural environment. This work presents an analysis of climate change scenarios with simulations from the EURO-CORDEX project using the excess heat factor over the Iberian Peninsula. We focus on climate change projections of the heatwave intensity and spatial distribution, which are evaluated for the near future (2021‚Äì2050) relative to a reference past climate (1971‚Äì2000). Heatwave projections show a general significant increase in intensity, frequency, duration and spatial extent for the whole region. The average change in heatwave intensity is 104% for the whole Iberian Peninsula for the near future 2021‚Äì2050. The largest changes occur in the eastern-central region, rising to 150% for the Mediterranean coast and the Pyrenees. The greater spatial extent of heatwaves strongly suggests increased human exposure, increased energy demand, and implications for fire risk. This spatial trend is predicted to continue in the near future with increases in the maximum spatial heatwave extent ranging from 6% to 8% per decade.","tags":["ola de calor","intensidad","pen√≠nsula ib√©rica","extensi√≥n espacial","cambio clim√°tico","escenarios futuros"],"title":"Heatwave intensity on the Iberian Peninsula: Future climate projections","type":"publication"},{"authors":["BR Wright","B Laffineur","D Roy√©","G Armstrong","RJ Fensham"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1617235200,"objectID":"44f54c4d8304ac129455ebd3eda87f28","permalink":"https://dominicroye.github.io/es/publication/2021-megafires-australia-frontiers-ecology/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/es/publication/2021-megafires-australia-frontiers-ecology/","section":"publication","summary":"Large, high-severity wildfires, or ‚Äòmegafires‚Äô, occur periodically in arid Australian spinifex (Triodia spp.) grasslands after high rainfall periods that trigger fuel accumulation. It has been suggested that these fires are unprecedented in the modern era and were formerly constrained by Aboriginal patch-burning that kept landscape fuel levels low. This assumption deserves scrutiny, as evidence from fire-prone systems globally indicates that weather factors often the primary determinant behind megafire incidence, and that fuel management does not mitigate such fires during periods of climatic extreme. We reviewed explorer‚Äôs diaries, anthropologist‚Äôs reports, and remotely sensed data from the Australian Western Desert for evidence of large rainfall linked fires during the pre-contact period when traditional Aboriginal patch-burning was still being practiced. We used only observations that contained empiric estimates of fire sizes. Concurrently, we employed remote rainfall data and the Oceanic Ni√±o Index to relate fire size to likely seasonal conditions at the time the observations were made. Numerous records were found of small fires during periods of average and below-average rainfall conditions, but no evidence of large-scale fires during these times. By contrast, there was strong evidence of large-scale wildfires during a high-rainfall period in the early 1870s, some of which are estimated to have burnt areas up to 700 000 ha. Our literature review also identified several Western Desert Aboriginal mythologies that refer to large-scale conflagrations. As oral traditions sometimes corroborate historic events, these myths may add further evidence that large fires are an inherent feature of spinifex grassland fire regimes. Overall, the results suggest that, contrary to predictions of the patch-burn mosaic hypothesis, traditional Aboriginal burning did not modulate spinifex fire size during periods of extreme-high arid zone rainfall. The mechanism behind this appears to be plant assemblages in seral spinifex vegetation that comprise highly flammable non-spinifex tussock grasses that rapidly accumulate high fuel loads under favorable precipitation conditions. Our finding that fuel management does not prevent megafires under extreme conditions in arid Australia has parallels with the primacy of climatic factors as drivers of megafires in the forests of temperate Australia.","tags":["vegetaci√≥n √°rida","ecolog√≠a del fuego","retroalimentaciones de hierba-fuego","quema de parches","manejo de tierras ind√≠genas"],"title":"Rainfall-linked megafires as innate fire regime elements in arid Australian spinifex (Triodia spp.) grasslands","type":"publication"},{"authors":null,"categories":["sig","R","R:avanzado","visualizaci√≥n"],"content":"\r\rConsideraciones iniciales\rUna desventaja de los mapas coropletas es que estos suelen distorsionar la relaci√≥n entre la verdadera geograf√≠a subyacente y la variable representada. Se debe a que las divisiones administrativas no suelen coincidir con la realidad geogr√°fica, donde la gente vive. Adem√°s, grandes √°reas aparentan tener un peso con poca poblaci√≥n que no tienen realmente. Para reflejar mejor la realidad se hace uso de distribuciones m√°s realista de la poblaci√≥n como puede ser el uso de suelo. Con t√©cnicas de Sistemas de Informaci√≥n Geogr√°fica es posible redistribuir la variable de inter√©s en funci√≥n de una variable a menor unidad espacial.\nCuando disponemos de datos de puntos, el proceso de redistribuci√≥n simplemente es recortar √°reas de puntos con poblaci√≥n a base del uso de suelo, normalmente clasificado como urbano. En caso de pol√≠gonos tambi√©n podr√≠amos recortar con pol√≠gonos de uso de suelo, pero una alternativa interesante son los mismos datos en formato raster. Veremos c√≥mo podemos realizar un mapa dasim√©trico usando datos raster con una resoluci√≥n de 100 m. En este post usaremos datos de secciones censales de la renta media y el √≠ndice de Gini de Espa√±a. No s√≥lo haremos un mapa dasim√©trico, sino tambi√©n bivariante, representando con dos gamas de colores ambas variables en el mismo mapa.\n\rPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rpatchwork\rSimple gram√°tica para combinar ggplots separados en el mismo gr√°fico\r\rraster\rImportar, exportar y manipular raster\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rbiscale\rHerramientas y paletas para mapeo tem√°tico bivariado\r\rsysfonts\rCargar fuentes en R\r\rshowtext\rUsar fuentes m√°s f√°cilmente en gr√°ficos R\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;patchwork\u0026quot;)) install.packages(\u0026quot;patchwork\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;biscale\u0026quot;)) install.packages(\u0026quot;biscale\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# paquetes\rlibrary(tidyverse)\rlibrary(sf)\rlibrary(readxl)\rlibrary(biscale)\rlibrary(patchwork)\rlibrary(raster)\rlibrary(sysfonts)\rlibrary(showtext)\rlibrary(raster)\r\rPreparaci√≥n\rDatos\rPrimero descargamos todos los datos necesarios. Con excepci√≥n de los datos CORINE Land Cover (~200 MB), se pueden obtener los datos almacenados en este blog directamente v√≠a los enlaces indicados .\n\rCORINE Land Cover 2018 (geotiff): COPERNICUS\rDatos de renta e √≠ndice Gini (excel) [INE]: descarga\rL√≠mites censales de Espa√±a (vectorial) [INE]: descarga\r\r\rImportar\rLo primero que hacemos es importar el raster del uso de suelo, los datos de renta e √≠ndice de Gini y los l√≠mites censales.\n# raster de CORINE LAND COVER 2018\rurb \u0026lt;- raster(\u0026quot;U2018_CLC2018_V2020_20u1.tif\u0026quot;)\r# datos de renta y Gini\rrenta \u0026lt;- read_excel(\u0026quot;30824.xlsx\u0026quot;)\rgini \u0026lt;- read_excel(\u0026quot;37677.xlsx\u0026quot;)\r# l√≠mites censales del INE\rlimits \u0026lt;- read_sf(\u0026quot;SECC_CE_20200101.shp\u0026quot;) \r\rUsos de suelo\rEn este primer paso filtramos las secciones censales para obtener aquellas de la Comunidad Aut√≥noma de Madrid, y creamos los l√≠mites municipales. Para disolver los pol√≠gonos de secciones censales aplicamos la funci√≥n group_by() en combinaci√≥n con summarise().\n# filtramos la Comunidad Aut√≥noma de Madrid\rlimits \u0026lt;- filter(limits, NCA == \u0026quot;Comunidad de Madrid\u0026quot;)\r# obtenemos los l√≠mites municipales\rmun_limit \u0026lt;- group_by(limits, CUMUN) %\u0026gt;% summarise()\rEn el siguiente paso recortamos el raster de uso de suelo con los l√≠mites de Madrid. Recomiendo usar siempre primero la funci√≥n crop() y despu√©s mask(), la primera recorta a la extensi√≥n requerida y la segunda enmascara. Posteriormente, eliminamos todos los valores que correspondan a 1 o 2 (urbano continuo, discontinuo). Por √∫ltimo, proyectamos el raster.\n# proyectamos los l√≠mites limits_prj \u0026lt;- st_transform(limits, projection(urb))\r# acortamos y enmascaramos urb_mad \u0026lt;- crop(urb, limits_prj) %\u0026gt;% mask(limits_prj)\r# eliminamos p√≠xeles no urbanos urb_mad[!urb_mad %in% 1:2] \u0026lt;- NA # plot del raster\rplot(urb_mad)\r# proyectamos urb_mad \u0026lt;- projectRaster(urb_mad, crs = CRS(\u0026quot;+proj=longlat +datum=WGS84 +no_defs\u0026quot;))\rEn este siguiente paso, convertimos los datos raster en un objeto sf de puntos.\n# transformamos el raster a xyz y objeto sf urb_mad \u0026lt;- as.data.frame(urb_mad, xy = TRUE, na.rm = TRUE) %\u0026gt;%\rst_as_sf(coords = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), crs = 4326)\r# a√±adimos las columnas de las coordinadas\rurb_mad \u0026lt;- urb_mad %\u0026gt;% rename(urb = 1) %\u0026gt;% cbind(st_coordinates(urb_mad))\r\rRenta media e √≠ndice de Gini\rEl formato de los Excels no coincide con el original del INE, dado que he limpiado el formato antes con el objetivo de hacer m√°s f√°cil este post. Lo que nos queda es crear una columna con los c√≥digos de las secciones censales y excluir datos que corresponden a otro nivel administrativo.\n## datos renta y gini INE\rrenta_sec \u0026lt;- mutate(renta, NATCODE = str_extract(CUSEC, \u0026quot;[0-9]{5,10}\u0026quot;), nc_len = str_length(NATCODE),\rmun_name = str_remove(CUSEC, NATCODE) %\u0026gt;% str_trim()) %\u0026gt;%\rfilter(nc_len \u0026gt; 5)\rgini_sec \u0026lt;- mutate(gini, NATCODE = str_extract(CUSEC, \u0026quot;[0-9]{5,10}\u0026quot;), nc_len = str_length(NATCODE),\rmun_name = str_remove(CUSEC, NATCODE) %\u0026gt;% str_trim()) %\u0026gt;%\rfilter(nc_len \u0026gt; 5)\rEn el siguiente paso unimos ambas tablas con las secciones censales usando left_join() y convertimos columnas de inter√©s en modo num√©rico.\n# unimos ambas tablas de renta y Gini con los l√≠mites censales\rmad \u0026lt;- left_join(limits, renta_sec, by = c(\u0026quot;CUSEC\u0026quot;=\u0026quot;NATCODE\u0026quot;)) %\u0026gt;% left_join(gini_sec, by = c(\u0026quot;CUSEC\u0026quot;=\u0026quot;NATCODE\u0026quot;))\r# convertimos columnas en num√©rico mad \u0026lt;- mutate_at(mad, c(23:27, 30:31), as.numeric)\r\rVariable bivariante\rPara crear un mapa bivariante debemos construir una √∫nica variable que combina diferentes clases de dos variables. Normalmente son tres de cada una lo que lleva a nueve clases en total. En nuestro caso, la renta media y el √≠ndice Gini. El paquete biscale incluye funciones auxiliares para llevar a cabo este proceso. Con la funci√≥n bi_class() creamos esta variable de clasificaci√≥n usando cuantiles como algoritmo. Dado que en ambas variables encontramos valores ausentes, corregimos aquellas combinaciones entre ambas variables donde aparece un NA.\n## creamos clasificaci√≥n bivariante\rmapbivar \u0026lt;- bi_class(mad, GINI_2017, RNMP_2017, style = \u0026quot;quantile\u0026quot;, dim = 3) %\u0026gt;% mutate(bi_class = ifelse(str_detect(bi_class, \u0026quot;NA\u0026quot;), NA, bi_class))\r# resultado\rhead(dplyr::select(mapbivar, GINI_2017, RNMP_2017, bi_class))\r## Simple feature collection with 6 features and 3 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: 415538.9 ymin: 4451487 xmax: 469341.7 ymax: 4552422\r## Projected CRS: ETRS89 / UTM zone 30N\r## # A tibble: 6 x 4\r## GINI_2017 RNMP_2017 bi_class geometry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;MULTIPOLYGON [m]\u0026gt;\r## 1 NA NA \u0026lt;NA\u0026gt; (((446007.9 4552348, 446133.7 4552288, 446207.8 ~\r## 2 31 13581 2-2 (((460243.8 4487756, 460322.4 4487739, 460279 44~\r## 3 30 12407 2-2 (((457392.5 4486262, 457391.6 4486269, 457391.1 ~\r## 4 34.3 13779 3-2 (((468720.8 4481374, 468695.5 4481361, 468664.6 ~\r## 5 33.5 9176 3-1 (((417140.2 4451736, 416867.5 4451737, 416436.8 ~\r## 6 26.2 10879 1-1 (((469251.9 4480826, 469268.1 4480797, 469292.6 ~\rTerminamos redistribuyendo la variable de desigualdad sobre los p√≠xeles del uso de suelo urbano. La funci√≥n st_join() une los datos con los puntos del uso de suelo.\n## redistribuimos los p√≠xeles urbanos a la desigualdad\rmapdasi \u0026lt;- st_join(urb_mad, st_transform(mapbivar, 4326))\r\r\rConstrucci√≥n del mapa\rLeyenda y fuente\rAntes de construir ambos mapas debemos crear la leyenda usando la funci√≥n bi_legend(). En la funci√≥n definimos los t√≠tulos para cada variable, el n√∫mero de dimensiones y la gama de colores. Por √∫ltimo, a√±adimos la fuente de Montserrat para los t√≠tulos del gr√°fico final.\n# leyenda bivariante\rlegend2 \u0026lt;- bi_legend(pal = \u0026quot;DkViolet\u0026quot;,\rdim = 3,\rxlab = \u0026quot;M√°s desigual\u0026quot;,\rylab = \u0026quot;M√°s renta\u0026quot;,\rsize = 9)\r#descarga de fuente\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\rshowtext_auto()\r\rMapa dasim√©trico\rEste mapa construimos usando geom_tile() para los p√≠xeles y geom_sf() para los l√≠mites municipales. Adem√°s, ser√° el mapa de la derecha donde ubicamos tambi√©n la leyenda. Para a√±adir la leyenda hacemos uso de la funci√≥n annotation_custom() indicando la posici√≥n en las coordenadas geogr√°ficas del mapa. El paquete biscale tambi√©n nos ayuda con la definici√≥n del color a trav√©s de la funci√≥n bi_scale_fill().\np2 \u0026lt;- ggplot(mapdasi) + geom_tile(aes(X, Y, fill = bi_class), show.legend = FALSE) +\rgeom_sf(data = mun_limit, color = \u0026quot;grey80\u0026quot;, fill = NA, size = 0.2) +\rannotation_custom(ggplotGrob(legend2), xmin = -3.25, xmax = -2.65,\rymin = 40.55, ymax = 40.95) +\rbi_scale_fill(pal = \u0026quot;DkViolet\u0026quot;, dim = 3, na.value = \u0026quot;grey90\u0026quot;) +\rlabs(title = \u0026quot;dasim√©trico\u0026quot;, x = \u0026quot;\u0026quot;, y =\u0026quot;\u0026quot;) +\rbi_theme() +\rtheme(plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 30, face = \u0026quot;bold\u0026quot;)) +\rcoord_sf(crs = 4326)\r\rMapa coropleta\rEl mapa coropleta se construye de forma similar al mapa anterior con la diferencia de que usamos geom_sf().\np1 \u0026lt;- ggplot(mapbivar) + geom_sf(aes(fill = bi_class), colour = NA, size = .1, show.legend = FALSE) +\rgeom_sf(data = mun_limit, color = \u0026quot;white\u0026quot;, fill = NA, size = 0.2) +\rbi_scale_fill(pal = \u0026quot;DkViolet\u0026quot;, dim = 3, na.value = \u0026quot;grey90\u0026quot;) +\rlabs(title = \u0026quot;coropl√©tico\u0026quot;, x = \u0026quot;\u0026quot;, y =\u0026quot;\u0026quot;) +\rbi_theme() +\rtheme(plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 30, face = \u0026quot;bold\u0026quot;)) +\rcoord_sf(crs = 4326)\r\rCombinar ambos mapas\rCon ayuda del paquete patchwork combinamos ambos mapas en una √∫nica fila, primero el mapa coropleta y a su derecha el mapa dasim√©trico. M√°s detalles de la gram√°tica que se usa para la combinaci√≥n de gr√°ficos aqu√≠.\n# Combinamos p \u0026lt;- p1 | p2\rp\r\n\r\r","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1614556800,"objectID":"102a8a3188ac1f201eecb0927c957e91","permalink":"https://dominicroye.github.io/es/2021/mapa-dasimetrico-bivariante/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/es/2021/mapa-dasimetrico-bivariante/","section":"post","summary":"Una desventaja de los mapas coropletas es que estos suelen distorsionar la relaci√≥n entre la verdadera geograf√≠a subyacente y la variable representada. Se debe a que las divisiones administrativas no suelen coincidir con la realidad geogr√°fica, donde la gente vive. Adem√°s, grandes √°reas aparentan tener un peso con poca poblaci√≥n que no tienen realmente. Para reflejar mejor la realidad se hace uso de distribuciones m√°s realista de la poblaci√≥n como puede ser el uso de suelo. Con t√©cnicas de Sistemas de Informaci√≥n Geogr√°fica es posible redistribuir la variable de inter√©s en funci√≥n de una variable a menor unidad espacial.","tags":["bivariante","mapa","desigualdad","renta","Madrid","urbano"],"title":"Mapa dasim√©trico bivariante","type":"post"},{"authors":["P Fdez-Arroyabe","et al."],"categories":null,"content":"","date":1609977600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609977600,"objectID":"c43c3d47ab5228aa3fec38fcdeb70898","permalink":"https://dominicroye.github.io/es/publication/2020-glossary-electricity-ij-biometeo/","publishdate":"2021-01-07T00:00:00Z","relpermalink":"/es/publication/2020-glossary-electricity-ij-biometeo/","section":"publication","summary":"There is an increasing interest to study the interactions between atmospheric electrical parameters and living organisms at multiple scales. So far, relatively few studies have been published that focus on possible biological effects of atmospheric electric and magnetic fields. To foster future work in this area of multidisciplinary research, here we present a glossary of relevant terms. Its main purpose is to facilitate the process of learning and communication among the different scientific disciplines working on this topic. While some definitions come from existing sources, other concepts have been re-defined to better reflect the existing and emerging scientific needs of this multidisciplinary and transdisciplinary area of research.","tags":["fen√≥menos de electricidad atmosf√©rica","campo el√©ctrico atmosf√©rico","efectos biol√≥gicos","perfil biometeorol√≥gico","glosario"],"title":"Glossary on atmospheric electricity and its effects on biology","type":"publication"},{"authors":null,"categories":["visualizaci√≥n","R","R:intermedio"],"content":"\r\rRecientemente buscaba una representaci√≥n visual para mostrar los cambios diarios de la temperatura, precipitaci√≥n y el viento en una aplicaci√≥n xeo81.shinyapps.io/MeteoExtremosGalicia, lo que me llev√≥ a usar un heatmap en forma de calendario. La aplicaci√≥n shiny se actualiza cada cuatro horas con nuevos datos mostrando calendarios de cada estaci√≥n meteorol√≥gica. El heatmap como calendario permite visualizar cualquier variable con una referencia temporal diaria.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rragg\rSalidas gr√°ficas para R basados en la librer√≠a AGG\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ragg\u0026quot;)) install.packages(\u0026quot;ragg\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r# paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ragg)\rPara aquellos con menos experiencia con tidyverse, recomiendo una breve introducci√≥n en este blog post.\n\rDatos\rEn este ejemplo usaremos la precipitaci√≥n diaria de Santiago de Compostela de este a√±o 2020 (hasta el 20 de diciembre) descarga.\n# importamos los datos\rdat_pr \u0026lt;- read_csv(\u0026quot;precipitation_santiago.csv\u0026quot;)\rdat_pr\r## # A tibble: 355 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-01-01 0 ## 2 2020-01-02 0 ## 3 2020-01-03 5.4\r## 4 2020-01-04 0 ## 5 2020-01-05 0 ## 6 2020-01-06 0 ## 7 2020-01-07 0 ## 8 2020-01-08 1 ## 9 2020-01-09 3.8\r## 10 2020-01-10 0 ## # ... with 345 more rows\r\rPreparaci√≥n\rEn el primer paso debemos 1) complementar la serie temporal desde el 21 al 31 de diciembre con NA, 2) a√±adir el d√≠a de la semana, el mes, el n√∫mero de la semana y el d√≠a. En funci√≥n de si queremos que cada semana comience por Domingo o Lunes debemos indicarlo en la funci√≥n wday().\ndat_pr \u0026lt;- dat_pr %\u0026gt;% complete(date = seq(ymd(\u0026quot;2020-01-01\u0026quot;), ymd(\u0026quot;2020-12-31\u0026quot;), \u0026quot;day\u0026quot;)) %\u0026gt;%\rmutate(weekday = wday(date, label = T, week_start = 1), month = month(date, label = T, abbr = F),\rweek = isoweek(date),\rday = day(date))\rEn el siguiente paso corregimos las etiquetas de los d√≠as de la semana, es un bug dentro del paquete lubridate. Adem√°s debemos hacer un cambio en la semana del a√±o, lo que se debe a que en ciertos a√±os pueden quedar, por ejemplo, unos d√≠as al final de a√±o como primera semana del siguiente a√±o. Tambi√©n creamos dos nuevas columnas. Por una parte, categorizamos la precipitaci√≥n en 14 clases y por otra definimos un color de texto blanco para tonos m√°s oscuros.\ndat_pr \u0026lt;- mutate(dat_pr, weekday = factor(weekday, levels(weekday),\rstr_sub(levels(weekday), 1, 2)),\rweek = case_when(month == \u0026quot;diciembre\u0026quot; \u0026amp; week == 1 ~ 53,\rmonth == \u0026quot;enero\u0026quot; \u0026amp; week %in% 52:53 ~ 0,\rTRUE ~ week),\rpcat = cut(pr, c(-1, 0, .5, 1:5, 7, 9, 15, 20, 25, 30, 300)),\rtext_col = ifelse(pcat %in% c(\u0026quot;(15,20]\u0026quot;, \u0026quot;(20,25]\u0026quot;, \u0026quot;(25,30]\u0026quot;, \u0026quot;(30,300]\u0026quot;), \u0026quot;white\u0026quot;, \u0026quot;black\u0026quot;)) dat_pr \r## # A tibble: 366 x 8\r## date pr weekday month week day pcat text_col\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; ## 1 2020-01-01 0 mi enero 1 1 (-1,0] black ## 2 2020-01-02 0 ju enero 1 2 (-1,0] black ## 3 2020-01-03 5.4 vi enero 1 3 (5,7] black ## 4 2020-01-04 0 s√° enero 1 4 (-1,0] black ## 5 2020-01-05 0 do enero 1 5 (-1,0] black ## 6 2020-01-06 0 lu enero 2 6 (-1,0] black ## 7 2020-01-07 0 ma enero 2 7 (-1,0] black ## 8 2020-01-08 1 mi enero 2 8 (0.5,1] black ## 9 2020-01-09 3.8 ju enero 2 9 (3,4] black ## 10 2020-01-10 0 vi enero 2 10 (-1,0] black ## # ... with 356 more rows\r\rVisualizaci√≥n\rPrimero creamos una rampa de color a partir de colores Brewer.\n# rampa de color\rpubu \u0026lt;- RColorBrewer::brewer.pal(9, \u0026quot;PuBu\u0026quot;)\rcol_p \u0026lt;- colorRampPalette(pubu)\rAntes de construir el gr√°fico definimos un estilo personalizado como funci√≥n. Para ello, especificamos todos los elementos y sus modificaciones con ayuda de la funci√≥n theme().\ntheme_calendar \u0026lt;- function(){\rtheme(aspect.ratio = 1/2,\raxis.title = element_blank(),\raxis.ticks = element_blank(),\raxis.text.y = element_blank(),\raxis.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rpanel.grid = element_blank(),\rpanel.background = element_blank(),\rstrip.background = element_blank(),\rstrip.text = element_text(family = \u0026quot;Montserrat\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 15),\rlegend.position = \u0026quot;top\u0026quot;,\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5),\rlegend.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 9, hjust = 1),\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = 1, size = 8),\rpanel.border = element_rect(colour = \u0026quot;grey\u0026quot;, fill=NA, size=1),\rplot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5, size = 26, face = \u0026quot;bold\u0026quot;, margin = margin(0,0,0.5,0, unit = \u0026quot;cm\u0026quot;)),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5, size = 16)\r)\r}\rFinalmente, creamos el gr√°fico usando geom_tile() y especificamos como eje X el d√≠a de la semana y como eje Y el n√∫mero de la semana. Como pod√©is observar en la variable de la semana (-week) cambio el signo con el objetivo de que el primer d√≠a de cada mes este en la primera fila. Con geom_text() a√±adimos el n√∫mero de cada d√≠a con su color seg√∫n lo que definimos anteriormente. En guides hacemos los ajustes de la barra de color y en scale_fill/colour_manual() definimos los colores correspondientes. Un importante paso lo encontramos en facet_wrap() donde especificamos las facetas de cada mes. Las facetas deben tener escalas libres y lo √≥ptimo ser√≠a una distribuci√≥n de 4x3 facetas. Es posible modificar la posici√≥n del n√∫mero de d√≠a a otra posici√≥n usando los argumentos nudge_* en geom_text() (por ej. esquina abajo derecha: nudge_x = .35, nudge_y = -.25).\n ggplot(dat_pr, aes(weekday, -week, fill = pcat)) +\rgeom_tile(colour = \u0026quot;white\u0026quot;, size = .4) + geom_text(aes(label = day, colour = text_col), size = 2.5) +\rguides(fill = guide_colorsteps(barwidth = 25, barheight = .4,\rtitle.position = \u0026quot;top\u0026quot;)) +\rscale_fill_manual(values = c(\u0026quot;white\u0026quot;, col_p(13)),\rna.value = \u0026quot;grey90\u0026quot;, drop = FALSE) +\rscale_colour_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;), guide = FALSE) + facet_wrap(~ month, nrow = 4, ncol = 3, scales = \u0026quot;free\u0026quot;) +\rlabs(title = \u0026quot;¬øC√≥mo est√° siendo el 2020 en Santiago?\u0026quot;, subtitle = \u0026quot;Precipitaci√≥n\u0026quot;,\rcaption = \u0026quot;Datos: Meteogalicia\u0026quot;,\rfill = \u0026quot;mm\u0026quot;) +\rtheme_calendar()\rPara exportar haremos uso del paquete ragg, que proporciona mayor rendimiento y mayor calidad que los dispositivos r√°ster est√°ndar proporcionados por grDevices.\nggsave(\u0026quot;pr_calendar.png\u0026quot;, height = 10, width = 8, device = agg_png())\rEn otros calendarios he a√±adido la direcci√≥n predominante del viento de cada d√≠a como flecha usando geom_arrow() del paquete metR (se puede ver en la mencionada aplicaci√≥n).\n\n\r","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1608422400,"objectID":"1993d8dd233bcb5c9057ac8811b45c4e","permalink":"https://dominicroye.github.io/es/2020/un-heatmap-como-calendario/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/es/2020/un-heatmap-como-calendario/","section":"post","summary":"Recientemente buscaba una representaci√≥n visual para mostrar los cambios diarios de la temperatura, precipitaci√≥n y el viento en una aplicaci√≥n [xeo81.shinyapps.io/MeteoExtremosGalicia](https://xeo81.shinyapps.io/MeteoExtremosGalicia/), lo que me llev√≥ a usar un heatmap en forma de calendario. La aplicaci√≥n [shiny](https://shiny.rstudio.com/) se actualiza cada cuatro horas con nuevos datos mostrando calendarios de cada estaci√≥n meteorol√≥gica.","tags":["calendario","temperatura","clima","heatmap"],"title":"Un heatmap como calendario","type":"post"},{"authors":["C I√±iguez","D Roy√©","A Tob√≠as"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1606780800,"objectID":"27e4ca05b49a88768f6b8aeb6904c20c","permalink":"https://dominicroye.github.io/es/publication/2021-morbi-mortality-spain-environmental-research/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/es/publication/2021-morbi-mortality-spain-environmental-research/","section":"publication","summary":"Background: Climate change is a severe public health challenge. Understanding to what extent fatal and non-fatal consequences of specific diseases are associated with temperature may help to improve the effectiveness of preventive public health efforts. This study examines the effects of temperature on deaths and hospital admissions by cardiovascular and respiratory diseases, empathizing the difference between mortality and morbidity. Methods: Daily counts for mortality and hospital admissions by cardiovascular and respiratory diseases were collected for the 52 provincial capital cities in Spain, between 1990 and 2014. The association with temperature in each city was investigated by means of distributed lag non-linear models using quasiPoisson regression. City-specific exposure-response curves were pooled by multivariate random-effects meta-analysis to obtain countrywide risk estimates of mortality and hospitalizations due to heat and cold, and attributable fractions were computed. Results: Heat and cold exposure were identified to be associated with increased risk of cardiovascular and respiratory mortality. Heat was not found to have an impact on hospital admissions. The estimated fraction of mortality attributable to cold was of greater magnitude in hospitalizations (17.5% for cardiovascular and 12.5% for respiratory diseases) compared to deaths (9% and 2.7%, respectively). Conclusions: There were noteworthy differences between temperature-related mortality and hospital admissions regarding cardiovascular and respiratory diseases, hence reinforcing the convenience of cause-specific measures to prevent temperature-related deaths.","tags":["temperatura","mortalidad","ingresos hospitalarios","cardiovascular","respiratorio","Espa√±a","modelos no lineales de retardo distribuido"],"title":"Contrasting patterns of temperature related mortality and hospitalization by cardiovascular and respiratory diseases in 52 Spanish cities","type":"publication"},{"authors":["S Gestal","D Roy√©","L S√°nchez Santos","A Figueiras"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1606780800,"objectID":"3470b5a9635dbb70b0a9278f6afe7308","permalink":"https://dominicroye.github.io/es/publication/2020-emergency-calls-ij-public-health/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/es/publication/2020-emergency-calls-ij-public-health/","section":"publication","summary":"Introduction and objectives. The increase in mortality and hospital admissions associated with high and low temperatures is well established. However, less is known about the influence of extreme ambient temperature conditions on cardiovascular ambulance dispatches. This study seeks to evaluate the effects of minimum and maximum daily temperatures on cardiovascular morbidity in the cities of Vigo and A Coru√±a in North-West Spain, using emergency medical calls during the period 2005‚Äì2017. Methods. For the purposes of analysis, we employed a quasi-Poisson time series regression model, within a distributed non-linear lag model by exposure variable and city. The relative risks of cold- and heat-related calls were estimated for each city and temperature model. Results. A total of 70,537 calls were evaluated, most of which were associated with low maximum and minimum temperatures on cold days in both cities. At maximum temperatures, significant cold-related effects were observed at lags of 3‚Äì6 days in Vigo and 5‚Äì11 days in A Coru√±a. At minimum temperatures, cold-related effects registered a similar pattern in both cities, with significant relative risks at lags of 4 to 12 days in A Coru√±a. Heat-related effects did not display a clearly significant pattern. Conclusions. An increase in cardiovascular morbidity is observed with moderately low temperatures without extremes being required to establish an effect. Public health prevention plans and warning systems should consider including moderate temperature range in the prevention of cardiovascular morbidity.","tags":["llamadas de emergencia","temperaturas extremas","cardiovascular","Galicia","Espa√±a","modelos de retardo distribuidos no lineales","morbilidad"],"title":"Impact of Extreme Temperatures on Ambulance Dispatches Due to Cardiovascular Causes in North-West Spain","type":"publication"},{"authors":null,"categories":["visualizaci√≥n","R","R:avanzado"],"content":"\r\rEn el campo de la visualizaci√≥n de datos, la animaci√≥n de datos espaciales en su dimensi√≥n temporal lleva a mostrar cambios y patrones fascinantes y muy visuales. A ra√≠z de una de las √∫ltimas publicaciones que he realizado en los RRSS me pidieron que hiciera un post acerca de c√≥mo lo cre√©. Pues bien, aqu√≠ vamos para empezar con datos de la Espa√±a peninsular. Pod√©is encontrar m√°s animaciones en la secci√≥n de gr√°ficos de este mismo blog.\nI couldn\u0026#39;t resist to make another animation. Smoothed daily maximum temperature throughout the year in Europe. #rstats #ggplot2 #dataviz #climate pic.twitter.com/ZC9L0vh3vR\n\u0026mdash; Dr. Dominic Roy√© (@dr_xeo) May 9, 2020  Paquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rrnaturalearth\rMapas vectoriales del mundo ‚ÄòNatural Earth‚Äô\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rraster\rImportar, exportar y manipular raster\r\rggthemes\rEstilos para ggplot2\r\rgifski\rCrear gifs\r\rshowtext\rUsar fuentes m√°s f√°cilmente en gr√°ficos R\r\rsysfonts\rCargar fuentes del sistema y fuentes de Google\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;ggthemes\u0026quot;)\rif(!require(\u0026quot;gifski\u0026quot;)) install.packages(\u0026quot;gifski\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# paquetes\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggthemes)\rlibrary(sf)\rlibrary(rnaturalearth)\rlibrary(extrafont)\rlibrary(showtext)\rlibrary(RColorBrewer)\rlibrary(gifski)\rPara aquellos con menos experiencia con tidyverse, recomiendo la breve introducci√≥n en este blog post.\n\rPreparaci√≥n\rDatos\rDescargamos los datos STEAD de la temperatura m√°xima (tmax_pen.nc) en formato netCDF desde el repositario del CSIC aqu√≠ (el tama√±o de los datos es de 2 GB). Se trata de un conjunto de datos con una resoluci√≥n espacial de 5 km y comprenden las temperaturas m√°ximas diarias desde 1901 a 2014. En la climatolog√≠a y la meteorolog√≠a, un formato de uso muy extendido es el de las bases de datos netCDF, que permiten obtener una estructura multidimensional e intercambiar los datos de forma independiente al sistema operativo empleado. Se trata de un formato espacio-temporal con una cuadr√≠cula regular o irregular. La estructura multidimensional en forma de matriz (array) permite usar no s√≥lo datos espacio-temporales sino tambi√©n multivariables. En nuestros datos tendremos un cubo de tres dimensiones: longitud, latitud y tiempo de la temperatura m√°xima.\nRoy√© 2015. S√©mata: Ciencias Sociais e Humanidades 27:11-37\n\r\rImportar los datos\rEl formato netCDF con extensi√≥n .nc lo podemos importar v√≠a dos paquetes principales: 1) ncdf4 y 2) raster. Aunque el paquete raster realmente lo que hace es usar el primer paquete para importar los datos. En este post usaremos el paquete raster dado que es algo m√°s f√°cil, con algunas funciones muy √∫tiles y m√°s universales para todo tipo de formato raster. Las funciones principales de importaci√≥n son: raster(), stack() y brick(). La primera funci√≥n s√≥lo permite importar una √∫nica capa, en cambio, las √∫ltimas dos funciones se emplean para datos multidimensionales. En nuestro caso s√≥lo tenemos una variable, por tanto no ser√≠a necesario hacer uso del argumento varname.\n# importamos los datos ncdf\rtmx \u0026lt;- brick(\u0026quot;tmax_pen.nc\u0026quot;, varname = \u0026quot;tx\u0026quot;)\r## Loading required namespace: ncdf4\rtmx # metadatos\r## class : RasterBrick ## dimensions : 190, 230, 43700, 41638 (nrow, ncol, ncell, nlayers)\r## resolution : 0.0585, 0.045 (x, y)\r## extent : -9.701833, 3.753167, 35.64247, 44.19247 (xmin, xmax, ymin, ymax)\r## crs : +proj=longlat +datum=WGS84 +no_defs ## source : tmax_pen.nc ## names : X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ... ## Time (days since 1901-01-01): 1, 41638 (min, max)\r## varname : tx\rEn la estructura del objeto RasterBrick vemos todos los metadatos necesarios: desde la resoluci√≥n, las dimensiones o el tipo de proyecci√≥n, hasta el nombre de la variable. Adem√°s nos indica que √∫nicamente apunta a los datos (source) y no los ha importado a la memoria RAM lo que facilita el trabajo con grandes conjuntos de datos.\nPara acceder a cualquier capa hacemos uso de [[ ]] con el √≠ndice correspondiente. As√≠ podemos plotear f√°cilmente cualquier d√≠a de los 41.638 d√≠as de los que disponemos.\n# mapear cualquier d√≠a\rplot(tmx[[200]], col = rev(heat.colors(7)))\r\rCalcular el promedio de la temperatura\rEn este paso el objetivo es calcular el promedio de la temperatura para cada d√≠a del a√±o. Por eso, lo primero que hacemos es crear un vector, indicando el d√≠a del a√±o para toda la serie temporal. En el paquete raster disponemos de la funci√≥n stackApply() que permite aplicar una funci√≥n sobre grupos de capas, o mejor dicho, √≠ndices. Dado que nuestro conjunto de datos es grande, incluimos esta funci√≥n en las funciones de paralelizaci√≥n.\nEmpezamos con las funciones beginClusterr() y endCluster() que inician y finalizan la paralelizaci√≥n. En la primera debemos indicar el n√∫mero de n√∫cleos que queremos usar. En este caso uso 4 de 7 posibles n√∫cleos, no obstante, se debe cambiar el n√∫mero seg√∫n las caracter√≠sticas de cada CPU, siendo la norma n-1. Entonces, la funci√≥n clusterR permite ejecutar funciones en paralelo con m√∫ltiples n√∫cleos. El primer argumento corresponde al objeto raster, el segundo a la funci√≥n empleada, y en forma de lista pasamos los argumentos de la funci√≥n stackApply(), los √≠ndices que crean los grupos y la funci√≥n usada para cada uno de los grupos. Si a√±adimos el argumento progress = 'text' se muestra una barra de progreso del c√°lculo.\n Para el conjunto de datos de EEUU hice un preprocesamiento, el c√°lculo del promedio en la nube a trav√©s de Google Earth Engine lo que hace todo el proceso m√°s r√°pido. En el caso de Australia, el preprocesamiento fue m√°s complejo ya que el conjunto de datos esta en archivos netCDF para cada a√±o.   # convertimos las fechas entre 1901 y 2014 a d√≠as del a√±o\rtime_days \u0026lt;- yday(seq(as_date(\u0026quot;1901-01-01\u0026quot;), as_date(\u0026quot;2014-12-31\u0026quot;), \u0026quot;day\u0026quot;))\r# calculamos el promedio beginCluster(4)\rtmx_mean \u0026lt;- clusterR(tmx, stackApply, args = list(indices = time_days, fun = mean))\rendCluster()\r\rSuavizar la variabilidad de las temperaturas\rAntes de pasar a suavizar las series temporales de nuestro RasterBrick, un ejemplo del por qu√© lo hacemos. Extraemos un p√≠xel de nuestro conjunto de datos en las coordenadas -1¬∫ de longitud y 40¬∫ de latitud usando la funci√≥n extract(). Dado que la funci√≥n con el mismo nombre aparece en varios paquetes, debemos cambiar a la forma nombre_paquete::nombre_funci√≥n. El resultado es una matriz con una fila correspondiente al p√≠xel y 366 columnas de los d√≠as del a√±o. El siguiente paso es la creaci√≥n de un data.frame con una fecha dummy y la temperatura m√°xima extra√≠da.\n# extraemos un p√≠xel\rpoint_ts \u0026lt;- raster::extract(tmx_mean, matrix(c(-1, 40), nrow = 1))\rdim(point_ts) # dimensiones \r## [1] 1 366\r# creamos un data.frame\rdf \u0026lt;- data.frame(date = seq(as_date(\u0026quot;2000-01-01\u0026quot;), as_date(\u0026quot;2000-12-31\u0026quot;), \u0026quot;day\u0026quot;),\rtmx = point_ts[1,])\r# visualizamos la temperatura m√°xima ggplot(df, aes(date, tmx)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rlabs(y = \u0026quot;Temperatura m√°xima\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rEl gr√°fico muestra claramente la todav√≠a existente variabilidad, lo que har√≠a fluctuar bastante una animaci√≥n. Por eso, creamos una funci√≥n de suavizado basado en un ajuste de regresi√≥n polinomial local (LOESS), m√°s detalles los encontr√°is en la ayuda de la funci√≥n loess(). El argumento m√°s importante es span que determina el grado de suavizado de la funci√≥n, cuanto m√°s peque√±o el valor menos suave ser√° la curva. El mejor resultado me ha dado un valor del 0,5.\ndaily_smooth \u0026lt;- function(x, span = 0.5){\rif(all(is.na(x))){\rreturn(x) } else {\rdf \u0026lt;- data.frame(yd = 1:366, ta = x)\rm \u0026lt;- loess(ta ~ yd, span = span, data = df)\rest \u0026lt;- predict(m, 1:366)\rreturn(est)\r}\r}\rAplicamos nuestra nueva funci√≥n de suavizado a la serie temporal extra√≠da y hacemos algunos cambios para poder visualizar la diferencia entre los datos originales y suavizados.\n# suavizamos la temperatura\rdf \u0026lt;- mutate(df, tmx_smoothed = daily_smooth(tmx)) %\u0026gt;% pivot_longer(2:3, names_to = \u0026quot;var\u0026quot;, values_to = \u0026quot;temp\u0026quot;)\r# visualizamos la diferencia ggplot(df, aes(date, temp, colour = var)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rscale_colour_manual(values = c(\u0026quot;#f4a582\u0026quot;, \u0026quot;#b2182b\u0026quot;)) +\rlabs(y = \u0026quot;Temperatura m√°xima\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rComo vemos en el gr√°fico la curva suavizada sigue muy bien la curva original. En el siguiente paso empleamos nuestra funci√≥n sobre el RasterBrick usando la funci√≥n calc(). La funci√≥n devuelve tantas capas como las que devuelve la funci√≥n empleada a cada de las series temporales.\n# suavizar el RasterBrick\rtmx_smooth \u0026lt;- calc(tmx_mean, fun = daily_smooth)\r\r\rVisualizaci√≥n\rPreparaci√≥n\rPara visualizar las temperaturas m√°ximas durante todo el a√±o, primero, convertimos el RasterBrick a un data.frame, incluyendo longitud y latitud, pero eliminando todas las series temporales sin valores (NA).\n# convertir a data.frame\rtmx_mat \u0026lt;- as.data.frame(tmx_smooth, xy = TRUE, na.rm = TRUE)\r# renombrar las columnas\rtmx_mat \u0026lt;- set_names(tmx_mat, c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, str_c(\u0026quot;D\u0026quot;, 1:366)))\rstr(tmx_mat[, 1:10])\r## \u0026#39;data.frame\u0026#39;: 20676 obs. of 10 variables:\r## $ lon: num -8.03 -7.98 -7.92 -7.86 -7.8 ...\r## $ lat: num 43.8 43.8 43.8 43.8 43.8 ...\r## $ D1 : num 10.5 10.3 10 10.9 11.5 ...\r## $ D2 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D3 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D4 : num 10.6 10.4 10.1 10.9 11.5 ...\r## $ D5 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D6 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D7 : num 10.6 10.4 10.2 11 11.6 ...\r## $ D8 : num 10.6 10.4 10.2 11 11.6 ...\rSegundo, importamos los l√≠mites administrativos con la funci√≥n ne_countries() del paquete rnaturalearth limitando la extensi√≥n a la regi√≥n de la Pen√≠nsula Ib√©rica, el sur de Francia y el norte de √Åfrica.\n# importamos los l√≠mites globales\rmap \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;) %\u0026gt;% st_cast(\u0026quot;MULTILINESTRING\u0026quot;)\r# limitamos la extensi√≥n\rmap \u0026lt;- st_crop(map, xmin = -10, xmax = 5, ymin = 35, ymax = 44) \r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r# mapa de los l√≠mites\rplot(map)\r## Warning: plotting the first 9 out of 94 attributes; use max.plot = 94 to plot\r## all\rTercero, creamos un vector con etiquetas del d√≠a del a√±o para incluirlas en la animaci√≥n. Adem√°s, definimos los cortes de la temperatura m√°xima, adaptados a la distribuci√≥n de nuestros datos, para obtener una categorizaci√≥n con un total de 20 clases.\nCuarto, aplicamos la funci√≥n cut() con los cortes a todas las columnas con las temperaturas de cada d√≠a del a√±o.\n# etiquetas de los d√≠as del a√±o\rlab \u0026lt;- as_date(0:365, \u0026quot;2000-01-01\u0026quot;) %\u0026gt;% format(\u0026quot;%d %B\u0026quot;)\r# cortes para la temperatura\rct \u0026lt;- c(-5, 0, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 40, 45)\r# datos categorizados con los cortes fijados\rtmx_mat_cat \u0026lt;- mutate_at(tmx_mat, 3:368, cut, breaks = ct)\rQuinto, descargamos la fuente Montserrat y definimos los colores correspondientes a las clases creadas.\n# descarga de la fuente\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# uso de showtext con DPI 300\rshowtext_opts(dpi = 300)\rshowtext_auto()\r# definimos una rampa de colores\rcol_spec \u0026lt;- colorRampPalette(rev(brewer.pal(11, \u0026quot;Spectral\u0026quot;)))\r\rMapa est√°tico\rEn esta primera visualizaci√≥n hacemos un mapa del 29 de mayo (d√≠a 150). No voy a explicar todos los detalles de la construcci√≥n con ggplot2, no obstante, es importante destacar que hago uso de la funci√≥n aes_string() en lugar de aes() para poder usar los nombres de las columnas en formato de car√°cter. Con la funci√≥n geom_raster() a√±adimos los datos en rejilla de temperatura como primera capa del gr√°fico y con geom_sf() los l√≠mites de clase sf. Por √∫ltimo, la funci√≥n guide_colorsteps() permite crear una bonita leyenda basada en las clases creadas por la funci√≥n cut().\nggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = \u0026quot;D150\u0026quot;)) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Promedio de la temperatura m√°xima durante el a√±o en Espa√±a\u0026quot;, subtitle = lab[150], caption = \u0026quot;Per√≠odo de referencia 1901-2014. Datos: STEAD\u0026quot;,\rfill = \u0026quot;¬∫C\u0026quot;)\r\rAnimaci√≥n de todo el a√±o\rLa animaci√≥n final consiste en crear un gif a partir de todas las im√°genes de los 366 d√≠as, en principio, se podr√≠a usar el paquete gganimate, pero en mi experiencia es m√°s lento, dado que requiere un data.frame en formato largo. En este ejemplo una tabla larga tendr√≠a m√°s de siete millones de filas, por eso, lo que hacemos es usar un bucle sobre las columnas y unir todas las im√°genes creadas con el paquete gifski que tambi√©n usa gganimate para la reproducci√≥n en formato gif.\nAntes del bucle creamos un vector con los pasos temporales o nombres de las columnas y otro vector con el nombre de las im√°genes, incluida el nombre de la carpeta. Con el objetivo de obtener una lista de im√°genes ordenadas por su n√∫mero debemos mantener tres cifras rellenando las posiciones a la izquierda con ceros.\ntime_step \u0026lt;- str_c(\u0026quot;D\u0026quot;, 1:366)\rfiles \u0026lt;- str_c(\u0026quot;./ta_anima/D\u0026quot;, str_pad(1:366, 3, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;), \u0026quot;.png\u0026quot;)\rPor √∫ltimo, incluimos la construcci√≥n anterior del gr√°fico en un bucle for.\nfor(i in 1:366){\rggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = time_step[i])) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Promedio de la temperatura m√°xima durante el a√±o en Espa√±a\u0026quot;, subtitle = lab[i], caption = \u0026quot;Per√≠odo de referencia 1901-2014. Datos: STEAD\u0026quot;,\rfill = \u0026quot;¬∫C\u0026quot;)\rggsave(files[i], width = 8.28, height = 7.33, type = \u0026quot;cairo\u0026quot;)\r}\rDespu√©s de haber creado im√°genes para cada d√≠a del a√±o, √∫nicamente nos queda por crear el gif.\ngifski(files, \u0026quot;tmx_spain.gif\u0026quot;, width = 800, height = 700, loop = FALSE, delay = 0.05)\r\n\r\r","date":1602374400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1602374400,"objectID":"86d926daca3de9f574ec3407407f693f","permalink":"https://dominicroye.github.io/es/2020/animacion-climatica-de-la-temperatura-maxima/","publishdate":"2020-10-11T00:00:00Z","relpermalink":"/es/2020/animacion-climatica-de-la-temperatura-maxima/","section":"post","summary":"En el campo de la visualizaci√≥n de datos, la animaci√≥n de datos espaciales en su dimensi√≥n temporal lleva a mostrar cambios y patrones fascinantes y muy visuales. A ra√≠z de una de las √∫ltimas publicaciones que he realizado en los RRSS me pidieron que hiciera un post acerca de c√≥mo lo cre√©. Pues bien, aqu√≠ vamos para empezar con datos de la Espa√±a peninsular.","tags":["animaci√≥n","temperatura","clima","SIG"],"title":"Animaci√≥n clim√°tica de la temperatura m√°xima","type":"post"},{"authors":null,"categories":["sig","R","R:avanzado"],"content":"\r\rRecientemente cre√© una visualizaci√≥n de la distribuci√≥n de las direcciones del flujo fluvial y tambi√©n de las orientaciones costeras. A ra√≠z de su publicaci√≥n en los RRSS (aqu√≠) me pidieron que hiciera un post acerca de c√≥mo lo hice. Pues bien, aqu√≠ vamos para empezar con un ejemplo de los r√≠os, la orientaci√≥n costera es algo m√°s compleja. Lo mismo hice para una selecci√≥n de r√≠os europeos aqu√≠ en este tweet. No obstante, originalmente empec√© con la orientaci√≥n de las costas europeas.\nHave you ever wondered where the European #coasts are oriented? #rstats #ggplot2 #geography #dataviz pic.twitter.com/tpWVxSoHlw\n\u0026mdash; Dr. Dominic Roy√© (@dr_xeo) May 26, 2020  Paquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rremotes\rInstalaci√≥n desde repositorios remotos\r\rqgisprocess\rInterfaz entre R y QGIS\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rggtext\rSoporte para la representaci√≥n de texto mejorado con ggplot2\r\rsysfonts\rCargar fuentes en R\r\rshowtext\rUsar fuentes m√°s f√°cilmente en gr√°ficos R\r\rcircular\rFunciones para trabajar con datos circulares\r\rgeosphere\rTrigonometr√≠a esf√©rica para aplicaciones geogr√°ficas\r\r\r\rEn el caso del paquete qgisprocess es necesario instalar QIGS \u0026gt;= 3.16 aqu√≠. M√°s adelante explicar√© la raz√≥n del uso de QGIS.\n# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;remotes\u0026quot;)) install.packages(\u0026quot;remotes\u0026quot;)\rif(!require(\u0026quot;qgisprocess\u0026quot;)) remotes::install_github(\u0026quot;paleolimbot/qgisprocess\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggtext\u0026quot;)) install.packages(\u0026quot;ggtext\u0026quot;)\rif(!require(\u0026quot;circular\u0026quot;)) install.packages(\u0026quot;circular\u0026quot;)\rif(!require(\u0026quot;geosphere\u0026quot;)) install.packages(\u0026quot;geosphere\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# paquetes\rlibrary(sf)\rlibrary(tidyverse)\rlibrary(ggtext)\rlibrary(circular)\rlibrary(geosphere)\rlibrary(qgisprocess)\rlibrary(showtext)\rlibrary(sysfonts)\r\rConsideraciones iniciales\rLos √°ngulos en l√≠neas vectoriales se basan en el √°ngulo entre dos v√©rtices, y el n√∫mero de v√©rtices depende de la complejidad, y en consecuencia de la resoluci√≥n, de los datos vectoriales. Por tanto, puede haber diferencias en usar distintas resoluciones de una l√≠nea vectorial, sea de la costa o del r√≠o como en este ejemplo. Una l√≠nea recta simplemente se construye con dos puntos de longitud y latitud.\nRelacionado con ello est√° la fractalidad, una estructura aparentemente irregular pero que se repite a diferentes escalas, de la l√≠nea de costa o tambi√©n del r√≠o. La caracter√≠stica m√°s parad√≥jica es que la longitud de una l√≠nea costera depende de la escala de medida, cuanto menor es el incremento de medida, la longitud medida se incrementa.\nExisten dos posibiliades de obtener los √°ngulos de los v√©rtices. En la primera calculamos el √°ngulo entre todos los v√©rtices consecutivos.\nPor ejemplo, imagin√©monos dos puntos, Madrid (-3.71, 40.43) y Barcelona (2.14, 41.4).\n¬øCu√°l es el √°ngulo de su l√≠nea recta?\nbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4))\r## [1] 77.62391\rVemos que es el de 77¬∫, o sea, direcci√≥n noreste. Pero, ¬øy si voy de Barcelona a Madrid?\nbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43))\r## [1] 257.6239\rEl ang√∫lo es diferente porque nos movemos desde el noreste al suroeste. Podemos invertir f√°cilmente el √°ngulo para obtener el movimiento contrario.\n# √°ngulo contrario de Barcelona -\u0026gt; Madrid\rbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43)) - 180\r## [1] 77.62391\r# √°ngulo contrario de Madrid -\u0026gt; Barcelona\rbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4)) + 180\r## [1] 257.6239\rLa direcci√≥n en la que calculamos los √°ngulos es importante. En el caso de los r√≠os se espera que sea la direcci√≥n de flujo de origen a la desembocadura, ahora bien, un problema puede ser que los v√©rtices, que construyen las l√≠neas, no est√©n ordenados geogr√°ficamente en la tabla de atributos. Otro problema puede ser que los v√©rtices empiecen en la desembocadura lo que dar√≠a al ang√∫lo inverso como lo hemos visto antes.\nSin embargo, hay una forma m√°s f√°cil. Podemos aprovechar los atributos de los sistemas de coordenadas proyectados (proyecci√≥n Robinson, etc) que incluyen el √°ngulo entre los v√©rtices. Este √∫ltimo enfoque lo vamos usar en este post. A√∫n as√≠, debemos prestar mucha atenci√≥n a los resultados seg√∫n lo dicho anteriormente.\n\rPreparaci√≥n\rDatos\rDescargamos las l√≠neas centrales de los r√≠os m√°s grandes del mundo (descarga), accesible tambi√©n en Zeenatul Basher et al.¬†2018.\n\rImportar y proyectar\rLo primero que hacemos es importar, proyectar y eliminar la tercera dimensi√≥n Z, usando el encadenamiento de las siguientes functions: st_read() nos ayuda a importar cualquier formato vectorial, st_zm() elimina la dimensi√≥n Z o M de una geometr√≠a vectorial y st_transform() proyecta los datos vectoriales a la nueva proyecci√≥n en formato proj4. La combinaci√≥n de las funciones la realizamos con el famoso pipe (%\u0026gt;%) que facilita la aplicaci√≥n de una secuencia de funciones sobre un conjunto de datos, m√°s detalles en este post. Todas las funciones del paquete sf comienzan por st_* haciendo referencia al car√°cter espacial de su aplicaci√≥n, similar a PostGIS. Igualmente, y al mismo estilo que PostGIS, se usan verbos como nombres de funci√≥n.\nproj_rob \u0026lt;- \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m no_defs\u0026quot;\rriver_line \u0026lt;- st_read(\u0026quot;RiverHRCenterlinesCombo.shp\u0026quot;) %\u0026gt;% st_zm() %\u0026gt;% st_transform(proj_rob)\r## Reading layer `RiverHRCenterlinesCombo\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\es\\post\\2020-07-24-direcciones-del-flujo-fluvial\\RiverHRCenterlinesCombo.shp\u0026#39; ## using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 78 features and 6 fields\r## Geometry type: MULTILINESTRING\r## Dimension: XYZ\r## Bounding box: xmin: -164.7059 ymin: -36.97094 xmax: 151.5931 ymax: 72.64474\r## z_range: zmin: 0 zmax: 0\r## Geodetic CRS: WGS 84\r\rExtraer los √°ngulos\rEn el siguiente paso debemos extraer los √°ngulos de los v√©rtices. Desgraciadamente, hasta donde sepa, no es posible extraer los atributos con alguna funci√≥n del paquete sf. Aunque la funci√≥n st_coordinates() nos devuelve las coordenadas, no incluye otros atributos. Por eso, debemos usar otra forma, y es que el open software Quantum GIS extrae todos los atributos de los v√©rtices. Podr√≠amos importar los datos vectoriales en QGIS Desktop y exportar los v√©rtices desde all√≠, pero tambi√©n es posible acceder a las funciones de QGIS desde R directamente.\nPara ello, tenemos que tener instalado QGIS en OSGeo4W. El paquete qgisprocess nos permite de forma muy f√°cil usar las funciones del programa en R. Primero empleamos la funci√≥n qgis_configure() para definir todas las rutas necesarias de QGIS.\n# rutas a QGIS\rqgis_configure()\r## getOption(\u0026#39;qgisprocess.path\u0026#39;) was not found.\r## Sys.getenv(\u0026#39;R_QGISPROCESS_PATH\u0026#39;) was not found.\r## Trying \u0026#39;qgis_process\u0026#39; on PATH\r## Error in processx::run(\u0026quot;cmd.exe\u0026quot;, c(\u0026quot;/c\u0026quot;, \u0026quot;call\u0026quot;, path, args), ...): System command \u0026#39;cmd.exe\u0026#39; failed, exit status: 1, stderr:\r## E\u0026gt; \u0026quot;qgis_process\u0026quot; no se reconoce como un comando interno o externo,\r## E\u0026gt; programa o archivo por lotes ejecutable.\r## Found 1 QGIS installation containing \u0026#39;qgis_process\u0026#39;:\r## C:/Program Files/QGIS 3.18/bin/qgis_process-qgis.bat\r## Trying command \u0026#39;C:/Program Files/QGIS 3.18/bin/qgis_process-qgis.bat\u0026#39;\r## Success!\r## QGIS version: 3.18.1-Z√ºrich\r## Metadata of 986 algorithms queried and stored in cache.\r## Run `qgis_algorithms()` to see them.\rLa funci√≥n qgis_algorithms() nos ayuda a buscar diferentes herramientas de QGIS. Adem√°s la funci√≥n qgis_show_help() especifica la forma de uso con todos los argumentos requeridos.\n# buscar herramientas\rqgis_algorithms()\r## # A tibble: 986 x 5\r## provider provider_title algorithm algorithm_id algorithm_title\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 3d QGIS (3D) 3d:tessellate tessellate Tessellate ## 2 gdal GDAL gdal:aspect aspect Aspect ## 3 gdal GDAL gdal:assignprojection assignproje~ Assign project~\r## 4 gdal GDAL gdal:buffervectors buffervecto~ Buffer vectors ## 5 gdal GDAL gdal:buildvirtualraster buildvirtua~ Build virtual ~\r## 6 gdal GDAL gdal:buildvirtualvector buildvirtua~ Build virtual ~\r## 7 gdal GDAL gdal:cliprasterbyextent cliprasterb~ Clip raster by~\r## 8 gdal GDAL gdal:cliprasterbymaskla~ cliprasterb~ Clip raster by~\r## 9 gdal GDAL gdal:clipvectorbyextent clipvectorb~ Clip vector by~\r## 10 gdal GDAL gdal:clipvectorbypolygon clipvectorb~ Clip vector by~\r## # ... with 976 more rows\r# uso de la herramienta\rqgis_show_help(\u0026quot;native:extractvertices\u0026quot;)\r## Extract vertices (native:extractvertices)\r## ## ----------------\r## Description\r## ----------------\r## This algorithm takes a line or polygon layer and generates a point layer with points representing the vertices in the input lines or polygons. The attributes associated to each point are the same ones associated to the line or polygon that the point belongs to.\r## ## Additional fields are added to the point indicating the vertex index (beginning at 0), the vertex‚Äôs part and its index within the part (as well as its ring for polygons), distance along original geometry and bisector angle of vertex for original geometry.\r## ## ----------------\r## Arguments\r## ----------------\r## ## INPUT: Input layer\r## Argument type: source\r## Acceptable values:\r## - Path to a vector layer\r## OUTPUT: Vertices\r## Argument type: sink\r## Acceptable values:\r## - Path for new vector layer\r## ## ----------------\r## Outputs\r## ----------------\r## ## OUTPUT: \u0026lt;outputVector\u0026gt;\r## Vertices\rEn nuestro caso la herramienta para extraer los v√©rtices es simple y s√≥lo lleva una entrada y una salida. La funci√≥n qgis_run_algorithm()() ejecuta una herramienta de QGIS indicando el algoritmo y sus argumentos. La ventaja de usar el algoritmo directamente desde R es que podemos pasar objetos de clase sf (o sp) y raster que tenemos importados o creados en R. Como salida creamos un geojson, tambi√©n podr√≠a ser de otro formato vectorial, y lo guardamos en una carpeta temporal. Para obtener el resultado de QGIS s√≥lo necesitamos emplear la funci√≥n qgis_output().\nriver_vertices \u0026lt;- qgis_run_algorithm(alg = \u0026quot;native:extractvertices\u0026quot;,\rINPUT = river_line,\rOUTPUT = file.path(tempdir(), \u0026quot;rivers_world_vertices.geojson\u0026quot;))\r## Running cmd.exe /c call \\\r## \u0026quot;C:/Program Files/QGIS 3.18/bin/qgis_process-qgis.bat\u0026quot; run \\\r## \u0026quot;native:extractvertices\u0026quot; \\\r## \u0026quot;--INPUT=C:\\Users\\xeo19\\AppData\\Local\\Temp\\RtmpIns79r\\file67a81d8f1f66\\file67a840795857.gpkg\u0026quot; \\\r## \u0026quot;--OUTPUT=C:\\Users\\xeo19\\AppData\\Local\\Temp\\RtmpIns79r/rivers_world_vertices.geojson\u0026quot;\r## ## ----------------\r## Inputs\r## ----------------\r## ## INPUT: C:\\Users\\xeo19\\AppData\\Local\\Temp\\RtmpIns79r\\file67a81d8f1f66\\file67a840795857.gpkg\r## OUTPUT: C:\\Users\\xeo19\\AppData\\Local\\Temp\\RtmpIns79r/rivers_world_vertices.geojson\r## ## ## 0...10...20...30...40...50...60...70...80...90...\r## ----------------\r## Results\r## ----------------\r## ## OUTPUT: C:\\Users\\xeo19\\AppData\\Local\\Temp\\RtmpIns79r/rivers_world_vertices.geojson\rriver_vertices \u0026lt;- st_read(qgis_output(river_vertices, \u0026quot;OUTPUT\u0026quot;))\r## Reading layer `rivers_world_vertices\u0026#39; from data source ## `C:\\Users\\xeo19\\AppData\\Local\\Temp\\RtmpIns79r\\rivers_world_vertices.geojson\u0026#39; ## using driver `GeoJSON\u0026#39;\r## Simple feature collection with 339734 features and 12 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -12117400 ymin: -3953778 xmax: 13751910 ymax: 7507359\r## Geodetic CRS: WGS 84\r Actualmente en Windows parece haber problemas con la librer√≠a de proj. En principio si termina creando el objeto river_vertices no debes preocuparte.   \rSelecci√≥n\rAntes de seguir con la estimaci√≥n de la distribuci√≥n de los √°ngulos, filtramos algunos r√≠os de inter√©s. Las funciones de la colecci√≥n tidyverse son compatibles con el paquete sf. En el √∫ltimo post hice una introducci√≥n a tidyverse aqu√≠.\nriver_vertices \u0026lt;- filter(river_vertices, NAME %in% c(\u0026quot;Mississippi\u0026quot;, \u0026quot;Colorado\u0026quot;, \u0026quot;Amazon\u0026quot;, \u0026quot;Nile\u0026quot;, \u0026quot;Orange\u0026quot;, \u0026quot;Ganges\u0026quot;, \u0026quot;Yangtze\u0026quot;, \u0026quot;Danube\u0026quot;,\r\u0026quot;Mackenzie\u0026quot;, \u0026quot;Lena\u0026quot;, \u0026quot;Murray\u0026quot;, \u0026quot;Niger\u0026quot;)\r) river_vertices \r## Simple feature collection with 94702 features and 12 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -10377520 ymin: -3953778 xmax: 13124340 ymax: 7507359\r## Geodetic CRS: WGS 84\r## First 10 features:\r## fid NAME SYSTEM name_alt scalerank rivernum Length_km vertex_index\r## 1 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 0\r## 2 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 1\r## 3 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 2\r## 4 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 3\r## 5 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 4\r## 6 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 5\r## 7 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 6\r## 8 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 7\r## 9 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 8\r## 10 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 9\r## vertex_part vertex_part_index distance angle geometry\r## 1 0 0 0.000 31.096005 POINT (3037149 1672482)\r## 2 0 1 1208.130 22.456672 POINT (3037772 1673517)\r## 3 0 2 2324.160 8.602259 POINT (3038039 1674600)\r## 4 0 3 3656.452 8.573580 POINT (3038118 1675930)\r## 5 0 4 5735.538 24.406889 POINT (3038612 1677950)\r## 6 0 5 6758.322 25.134763 POINT (3039200 1678787)\r## 7 0 6 10432.834 6.998982 POINT (3040164 1682333)\r## 8 0 7 14865.136 4.239641 POINT (3040070 1686764)\r## 9 0 8 16563.207 358.730530 POINT (3040356 1688438)\r## 10 0 9 18376.526 347.480822 POINT (3039972 1690210)\r\r\rEstimar la distribuci√≥n\rPara visualizar la distribuci√≥n podemos usar, o bien un histograma o un gr√°fico de densidad. Pero en el caso de estimar la funci√≥n de densidad de probabilidad nos encontramos con un problema matem√°tico a la hora de aplicarlo a datos circulares. No debemos usar la funci√≥n estandar de R density() dado que en nuestros datos una direcci√≥n de 360¬∫ es la misma a 0¬∫, lo que provocar√≠a errores en este rango de valores. Es un problema general para diferentes m√©tricas estad√≠sticas. M√°s detalles estad√≠sticos se explican en el paquete circular. Este paquete permite definir las caracter√≠sticas de los datos circulares (unidad, tipo de datos, rotaci√≥n, etc.) como una clase de objeto en R.\nPor tanto, lo que hacemos es construir una funci√≥n que estime la densidad y devuelva una tabla con los √°ngulos (x) y las estimaciones de densidad (y). Dado que los r√≠os tienen diferentes longitudes, y queremos ver diferencias independientemente de ello, normalizamos las estimaciones usando el valor m√°ximo. A diferencia de la funci√≥n density(), en la que el ancho de banda de suavizado bw es optimizado, aqu√≠ es requerido indicarlo. Es similar a definir el ancho de barra en un histograma. Existe una funci√≥n de optimizaci√≥n para la banda, bw.nrd.circular() que se podr√≠a emplear aqu√≠.\ndens_circ \u0026lt;- function(x){\rdens \u0026lt;- density.circular(circular(x$angle, units = \u0026quot;degrees\u0026quot;),\rbw = 70, kernel = \u0026quot;vonmises\u0026quot;,\rcontrol.circular = list(units = \u0026quot;degrees\u0026quot;))\rdf \u0026lt;- data.frame(x = dens$x, y = dens$y/max(dens$y))\rreturn(df)\r}\rPara finalizar, estimamos la densidad de cada r√≠o de nuestra selecci√≥n. Empleamos la funci√≥n split() de R Base para obtener una tabla de cada r√≠o en una lista. Despu√©s aplicamos con la funci√≥n map_df() del paquete purrr nuestra funci√≥n de estimaci√≥n de densidad a la lista. El sufijo _df permite que obtengamos una tabla unida, en lugar de una lista con los resultados de cada r√≠o. No obstante, es necesario indicar el nombre de la columna con el argumento .id, la que contendr√° el nombre de cada r√≠o. En caso contrario no sabr√≠amos diferenciar los resultados. Tambi√©n aqu√≠ recomiendo leer m√°s detalles en el √∫ltimo post sobre tidyverse aqu√≠.\ndens_river \u0026lt;- split(river_vertices, river_vertices$NAME) %\u0026gt;% map_df(dens_circ, .id = \u0026quot;river\u0026quot;)\r# resultado\rhead(dens_river)\r## river x y\r## 1 Amazon 0.000000 0.2399907\r## 2 Amazon 0.704501 0.2492548\r## 3 Amazon 1.409002 0.2585758\r## 4 Amazon 2.113503 0.2679779\r## 5 Amazon 2.818004 0.2774859\r## 6 Amazon 3.522505 0.2871232\r\rVisualizaci√≥n\rAhora ya s√≥lo nos queda la visualizaci√≥n mediante el famoso paquete ggplot. Primero a√±adimos una nueva fuente Montserrat para usarla en este gr√°fico.\n# descarga de fuente\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# usar showtext para fuentes\rshowtext_opts(dpi = 200)\rshowtext_auto() \rEn el siguiente paso creamos dos objetos con el t√≠tulo y con una nota de pie. En el t√≠tulo estamos usando un c√≥digo html para dar color a una parte de texto en sustituci√≥n de una leyenda. Se puede usar html de forma muy f√°cil con el paquete ggtext.\n# t√≠tulo con html title \u0026lt;- \u0026quot;Relative distribution of river \u0026lt;span style=\u0026#39;color:#011FFD;\u0026#39;\u0026gt;\u0026lt;strong\u0026gt;flow direction\u0026lt;/strong\u0026gt;\u0026lt;/span\u0026gt; in the world\u0026quot;\rcaption \u0026lt;- \u0026quot;Based on data from Zeenatul Basher, 20180215\u0026quot;\rLa cuadr√≠cula de fondo que crea ggplot por defecto para coordenadas polares no me convenci√≥, por eso creamos una tabla con las l√≠neas de fondo del eje x.\ngrid_x \u0026lt;- tibble(x = seq(0, 360 - 22.5, by = 22.5), y = rep(0, 16), xend = seq(0, 360 - 22.5, by = 22.5), yend = rep(Inf, 16))\rA continuaci√≥n definimos todos los estilos del gr√°fico. Lo m√°s importante en este paso es la funci√≥n element_textbox() del paquete ggtext para poder interpretar nuestro c√≥digo html incorporado al t√≠tulo.\ntheme_polar \u0026lt;- function(){ theme_minimal() %+replace%\rtheme(axis.title.y = element_blank(),\raxis.text.y = element_blank(),\rlegend.title = element_blank(),\rplot.title = element_textbox(family = \u0026quot;Montserrat\u0026quot;, hjust = 0.5, colour = \u0026quot;white\u0026quot;, size = 15),\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\raxis.text.x = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\rstrip.text = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rpanel.grid = element_blank()\r)\r}\rPara terminar construimos el gr√°fico: 1) Usamos la funci√≥n geom_hline() con diferentes puntos de intersecci√≥n en y para crear la cuadr√≠cula de fondo. La funci√≥n geom_segment() crea la cuadr√≠cula en x. 2) El √°rea de densidad la creamos usando la funci√≥n geom_area(). 3) En scale_x_continous() definimos un l√≠mite inferior\rnegativo para que no colapse en un punto peque√±o. Las etiquetas de las ocho direcciones principales las indicamos en la funci√≥n scale_y_continous(), y 4) Por √∫ltimo, cambiamos a un sistema de coordenadas polar y fijamos la variable para crear facetas.\nggplot() +\rgeom_hline(yintercept = c(0, .2, .4, .6, .8, 1), colour = \u0026quot;white\u0026quot;) +\rgeom_segment(data = grid_x , aes(x = x, y = y, xend = xend, yend = yend), linetype = \u0026quot;dashed\u0026quot;, col = \u0026quot;white\u0026quot;) +\rgeom_area(data = dens_river, aes(x = x, y = y, ymin = 0, ymax = y), alpha = .7, colour = NA, show.legend = FALSE,\rfill = \u0026quot;#011FFD\u0026quot;) + scale_y_continuous(limits = c(-.2, 1), expand = c(0, 0)) +\rscale_x_continuous(limits = c(0, 360), breaks = seq(0, 360 - 22.5, by = 22.5),\rminor_breaks = NULL,\rlabels = c(\u0026quot;N\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NE\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;E\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SE\u0026quot;, \u0026quot;\u0026quot;,\r\u0026quot;S\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SW\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NW\u0026quot;, \u0026quot;\u0026quot;)) +\rcoord_polar() + facet_wrap(river ~ ., ncol = 4) +\rlabs(title = title, caption = caption, x = \u0026quot;\u0026quot;) +\rtheme_polar()\r## Warning: Ignoring unknown aesthetics: ymin, ymax\r\n\r","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1595548800,"objectID":"069724862a0881269f82d0ff665195ba","permalink":"https://dominicroye.github.io/es/2020/direcciones-del-flujo-fluvial/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/es/2020/direcciones-del-flujo-fluvial/","section":"post","summary":"Recientemente cre√© una visualizaci√≥n de la distribuci√≥n de las direcciones del flujo fluvial y  tambi√©n de las orientaciones costeras. A ra√≠z de su publicaci√≥n en los RRSS me pidieron que hiciera un post acerca de c√≥mo lo hice. Pues bien, aqu√≠ vamos para empezar con un ejemplo de los r√≠os, la orientaci√≥n costera es algo m√°s compleja.","tags":["direcciones","rios","fluvial","orientaci√≥n","distribuci√≥n"],"title":"Direcciones del flujo fluvial","type":"post"},{"authors":null,"categories":["tidyverse","R","R:principante"],"content":"\r\r\r1 Tidyverse\r2 Gu√≠a de estilo\r3 Pipe %\u0026gt;%\r4 Paquetes de Tidyverse\r\r4.1 Lectura y escritura\r4.2 Manipulaci√≥n de caracteres\r4.3 Manejo de fechas y horas\r4.4 Manipulaci√≥n de tablas y vectores\r\r4.4.1 Selecionar y renombrar\r4.4.2 Filtrar y ordenar\r4.4.3 Agrupar y resumir\r4.4.4 Unir tablas\r4.4.5 Tablas largas y anchas\r\r4.5 Visualizar datos\r\r4.5.1 Gr√°fico de linea y puntos\r4.5.2 Boxplot\r4.5.3 Heatmap\r\r4.6 Aplicar funciones sobre vectores o listas\r\r\r\r1 Tidyverse\rEl universo de los paquetes de tidyverse, una colecci√≥n de paquetes de funciones para un uso especialmente enfocado en la ciencia de datos, abri√≥ un antes y despu√©s en la programaci√≥n de R. En este post voy a resumir muy brevemente lo m√°s esencial para inicarse en este mundo. La gram√°tica sigue en todas las funciones una estructura com√∫n. Lo m√°s esencial es que el primer argumento es el objeto y a continuaci√≥n viene el resto de argumentos. Adem√°s, se proporciona un conjunto de verbos que facilitan el uso de las funciones. En la actualidad, la filosof√≠a de las funciones tambi√©n se refleja en otros paquetes que hacen compatible su uso con la colecci√≥n de tidyverse. Por ejemplo, el paquete sf (simple feature) para el tratamiento de datos vectoriales, permite el uso de m√∫ltiples funciones que encontramos en el paquete dplyr.\nEl n√∫cleo de la colecci√≥n lo constituyen los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rggplot2\rGram√°tica para la creaci√≥n de gr√°ficos\r\rpurrr\rProgramaci√≥n funcional de R\r\rtibble\rSistema moderno y efectivo de tablas\r\rdplyr\rGramatica para la manipulaci√≥n de datos\r\rtidyr\rConjunto de funciones para ordenar datos\r\rstringr\rConjunto de funciones para trabajar con caracteres\r\rreadr\rUna forma f√°cil y r√°pida para importar datos\r\rforcats\rHerramientas y funciones para trabajar f√°cilmente con factores\r\r\r\rAdem√°s de los paquetes menciondos, tambi√©n se usa muy frecuentemente lubridate para trabajar con fechas y horas, y tambi√©n readxl que nos permite importar archivos en formato Excel. Para conocer todos los paquetes disponibles podemos emplear la funci√≥n tidyverse_packages().\n## [1] \u0026quot;broom\u0026quot; \u0026quot;cli\u0026quot; \u0026quot;crayon\u0026quot; \u0026quot;dbplyr\u0026quot; ## [5] \u0026quot;dplyr\u0026quot; \u0026quot;dtplyr\u0026quot; \u0026quot;forcats\u0026quot; \u0026quot;googledrive\u0026quot; ## [9] \u0026quot;googlesheets4\u0026quot; \u0026quot;ggplot2\u0026quot; \u0026quot;haven\u0026quot; \u0026quot;hms\u0026quot; ## [13] \u0026quot;httr\u0026quot; \u0026quot;jsonlite\u0026quot; \u0026quot;lubridate\u0026quot; \u0026quot;magrittr\u0026quot; ## [17] \u0026quot;modelr\u0026quot; \u0026quot;pillar\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;readr\u0026quot; ## [21] \u0026quot;readxl\u0026quot; \u0026quot;reprex\u0026quot; \u0026quot;rlang\u0026quot; \u0026quot;rstudioapi\u0026quot; ## [25] \u0026quot;rvest\u0026quot; \u0026quot;stringr\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;tidyr\u0026quot; ## [29] \u0026quot;xml2\u0026quot; \u0026quot;tidyverse\u0026quot;\rEs muy f√°cil encontrarnos con conflicos de funciones, o sea, que el mismo nombre de funci√≥n exista en varios paquetes. Para evitarlo, podemos escribir el nombre del paquete delante de la funci√≥n que queremos usar, separados por el s√≠mbolo de dos puntos escrito dos veces (package_name::function_name).\nAntes de empezar con los paquetes, espero que sea verdaderamente una breve introducci√≥n, algunos comentarios sobre el estilo al programar en R.\n\r2 Gu√≠a de estilo\rEn R no existe una gu√≠a de estilo universal, o sea, en la sintaxis de R no es necesario seguir normas concretas para nuestros scripts. Es recomendable trabajar de forma homog√©nea y clara a la hora de escribir con un estilo uniforme y legible. La colecci√≥n de tidyverse tiene una guia propia (https://style.tidyverse.org/).\nLas recomendaciones m√°s importes son:\n\rEvitar usar m√°s de 80 caracteres por l√≠nea para permitir leer el c√≥digo completo.\rUsar siempre un espacio despu√©s de una coma, nunca antes.\rLos operadores (==, +, -, \u0026lt;-, %\u0026gt;%, etc.) deben tener un espacio antes y despu√©s.\rNo hay espacio entre el nombre de una funci√≥n y el primer par√©ntesis, ni entre el √∫ltimo arguemento y el par√©ntesis final de una funci√≥n.\rEvitar reutilizar nombres de funciones y variables comunes (c \u0026lt;- 5 vs.¬†c())\rOrdenar el script separando las partes con la forma de comentario # Importar datos -----\rSe deben evitar tildes o s√≠mbolos especiales en nombres, archivos, rutas, etc.\rNombres de los objetos deben seguir una estructura constante: day_one, day_1.\r\rEs aconsejable usar una correcta indentaci√≥n para multiples argumentos de una funci√≥n o funciones encadenadas por el operador pipe (%\u0026gt;%).\n\r3 Pipe %\u0026gt;%\rPara facilitar el trabajo en la gesti√≥n, manipulaci√≥n y visualizaci√≥n de datos, el paquete magrittr introduce el operador llamado pipe en la forma %\u0026gt;% con el objetivo de combinar varias funciones sin la necesidad de asignar el resultado a un nuevo objeto. El operador pipe pasa a la salida de una funci√≥n aplicada al primer argumento de la siguiente funci√≥n. Esta forma de combinar funciones permite encadenar varios pasos de forma simult√°nea. En el siguiente ejemplo, muy sencillo, pasamos el vector 1:5 a la funci√≥n mean() para calcular el promedio.\n1:5 %\u0026gt;% mean()\r## [1] 3\r\r4 Paquetes de Tidyverse\r4.1 Lectura y escritura\rEl paquete readr facilita la lectura o escritura de m√∫ltiples formatos de archivo usando funciones que comienzan por read_* o write_*.\rEn comparaci√≥n con R Base las funciones son m√°s r√°pidas, ayudan a limpiar los nombres de las columnas y las fechas son convertidas autom√°ticamente. Las tablas importadas son de clase tibble (tbl_df), una versi√≥n moderna de data.frame del paquete tibble. En el mismo sentido se puede usar la funci√≥n read_excel() del paquete readxl para importar datos de hojas de Excel (m√°s detalles tambi√©n en esta entrada de mi blog). En el siguiente ejemplo importamos los datos de la movilidad registrada por Google (enlace) durante los √∫ltimos meses a causa de la pandemia COVID-19 (descarga).\n\r\rFunci√≥n lectura\rDescripci√≥n\r\r\r\rread_csv() o read_csv2()\rcoma o punto-coma (CSV)\r\rread_delim()\rseparador general\r\rread_table()\respacio blanco\r\r\r\r# cargar el paquete\rlibrary(tidyverse)\rgoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Rows: 516697 Columns: 13\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (6): country_region_code, country_region, sub_region_1, sub_region_2, i...\r## dbl (6): retail_and_recreation_percent_change_from_baseline, grocery_and_ph...\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rgoogle_mobility\r## # A tibble: 516,697 x 13\r## country_region_code country_region sub_region_1 sub_region_2 iso_3166_2_code\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 2 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 3 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 4 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 5 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 9 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 10 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # ... with 516,687 more rows, and 8 more variables: census_fips_code \u0026lt;chr\u0026gt;,\r## # date \u0026lt;date\u0026gt;, retail_and_recreation_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # grocery_and_pharmacy_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # parks_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # transit_stations_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # workplaces_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # residential_percent_change_from_baseline \u0026lt;dbl\u0026gt;\rDebemos prestar atenci√≥n a los nombres de los argumentos, ya que cambian en las funciones de readr. Por ejemplo, el argumento conocido header = TRUE de read.csv() es en este caso col_names = TRUE. Podemos encontrar m√°s detalles en el Cheat-Sheet de readr .\n\r4.2 Manipulaci√≥n de caracteres\rCuando se requiere manipular cadenas de texto usamos el paquete stringr, cuyas funciones siempre empiezan por str_* seguidas por un verbo y el primer argumento.\nAlgunas de estas funciones son las siguientes:\n\r\rFunci√≥n\rDescripci√≥n\r\r\r\rstr_replace()\rreemplazar patrones\r\rstr_c()\rcombinar characteres\r\rstr_detect()\rdetectar patrones\r\rstr_extract()\rextraer patrones\r\rstr_sub()\rextraer por posici√≥n\r\rstr_length()\rlongitud de la cadena de caracteres\r\r\r\rSe suelen usar expresiones regulares para patrones de caracteres. Por ejemplo, la expresi√≥n regular [aeiou] coincide con cualquier caracter √∫nico que sea una vocal. El uso de corchetes [] corresponde a clases de caracteres. Por ejemplo, [abc] corresponde a cada letra independientemente de la posici√≥n. [a-z] o [A-Z] o [0-9] cada uno entre a y z √≥ 0 y 9. Y por √∫ltimo, [:punct:] puntuaci√≥n, etc. Con llaves ‚Äú{}‚Äù podemos indicar el n√∫mero del elemento anterior {2} ser√≠a dos veces, {1,2} entre una y dos, etc. Adem√°s con $o ^ podemos indicar si el patr√≥n empieza al principio o termina al final. Podemos encontrar m√°s detalles y patrones en el Cheat-Sheet de stringr.\n# reemplazamos \u0026#39;er\u0026#39; al final por vac√≠o\rstr_replace(month.name, \u0026quot;er$\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;March\u0026quot; \u0026quot;April\u0026quot; \u0026quot;May\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;Septemb\u0026quot; \u0026quot;Octob\u0026quot; \u0026quot;Novemb\u0026quot; \u0026quot;Decemb\u0026quot;\rstr_replace(month.name, \u0026quot;^Ma\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;rch\u0026quot; \u0026quot;April\u0026quot; \u0026quot;y\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;September\u0026quot; \u0026quot;October\u0026quot; \u0026quot;November\u0026quot; \u0026quot;December\u0026quot;\r# combinar caracteres\ra \u0026lt;- str_c(month.name, 1:12, sep = \u0026quot;_\u0026quot;)\ra\r## [1] \u0026quot;January_1\u0026quot; \u0026quot;February_2\u0026quot; \u0026quot;March_3\u0026quot; \u0026quot;April_4\u0026quot; \u0026quot;May_5\u0026quot; ## [6] \u0026quot;June_6\u0026quot; \u0026quot;July_7\u0026quot; \u0026quot;August_8\u0026quot; \u0026quot;September_9\u0026quot; \u0026quot;October_10\u0026quot; ## [11] \u0026quot;November_11\u0026quot; \u0026quot;December_12\u0026quot;\r# colapsar combinaci√≥n\rstr_c(month.name, collapse = \u0026quot;, \u0026quot;)\r## [1] \u0026quot;January, February, March, April, May, June, July, August, September, October, November, December\u0026quot;\r# dedectamos patrones\rstr_detect(a, \u0026quot;_[1-5]{1}\u0026quot;)\r## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE\r# extraemos patrones\rstr_extract(a, \u0026quot;_[1-9]{1,2}\u0026quot;)\r## [1] \u0026quot;_1\u0026quot; \u0026quot;_2\u0026quot; \u0026quot;_3\u0026quot; \u0026quot;_4\u0026quot; \u0026quot;_5\u0026quot; \u0026quot;_6\u0026quot; \u0026quot;_7\u0026quot; \u0026quot;_8\u0026quot; \u0026quot;_9\u0026quot; \u0026quot;_1\u0026quot; \u0026quot;_11\u0026quot; \u0026quot;_12\u0026quot;\r# extraermos los caracteres en las posiciones entre 1 y 2\rstr_sub(month.name, 1, 2)\r## [1] \u0026quot;Ja\u0026quot; \u0026quot;Fe\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ap\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Au\u0026quot; \u0026quot;Se\u0026quot; \u0026quot;Oc\u0026quot; \u0026quot;No\u0026quot; \u0026quot;De\u0026quot;\r# longitud de cada mes\rstr_length(month.name)\r## [1] 7 8 5 5 3 4 4 6 9 7 8 8\r# con pipe, el \u0026#39;.\u0026#39; representa al objeto que pasa el operador %\u0026gt;%\rstr_length(month.name) %\u0026gt;% str_c(month.name, ., sep = \u0026quot;.\u0026quot;)\r## [1] \u0026quot;January.7\u0026quot; \u0026quot;February.8\u0026quot; \u0026quot;March.5\u0026quot; \u0026quot;April.5\u0026quot; \u0026quot;May.3\u0026quot; ## [6] \u0026quot;June.4\u0026quot; \u0026quot;July.4\u0026quot; \u0026quot;August.6\u0026quot; \u0026quot;September.9\u0026quot; \u0026quot;October.7\u0026quot; ## [11] \u0026quot;November.8\u0026quot; \u0026quot;December.8\u0026quot;\rUna funci√≥n muy √∫til es str_glue() para interpolar caracteres.\nname \u0026lt;- c(\u0026quot;Juan\u0026quot;, \u0026quot;Michael\u0026quot;)\rage \u0026lt;- c(50, 80) date_today \u0026lt;- Sys.Date()\rstr_glue(\r\u0026quot;My name is {name}, \u0026quot;,\r\u0026quot;I\u0026#39;am {age}, \u0026quot;,\r\u0026quot;and my birth year is {format(date_today-age*365, \u0026#39;%Y\u0026#39;)}.\u0026quot;\r)\r## My name is Juan, I\u0026#39;am 50, and my birth year is 1972.\r## My name is Michael, I\u0026#39;am 80, and my birth year is 1942.\r\r4.3 Manejo de fechas y horas\rEl paquete lubridate ayuda en el manejo de fechas y horas. Nos permite crear los objetos reconocidos por R con funciones (como ymd() √≥ ymd_hms()) y hacer c√°lculos.\nDebemos conocer las siguientes abreviaturas:\n\rymd: representa y:year, m:month, d:day\rhms: representa h:hour, m:minutes, s:seconds\r\r# paquete\rlibrary(lubridate)\r## ## Attaching package: \u0026#39;lubridate\u0026#39;\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## date, intersect, setdiff, union\r# vector de fechas\rdat \u0026lt;- c(\u0026quot;1999/12/31\u0026quot;, \u0026quot;2000/01/07\u0026quot;, \u0026quot;2005/05/20\u0026quot;,\u0026quot;2010/03/25\u0026quot;)\r# vector de fechas y horas\rdat_time \u0026lt;- c(\u0026quot;1988-08-01 05:00\u0026quot;, \u0026quot;2000-02-01 22:00\u0026quot;)\r# convertir a clase date\rdat \u0026lt;- ymd(dat) dat\r## [1] \u0026quot;1999-12-31\u0026quot; \u0026quot;2000-01-07\u0026quot; \u0026quot;2005-05-20\u0026quot; \u0026quot;2010-03-25\u0026quot;\r# otras formatos\rdmy(\u0026quot;05-02-2000\u0026quot;)\r## [1] \u0026quot;2000-02-05\u0026quot;\rymd(\u0026quot;20000506\u0026quot;)\r## [1] \u0026quot;2000-05-06\u0026quot;\r# convertir a POSIXct\rdat_time \u0026lt;- ymd_hm(dat_time)\rdat_time\r## [1] \u0026quot;1988-08-01 05:00:00 UTC\u0026quot; \u0026quot;2000-02-01 22:00:00 UTC\u0026quot;\r# diferentes formatos en un vector dat_mix \u0026lt;- c(\u0026quot;1999/12/05\u0026quot;, \u0026quot;05-09-2008\u0026quot;, \u0026quot;2000/08/09\u0026quot;, \u0026quot;25-10-2019\u0026quot;)\r# indicar formato con la convenci√≥n conocida en ?strptime\rparse_date_time(dat_mix, order = c(\u0026quot;%Y/%m/%d\u0026quot;, \u0026quot;%d-%m-%Y\u0026quot;))\r## [1] \u0026quot;1999-12-05 UTC\u0026quot; \u0026quot;2008-09-05 UTC\u0026quot; \u0026quot;2000-08-09 UTC\u0026quot; \u0026quot;2019-10-25 UTC\u0026quot;\rM√°s funciones √∫tiles:\n# extraer el a√±o\ryear(dat)\r## [1] 1999 2000 2005 2010\r# el mes\rmonth(dat)\r## [1] 12 1 5 3\rmonth(dat, label = TRUE) # como etiqueta\r## [1] dic ene may mar\r## 12 Levels: ene \u0026lt; feb \u0026lt; mar \u0026lt; abr \u0026lt; may \u0026lt; jun \u0026lt; jul \u0026lt; ago \u0026lt; sep \u0026lt; ... \u0026lt; dic\r# el d√≠a de la semana\rwday(dat)\r## [1] 6 6 6 5\rwday(dat, label = TRUE) # como etiqueta\r## [1] vi\\\\. vi\\\\. vi\\\\. ju\\\\.\r## Levels: do\\\\. \u0026lt; lu\\\\. \u0026lt; ma\\\\. \u0026lt; mi\\\\. \u0026lt; ju\\\\. \u0026lt; vi\\\\. \u0026lt; s√°\\\\.\r# la hora\rhour(dat_time)\r## [1] 5 22\r# sumar 10 d√≠as\rdat + days(10)\r## [1] \u0026quot;2000-01-10\u0026quot; \u0026quot;2000-01-17\u0026quot; \u0026quot;2005-05-30\u0026quot; \u0026quot;2010-04-04\u0026quot;\r# sumar 1 mes\rdat + months(1)\r## [1] \u0026quot;2000-01-31\u0026quot; \u0026quot;2000-02-07\u0026quot; \u0026quot;2005-06-20\u0026quot; \u0026quot;2010-04-25\u0026quot;\rPor √∫ltimo, la funci√≥n make_date() es muy √∫til en crear fechas a partir de diferentes partes de las mismas como puede ser el a√±o, mes, etc.\n# crear fecha a partir de sus elementos, aqu√≠ con a√±o y mes\rmake_date(2000, 5)\r## [1] \u0026quot;2000-05-01\u0026quot;\r# crear fecha con hora make_datetime(2005, 5, 23, 5)\r## [1] \u0026quot;2005-05-23 05:00:00 UTC\u0026quot;\rPodemos encontrar m√°s detalles en el Cheat-Sheet de lubridate.\n\r4.4 Manipulaci√≥n de tablas y vectores\rLos paquetes dplyr y tidyr nos proporciona una gram√°tica de manipulaci√≥n de datos con un conjunto de verbos √∫tiles para resolver los problemas m√°s comunes. Las funciones m√°s importantes son:\n\r\rFunci√≥n\rDescripci√≥n\r\r\r\rmutate()\ra√±adir nuevas variables o modificar existentes\r\rselect()\rseleccionar variables\r\rfilter()\rfiltrar\r\rsummarise()\rresumir/reducir\r\rarrange()\rordenar\r\rgroup_by()\ragrupar\r\rrename()\rrenombrar columnas\r\r\r\rEn caso de que no lo hayas hecho antes, importamos los datos de movilidad.\ngoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Rows: 516697 Columns: 13\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (6): country_region_code, country_region, sub_region_1, sub_region_2, i...\r## dbl (6): retail_and_recreation_percent_change_from_baseline, grocery_and_ph...\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r4.4.1 Selecionar y renombrar\rPodemos selecionar o eliminar columnas con la funci√≥n select(), usando el nombre o √≠ndice de la(s) columna(s). Para suprimir columnas hacemos uso del signo negativo. La funci√≥n rename ayuda en renombrar columnas o bien con el mismo nombre o con su √≠ndice.\nresidential_mobility \u0026lt;- select(google_mobility, country_region_code:sub_region_1, date, residential_percent_change_from_baseline) %\u0026gt;% rename(resi = 5)\r\r4.4.2 Filtrar y ordenar\rPara filtrar datos, empleamos filter() con operadores l√≥gicos (|, ==, \u0026gt;, etc) o funciones que devuelven un valor l√≥gico (str_detect(), is.na(), etc.). La funci√≥n arrange() ordena de menor a mayor por una o m√∫ltiples variables (con el signo negativo - se invierte el orden de mayor a menor).\nfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;)\r## # A tibble: 304,648 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States \u0026lt;NA\u0026gt; 2020-02-15 -1\r## 2 US United States \u0026lt;NA\u0026gt; 2020-02-16 -1\r## 3 US United States \u0026lt;NA\u0026gt; 2020-02-17 5\r## 4 US United States \u0026lt;NA\u0026gt; 2020-02-18 1\r## 5 US United States \u0026lt;NA\u0026gt; 2020-02-19 0\r## 6 US United States \u0026lt;NA\u0026gt; 2020-02-20 1\r## 7 US United States \u0026lt;NA\u0026gt; 2020-02-21 0\r## 8 US United States \u0026lt;NA\u0026gt; 2020-02-22 -1\r## 9 US United States \u0026lt;NA\u0026gt; 2020-02-23 -1\r## 10 US United States \u0026lt;NA\u0026gt; 2020-02-24 0\r## # ... with 304,638 more rows\rfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;, sub_region_1 == \u0026quot;New York\u0026quot;)\r## # A tibble: 7,068 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States New York 2020-02-15 0\r## 2 US United States New York 2020-02-16 -1\r## 3 US United States New York 2020-02-17 9\r## 4 US United States New York 2020-02-18 3\r## 5 US United States New York 2020-02-19 2\r## 6 US United States New York 2020-02-20 2\r## 7 US United States New York 2020-02-21 3\r## 8 US United States New York 2020-02-22 -1\r## 9 US United States New York 2020-02-23 -1\r## 10 US United States New York 2020-02-24 0\r## # ... with 7,058 more rows\rfilter(residential_mobility, resi \u0026gt; 50) %\u0026gt;% arrange(-resi)\r## # A tibble: 32 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 KW Kuwait Al Farwaniyah Governorate 2020-05-14 56\r## 2 KW Kuwait Al Farwaniyah Governorate 2020-05-21 55\r## 3 SG Singapore \u0026lt;NA\u0026gt; 2020-05-01 55\r## 4 KW Kuwait Al Farwaniyah Governorate 2020-05-28 54\r## 5 PE Peru Metropolitan Municipalit~ 2020-04-10 54\r## 6 EC Ecuador Pichincha 2020-03-27 53\r## 7 KW Kuwait Al Farwaniyah Governorate 2020-05-11 53\r## 8 KW Kuwait Al Farwaniyah Governorate 2020-05-13 53\r## 9 KW Kuwait Al Farwaniyah Governorate 2020-05-20 53\r## 10 SG Singapore \u0026lt;NA\u0026gt; 2020-04-10 53\r## # ... with 22 more rows\r\r4.4.3 Agrupar y resumir\r¬øD√≥nde encontramos mayor variabilidad entre regiones en cada pa√≠s el d√≠a 1 de abril de 2020?\nPara responder a esta pregunta, primero filtramos los datos y despu√©s agrupamos por la columna de pa√≠s. Cuando empleamos la funci√≥n summarise() posterior a la agrupaci√≥n, nos permite resumir por estos grupos. Incluso, la combinaci√≥n del group_by() con la funci√≥n mutate() permite modificar columnas por grupos. En summarise() calculamos el valor m√°ximo, m√≠nimo y la diferencia entre ambos extremos creando nuevas columnas.\nresi_variability \u0026lt;- residential_mobility %\u0026gt;% filter(date == ymd(\u0026quot;2020-04-01\u0026quot;),\r!is.na(sub_region_1)) %\u0026gt;% group_by(country_region) %\u0026gt;% summarise(mx = max(resi, na.rm = TRUE), min = min(resi, na.rm = TRUE),\rrange = abs(mx)-abs(min))\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in min(resi, na.rm = TRUE): ning√∫n argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ning√∫n argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ning√∫n argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ning√∫n argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ning√∫n argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ning√∫n argumento finito para min; retornando\r## Inf\rarrange(resi_variability, -range)\r## # A tibble: 94 x 4\r## country_region mx min range\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Nigeria 43 6 37\r## 2 United States 35 6 29\r## 3 India 36 15 21\r## 4 Malaysia 45 26 19\r## 5 Philippines 40 21 19\r## 6 Vietnam 28 9 19\r## 7 Colombia 41 24 17\r## 8 Ecuador 44 27 17\r## 9 Argentina 35 19 16\r## 10 Chile 30 14 16\r## # ... with 84 more rows\r\r4.4.4 Unir tablas\r¬øC√≥mo podemos filtrar los datos para obtener un subconjunto de Europa?\nPara ello, importamos datos espaciales con el c√≥digo de pa√≠s y una columna de las regiones. Explicaciones detalladas sobre el paquete sf (simple feature) para trabajar con datos vectoriales, lo dejaremos para otro post.\nlibrary(rnaturalearth) # paquete de datos vectoriales\r# datos de pa√≠ses\rwld \u0026lt;- ne_countries(returnclass = \u0026quot;sf\u0026quot;)\r# filtramos los pa√≠ses con c√≥digo y seleccionamos las dos columnas de inter√©s\rwld \u0026lt;- filter(wld, !is.na(iso_a2)) %\u0026gt;% select(iso_a2, subregion)\r# plot\rplot(wld)\rOtras funciones de dplyr nos permiten unir tablas: *_join(). Seg√∫n hacia qu√© tabla (izquierda o derecha) se quiere unir, cambia la funci√≥n : left_join(), right_join() o incluso full_join(). El argumento by no es necesario siempre y cuando ambas tablas tienen una columna en com√∫n. No obstante, en este caso la columna de fusi√≥n es diferente, por eso, usamos el modo c(\"country_region_code\"=\"iso_a2\"). El paquete forcats de tidyverse tiene muchas funciones √∫tiles para manejar variables categ√≥ricas (factors), variables que tienen un conjunto fijo y conocido de valores posibles. Todas las funciones de forcats tienen el prefijo fct_*. Por ejemplo, en este caso usamos fct_reorder() para reordenar las etiquetas de los pa√≠ses en orden de la m√°xima basada en los registros de movibilidad residencial. Finalmente, creamos una nueva columna ‚Äòresi_real‚Äô para cambiar el valor de referencia, el promedio (baseline), fijado en 0 a 100.\nsubset_europe \u0026lt;- filter(residential_mobility, is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;%\rmutate(resi_real = resi + 100,\rregion = fct_reorder(country_region, resi, .fun = \u0026quot;max\u0026quot;, .desc = FALSE)) %\u0026gt;% select(-geometry, -sub_region_1)\rstr(subset_europe)\r## tibble [3,988 x 7] (S3: tbl_df/tbl/data.frame)\r## $ country_region_code: chr [1:3988] \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; ...\r## $ country_region : chr [1:3988] \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; ...\r## $ date : Date[1:3988], format: \u0026quot;2020-02-15\u0026quot; \u0026quot;2020-02-16\u0026quot; ...\r## $ resi : num [1:3988] -2 -2 0 0 1 0 1 -2 0 -1 ...\r## $ subregion : chr [1:3988] \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; ...\r## $ resi_real : num [1:3988] 98 98 100 100 101 100 101 98 100 99 ...\r## $ region : Factor w/ 35 levels \u0026quot;Belarus\u0026quot;,\u0026quot;Ukraine\u0026quot;,..: 18 18 18 18 18 18 18 18 18 18 ...\r\r4.4.5 Tablas largas y anchas\rAntes de pasar a la visualizaci√≥n con ggplot2. Es muy habitual modificar la tabla entre dos formatos principales. Una tabla es tidy cuando 1) cada variable es una columna 2) cada observaci√≥n/caso es una fila y 3) cada tipo de unidad observacional forma una tabla.\n# subconjunto mobility_selection \u0026lt;- select(subset_europe, country_region_code, date:resi)\rmobility_selection\r## # A tibble: 3,988 x 3\r## country_region_code date resi\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 AT 2020-02-15 -2\r## 2 AT 2020-02-16 -2\r## 3 AT 2020-02-17 0\r## 4 AT 2020-02-18 0\r## 5 AT 2020-02-19 1\r## 6 AT 2020-02-20 0\r## 7 AT 2020-02-21 1\r## 8 AT 2020-02-22 -2\r## 9 AT 2020-02-23 0\r## 10 AT 2020-02-24 -1\r## # ... with 3,978 more rows\r# tabla ancha\rmobi_wide \u0026lt;- pivot_wider(mobility_selection, names_from = country_region_code,\rvalues_from = resi)\rmobi_wide\r## # A tibble: 114 x 36\r## date AT BA BE BG BY CH CZ DE DK EE ES\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 -2 -1 -1 0 -1 -1 -2 -1 0 0 -2\r## 2 2020-02-16 -2 -1 1 -3 0 -1 -1 0 1 0 -2\r## 3 2020-02-17 0 -1 0 -2 0 1 0 0 1 1 -1\r## 4 2020-02-18 0 -1 0 -2 0 1 0 1 1 1 0\r## 5 2020-02-19 1 -1 0 -1 -1 1 0 1 1 0 -1\r## 6 2020-02-20 0 -1 0 0 -1 0 0 1 1 0 -1\r## 7 2020-02-21 1 -2 0 -1 -1 1 0 2 1 1 -2\r## 8 2020-02-22 -2 -1 0 0 -2 -2 -3 0 1 0 -2\r## 9 2020-02-23 0 -1 0 -3 -1 -1 0 0 0 -2 -3\r## 10 2020-02-24 -1 -1 4 -1 0 0 0 4 0 16 0\r## # ... with 104 more rows, and 24 more variables: FI \u0026lt;dbl\u0026gt;, FR \u0026lt;dbl\u0026gt;, GB \u0026lt;dbl\u0026gt;,\r## # GR \u0026lt;dbl\u0026gt;, HR \u0026lt;dbl\u0026gt;, HU \u0026lt;dbl\u0026gt;, IE \u0026lt;dbl\u0026gt;, IT \u0026lt;dbl\u0026gt;, LT \u0026lt;dbl\u0026gt;, LU \u0026lt;dbl\u0026gt;,\r## # LV \u0026lt;dbl\u0026gt;, MD \u0026lt;dbl\u0026gt;, MK \u0026lt;dbl\u0026gt;, NL \u0026lt;dbl\u0026gt;, NO \u0026lt;dbl\u0026gt;, PL \u0026lt;dbl\u0026gt;, PT \u0026lt;dbl\u0026gt;,\r## # RO \u0026lt;dbl\u0026gt;, RS \u0026lt;dbl\u0026gt;, RU \u0026lt;dbl\u0026gt;, SE \u0026lt;dbl\u0026gt;, SI \u0026lt;dbl\u0026gt;, SK \u0026lt;dbl\u0026gt;, UA \u0026lt;dbl\u0026gt;\r# tabla larga\rpivot_longer(mobi_wide,\r2:36,\rnames_to = \u0026quot;country_code\u0026quot;,\rvalues_to = \u0026quot;resi\u0026quot;)\r## # A tibble: 3,990 x 3\r## date country_code resi\r## \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 AT -2\r## 2 2020-02-15 BA -1\r## 3 2020-02-15 BE -1\r## 4 2020-02-15 BG 0\r## 5 2020-02-15 BY -1\r## 6 2020-02-15 CH -1\r## 7 2020-02-15 CZ -2\r## 8 2020-02-15 DE -1\r## 9 2020-02-15 DK 0\r## 10 2020-02-15 EE 0\r## # ... with 3,980 more rows\rOtro grupo de funciones a las que deber√≠as echar un vistazo son: separate(), case_when(), complete(). Podemos encontrar m√°s detalles en el Cheat-Sheet de dplyr\n\r\r4.5 Visualizar datos\rggplot2 es un sistema moderno, y con una enorme variedad de opciones, para visualizaci√≥n de datos. A diferencia del sistema gr√°fico de R Base se utiliza una gram√°tica diferente.\rLa gram√°tica de los gr√°ficos (grammar of graphics, de all√≠ ‚Äúgg‚Äù) consiste en la suma de varias capas u objetos independientes que se combinan usando + para construir el gr√°fico final. ggplot diferencia entre los datos, lo que se visualiza y la forma en que se visualiza.\n\rdata: nuestro conjunto de datos (data.frame o tibble)\n\raesthetics: con la funci√≥n aes() indicamos las variables que corresponden a los ejes x, y, z,‚Ä¶ o, cuando se pretende aplicar par√°metros gr√°ficos (color, size, shape) seg√∫n una variable. Es posible incluir aes() en ggplot() o en la funci√≥n correspondiente a una geometr√≠a geom_*.\n\rgeometries: son objetos geom_* que indican la geometr√≠a a usar, (p.¬†ej.: geom_point(), geom_line(), geom_boxplot(), etc.).\n\rscales: son objetos de tipo scales_* (p.¬†ej.: scale_x_continous(), scale_colour_manual()) para manipular las ejes, definir colores, etc.\n\rstatistics: son objetos stat_* (p.ej.: stat_density()) que permiten aplicar transformaciones estad√≠sticas.\n\r\rPodemos encontrar m√°s detalles en el Cheat-Sheet de ggplot2. ggplot es complementado constantemente con extensiones para geometr√≠as u otras opciones gr√°ficas (https://exts.ggplot2.tidyverse.org/ggiraph.html), para obtener ideas gr√°ficas, debes echarle un vistazo a la Galer√≠a de Gr√°ficos R (https://www.r-graph-gallery.com/).\n4.5.1 Gr√°fico de linea y puntos\rCreamos un subconjunto de nuestros datos de movilidad para residencias y parques, filtrando los registros de regiones italianas. Adem√°s, dividimos los valores de movilidad en porcentaje por 100 para obtener la fracci√≥n, ya que ggplot2 nos permite indicar la unidad de porcentaje en el argumento de las etiquetas (√∫ltimo gr√°fico de esta secci√≥n).\n# creamos el subconjunto\rit \u0026lt;- filter(google_mobility, country_region == \u0026quot;Italy\u0026quot;, is.na(sub_region_1)) %\u0026gt;% mutate(resi = residential_percent_change_from_baseline/100, parks = parks_percent_change_from_baseline/100)\r# gr√°fico de l√≠nea ggplot(it, aes(date, resi)) + geom_line()\r# gr√°fico de dispersi√≥n con l√≠nea de correlaci√≥n\rggplot(it, aes(parks, resi)) + geom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rPara modificar los ejes, empleamos las diferentes funciones de scale_* que debemos adpatar a las escalas de medici√≥n (date, discrete, continuous, etc.). La funci√≥n labs() nos ayuda en definir los t√≠tulos de ejes, del gr√°fico y de la leyenda. Por √∫ltimo, a√±adimos con theme_light() el estilo del gr√°fico (otros son theme_bw(), theme_minimal(), etc.). Tambi√©n podr√≠amos hacer cambios de todos los elementos gr√°ficos a trav√©s de theme().\n# time serie plot\rggplot(it, aes(date, resi)) + geom_line(colour = \u0026quot;#560A86\u0026quot;, size = 0.8) +\rscale_x_date(date_breaks = \u0026quot;10 days\u0026quot;, date_labels = \u0026quot;%d %b\u0026quot;) +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r# scatter plot\rggplot(it, aes(parks, resi)) + geom_point(alpha = .4, size = 2) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rscale_x_continuous(breaks = seq(-1, 1.4, 0.2), labels = scales::percent) +\rscale_y_continuous(breaks = seq(-1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;Park mobility\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\r4.5.2 Boxplot\rPodemos visualizar diferentes aspectos de los datos de movilidad con otras geometr√≠as. Aqu√≠ creamos boxplots por cada pa√≠s europeo representando la variabilidad de movilidad entre y en los pa√≠ses durante la pandemia del COVID-19.\n# subconjunto\rsubset_europe_reg \u0026lt;- filter(residential_mobility, !is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;% mutate(resi = resi/100, country_region = fct_reorder(country_region, resi))\r# boxplot\rggplot(subset_europe_reg, aes(country_region, resi, fill = subregion)) + geom_boxplot() +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;) +\rcoord_flip() +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;, fill = \u0026quot;\u0026quot;) +\rtheme_minimal()\r\r4.5.3 Heatmap\rPara visualizar la tendencia de todos los pa√≠ses europeos es recomendable usar un heatmap en lugar de un bulto de l√≠neas. Antes de constuir el gr√°fico, creamos un vector de fechas para las etiquetas con los domingos en el per√≠odo de registros.\n# secuencia de fechas\rdf \u0026lt;- data.frame(d = seq(ymd(\u0026quot;2020-02-15\u0026quot;), ymd(\u0026quot;2020-06-07\u0026quot;), \u0026quot;day\u0026quot;))\r# filtramos los domingos creando el d√≠a de la semana\rsundays \u0026lt;- df %\u0026gt;% mutate(wd = wday(d, week_start = 1)) %\u0026gt;% filter(wd == 7) %\u0026gt;% pull(d)\rSi queremos usar etiquetas en otras lenguas, es necesario cambiar la configuraci√≥n regional del sistema.\nSys.setlocale(\u0026quot;LC_TIME\u0026quot;, \u0026quot;English\u0026quot;)\r## [1] \u0026quot;English_United States.1252\u0026quot;\rEl relleno de color para los boxplots lo dibujamos por cada regi√≥n de los pa√≠ses europeos. Podemos fijar el tipo de color con scale_fill_*, en este caso, de las gamas viridis.\nAdem√°s, la funci√≥n guides() nos permite modificar la barra de color de la leyenda. Por √∫ltimo, aqu√≠ vemos el uso de theme() con cambios adicionales a theme_minimal().\n# headmap\rggplot(subset_europe, aes(date, region, fill = resi_real)) +\rgeom_tile() +\rscale_x_date(breaks = sundays,\rdate_labels = \u0026quot;%d %b\u0026quot;) +\rscale_fill_viridis_c(option = \u0026quot;A\u0026quot;, breaks = c(91, 146),\rlabels = c(\u0026quot;Less\u0026quot;, \u0026quot;More\u0026quot;), direction = -1) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;top\u0026quot;, title = element_text(size = 14),\rpanel.grid.major.x = element_line(colour = \u0026quot;white\u0026quot;, linetype = \u0026quot;dashed\u0026quot;),\rpanel.grid.minor.x = element_blank(),\rpanel.grid.major.y = element_blank(),\rpanel.ontop = TRUE,\rplot.margin = margin(r = 1, unit = \u0026quot;cm\u0026quot;)) +\rlabs(y = \u0026quot;\u0026quot;, x = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, title = \u0026quot;Mobility trends for places of residence\u0026quot;,\rcaption = \u0026quot;Data: google.com/covid19/mobility/\u0026quot;) +\rguides(fill = guide_colorbar(barwidth = 10, barheight = .5,\rlabel.position = \u0026quot;top\u0026quot;, ticks = FALSE)) +\rcoord_cartesian(expand = FALSE)\r\r\r4.6 Aplicar funciones sobre vectores o listas\rEl paquete purrr contiene un conjunto de funciones avanzadas de programaci√≥n funcional para trabajar con funciones y vectores. La familia de funciones lapply() conocido de R Basecorresponde a las funciones de map() en este paquete. Una de las mayores ventajas es poder reducir el uso de bucles (for, etc.).\n# lista con dos vectores\rvec_list \u0026lt;- list(x = 1:10, y = 50:70)\r# calculamos el promedio para cada uno\rmap(vec_list, mean)\r## $x\r## [1] 5.5\r## ## $y\r## [1] 60\r# podemos cambiar tipo de salida map_* (dbl, chr, lgl, etc.)\rmap_dbl(vec_list, mean)\r## x y ## 5.5 60.0\rUn ejemplo m√°s complejo. Calculamos el coeficiente de correlaci√≥n entre la movilidad residencial y la de los parques en todos los pa√≠ses europeos. Para obtener un resumen tidy de un modelo o un test usamos la funci√≥n tidy() del paquete broom.\nlibrary(broom) # tidy outputs\r# funci√≥n adaptada cor_test \u0026lt;- function(x, formula) { df \u0026lt;- cor.test(as.formula(formula), data = x) %\u0026gt;% tidy()\rreturn(df)\r}\r# preparamos los datos\reurope_reg \u0026lt;- filter(google_mobility, !is.na(sub_region_1),\r!is.na(residential_percent_change_from_baseline)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;))\r# aplicamos la funci√≥n a cada pa√≠s creando una lista\reurope_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;) \r## $AT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.360 -12.3 2.68e-32 1009 -0.413 -0.305 Pearson\u0026#39;~ two.sided ## ## $BE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.312 -6.06 3.67e-9 340 -0.405 -0.213 Pears~ two.sided ## ## $BG\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pearson~ two.sided ## ## $CH\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0786 -2.91 0.00370 1360 -0.131 -0.0256 Pearson\u0026#39;s~ two.sided ## ## $CZ\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0837 -3.35 0.000824 1593 -0.132 -0.0347 Pearson\u0026#39;~ two.sided ## ## $DE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.00239 0.102 0.919 1814 -0.0436 0.0484 Pearson\u0026#39;s~ two.sided ## ## $DK\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.237 5.81 1.04e-8 567 0.158 0.313 Pears~ two.sided ## ## $EE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.235 -2.88 0.00462 142 -0.384 -0.0740 Pearson\u0026#39;s~ two.sided ## ## $ES\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.825 -65.4 0 2005 -0.839 -0.811 Pearson\u0026#39;s~ two.sided ## ## $FI\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.0427 1.42 0.155 1106 -0.0162 0.101 Pearson\u0026#39;s~ two.sided ## ## $FR\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.698 -37.4 3.29e-216 1474 -0.723 -0.671 Pearson~ two.sided ## ## $GB\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.105 -11.0 9.19e-28 10712 -0.124 -0.0865 Pearson\u0026#39;~ two.sided ## ## $GR\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.692 -27.0 1.03e-114 796 -0.726 -0.654 Pearson~ two.sided ## ## $HR\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.579 -21.9 9.32e-87 954 -0.620 -0.536 Pearson\u0026#39;~ two.sided ## ## $HU\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.342 -15.6 6.71e-52 1843 -0.382 -0.301 Pearson\u0026#39;~ two.sided ## ## $IE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.222 -8.45 7.49e-17 1378 -0.271 -0.171 Pearson\u0026#39;~ two.sided ## ## $IT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.831 -71.0 0 2250 -0.844 -0.818 Pearson\u0026#39;s~ two.sided ## ## $LT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.204 -5.45 7.17e-8 686 -0.274 -0.131 Pears~ two.sided ## ## $LV\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.544 -6.87 3.84e-10 112 -0.662 -0.401 Pearson\u0026#39;~ two.sided ## ## $NL\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.143 5.31 0.000000125 1356 0.0903 0.195 Pears~ two.sided ## ## $NO\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.0483 1.69 0.0911 1221 -0.00774 0.104 Pearson\u0026#39;s~ two.sided ## ## $PL\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.531 -26.7 6.08e-133 1815 -0.564 -0.498 Pearson~ two.sided ## ## $PT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.729 -46.9 2.12e-321 1938 -0.749 -0.707 Pearson~ two.sided ## ## $RO\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.640 -56.0 0 4517 -0.657 -0.623 Pearson\u0026#39;s~ two.sided ## ## $SE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.106 3.93 0.0000909 1367 0.0529 0.158 Pearson~ two.sided ## ## $SI\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.627 -11.4 1.98e-23 200 -0.704 -0.535 Pearson\u0026#39;~ two.sided ## ## $SK\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.196 -5.70 1.65e-8 810 -0.262 -0.129 Pears~ two.sided\rComo ya hemos visto anteriormente, existen subfunciones de map_* para obtener en lugar de una lista un objeto de otra clase, aqu√≠ de data.frame.\ncor_mobility \u0026lt;- europe_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map_df(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;, .id = \u0026quot;country_code\u0026quot;)\rarrange(cor_mobility, estimate)\r## # A tibble: 27 x 9\r## country_code estimate statistic p.value parameter conf.low conf.high method\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 IT -0.831 -71.0 0 2250 -0.844 -0.818 Pears~\r## 2 ES -0.825 -65.4 0 2005 -0.839 -0.811 Pears~\r## 3 PT -0.729 -46.9 2.12e-321 1938 -0.749 -0.707 Pears~\r## 4 FR -0.698 -37.4 3.29e-216 1474 -0.723 -0.671 Pears~\r## 5 GR -0.692 -27.0 1.03e-114 796 -0.726 -0.654 Pears~\r## 6 BG -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pears~\r## 7 RO -0.640 -56.0 0 4517 -0.657 -0.623 Pears~\r## 8 SI -0.627 -11.4 1.98e- 23 200 -0.704 -0.535 Pears~\r## 9 HR -0.579 -21.9 9.32e- 87 954 -0.620 -0.536 Pears~\r## 10 LV -0.544 -6.87 3.84e- 10 112 -0.662 -0.401 Pears~\r## # ... with 17 more rows, and 1 more variable: alternative \u0026lt;chr\u0026gt;\rOtros ejemplos pr√°cticos aqu√≠ en este post or este otro. Podemos encontrar m√°s detalles en el Cheat-Sheet de purrr.\n\n\r\r","date":1591401600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1591401600,"objectID":"5b07a046d9393968e3c49657d6c1e1af","permalink":"https://dominicroye.github.io/es/2020/una-muy-breve-introduccion-a-tidyverse/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/es/2020/una-muy-breve-introduccion-a-tidyverse/","section":"post","summary":"El universo de los paquetes de tidyverse, una colecci√≥n de paquetes de funciones para un uso especialmente enfocado en la ciencia de datos, abri√≥ un antes y despu√©s en la programaci√≥n de R. En este post voy a resumir muy brevemente lo m√°s esencial para inicarse en este mundo. La gram√°tica sigue en todas las funciones una estructura com√∫n. Lo m√°s esencial es que el primer argumento es el objeto y a continuaci√≥n viene el resto de argumentos. Adem√°s, se proporciona un conjunto de verbos que facilitan el uso de las funciones. En la actualidad, la filosof√≠a de las funciones tambi√©n se refleja en otros paquetes que hacen compatible su uso con la colecci√≥n de tidyverse.","tags":["introducci√≥n","visualizaci√≥n","gesti√≥n","datos","COVID-19"],"title":"Una muy breve introducci√≥n a Tidyverse","type":"post"},{"authors":["A Santurt√∫n","R Almendra","P Fdez-Arroyabe","A Sanchez-Lorenzo","D Roy√©","MT Zarrabeitia","P Santana"],"categories":null,"content":"","date":1586822400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1586822400,"objectID":"b7804ed138a0b217293985653c0122c4","permalink":"https://dominicroye.github.io/es/publication/2020-hospital-admissions-cardio-thermal-indices-stoten/","publishdate":"2020-04-14T00:00:00Z","relpermalink":"/es/publication/2020-hospital-admissions-cardio-thermal-indices-stoten/","section":"publication","summary":"The natural environment has been considered an important determinant of cardiovascular morbidity. This work seeks to assess the impact of the winter thermal environment on hospital admissions from diseases of the circulatory system by using three biometeorological indices in five regions of the Iberian Peninsula. A theoretical index based on a thermophysiological model (Universal Thermal Climate Index [UTCI]) and two experimental biometeorological ones (Net Effective Temperature [NET] and Apparent Temperature [AT]) were estimated in two metropolitan areas of Portugal (Porto and Lisbon) and in three provinces of Spain (Madrid, Barcelona and Valencia). Subsequently, their relationship with hospital admissions, adjusted by NO2 concentration, time, and day of the week, was analyzed using a Generalized Additive Model. As the estimation method, a semi-parametric quasi-Poisson regression was used. Around 53% of the hospitalizations occurred during the cold periods. The admissions rate followed an upward trend over the 9-year period in both capitals (Madrid and Lisbon) as well as in Barcelona. An inverse and statistically significant relationship was found between thermal comfort and hospital admissions in the five regions (p","tags":["enfermedades del sistema circulatorio","temperatura del aire","temperatura neta efectiva","temperatura aparente","√≠ndice UTCI"],"title":"Predictive value of three thermal confort indices in low temperatures on cardiovascular morbidity in the Iberian Peninsula","type":"publication"},{"authors":null,"categories":["visualizaci√≥n","R","R:intermedio"],"content":"\r\rCuando visualizamos anomal√≠as de precipitaci√≥n y temperatura, simplemente usamos series temporales en un gr√°fico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, para tener una imagen global necesitamos ambas anomal√≠as en un √∫nico gr√°fico. As√≠ podr√≠amos responder directamente a la pregunta de si una estaci√≥n del a√±o o un mes en concreto fue seco-c√°lido o h√∫medo-fr√≠o, e incluso comparar estas anomal√≠as en el contexto de a√±os anteriores.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rggrepel\rEtiquetas sin superposici√≥n con ggplot2\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggrepel\u0026quot;)) install.packages(\u0026quot;ggrepel\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggrepel)\r\rPreparar los datos\rPrimero importamos la precipitaci√≥n y temperatura diaria de la estaci√≥n meteorol√≥gica seleccionada (descarga). Usaremos los datos de Tenerife Sur (Espa√±a) [1981-2020] accesible a trav√©s de Open Data AEMET. En R existe un paquete meteoland que facilita la descarga con funciones espec√≠ficas para acceder a los datos de AEMET, Meteogalicia y Meteocat.\nPaso 1: importar los datos\rImportamos los datos en formato csv, siendo la primera columna la fecha, la segunda la precipitaci√≥n (pr) y la √∫ltima la temperatura media diaria (ta).\ndata \u0026lt;- read_csv(\u0026quot;meteo_tenerife.csv\u0026quot;) \r## Rows: 14303 Columns: 3\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (2): pr, ta\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rdata\r## # A tibble: 14,303 x 3\r## date pr ta\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1981-01-02 0 17.6\r## 2 1981-01-03 0 16.8\r## 3 1981-01-04 0 17.4\r## 4 1981-01-05 0 17.6\r## 5 1981-01-06 0 17 ## 6 1981-01-07 0 17.6\r## 7 1981-01-08 0 18.6\r## 8 1981-01-09 0 19.8\r## 9 1981-01-10 0 21.5\r## 10 1981-01-11 3.8 17.6\r## # ... with 14,293 more rows\r\rPaso 2: preparar los datos\rEn el segundo paso preparamos los datos para calcular las anomal√≠as. Para ello, creamos tres nuevas columnas: el mes, el a√±o y la estaci√≥n del a√±o. Como nuestro objetivo son las anomal√≠as invernales no podemos usar el a√±o natural, ya que el invierno comprende el mes de diciembre de un a√±o y los meses de enero y febrero del siguiente. La funci√≥n personalizada meteo_yr() extrae el a√±o de una fecha indicando el mes inicial. El concepto es similar al a√±o hidrol√≥gico en el que se empieza en octubre.\nmeteo_yr \u0026lt;- function(dates, start_month = NULL) {\r# convertir a POSIXlt\rdates.posix \u0026lt;- as.POSIXlt(dates)\r# la compensaci√≥n\roffset \u0026lt;- ifelse(dates.posix$mon \u0026gt;= start_month - 1, 1, 0)\r# nuevo a√±o\radj.year = dates.posix$year + 1900 + offset\rreturn(adj.year)\r}\rUsaremos las funciones de la colecci√≥n de paquetes tidyverse (https://www.tidyverse.org/). La funci√≥n mutate() ayuda a a√±adir nuevas columnas o a cambiar otras existentes. Para definir las estaciones del a√±o, empleamos la funci√≥n case_when() del paquete dplyr lo que tiene muchas ventajas en comparaci√≥n con una cadena de ifelse(). En la funci√≥n case_when() usamos f√≥rmulas en dos tiempos, por un lado la condici√≥n y por otro la acci√≥n cuando se cumpla esa condici√≥n. En R una f√≥rmula de dos tiempos o dos lados se consistuye con el operador ~. El operador binario %in% nos permite filtrar varios valores en un conjunto.\ndata \u0026lt;- mutate(data, winter_yr = meteo_yr(date, 12),\rmonth = month(date), season = case_when(month %in% c(12,1:2) ~ \u0026quot;Winter\u0026quot;,\rmonth %in% 3:5 ~ \u0026quot;Spring\u0026quot;,\rmonth %in% 6:8 ~ \u0026quot;Summer\u0026quot;,\rmonth %in% 9:11 ~ \u0026quot;Autum\u0026quot;))\rdata\r## # A tibble: 14,303 x 6\r## date pr ta winter_yr month season\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1981-01-02 0 17.6 1981 1 Winter\r## 2 1981-01-03 0 16.8 1981 1 Winter\r## 3 1981-01-04 0 17.4 1981 1 Winter\r## 4 1981-01-05 0 17.6 1981 1 Winter\r## 5 1981-01-06 0 17 1981 1 Winter\r## 6 1981-01-07 0 17.6 1981 1 Winter\r## 7 1981-01-08 0 18.6 1981 1 Winter\r## 8 1981-01-09 0 19.8 1981 1 Winter\r## 9 1981-01-10 0 21.5 1981 1 Winter\r## 10 1981-01-11 3.8 17.6 1981 1 Winter\r## # ... with 14,293 more rows\r\rPaso 3: estimar las anomal√≠as invernales\rEn el siguiente paso creamos un subset del invierno. Despu√©s agrupamos por el a√±o meteorol√≥gico y calculamos la suma y el promedio para la precipitaci√≥n y la temperatura, respectivamente. Para facilitar el trabajo el paquete magrittr introduce el operador llamado pipe en la forma %\u0026gt;% con el objetivo de combinar varias funciones sin la necesidad de asignar el resultado a un nuevo objeto. El operador pipe pasa la salida de una funci√≥n aplicada al primer argumento de la siguiente funci√≥n. Esta forma de combinar funciones permite encadenar varios pasos de forma simult√°nea. Se debe entender y pronunciar el %\u0026gt;% como ‚Äúluego‚Äù (then).\ndata_inv \u0026lt;- filter(data, season == \u0026quot;Winter\u0026quot;) %\u0026gt;% group_by(winter_yr) %\u0026gt;%\rsummarise(pr = sum(pr, na.rm = TRUE),\rta = mean(ta, na.rm = TRUE))\rS√≥lo nos quedan por calcular las anomal√≠as de precipitaci√≥n y temperatura. Las columnas pr_mean y ta_mean contendr√°n el promedio climatico, la referencia de las anomal√≠as respecto al periodo normal 1981-2010. Por eso debemos filtrar los valores al periodo antes de 2010, lo que haremos de la forma habitual de filtrado de vectores en R. Una vez que tenemos las referencias estimamos las anomal√≠as pr_anom y ta_anom. Para facilitar la interpretaci√≥n, en el caso de la precipitaci√≥n lo expresamos en porcentaje, pero poniendo el promedio en 0% en lugar del 100%.\nAdem√°s, a√±adimmos tres columnas con informaci√≥n necesaria en la creaci√≥n del gr√°fico. 1) labyr contiene el a√±o de cada anomal√≠a siempre y cuando haya sido mayor/menor del -+10% o -+0,5¬∫C, respectivamente (lo hago para que no haya demasiadas etiquetas), 2) symb_point una variable dummy para poder crear un simbolo diferencial entre los casos de (1), y 3) lab_font destacaremos en negrita el a√±o 2020.\ndata_inv \u0026lt;- mutate(data_inv, pr_mean = mean(pr[winter_yr \u0026lt;= 2010]), ta_mean = mean(ta[winter_yr \u0026lt;= 2010]),\rpr_anom = (pr*100/pr_mean)-100, ta_anom = ta-ta_mean,\rlabyr = case_when(pr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr),\rsymb_point = ifelse(!is.na(labyr), \u0026quot;yes\u0026quot;, \u0026quot;no\u0026quot;),\rlab_font = ifelse(labyr == 2020, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)\r)\r\r\rCrear el gr√°fico\rEl gr√°fico lo construiremos a√±adiendo capa por capa los diferentes elementos: 1) el fondo con las diferentes cuadr√≠culas (Seco-C√°lido, Seco-Fr√≠o, etc.) 2) los puntos y etiquetas, y 3) los √∫ltimos ajustes de estilo.\nParte 1\rLa idea es que tengamos los puntos con anomal√≠as seco-c√°lido en el cuadrante I (arriba-derecha) y los de h√∫medo-fr√≠o en el cuadrante III (abajo-izquierda). Por eso, debemos invertir el signo en las anomal√≠as de precipitaci√≥n. Despu√©s creamos un data.frame con las posiciones de las etiquetas de los cuatro cuadrantes. Para las posiciones en x y y se usan Inf y -Inf lo que equivale al punto m√°ximo dentro del panel. No obstante, es necesario ajustar la posici√≥n hacia los puntos extremos dentro del marco gr√°fico con los argumentos conocidos de ggplot2: hjust y vjust.\ndata_inv_p \u0026lt;- mutate(data_inv, pr_anom = pr_anom * -1)\rbglab \u0026lt;- data.frame(x = c(-Inf, Inf, -Inf, Inf), y = c(Inf, Inf, -Inf, -Inf),\rhjust = c(1, 1, 0, 0), vjust = c(1, 0, 1, 0),\rlab = c(\u0026quot;H√∫medo-C√°lido\u0026quot;, \u0026quot;Seco-C√°lido\u0026quot;,\r\u0026quot;H√∫medo-Fr√≠o\u0026quot;, \u0026quot;Seco-Fr√≠o\u0026quot;))\rbglab\r## x y hjust vjust lab\r## 1 -Inf Inf 1 1 H√∫medo-C√°lido\r## 2 Inf Inf 1 0 Seco-C√°lido\r## 3 -Inf -Inf 0 1 H√∫medo-Fr√≠o\r## 4 Inf -Inf 0 0 Seco-Fr√≠o\r\rParte 2\rEn la segunda podemos empezar a construir el gr√°fico a√±adiendo todos los elementos gr√°ficos. En esta parte creamos el fondo con los diferentes colores de cada cuadrante. La funci√≥n annotate() permite a√±adir capas de geometr√≠a sin el uso de variables dentro de un data.frame. Con la funci√≥n geom_hline() y geom_vline() marcamos los cuadrantes en horizontal y vertical usando una linea discontinua. Por √∫ltimo, dibujamos las etiquetas de cada cuadrante, empleando la funci√≥n geom_text(). Cuando usamos diferentes fuentes de data.frames, uno diferente al principal usado en ggplot(), debemos indicarlo con el argumento data en la funci√≥n de geomtr√≠a correspondiente.\ng1 \u0026lt;- ggplot(data_inv_p, aes(pr_anom, ta_anom)) +\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = 0, ymax = Inf, fill = \u0026quot;#fc9272\u0026quot;, alpha = .6) + #humedo-calido\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = 0, ymax = Inf, fill = \u0026quot;#cb181d\u0026quot;, alpha = .6) + #seco-calido\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = -Inf, ymax = 0, fill = \u0026quot;#2171b5\u0026quot;, alpha = .6) + #humedo-frio\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = -Inf, ymax = 0, fill = \u0026quot;#c6dbef\u0026quot;, alpha = .6) + #seco-frio\rgeom_hline(yintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_vline(xintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = bglab, aes(x, y, label = lab, hjust = hjust, vjust = vjust),\rfontface = \u0026quot;italic\u0026quot;, size = 5, angle = 90, colour = \u0026quot;white\u0026quot;)\rg1\r\rParte 3\rEn la tercera parte simplemente a√±adimos los puntos de las anomal√≠as y las etiquetas de los a√±os. La funci√≥n geom_text_repel() es similar a la que conocemos por defecto en ggplot2, geom_text(), pero evita el sobrlapso entre s√≠.\ng2 \u0026lt;- g1 + geom_point(aes(fill = symb_point, colour = symb_point),\rsize = 2.8, shape = 21, show.legend = FALSE) +\rgeom_text_repel(aes(label = labyr, fontface = lab_font),\rmax.iter = 5000, size = 3.5) g2\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\rParte 4\rEn la √∫ltima parte ajustamos, adem√°s del estilo general, los ejes, el tipo de color y el (sub)t√≠tulo. Recuerda que cambiamos el signo en las anomal√≠as de precipitaci√≥n. Por eso, debemos usar los argumentos breaks y labels en la funci√≥n scale_x_continouous() para revertir el signo en las etiquetas correspondientes a los cortes.\ng3 \u0026lt;- g2 + scale_x_continuous(\u0026quot;Anomal√≠a de precipitaci√≥n en %\u0026quot;,\rbreaks = seq(-100, 250, 10) * -1,\rlabels = seq(-100, 250, 10),\rlimits = c(min(data_inv_p$pr_anom), 100)) +\rscale_y_continuous(\u0026quot;Anomal√≠a de temperatura media en ¬∫C\u0026quot;,\rbreaks = seq(-2, 2, 0.5)) +\rscale_fill_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;)) +\rscale_colour_manual(values = rev(c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;))) +\rlabs(title = \u0026quot;Anomal√≠as invernales en Tenerife Sur\u0026quot;, caption = \u0026quot;Datos: AEMET\\nPeriodo normal 1981-2010\u0026quot;) +\rtheme_bw()\rg3\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\n\r\r","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1585440000,"objectID":"d06c7cd6ab9f372fccae68c86f97adcc","permalink":"https://dominicroye.github.io/es/2020/visualizar-anomalias-climaticas/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/es/2020/visualizar-anomalias-climaticas/","section":"post","summary":"Cuando visualizamos anomal√≠as de precipitaci√≥n y temperatura, simplemente usamos series temporales en un gr√°fico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, para tener una imagen global necesitamos ambas anomal√≠as en un √∫nico gr√°fico. As√≠ podr√≠amos responder directamente a la pregunta de si una estaci√≥n del a√±o o un mes en concreto fue seco-c√°lido o h√∫medo-fr√≠o, e incluso comparar estas anomal√≠as en el contexto de a√±os anteriores.","tags":["anomal√≠a","precipitaci√≥n","temperatura","clima","puntos"],"title":"Visualizar anomal√≠as clim√°ticas","type":"post"},{"authors":["R Monjo","D Roy√©","J Martin-Vide"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1581379200,"objectID":"a7fd692ee43756f99192bf1657a2431c","permalink":"https://dominicroye.github.io/es/publication/2020-drought-fractality-essd/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/es/publication/2020-drought-fractality-essd/","section":"publication","summary":"Drought duration strongly depends on the definition thereof. In meteorology, dryness is habitually measured by means of fixed thresholds (e.g. 0.1 or 1 mm usually define dry spells) or climatic mean values (as is the case of the Standard-ised Precipitation Index), but this also depends on the aggregation time interval considered. However, robust measurements of drought duration are required for analysing the statistical significance of possible changes. Herein we have climatically classified the drought duration around the world according to their similarity to the voids of the Cantor set. Dryness time structure 5 can be concisely measured by the n-index (from the regular/irregular alternation of dry/wet spells), which is closely related to the Gini index and to a Cantor-based exponent. This enables the worlds climates to be classified into six large types based upon a new measure of drought duration. We performed the dry-spell analysis using the full global gridded daily Multi-Source Weighted-Ensemble Precipitation (MSWEP) dataset. The MSWEP combines gauge-, satellite-, and reanalysis-based data to provide reliable precipitation estimates. The study period comprises the years 1979-2016 (total of 45165 days), and a spatial 10 resolution of 0.5¬∫, with a total of 259,197 grid points. Data set is publicly available at 10.5281/zenodo.3247041 (Monjo et al., 2019).","tags":["sequ√≠a","clasificaci√≥n","mundo","lacunaridad","patrones espacio-temporales","per√≠odos de sequ√≠a"],"title":"Meteorological drought lacunarity around the world and its classification","type":"publication"},{"authors":["D Roy√©","F Tedim","J Martin-Vide","M Salis","J Vendrell","R Lovreglio","C Bouillon","V Leone"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1581379200,"objectID":"1f7b33fb7f23393295d2d37d98d160b3","permalink":"https://dominicroye.github.io/es/publication/2019-wildfire-ci-europe-land-degradation/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/es/publication/2019-wildfire-ci-europe-land-degradation/","section":"publication","summary":"The most widely used metrics to characterize wildfire regime and estimate the impact of wildfires are total burnt area (BA) and the number of fire events (FE). However, these are insufficient to analyse the threat to society of a new fire regime characterized by a higher occurrence of very large events. To overcome this weakness, we propose the use of a Concentration Index (CIB) which makes it possible to identify spatio-temporal patterns. The frequency distribution of BA follows a negative exponential distribution almost everywhere, in which a small minority of FE are responsible of the majority of BA. In this article, the spatio-temporal behaviour of BA is analysed in Western Mediterranean Europe, with particular focus on Portugal, Spain, France and Italy, using data from the European Forest Fire Information System and national wildfire databases. This is the first time that the CI has been applied to wildfire events. This research shows that, in most Mediterranean European countries, the amount of BA is increasingly related with a lower number of fires. The spatio-temporal distribution of CIB shows high variability in all of the countries analysed in Europe. Portugal and Spain show increasing significant trends of CIB +7.6% (p-value = 0.001) and +1.3% per decade (p-value = 0.003). Statistically significant correlations for Portugal, Spain and Italy are also found between the annual CIB and several teleconnection indices. The application of the CIB demonstrates its discriminatory ability, which is a key point in detecting vulnerable areas and temporal trends under climate change.","tags":["incendio forestal","√≠ndice de concentraci√≥n","Europa","teleconexi√≥n","patrones espacio-temporales"],"title":"Wildfire burnt area patterns and trends in Western Mediterranean Europe via the application of a concentration index","type":"publication"},{"authors":["D Roy√©","A Tob√≠as","C I√±iguez"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1581033600,"objectID":"d9a5421286714af3d1a6ee2d3589aab5","permalink":"https://dominicroye.github.io/es/publication/2020-era5-reanalysis-mortality-environmental-research/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/es/publication/2020-era5-reanalysis-mortality-environmental-research/","section":"publication","summary":"Background: Most studies use temperature observation data from weather stations near the analyzed region or city as the reference point for the exposure-response association. Climatic reanalysis data sets have already been used for climate studies, but are not yet used routinely in environmental epidemiology. Methods: We compared the mortality-temperature association using weather station temperature and ERA-5 reanalysis data for the 52 provincial capital cities in Spain, using time-series regression with distributed lag non-linear models. Results: The shape of temperature distribution is very close between the weather station and ERA-5 reanalysis data (correlation from 0.90 to 0.99). The overall cumulative exposure-response curves are very similar in their shape and risks estimates for cold and heat effects, although risk estimates for ERA-5 were slightly lower than for weather station temperature. Conclusions: Reanalysis data allow the estimation of the health effects of temperature, even in areas located far from weather stations or without any available.","tags":["Espa√±a","temperatura","rean√°lisis","ERA-5","mortalidad","estaciones meteorol√≥gicas"],"title":"Comparison of temperature-mortality associations using observed weather station and reanalysis data in 52 Spanish cities","type":"publication"},{"authors":null,"categories":["an√°lisis espacial","R","R:principante","gis"],"content":"\r\rEl primer post del a√±o 2020, lo dedicar√© a una consulta que me hicieron recientemente. Me plantearon la pregunta de c√≥mo se podr√≠a calcular la distancia m√°s corta entre diferentes puntos y c√≥mo saber c√∫al es el punto m√°s pr√≥ximo a uno dado. Cuando trabajamos con datos espaciales en R, en la actualidad lo m√°s f√°cil es usar el paquete sf en combinaci√≥n con la colecci√≥n de paquetes tidyverse. Adem√°s usamos el paquete units que es muy √∫til para trabajar con unidades de medida.\nPaquetes\r\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\runits\rProporciona unidades de medida para vectores R: conversi√≥n, derivaci√≥n, simplificaci√≥n\r\rmaps\rMapas geogr√°ficos y conjuntos de datos\r\rrnaturalearth\rMapas vectoriales del mundo ‚ÄòNatural Earth‚Äô\r\r\r\r# instalamos los paquetes necesarios\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;units\u0026quot;)) install.packages(\u0026quot;units\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;maps\u0026quot;)) install.packages(\u0026quot;maps\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r# cargamos los paquetes\rlibrary(maps)\rlibrary(sf) library(tidyverse)\rlibrary(units)\rlibrary(rnaturalearth)\r\rUnidades de medida\rEl uso de vectores y matrices de clase units nos permite calcular y transformar unidades de medida.\n# longitud\rl \u0026lt;- set_units(1:10, m)\rl\r## Units: [m]\r## [1] 1 2 3 4 5 6 7 8 9 10\r# convertir a otras unidades\rset_units(l, cm)\r## Units: [cm]\r## [1] 100 200 300 400 500 600 700 800 900 1000\r# sumar diferentes unidades\rset_units(l, cm) + l\r## Units: [cm]\r## [1] 200 400 600 800 1000 1200 1400 1600 1800 2000\r# area\ra \u0026lt;- set_units(355, ha)\rset_units(a, km2)\r## 3.55 [km2]\r# velocidad\rvel \u0026lt;- set_units(seq(20, 50, 10), km/h)\rset_units(vel, m/s)\r## Units: [m/s]\r## [1] 5.555556 8.333333 11.111111 13.888889\r\rCapitales del mundo\rVamos a usar las capitales de todo el mundo con el objetivo de calcular la distancia a la capital m√°s pr√≥xima y indicar el nombre de la ciudad.\n# conjunto de ciudades del mundo con coordenadas\rhead(world.cities) # proviene del paquete maps\r## name country.etc pop lat long capital\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 31.31 34.34 0\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 31.32 34.35 0\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 30.55 72.11 0\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 29.36 47.98 0\r## 5 \u0026#39;Abud Palestine 2456 32.03 35.07 0\r## 6 \u0026#39;Abwein Palestine 3434 32.03 35.20 0\rPara convertir puntos con longitud y latitud en un objeto espacial de clase sf, empleamos la funci√≥n st_as_sf(), indicando las columnas de las coordenadas y el sistema de referencia de coordenadas (WSG84, epsg:4326).\n# convertimos los puntos en un objeto sf con CRS WSG84\rcities \u0026lt;- st_as_sf(world.cities, coords = c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;), crs = 4326)\rcities\r## Simple feature collection with 43645 features and 4 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -178.8 ymin: -54.79 xmax: 179.81 ymax: 78.93\r## Geodetic CRS: WGS 84\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 0 POINT (34.34 31.31)\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 0 POINT (34.35 31.32)\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 0 POINT (72.11 30.55)\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 0 POINT (47.98 29.36)\r## 5 \u0026#39;Abud Palestine 2456 0 POINT (35.07 32.03)\r## 6 \u0026#39;Abwein Palestine 3434 0 POINT (35.2 32.03)\r## 7 \u0026#39;Adadlay Somalia 9198 0 POINT (44.65 9.77)\r## 8 \u0026#39;Adale Somalia 5492 0 POINT (46.3 2.75)\r## 9 \u0026#39;Afak Iraq 22706 0 POINT (45.26 32.07)\r## 10 \u0026#39;Afif Saudi Arabia 41731 0 POINT (42.93 23.92)\rEn el pr√≥ximo paso, simplemente filtramos por las capitales codificadas en la columna capital con 1. La ventaja del paquete sf es la posibilidad de aplicar funciones de la colecci√≥n tidyverse para manipular los atributos. Adem√°s, a√±adimos una columna con nuevas etiquetas usando la funci√≥n str_c() del paquete stringr, la c√∫al es similar a la de R Base paste().\n# filtramos por las capitales\rcapitals \u0026lt;- filter(cities, capital == 1)\r# creamos una nueva etiqueta combinando nombre y pa√≠s\rcapitals \u0026lt;- mutate(capitals, city_country = str_c(name, \u0026quot; (\u0026quot;, country.etc, \u0026quot;)\u0026quot;))\rcapitals \r## Simple feature collection with 230 features and 5 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -176.13 ymin: -51.7 xmax: 179.2 ymax: 78.21\r## Geodetic CRS: WGS 84\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Amman Jordan 1303197 1 POINT (35.93 31.95)\r## 2 Abu Dhabi United Arab Emirates 619316 1 POINT (54.37 24.48)\r## 3 Abuja Nigeria 178462 1 POINT (7.17 9.18)\r## 4 Accra Ghana 2029143 1 POINT (-0.2 5.56)\r## 5 Adamstown Pitcairn 51 1 POINT (-130.1 -25.05)\r## 6 Addis Abeba Ethiopia 2823167 1 POINT (38.74 9.03)\r## 7 Agana Guam 1041 1 POINT (144.75 13.47)\r## 8 Algiers Algeria 2029936 1 POINT (3.04 36.77)\r## 9 Alofi Niue 627 1 POINT (-169.92 -19.05)\r## 10 Amsterdam Netherlands 744159 1 POINT (4.89 52.37)\r## city_country\r## 1 \u0026#39;Amman (Jordan)\r## 2 Abu Dhabi (United Arab Emirates)\r## 3 Abuja (Nigeria)\r## 4 Accra (Ghana)\r## 5 Adamstown (Pitcairn)\r## 6 Addis Abeba (Ethiopia)\r## 7 Agana (Guam)\r## 8 Algiers (Algeria)\r## 9 Alofi (Niue)\r## 10 Amsterdam (Netherlands)\r\rCalcular distancias\rLa distancia geogr√°fica (euclidiana o de gran c√≠rculo) se calcula con la funci√≥n st_distance(), o bien entre dos puntos, entre un punto y otros m√∫ltiples o entre todos. En el √∫ltimo caso obtenemos una matriz sim√©trica de distancias (NxN), tomados por pares de un conjunto. En la diagonal encontramos las combinaciones entre los mismos puntos dando todas nulas.\n\r\r\rA\rB\rC\r\rA\r0\r310\r323\r\rB\r310\r0\r133\r\rC\r323\r133\r0\r\r\r\rCuando queremos saber, por ejemplo, la distancia de Amsterdam a Abu Dhabi, Washington y Tokyo pasamos dos objetos espaciales.\n# calcular la distancia\rdist_amsterdam \u0026lt;- st_distance(slice(capitals, 10), slice(capitals, c(2, 220, 205)))\rdist_amsterdam # distancia en metros\r## Units: [m]\r## [,1] [,2] [,3]\r## [1,] 5163124 6187634 9293710\rEl resultado es una matriz de una fila o de una columna (en funci√≥n del orden de los objetos) con clase de units. As√≠ es posible cambiar f√°cilmente a otra unidad de medida. Si queremos obtener un vector sin clase units, √∫nicamente aplicamos la funci√≥n as.vector().\n# cambiamos de m a km\rset_units(dist_amsterdam, \u0026quot;km\u0026quot;)\r## Units: [km]\r## [,1] [,2] [,3]\r## [1,] 5163.124 6187.634 9293.71\r# units class a vector\ras.vector(dist_amsterdam)\r## [1] 5163124 6187634 9293710\rA continuaci√≥n estimamos la matriz de distancia entre todas las capitales. Es importante convertir los valores nulos a NA para obtener posteriormente el √≠ndice correcto de la matriz.\n# calcular la distancia\rm_distance \u0026lt;- st_distance(capitals)\r# matriz\rdim(m_distance)\r## [1] 230 230\r# cambiamos de m a km\rm_distance_km \u0026lt;- set_units(m_distance, km)\r# reemplazamos la distance de 0 con NA\rm_distance_km[m_distance_km == set_units(0, km)] \u0026lt;- NA\r Cuando el resultado es de clase units es necesario usar la misma clase para poder hacer consultas logicas. Por ejemplo, set_units(1, m) == set_units(1, m) vs.¬†set_units(1, m) == 1.   Con el objetivo de obtener la distancia m√°s corta, adem√°s de la posici√≥n de la misma, usamos la funci√≥n apply() que a su vez nos permite aplicar la funci√≥n which.min() y min() sobre cada fila. Tambi√©n ser√≠a posible emplear la funci√≥n sobre columnas que dar√≠a el mismo resulado. Para finalizar, a√±adimos los resultados como nuevas columnas con la funci√≥n mutate(). Las posiciones en pos nos permiten obtener los nombres de las ciudades m√°s pr√≥ximas.\n# obtenemos la posici√≥n de la ciudad y la distancia\rpos \u0026lt;- apply(m_distance_km, 1, which.min)\rdist \u0026lt;- apply(m_distance_km, 1, min, na.rm = TRUE)\r# a√±adimos la distancia y obtenemos el nombre de la ciudad\rcapitals \u0026lt;- mutate(capitals, nearest_city = city_country[pos], geometry_nearest = geometry[pos],\rdistance_city = dist)\r\rMapa de distancias a la pr√≥xima capital\rPor √∫ltimo, construimos un mapa representando la distancia en circulos proporcionales. Para ello, usamos la gram√°tica habitual de ggplot() a√±adiendo la geometr√≠a geom_sf(), primero para el mapamundi de fondo y despu√©s para los circulos de las ciudades. En aes() indicamos con el argumento size = distance_city la variable que debe ser mapeado proporcionalmente. La funci√≥n theme_void() elimina todos los elementos de estilo. Adem√°s, definimos con la funci√≥n coord_sf() una nueva proyecci√≥n indicando el formato proj4.\n# mapamundi\rworld \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;)\r# mapa ggplot(world) +\rgeom_sf(fill = \u0026quot;black\u0026quot;, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = capitals, aes(size = distance_city),\ralpha = 0.7,\rfill = \u0026quot;#bd0026\u0026quot;,\rshape = 21,\rshow.legend = \u0026#39;point\u0026#39;) +\rcoord_sf(crs = \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\u0026quot;) +\rlabs(size = \u0026quot;Distance (km)\u0026quot;, title = \u0026quot;Distance to the next capital\u0026quot;) +\rtheme_void()\r\n\r","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1579392000,"objectID":"a44ced12e2fc0e7bb986f86b611ad4bb","permalink":"https://dominicroye.github.io/es/2020/distancias-geograficas/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/es/2020/distancias-geograficas/","section":"post","summary":"El primer post del a√±o 2020, lo dedicar√© a una consulta que me hicieron recientemente. Me plantearon la pregunta de c√≥mo se podr√≠a calcular la distancia m√°s corta entre diferentes puntos y c√≥mo saber c√∫al es el punto m√°s pr√≥ximo a uno dado. Cuando trabajamos con datos espaciales en R, en la actualidad lo m√°s f√°cil es usar el paquete ``sf`` en combinaci√≥n con la colecci√≥n de paquetes ``tidyverse``. Adem√°s usamos el paquete ``units`` que es muy √∫til para trabajar con unidades de medida.","tags":["distancia","puntos","ciudades"],"title":"Distancias geogr√°ficas","type":"post"},{"authors":["F Tedim","V Leone","M Coughlan","C Bouillon","G Xanthopoulos","D Roy√©","F.J.M. Correia","C Ferreira"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1577836800,"objectID":"3a9da827765883a44fd563da52d3563f","permalink":"https://dominicroye.github.io/es/publication/2019-extreme-wildfire-definitions-elsevier/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/es/publication/2019-extreme-wildfire-definitions-elsevier/","section":"publication","summary":"Extreme wildfires events (EWEs) represent a minority among all wildfires but are a true challenge for societies as they exceed the current control capacity even in the best prepared regions of the world and they create destruction and a disproportionately number of fatalities. Recent events in Portugal, Chile, Greece, Australia, Canada, and the USA provide evidence that EWEs are an escalating worldwide problem, exceeding all previous records. Despite the challenges put by climate change, the occurrence of EWEs and disasters is not an ecological inevitability. In this chapter the rationale of the definition of EWEs and the integration of potential consequences on people and assets in a novel wildfire classification scheme are proposed and discussed. They are excellent instruments to enhance wildfire risk and crisis communication programs and to define appropriate prevention, mitigation, and response measures which are crucial to build up citizens safety.","tags":["capacidad de control","evento de desastre extremo de incendios forestales (EWE)","intensidad del fuego","mitigaci√≥n","preparaci√≥n","prevenci√≥n","tasa de propagaci√≥n","sistema socioecon√≥mico (SES)","clasificaci√≥n de incendios forestales"],"title":"Extreme wildfire events: the definition","type":"publication"},{"authors":["D Roy√©","R Codesido","A Tob√≠as","M Taracido"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1577836800,"objectID":"c7d58cef5a9424f0434cf5b5e6ddf591","permalink":"https://dominicroye.github.io/es/publication/2020-ehf-mortalidad-environmental-research/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/es/publication/2020-ehf-mortalidad-environmental-research/","section":"publication","summary":"In the current context of climate change, heat waves have become a significant problem for human health. This study assesses the effects of heat wave intensity on mortality (natural, respiratory and cardiovascular causes) in four of the largest cities of Spain (Barcelona, Bilbao, Madrid and Seville) during the period between 1990 and 2014. To model the heat wave severity the Excess Heat Factor (EHF) was used. The EHF is a two-component index. The first is the comparison of the three-day average daily mean temperature with the 95th percentile. The second component is a measure of the temperatures reached during the three-day period compared with the recent past (the previous 30 days). The city-specific exposure-response curves showed a non-linear J-shaped relationship between mortality and the EHF. Overall city-specific mortality risk estimates for 1th vs. 99th percentile increases range from the highest mortality risk with 2.73 (95% CI: 2.34-3.18) in Seville to a risk of 1.78 (95% CI: 1.62-1.97) and 1.78 (95% CI: 1.45-2.19) in Barcelona and Bilbao, respectively. When we compare our results with risk estimates for the analyzed Spanish cities in other studies, the heat wave related mortality risks seem to be clearly higher. Furthermore, it has been demonstrated that different heat wave days of the same event do not present the same degree of severity/intensity. Thus, the intensity of a heat wave is an important mortality risk indicator during heat wave days. Due to the low number of studies on the EHF as a heat wave intensity indicator and heat-related mortality and morbidity, further research is required to validate its application in other geographic areas and focus populations.","tags":["Espa√±a","ola de calor","factor de exceso de calor","mortalidad","temperatura extrema"],"title":"Heat wave intensity and daily mortality in four of the largest cities of Spain","type":"publication"},{"authors":null,"categories":["visualizaci√≥n","R","R:principante","gis"],"content":"\r\rLa Direcci√≥n General del Catastro de Espa√±a dispone de informaci√≥n espacial de toda la edificaci√≥n a excepci√≥n del Pa√≠s Vasco y Navarra. Este conjunto de datos forma parte de la implantaci√≥n de INSPIRE, la Infraestructura de Informaci√≥n Espacial en Europa. M√°s informaci√≥n podemos encontrar aqu√≠. Utilizaremos los enlaces (urls) en formato ATOM, que es un formato de redifusi√≥n de tipo RSS, permiti√©ndonos obtener el enlace de descarga para cada municipio.\n Esta entrada de blog es una versi√≥n reducida del caso pr√°ctico que pod√©is encontrar en nuestra reciente publicaci√≥n - Introducci√≥n a los SIG con R - publicado por Dominic Roy√© y Roberto Serrano-Notivoli.   Paquetes\r\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rfs\rProporciona una interfaz uniforme y multiplataforma para las operaciones del sistema de archivos\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rfeedeR\rImportar formatos de redifusi√≥n RSS\r\rtmap\rF√°cil creaci√≥n de mapas tem√°ticos\r\rclassInt\rPara crear intervalos de clase univariantes\r\rsysfonts\rCarga familias tipogr√°ficas del sistema y de Google\r\rshowtext\rUsar familias tipogr√°ficas m√°s f√°cilmente en gr√°ficos R\r\r\r\r# instalamos los paquetes necesarios\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;feedeR\u0026quot;)) install.packages(\u0026quot;feedeR\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;tmap\u0026quot;)) install.packages(\u0026quot;tmap\u0026quot;)\rif(!require(\u0026quot;classInt\u0026quot;)) install.packages(\u0026quot;classInt\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;rvest\u0026quot;)) install.packages(\u0026quot;rvest\u0026quot;)\r# cargamos los paquetes\rlibrary(feedeR)\rlibrary(sf) library(fs)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(classInt)\rlibrary(tmap)\rlibrary(rvest)\r\rEnlaces de descarga\rLa primera url nos dar√° acceso a un listado de provincias, sedes territoriales (no siempre coinciden con la provincia), con nuevos enlaces RSS los cuales incluyen los enlaces finales de descarga para cada municipio. En este caso, descargaremos el edificado de Valencia. Los datos del Catastro se actualizan cada seis meses.\nurl \u0026lt;- \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.bu.atom.xml\u0026quot;\r# importamos los RSS con enlaces de provincias\rprov_enlaces \u0026lt;- feed.extract(url)\rstr(prov_enlaces) #estructura es lista\r## List of 4\r## $ title : chr \u0026quot;Download service of Buildings. Territorial Office\u0026quot;\r## $ link : chr \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.BU.atom.xml\u0026quot;\r## $ updated: POSIXct[1:1], format: \u0026quot;2022-03-14\u0026quot;\r## $ items : tibble [52 x 5] (S3: tbl_df/tbl/data.frame)\r## ..$ title : chr [1:52] \u0026quot;Territorial office 02 Albacete\u0026quot; \u0026quot;Territorial office 03 Alicante\u0026quot; \u0026quot;Territorial office 04 Almer√≠a\u0026quot; \u0026quot;Territorial office 05 Avila\u0026quot; ...\r## ..$ date : POSIXct[1:52], format: \u0026quot;2022-03-14\u0026quot; \u0026quot;2022-03-14\u0026quot; ...\r## ..$ link : chr [1:52] \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/02/ES.SDGC.bu.atom_02.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/03/ES.SDGC.bu.atom_03.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/04/ES.SDGC.bu.atom_04.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/05/ES.SDGC.bu.atom_05.xml\u0026quot; ...\r## ..$ description: chr [1:52] \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; ...\r## ..$ hash : chr [1:52] \u0026quot;d21ebb7975e59937\u0026quot; \u0026quot;bdba5e149f09e9d8\u0026quot; \u0026quot;03bcbcc7c5be2e17\u0026quot; \u0026quot;8a154202dd778143\u0026quot; ...\r# extraemos la tabla con los enlaces\rprov_enlaces_tab \u0026lt;- as_tibble(prov_enlaces$items) %\u0026gt;% mutate(title = repair_encoding(title))\r## Warning: `html_encoding_repair()` was deprecated in rvest 1.0.0.\r## Instead, re-load using the `encoding` argument of `read_html()`\r## This warning is displayed once every 8 hours.\r## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\r## Best guess: UTF-8 (100% confident)\rprov_enlaces_tab\r## # A tibble: 52 x 5\r## title date link description hash ## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;\r## 1 \u0026quot;Territorial office 02 Albacete\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ d21e~\r## 2 \u0026quot;Territorial office 03 Alicante\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ bdba~\r## 3 \u0026quot;Territorial office 04 Almer√≠a\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 03bc~\r## 4 \u0026quot;Territorial office 05 Avila\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 8a15~\r## 5 \u0026quot;Territorial office 06 Badajoz\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 7d3f~\r## 6 \u0026quot;Territorial office 07 Baleares \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 9c08~\r## 7 \u0026quot;Territorial office 08 Barcelona\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ ff72~\r## 8 \u0026quot;Territorial office 09 Burgos \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ b431~\r## 9 \u0026quot;Territorial office 10 C√°ceres \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ f79c~\r## 10 \u0026quot;Territorial office 11 C√°diz \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ d702~\r## # ... with 42 more rows\rAccedemos y descargamos los datos de Valencia. Para encontrar el enlace final de descarga usamos la funci√≥n filter() del paquete dplyr buscando el nombre de la sede territorial y posteriormente el nombre del municipio en may√∫sculas con la funci√≥n str_detect() de stringr. La funci√≥n pull() nos permite extraer una columna de un data.frame.\n Actualmente la funci√≥n feed.extract() no importa correctamente en el encoding UTF-8 (Windows). Por eso, en algunas ciudades pueden aparecer una mala codificaci√≥n de caracteres especiales ‚ÄúC√É¬°diz‚Äù. Para subsanar este problema aplicamos la funci√≥n repair_encoding() del paquete rvest. √Å√∫n as√≠ pueden surgir problemas que deban corregirse manualmente.   # filtramos la provincia y obtenemos la url RSS\rval_atom \u0026lt;- filter(prov_enlaces_tab, str_detect(title, \u0026quot;Valencia\u0026quot;)) %\u0026gt;% pull(link)\r# importamos la RSS\rval_enlaces \u0026lt;- feed.extract(val_atom)\r# obtenemos la tabla con los enlaces de descarga\rval_enlaces_tab \u0026lt;- val_enlaces$items\rval_enlaces_tab \u0026lt;- mutate(val_enlaces_tab, title = repair_encoding(title),\rlink = repair_encoding(link)) \r## Best guess: UTF-8 (80% confident)\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r## Best guess: UTF-8 (80% confident)\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r# filtramos la tabla con el nombre de la ciudad\rval_link \u0026lt;- filter(val_enlaces_tab, str_detect(title, \u0026quot;VALENCIA\u0026quot;)) %\u0026gt;% pull(link)\rval_link\r## [1] \u0026quot;http://www.catastro.minhap.es/INSPIRE/Buildings/46/46900-VALENCIA/A.ES.SDGC.BU.46900.zip\u0026quot;\r\rDescarga de datos\rLa descarga se realiza con la funci√≥n download.file() que √∫nicamente tiene dos argumentos principales, el enlace de descarga y la ruta con el nombre del archivo. En este caso hacemos uso de la funci√≥n tempfile(), que nos es √∫til para crear archivos temporales, es decir, archivos que √∫nicamente existen en la memor√≠a RAM por un tiempo determinado.\rEl archivo que descargamos tiene extensi√≥n *.zip, por lo que debemos descomprimirlo con otra funci√≥n (unzip()), que requiere el nombre del archivo y el nombre de la carpeta donde lo queremos descomprimir. Por √∫ltimo, la funci√≥n URLencode() codifica una direcci√≥n URL que contiene caracteres especiales.\n# creamos un archivo temporal temp \u0026lt;- tempfile()\r# descargamos los datos\rdownload.file(URLencode(val_link), temp)\r# descomprimimos a una carpeta llamda buildings\runzip(temp, exdir = \u0026quot;buildings_valencia\u0026quot;) # cambia el nombre seg√∫n la ciudad\r\rImportar los datos\rPara importar los datos utilizamos la funci√≥n dir_ls() del paquete fs, que nos permite obtener los archivos y carpetas de una ruta concreta al mismo tiempo que filtramos por un patr√≥n de texto (regexp: expresi√≥n regular). Aplicamos la funci√≥n st_read() del paquete sf al archivo espacial de formato Geography Markup Language (GML).\n# obtenemos la ruta con el archivo\rfile_val \u0026lt;- dir_ls(\u0026quot;buildings_valencia\u0026quot;, regexp = \u0026quot;building.gml\u0026quot;) # cambia el nombre de carpeta si es necesario\r# importamos los datos\rbuildings_val \u0026lt;- st_read(file_val)\r## Reading layer `Building\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\es\\post\\2019-11-01-visualizar-crecimiento-urbano\\buildings_valencia\\A.ES.SDGC.BU.46900.building.gml\u0026#39; ## using driver `GML\u0026#39;\r## Simple feature collection with 36284 features and 24 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: 720570.9 ymin: 4351286 xmax: 734981.9 ymax: 4382906\r## Projected CRS: ETRS89 / UTM zone 30N\r\rPreparaci√≥n de los datos\r√önicamente convertimos la columna de la edad del edificio (beginning) en clase Date. La columna de la fecha contiene algunas fechas en formato --01-01 lo que no corresponde a ninguna fecha reconocible. Por eso, reemplazamos el primer - por 0000.\n# buildings_val \u0026lt;- mutate(buildings_val, beginning = str_replace(beginning, \u0026quot;^-\u0026quot;, \u0026quot;0000\u0026quot;) %\u0026gt;% ymd_hms() %\u0026gt;% as_date()\r)\r## Warning: 4 failed to parse.\r\rGr√°fico de distribuci√≥n\rAntes de crear el mapa de la edad del edificado, lo que reflejar√° el crecimiento urbano, haremos un gr√°fico de distribuci√≥n de la fecha de construcci√≥n de los edificios. Podremos identificar claramente per√≠odos de expansi√≥n urbana. Usaremos el paquete ggplot2 con la geometr√≠a de geom_density() para este objetivo. La funci√≥n font_add_google() del paquete sysfonts nos permite descargar e incluir familias tipogr√°ficas de Google.\n#descarga de familia tipogr√°fica\rsysfonts::font_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r#usar showtext para familias tipogr√°ficas\rshowtext::showtext_auto() \r#limitamos al periodo posterior a 1750\rfilter(buildings_val, beginning \u0026gt;= \u0026quot;1750-01-01\u0026quot;) %\u0026gt;%\rggplot(aes(beginning)) + geom_density(fill = \u0026quot;#2166ac\u0026quot;, alpha = 0.7) +\rscale_x_date(date_breaks = \u0026quot;20 year\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) +\rtheme_minimal(base_family = \u0026quot;Montserrat\u0026quot;) +\rlabs(y = \u0026quot;\u0026quot;,x = \u0026quot;\u0026quot;, title = \u0026quot;Evoluci√≥n del desarrollo urbano\u0026quot;)\r\rBuffer de 2,5 km de Valencia\rPara poder visualizar bien la distribuci√≥n del crecimiento, limitamos el mapa a un radio de 2,5 km desde el centro de la ciudad. Usamos la funci√≥n geocode_OSM() del paquete tmaptools para obtener las coordenadas de Valencia en clase sf. Despu√©s proyectamos los puntos al sistema que usamos para el edificado (EPSG:25830). La funci√≥n st_crs() nos devuelve el sistema de coordenadas de un objeto espacial sf. Como √∫ltimo paso creamos con la funci√≥n st_buffer() un buffer con 2500 m y la intersecci√≥n con nuestros datos de los edificios. Tambi√©n es posible crear un buffer en forma de un rect√°ngulo indicando el tipo de estilo con el argumento endCapStyle = \"SQUARE\".\n# obtenemos las coordinadas de Valencia\rciudad_point \u0026lt;- tmaptools::geocode_OSM(\u0026quot;Valencia\u0026quot;, as.sf = TRUE)\r# proyectamos los datos\rciudad_point \u0026lt;- st_transform(ciudad_point, st_crs(buildings_val))\r# creamos un buffer\rpoint_bf \u0026lt;- st_buffer(ciudad_point, 2500) # radio de 2500 m\r# obtenemos la intersecci√≥n entre el buffer y la edificaci√≥n\rbuildings_val25 \u0026lt;- st_intersection(buildings_val, point_bf)\r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r\rPreparar los datos para el mapas\rPara poder visualizar bien las diferentes √©pocas de crecimiento, categorizamos el a√±o en 15 grupos empleando cuartiles. Tambi√©n es posible modificar el n√∫mero de clases o bien el m√©todo aplicado (p.j. jenks, fisher, etc), m√°s detalles encontr√°is en la ayuda ?classIntervals.\n#encontrar 15 clases\rbr \u0026lt;- classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;)\r## Warning in classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;): var\r## has missing values, omitted in finding classes\r#crear etiquetas\rlab \u0026lt;- names(print(br, under = \u0026quot;\u0026lt;\u0026quot;, over = \u0026quot;\u0026gt;\u0026quot;, cutlabels = FALSE))\r## style: quantile\r## \u0026lt; 1890 1890 - 1912 1912 - 1925 1925 - 1930 1930 - 1940 1940 - 1950 ## 932 1350 947 594 1703 1054 ## 1950 - 1958 1958 - 1962 1962 - 1966 1966 - 1970 1970 - 1973 1973 - 1978 ## 1453 1029 1223 1158 1155 1190 ## 1978 - 1988 1988 - 1999 \u0026gt; 1999 ## 1149 1111 1244\r#categorizar el a√±o\rbuildings_val25 \u0026lt;- mutate(buildings_val25, yr_cl = cut(year(beginning), br$brks, labels = lab, include.lowest = TRUE))\r\rMapa de Valencia\rEl mapa creamos con el paquete tmap. Es una interesante alternativa a ggplot2. Se trata de un paquete de funciones especializadas en crear mapas tem√°ticos. La filosof√≠a del paquete sigue a la de ggplot2, creando multiples capas con diferentes funciones, que siempre empiezan con tm_* y se combinan con +. La construcci√≥n de un mapa con tmap siempre comienza con tm_shape(), donde se definen los datos que queremos dibujar. Luego agregamos la geometr√≠a correspondiente al tipo de datos (tm_polygon(), tm_border(), tm_dots() o incluso tm_raster()). La funci√≥n tm_layout() ayuda a configurar el estilo del mapa.\nCuando necesitamos m√°s colores del m√°ximo permitido por RColorBrewer podemos pasar los colores a la funci√≥n colorRampPalette(). Esta funci√≥n interpola para un mayor n√∫mero m√°s colores de la gama.\n#colores\rcol_spec \u0026lt;- RColorBrewer::brewer.pal(11, \u0026quot;Spectral\u0026quot;)\r#funci√≥n de una gama de colores\rcol_spec_fun \u0026lt;- colorRampPalette(col_spec)\r#crear los mapas\rtm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15), # adapta al n√∫mero clases\rtextNA = \u0026quot;Sin dato\u0026quot;,\rtitle = \u0026quot;\u0026quot;) +\rtm_layout(bg.color = \u0026quot;black\u0026quot;,\router.bg.color = \u0026quot;black\u0026quot;,\rlegend.outside = TRUE,\rlegend.text.color = \u0026quot;white\u0026quot;,\rlegend.text.fontfamily = \u0026quot;Montserrat\u0026quot;, panel.label.fontfamily = \u0026quot;Montserrat\u0026quot;,\rpanel.label.color = \u0026quot;white\u0026quot;,\rpanel.label.bg.color = \u0026quot;black\u0026quot;,\rpanel.label.size = 5,\rpanel.label.fontface = \u0026quot;bold\u0026quot;)\rPodemos exportar nuestro mapa usando la funci√≥n tmap_save(\"nombre.png\", dpi = 300). Recomiendo usar el argumento dpi = 300 para obtener una buena calidad de imagen.\nUna alternativa al paquete tmap es ggplot2.\n#crear el mapa\rggplot(buildings_val25) +\rgeom_sf(aes(fill = yr_cl), colour = \u0026quot;transparent\u0026quot;) +\rscale_fill_manual(values = col_spec_fun(15)) + # adapta al n√∫mero clases\rlabs(title = \u0026quot;VAL√àNCIA\u0026quot;, fill = \u0026quot;\u0026quot;) +\rguides(fill = guide_legend(keywidth = .7, keyheight = 2.7)) +\rtheme_void(base_family = \u0026quot;Montserrat\u0026quot;) +\rtheme(panel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rlegend.justification = .5,\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;, size = 12),\rplot.title = element_text(colour = \u0026quot;white\u0026quot;, hjust = .5, size = 60,\rmargin = margin(t = 30)),\rplot.caption = element_text(colour = \u0026quot;white\u0026quot;,\rmargin = margin(b = 20), hjust = .5, size = 16),\rplot.margin = margin(r = 40, l = 40))\rPara exportar el resultado de ggplot podemos emplear la funci√≥n ggsave(\"nombre.png\").\n\rMapa din√°mico en leaflet\rUna ventaja muy interesante es la funci√≥n tmap_leaflet() del paquete tmap para pasar de forma sencilla un mapa creado en el mismo marco a leaflet.\n#mapa tmap de Santiago\rm \u0026lt;- tm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15), # adapta al n√∫mero clases\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;)\r#mapa din√°mico\rtmap_leaflet(m)\r\r\n\r","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1572566400,"objectID":"125fc08a29c1485e05a88e58bafc7901","permalink":"https://dominicroye.github.io/es/2019/visualizar-el-crecimiento-urbano/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/es/2019/visualizar-el-crecimiento-urbano/","section":"post","summary":"La Direcci√≥n General del Catastro de Espa√±a dispone de informaci√≥n espacial de toda la edificaci√≥n a excepci√≥n del Pa√≠s Vasco y Navarra. Este conjunto de datos forma parte de la implantaci√≥n de [INSPIRE](https://inspire.ec.europa.eu/), la Infraestructura de Informaci√≥n Espacial en Europa. M√°s informaci√≥n podemos encontrar [aqu√≠](http://www.catastro.meh.es/webinspire/index.html). Utilizaremos los enlaces (*urls*) en formato *ATOM*, que es un formato de redifusi√≥n de tipo RSS, permiti√©ndonos obtener el enlace de descarga para cada municipio.","tags":["crecimiento urbano","ciudad","geografia urbana"],"title":"Visualizar el crecimiento urbano","type":"post"},{"authors":["D Roy√©","R Serrano-Notivoli"],"categories":null,"content":"","date":1570406400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1570406400,"objectID":"5853d900e7df9f45aa72fc9f8af349c1","permalink":"https://dominicroye.github.io/es/publication/2019-manual-introduccion-sig-con-r-publicaciones-unizar/","publishdate":"2019-10-07T00:00:00Z","relpermalink":"/es/publication/2019-manual-introduccion-sig-con-r-publicaciones-unizar/","section":"publication","summary":"R tiene, como lenguaje de programaci√≥n enfocado al an√°lisis estad√≠stico, todos los ingredientes para ser usado como herramienta de an√°lisis espacial y representaci√≥n cartogr√°Ô¨Åca: es gratuito, permite personalizar, replicar y compartir los an√°lisis de cualquier nivel de diÔ¨Åcultad y no tiene ninguna limitaci√≥n en cuanto a cantidad de informaci√≥n a procesar o tipos de formato diferentes para gestionar. Esto le sit√∫a en una situaci√≥n de ventaja que mejora d√≠a a d√≠a, gracias a su amplia comunidad de usuarios, respecto a un SIG (Sistema de Informaci√≥n Geogr√°Ô¨Åca) convencional. Este manual explica, sin necesidad de conocimientos previos, c√≥mo desarrollar con R todos los an√°lisis disponibles en un SIG, con ejemplos sencillos y multitud de casos pr√°cticos. Adem√°s, se muestran las enormes posibilidades de representaci√≥n cartogr√°Ô¨Åca, que van mucho m√°s all√° de la simple creaci√≥n de mapas. R permite, desde exportar a cualquier formato de archivo, hasta crear mapas din√°micos para supublicaci√≥n en Internet.","tags":["R","manual","visualisaci√≥n","SIG"],"title":"Introducci√≥n a los SIG con R","type":"publication"},{"authors":["S Mathbout","JA Lopez-Bustins","D Roy√©","J Martin-Vide","A Benhamrouche"],"categories":null,"content":"","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1565827200,"objectID":"a769dca70eaff7cf8140b44abac4490f","permalink":"https://dominicroye.github.io/es/publication/2019-teleconnections-mediterranean-ij-climatology/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/es/publication/2019-teleconnections-mediterranean-ij-climatology/","section":"publication","summary":"This study has addressed the spatiotemporal distribution of the daily rainfall concentration and its relation to the teleconnection patterns across the Mediterranean (MR). Daily Concentration Index (CI) and the ordered n index () are used at annual time scale to reveal the statistical structure of precipitation across the MR based on 233 daily rainfall series for the period 1975‚Äì2015. Eight teleconnection patterns ,North Atlantic Oscillation (NAO), Mediterranean Oscillation (MO), Western Mediterranean Oscillation (WeMO), Upper Level Mediterranean Oscillation index (ULMO), East Atlantic (EA) pattern, East Atlantic/West Russia (EATL/WRUS) pattern, Scandinavia (SCAND) pattern and Southern Oscillation (SO) at annual time scale are selected. The spatiotemporal patterns in precipitation concentration indices, annual precipitation and their teleconnections with previous large-scale circulations are investigated. Results show a strong connection between the CI and the (r = 0.70, p","tags":["mediterr√°neo","√≠ndice n","√≠ndice de concentraci√≥n","patrones de teleconexi√≥n","precipitaci√≥n diaria"],"title":"Spatiotemporal variability of daily precipitation concentration and its relationship to teleconnection patterns over the Mediterranean during 1975-2015","type":"publication"},{"authors":["D Roy√©","MT Zarrabeitia","P Fdez-Arroyabe","A √Ålvarez-Guti√©rrez","A Santurt√∫n"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1564617600,"objectID":"1f89cdb97a76af08cfdaa3e92bb4071d","permalink":"https://dominicroye.github.io/es/publication/2018-iam-cantabria-rev-esp-cardiologia/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/es/publication/2018-iam-cantabria-rev-esp-cardiologia/","section":"publication","summary":"Introduction and objectives. The role of the environment on cardiovascular health is becoming more prominent in the context of global change. The aim of this study was to analyze the relationship between apparent temperature (AT) and air pollutants and acute myocardial infarction (AMI) and to study the temporal pattern of this disease and its associated mortality. Methods. We performed a time-series study of admissions for AMI in Cantabria between 2001 and 2015. The association between environmental variables (including a biometeorological index, apparent AT) and AMI was analyzed using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of these variables on AMI, a lag non-linear model was fitted in a generalized additive model. Results. The incidence rate and the mortality followed a downward trend during the study period (CC=‚Äì0.714; P=.0002). An annual pattern was found in hospital admissions (P=.005), with the highest values being registered in winter; a weekly trend was also identified, reaching a minimum during the weekends (P=.000005). There was an inverse association between AT and the number of hospital admissions due to AMI and a direct association with particulate matter with a diameter smaller than 10 Œºm. Conclusions. Hospital admissions for AMI followed a downward trend between 2007 and 2015. Mortality associated with admissions due to this diagnosis has decreased. Predictive factors for this disease were AT and particulate matter with a diameter smaller than 10 Œºm.","tags":["infarto agudo del miocardio","temperatura aparente","contaminantes del aire","materia particulada"],"title":"Role of Apparent Temperature and Air Pollutants in Hospital Admissions for Acute Myocardial Infarction in the North of Spain","type":"publication"},{"authors":null,"categories":["visualizaci√≥n","R","R:intermedio"],"content":"\r\rNormalmente cuando visualizamos anomal√≠as de precipitaci√≥n mensual, simplemente usamos un gr√°fico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, no nos explica el contexto general de estas mismas anomal√≠as. Por ejemplo, ¬øcu√°l fue la anomal√≠a m√°s alta o m√°s baja en cada mes? En principio, podr√≠amos usar un boxplot para visualizar la distribuci√≥n de las anomal√≠as, pero en este caso concreto no encajar√≠an bien est√©ticamente, por lo que debemos buscar una alternativa. Aqu√≠ os presento una forma gr√°fica muy √∫til.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rreadr\rImportar datos\r\rggthemes\rEstilos para ggplot2\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\rcowplot\rF√°cil creaci√≥n de m√∫ltiples gr√°ficos con ggplot2\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;cowplot\u0026quot;)) install.packages(\u0026quot;cowplot\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#paquetes\rlibrary(tidyverse) #contiene readr\rlibrary(ggthemes)\rlibrary(cowplot)\rlibrary(lubridate)\r\rPreparar los datos\rPrimero importamos la precipitaci√≥n diaria de la estaci√≥n meteorol√≥gica seleccionada (descarga). Usaremos datos de Santiago de Compostela (Espa√±a) accesible a trav√©s de ECA\u0026amp;D.\nPaso 1: importar los datos\rNo s√≥lo importamos los datos en formato csv, sino tambi√©n hacemos los primeros cambios. Saltamos las primeras 21 filas que contienen informaci√≥n sobre la estaci√≥n meteorol√≥gica. Adem√°s, convertimos la fecha a clase date y reemplazamos valores ausentes (-9999) por NA. La precipitaci√≥n est√° en 0.1 mm, por tanto, debemos dividir los valores por 10. Despu√©s seleccionamos las columnas DATE y RR, y las renombramos.\ndata \u0026lt;- read_csv(\u0026quot;RR_STAID001394.txt\u0026quot;, skip = 21) %\u0026gt;%\rmutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10)) %\u0026gt;%\rselect(DATE:RR) %\u0026gt;% rename(date = DATE, pr = RR)\r## Rows: 27606 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rdata\r## # A tibble: 27,606 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1943-11-01 0.6\r## 2 1943-11-02 0 ## 3 1943-11-03 0 ## 4 1943-11-04 0 ## 5 1943-11-05 0 ## 6 1943-11-06 0 ## 7 1943-11-07 0 ## 8 1943-11-08 0 ## 9 1943-11-09 0 ## 10 1943-11-10 0 ## # ... with 27,596 more rows\r\rPaso 2: crear valores menusales\rEn el segundo paso calculamos las cantidades mensuales de precipitaci√≥n. Para ello, a) limitamos el per√≠odo a los a√±os posteriores a 1950, b) a√±adimos como variable el mes con etiqueta y el a√±o.\ndata \u0026lt;- mutate(data, mo = month(date, label = TRUE), yr = year(date)) %\u0026gt;%\rfilter(date \u0026gt;= \u0026quot;1950-01-01\u0026quot;) %\u0026gt;%\rgroup_by(yr, mo) %\u0026gt;% summarise(prs = sum(pr, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;yr\u0026#39;. You can override using the `.groups` argument.\rdata\r## # A tibble: 833 x 3\r## # Groups: yr [70]\r## yr mo prs\r## \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 ene 55.6\r## 2 1950 feb 349. ## 3 1950 mar 85.8\r## 4 1950 abr 33.4\r## 5 1950 may 272. ## 6 1950 jun 111. ## 7 1950 jul 35.4\r## 8 1950 ago 76.4\r## 9 1950 sep 85 ## 10 1950 oct 53 ## # ... with 823 more rows\r\rPaso 3: estimar las anomal√≠as\rAhora debemos estimar los valores normales de cada mes y unir esta tabla a nuestros datos principales para posteriormente poder calcular la anomal√≠a mensual. Expresamos la anomal√≠a en porcentaje y restamos 100 para fijar el promedio en 0. Adem√°s, creamos una variable que nos indica si la anomal√≠a es negativa o positiva, y otra con la fecha.\npr_ref \u0026lt;- filter(data, yr \u0026gt; 1981, yr \u0026lt;= 2010) %\u0026gt;%\rgroup_by(mo) %\u0026gt;%\rsummarise(pr_ref = mean(prs))\rdata \u0026lt;- left_join(data, pr_ref, by = \u0026quot;mo\u0026quot;)\rdata \u0026lt;- mutate(data, anom = (prs*100/pr_ref)-100, date = str_c(yr, as.numeric(mo), 1, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd(),\rsign= ifelse(anom \u0026gt; 0, \u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;) %\u0026gt;% factor(c(\u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;)))\rYa podemos hacer un primer ensayo de un gr√°fico de anomal√≠as (la versi√≥n cl√°sica), para ello filtramos el a√±o 2018. En este caso usamos la geometr√≠a de barras, recuerda que por defecto la funci√≥n geom_bar() aplica el conteo a la variable. No obstante, en este caso conocemos y, por tanto indicamos con el argumento stat = \"identity\" que debe usar el valor dado en aes().\nfilter(data, yr == 2018) %\u0026gt;%\rggplot(aes(date, anom, fill = sign)) + geom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(-100, 100, 20)) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rlabs(y = \u0026quot;Anomal√≠a de precipitaci√≥n (%)\u0026quot;, x = \u0026quot;\u0026quot;) +\rtheme_hc()\r\rPaso 4: calcular las medidas estad√≠sticas\rEn este √∫ltimo paso estimamos el valor m√°ximo, m√≠nimo, el cuantil 25%/75% y el rango intercuartil por mes de toda la serie temporal.\ndata_norm \u0026lt;- group_by(data, mo) %\u0026gt;%\rsummarise(mx = max(anom),\rmin = min(anom),\rq25 = quantile(anom, .25),\rq75 = quantile(anom, .75),\riqr = q75-q25)\rdata_norm\r## # A tibble: 12 x 6\r## mo mx min q25 q75 iqr\r## \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 ene 193. -89.6 -43.6 56.3 99.9\r## 2 feb 320. -96.5 -51.2 77.7 129. ## 3 mar 381. -100 -40.6 88.2 129. ## 4 abr 198. -93.6 -51.2 17.1 68.3\r## 5 may 141. -90.1 -45.2 17.0 62.2\r## 6 jun 419. -99.3 -58.2 50.0 108. ## 7 jul 311. -98.2 -77.3 27.1 104. ## 8 ago 264. -100 -68.2 39.8 108. ## 9 sep 241. -99.2 -64.9 48.6 113. ## 10 oct 220. -99.0 -54.5 4.69 59.2\r## 11 nov 137. -98.8 -44.0 39.7 83.7\r## 12 dic 245. -91.8 -49.8 36.0 85.8\r\r\rCrear el gr√°fico\rPara crear el gr√°fico de anomal√≠as con leyenda es necesario separar el gr√°fico principal de las leyendas.\nParte 1\rEn esta primera parte vamos a√±adiendo capa por capa los diferentes elementos: 1) el rango de anomal√≠as m√°ximo-m√≠nimo 2) el rango intercuartil 3) las anomal√≠as del a√±o 2018.\n#rango de anomal√≠as m√°ximo-m√≠nimo g1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#a√±adinos el rango intercuartil\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#a√±adimos las anomal√≠as del a√±o 2018\rg1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr == 2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE)\rg1.3\rFinalmente a√±adimos unos √∫ltimos ajustes de estilo.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Anomal√≠a de precipitaci√≥n (%)\u0026quot;,\rbreaks = seq(-100, 500, 25),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Anomal√≠a de precipitaci√≥n en Santiago de Compostela 2018\u0026quot;,\rcaption=\u0026quot;Dominic Roy√© (@dr_xeo) | Datos: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\r\rParte 2\rTodav√≠a nos falta una leyenda. Primero la creamos para los valores normales.\n#datos de la leyenda\rlegend \u0026lt;- filter(data_norm, mo == \u0026quot;ene\u0026quot;)\rlegend_lab \u0026lt;- gather(legend, stat, y, mx:q75) %\u0026gt;%\rmutate(stat = factor(stat, stat, c(\u0026quot;m√°ximo\u0026quot;,\r\u0026quot;m√≠nimo\u0026quot;,\r\u0026quot;Cuantil 25%\u0026quot;,\r\u0026quot;Cuantil 75%\u0026quot;)) %\u0026gt;%\ras.character())\r## Warning: attributes are not identical across measure variables;\r## they will be dropped\r#gr√°fico de la leyenda\rg2 \u0026lt;- legend %\u0026gt;% ggplot()+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;, width = 0.2) +\rgeom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;, width = 0.2) +\rgeom_text(data = legend_lab, aes(x = mo, y = y+c(12,-8,-10,12), label = stat), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.18, y = 40, label = \u0026quot;Per√≠odo 1950-2018\u0026quot;, angle = 90, size = 3) +\rtheme_void() + theme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg2\rSegundo, creamos otra leyenda para las anomal√≠as actuales.\nlegend2 \u0026lt;- filter(data, yr == 1950, mo %in% c(\u0026quot;ene\u0026quot;,\u0026quot;feb\u0026quot;)) %\u0026gt;% ungroup() %\u0026gt;% select(mo, anom, sign)\rlegend2[2,1] \u0026lt;- \u0026quot;ene\u0026quot;\rlegend_lab2 \u0026lt;- data.frame(mo = rep(\u0026quot;ene\u0026quot;, 3), anom= c(110, 3, -70), label = c(\u0026quot;Anomal√≠a positiva\u0026quot;, \u0026quot;Promedio\u0026quot;, \u0026quot;Anomal√≠a negativa\u0026quot;))\rg3 \u0026lt;- ggplot() + geom_bar(data = legend2,\raes(x = mo, y = anom, fill = sign),\ralpha = .6, colour = \u0026quot;NA\u0026quot;, stat = \u0026quot;identity\u0026quot;, show.legend = FALSE, width = 0.2) +\rgeom_segment(aes(x = .85, y = 0, xend = 1.15, yend = 0), linetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = legend_lab2, aes(x = mo, y = anom+c(10,5,-13), label = label), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.25, y = 20, label =\u0026quot;Referencia 1971-2010\u0026quot;, angle = 90, size = 3) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rtheme_void() +\rtheme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg3\r\rParte 3\rPara finalizar s√≥lo debemos unir el gr√°fico y las leyendas con ayuda del paquete cowplot. La funci√≥n princial de cowplot es plot_grid() que ayuda en combinar diferentes gr√°ficos. No obstante, en este caso se hace necesario usar unas funciones m√°s flexibles para crear formatos menos habituales. La funci√≥n ggdraw() configura la capa b√°sica del gr√°fico, y las funciones que est√°n destinadas a operar en esta capa comienzan con draw_*.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .3, width = 1, height = 0.6) +\rdraw_plot(g2, x = 0, y = .15, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .15, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomalia2016_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r\rM√∫ltiples facetas\rEn este apartado haremos el mismo gr√°fico como en el anterior, pero para varios a√±os.\nParte 1\r√önicamente debemos filtrar por un conjunto de a√±os, en este caso de 2016 a 2018, usando el operador %in%, adem√°s a√±adimos la funci√≥n facet_grid() a ggplot, lo que nos permite plotear los gr√°ficos seg√∫n una variable. La formula usada para la funci√≥n de facetas es similar al uso en modelos: variable_por_fila ~ variable_por_columna. Cuando no tenemos una variable en columna debemos usar el ..\n#rango de anomalias maximo-minimo g1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#a√±adinos el rango intercuartil\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#a√±adimos las anomal√≠as del a√±o 2016-2018\rg1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr %in% 2016:2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE) +\rfacet_grid(yr ~ .)\rg1.3\rFinalmente, a√±adimos unos √∫ltimos ajustes de estilo.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Anomal√≠a de precipitaci√≥n (%)\u0026quot;,\rbreaks = seq(-100, 500, 50),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Anomal√≠a de precipitaci√≥n en Santiago de Compostela\u0026quot;,\rcaption=\u0026quot;Dominic Roy√© (@dr_xeo) | Datos: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\rUsamos la misma leyenda universal creada para el gr√°fico anterior.\n\r\rParte 2\rPara finalizar, s√≥lo unimos el gr√°fico y las leyendas con ayuda del paquete cowplot. Lo √∫nico que debemos ajustar aqu√≠ son los argumentos en la funci√≥n draw_plot() para colocar correctamente las diferentes partes.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .18, width = 1, height = 0.8) +\rdraw_plot(g2, x = 0, y = .08, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .08, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomalia20162018_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1562457600,"objectID":"6e12309cec2969a7f723e804916a9e72","permalink":"https://dominicroye.github.io/es/2019/visualizar-anomalias-de-precipitacion-mensual/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/es/2019/visualizar-anomalias-de-precipitacion-mensual/","section":"post","summary":"Normalmente cuando visualizamos anomal√≠as de precipitaci√≥n mensual, simplemente usamos un gr√°fico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, no nos explica el contexto general de estas mismas anomal√≠as. Por ejemplo, ¬øcu√°l fue la anomal√≠a m√°s alta o m√°s baja en cada mes? En principio, podr√≠amos usar un *boxplot* para visualizar la distribuci√≥n de las anomal√≠as, pero en este caso concreto no encajar√≠an bien est√©ticamente, por lo que debemos buscar una alternativa. Aqu√≠ os presento una forma gr√°fica muy √∫til.","tags":["anomal√≠a","precipitaci√≥n","clima","boxplot"],"title":"Visualizar anomal√≠as de precipitaci√≥n mensual","type":"post"},{"authors":["A Mart√≠","J Taboada","D Roy√©","X Fonseca"],"categories":null,"content":"","date":1560384000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1560384000,"objectID":"4435d12996c0b605f0230622ff926476","permalink":"https://dominicroye.github.io/es/publication/2019-book-os-tempos-galicia-xerais/","publishdate":"2019-06-13T00:00:00Z","relpermalink":"/es/publication/2019-book-os-tempos-galicia-xerais/","section":"publication","summary":"Que r√©cords clim√°ticos se alcanzaron en Galicia? Cales son os lugares m√°is calorosos? E os m√°is fr√≠os? Onde chove m√°is? Onde se rexistran m√°is d√≠as de precipitaci√≥n? Que zonas gozan dun maior n√∫mero de horas de sol? Cales te√±en maior nebulosidade? Que lugares son os m√°is ventosos? Como est√° a afectar o cambio clim√°tico a Galicia? Neste libro atopar√°s as respostas a estas e a outras preguntas relacionadas co clima de Galicia e os diversos tipos de tempo que o caracterizan. Nas s√∫as p√°xinas expl√≠case como se producen os fen√≥menos meteorol√≥xicos m√°is habituais no noso territorio: as inversi√≥ns t√©rmicas, as n√©boas costeiras e orogr√°ficas, as illas de calor urbanas, os tipos de precipitaci√≥n, o efecto foehn, as brisas mari√±as, o arco da vella etc. A trav√©s de exemplos concretos, anal√≠zanse tam√©n os riscos clim√°ticos que afectan regularmente a Galicia, como vagas de calor, temporais de neve, ciclox√©neses explosivas e temporais de choiva e vento, tormentas, secas, tornados... Tam√©n poder√°s co√±ecer como est√° a cambiar o clima da nosa comunidade debido ao quecemento global e cales son os escenarios de futuro.","tags":["clima","Galicia","divulgaci√≥n"],"title":"Os tempos e o clima de Galicia","type":"publication"},{"authors":["A V√©lez","J Martin-Vide","D Roy√©","O Santaella"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1556668800,"objectID":"e253b468309e82e9373188106dd0d2ef","permalink":"https://dominicroye.github.io/es/publication/2018-concentration-index-puerto-rico-applied-climatology/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/es/publication/2018-concentration-index-puerto-rico-applied-climatology/","section":"publication","summary":"The present study analyzes spatial patterns of precipitation Concentration Index (CI) in Puerto Rico considering the daily precipitation data of precipitation-gauging stations during 1971-2010. The South and East interior parts of Puerto Rico are characterized by higher CI and the West and North-West parts show lower CI. The annual CI and the rainy season CI show a gradient from South-East to North-West and the dry season CI shows a gradient from South to North. Another difference between the rainy season CI and dry season CI is that the former shows the lowest values of CI while the latter shows the highest values of CI. The different types of seasonal precipitation seem to play a major role on the spatial CI distribution. However, the local relief plays a major role in the spatial patterns due to the effect of the air circulation by the mountains. These findings can contribute to basin-scale water resource management (ooding, soil erosion, etc.) and conservation of the ecological environment.","tags":["√≠ndice de concentraci√≥n","Puerto Rico","precipitaci√≥n","patrones espacio-temporales"],"title":"Spatial Analysis of Daily Precipitation Concentration in Puerto Rico","type":"publication"},{"authors":null,"categories":["estadistica","R","R:avanzado"],"content":"\r\rCuando pretendemos estimar la correlaci√≥n entre m√∫ltiples variables, la tarea se complica para obtener un resultado simple y limpio. Una forma sencilla es usar la funci√≥n tidy() del paquete {broom}. Como ejemplo, en este post vamos a estimar la correlaci√≥n entre la precipitaci√≥n anual de varias ciudades espa√±olas y varios √≠ndices de teleconexiones clim√°ticas: descarga. Los datos de las teleconexiones est√°n preprocesados, pero pueden ser descargados directamente desde crudata.uea.ac.uk. La preciptiaci√≥n diaria proviene de ECA\u0026amp;D.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rbroom\rConvierte resultados de funciones estad√≠sticas (lm, t.test, cor.test, etc.) en bonitas tablas\r\rfs\rProporciona una interfaz uniforme y multiplataforma para las operaciones del sistema de archivos\r\rlubridate\rF√°cil manipulaci√≥n de fechas y tiempos\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;broom\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(broom)\rlibrary(fs)\rlibrary(lubridate)\r\rImportar datos\rPrimero debemos importar la precipitaci√≥n diaria de las estaciones meteorol√≥gicas seleccionadas.\nCreamos un vector con todos los archivos de precipitaci√≥n con la funci√≥n dir_ls() del paquete {fs}.\rImportamos los datos con ayuda de la funci√≥n map_df() del paquete {purrr} que aplica otra funci√≥n a un vector o lista, y los une en una √∫nica tabla.\rSeleccionamos √∫nicamente las columnas que nos interesan, b) Convertimos la fecha en objeto date con la funci√≥n ymd() del paquete {lubridate}, c) Creamos una nueva columna yr con el a√±o, d) Dividimos la precipitaci√≥n entre 10 y reclasificamos valores ausentes -9999 por NA, e) Por √∫ltimo, reclasificamos la ID de cada estaci√≥n meteorol√≥gica, creando un factor con nuevas etiquetas.\r\r\rM√°s detalles sobre el uso de las funciones dir_ls() y map_df() en este √∫ltimo post.\n#archivos de la precipitaci√≥n\rfiles \u0026lt;- dir_ls(regexp = \u0026quot;txt\u0026quot;)\rfiles\r## RR_STAID001393.txt RR_STAID001394.txt RR_STAID002969.txt RR_STAID003946.txt ## RR_STAID003969.txt\r#importamos todos, uni√©ndolos en una √∫nica tabla\rpr \u0026lt;- files %\u0026gt;% map_df(read_csv, skip = 20)\r## Rows: 26329 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 27545 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 34729 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 24927 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 19813 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rpr\r## # A tibble: 133,343 x 5\r## STAID SOUID DATE RR Q_RR\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1393 20611 19470301 0 0\r## 2 1393 20611 19470302 5 0\r## 3 1393 20611 19470303 0 0\r## 4 1393 20611 19470304 33 0\r## 5 1393 20611 19470305 15 0\r## 6 1393 20611 19470306 0 0\r## 7 1393 20611 19470307 85 0\r## 8 1393 20611 19470308 3 0\r## 9 1393 20611 19470309 0 0\r## 10 1393 20611 19470310 0 0\r## # ... with 133,333 more rows\r#creamos los niveles del factor id \u0026lt;- unique(pr$STAID)\r#las etiquetas correspondientes\rlab \u0026lt;- c(\u0026quot;Bilbao\u0026quot;, \u0026quot;Santiago\u0026quot;, \u0026quot;Barcelona\u0026quot;, \u0026quot;Madrid\u0026quot;, \u0026quot;Valencia\u0026quot;)\r#primeros cambios\rpr \u0026lt;- select(pr, STAID, DATE, RR)%\u0026gt;% mutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10), STAID = factor(STAID, id, lab), yr = year(DATE)) pr\r## # A tibble: 133,343 x 4\r## STAID DATE RR yr\r## \u0026lt;fct\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1947-03-01 0 1947\r## 2 Bilbao 1947-03-02 0.5 1947\r## 3 Bilbao 1947-03-03 0 1947\r## 4 Bilbao 1947-03-04 3.3 1947\r## 5 Bilbao 1947-03-05 1.5 1947\r## 6 Bilbao 1947-03-06 0 1947\r## 7 Bilbao 1947-03-07 8.5 1947\r## 8 Bilbao 1947-03-08 0.3 1947\r## 9 Bilbao 1947-03-09 0 1947\r## 10 Bilbao 1947-03-10 0 1947\r## # ... with 133,333 more rows\rLo que todav√≠a nos hace falta es filtrar y calcular la suma anual de precipitaci√≥n. En principio, no es lo m√°s correcto sumar la precipitaci√≥n sin tener en cuenta que haya valores ausentes, pero nos sirve igualmente para este ensayo. Despu√©s, cambiamos el formato de la tabla con la funci√≥n spread(), pasando de una tabla larga a una ancha, es decir, queremos obtener una columna por estaci√≥n meteorol√≥gica.\npr_yr \u0026lt;- filter(pr, DATE \u0026gt;= \u0026quot;1950-01-01\u0026quot;, DATE \u0026lt; \u0026quot;2018-01-01\u0026quot;) %\u0026gt;%\rgroup_by(STAID, yr) %\u0026gt;%\rsummarise(pr = sum(RR, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;STAID\u0026#39;. You can override using the\r## `.groups` argument.\rpr_yr\r## # A tibble: 324 x 3\r## # Groups: STAID [5]\r## STAID yr pr\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1950 1342 ## 2 Bilbao 1951 1306.\r## 3 Bilbao 1952 1355.\r## 4 Bilbao 1953 1372.\r## 5 Bilbao 1954 1428.\r## 6 Bilbao 1955 1062.\r## 7 Bilbao 1956 1254.\r## 8 Bilbao 1957 968.\r## 9 Bilbao 1958 1272.\r## 10 Bilbao 1959 1450.\r## # ... with 314 more rows\rpr_yr \u0026lt;- spread(pr_yr, STAID, pr)\rpr_yr\r## # A tibble: 68 x 6\r## yr Bilbao Santiago Barcelona Madrid Valencia\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA\r## 2 1951 1306. 2344. 1072. 798. NA\r## 3 1952 1355. 1973. 415. 524. NA\r## 4 1953 1372. 973. 683. 365. NA\r## 5 1954 1428. 1348. 581. 246. NA\r## 6 1955 1062. 1769. 530. 473. NA\r## 7 1956 1254. 1533. 695. 480. NA\r## 8 1957 968. 1599. 635. 424. NA\r## 9 1958 1272. 2658. 479. 482. NA\r## 10 1959 1450. 2847. 1006 665. NA\r## # ... with 58 more rows\rEl siguiente paso es importar los √≠ndices de las teleconexiones.\n#teleconexiones\rtelecon \u0026lt;- read_csv(\u0026quot;teleconnections_indices.csv\u0026quot;)\r## Rows: 68 Columns: 9\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (9): yr, NAO, WeMO, EA, POL-EUAS, EATL/WRUS, MO, SCAND, AO\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rtelecon\r## # A tibble: 68 x 9\r## yr NAO WeMO EA `POL-EUAS` `EATL/WRUS` MO SCAND AO\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 0.49 0.555 -0.332 0.0217 -0.0567 0.335 0.301 -0.199 ## 2 1951 -0.07 0.379 -0.372 0.402 -0.419 0.149 -0.00667 -0.365 ## 3 1952 -0.37 0.693 -0.688 -0.0117 -0.711 0.282 0.0642 -0.675 ## 4 1953 0.4 -0.213 -0.727 -0.0567 -0.0508 0.216 0.0233 -0.0164 ## 5 1954 0.51 1.20 -0.912 0.142 -0.318 0.386 0.458 -0.000583\r## 6 1955 -0.64 0.138 -0.824 -0.0267 0.154 0.134 0.0392 -0.362 ## 7 1956 0.17 0.617 -1.29 -0.197 0.0617 0.256 0.302 -0.163 ## 8 1957 -0.02 0.321 -0.952 -0.638 -0.167 0.322 -0.134 -0.342 ## 9 1958 0.12 0.941 -0.243 0.138 0.661 0.296 0.279 -0.868 ## 10 1959 0.49 -0.055 -0.23 -0.0142 0.631 0.316 0.725 -0.0762 ## # ... with 58 more rows\rPor √∫ltimo nos falta unir ambas tablas por a√±o.\ndata_all \u0026lt;- left_join(pr_yr, telecon, by = \u0026quot;yr\u0026quot;)\rdata_all\r## # A tibble: 68 x 14\r## yr Bilbao Santiago Barcelona Madrid Valencia NAO WeMO EA\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA 0.49 0.555 -0.332\r## 2 1951 1306. 2344. 1072. 798. NA -0.07 0.379 -0.372\r## 3 1952 1355. 1973. 415. 524. NA -0.37 0.693 -0.688\r## 4 1953 1372. 973. 683. 365. NA 0.4 -0.213 -0.727\r## 5 1954 1428. 1348. 581. 246. NA 0.51 1.20 -0.912\r## 6 1955 1062. 1769. 530. 473. NA -0.64 0.138 -0.824\r## 7 1956 1254. 1533. 695. 480. NA 0.17 0.617 -1.29 ## 8 1957 968. 1599. 635. 424. NA -0.02 0.321 -0.952\r## 9 1958 1272. 2658. 479. 482. NA 0.12 0.941 -0.243\r## 10 1959 1450. 2847. 1006 665. NA 0.49 -0.055 -0.23 ## # ... with 58 more rows, and 5 more variables: `POL-EUAS` \u0026lt;dbl\u0026gt;,\r## # `EATL/WRUS` \u0026lt;dbl\u0026gt;, MO \u0026lt;dbl\u0026gt;, SCAND \u0026lt;dbl\u0026gt;, AO \u0026lt;dbl\u0026gt;\r\rTest de correlaci√≥n\rUn test de correlaci√≥n lo podemos hacer con la funci√≥n cor.test() de R Base. En este caso entre la precipitaci√≥n anual de Bilbao y el √≠ndice de NAO.\ncor_nao_bil \u0026lt;- cor.test(data_all$Bilbao, data_all$NAO,\rmethod=\u0026quot;spearman\u0026quot;)\r## Warning in cor.test.default(data_all$Bilbao, data_all$NAO, method = \u0026quot;spearman\u0026quot;):\r## Cannot compute exact p-value with ties\rcor_nao_bil\r## ## Spearman\u0026#39;s rank correlation rho\r## ## data: data_all$Bilbao and data_all$NAO\r## S = 44372, p-value = 0.2126\r## alternative hypothesis: true rho is not equal to 0\r## sample estimates:\r## rho ## 0.1531149\rstr(cor_nao_bil)\r## List of 8\r## $ statistic : Named num 44372\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## $ parameter : NULL\r## $ p.value : num 0.213\r## $ estimate : Named num 0.153\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ null.value : Named num 0\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ alternative: chr \u0026quot;two.sided\u0026quot;\r## $ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## $ data.name : chr \u0026quot;data_all$Bilbao and data_all$NAO\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;htest\u0026quot;\rVemos que el resultado est√° en un formato poco manejable. Nos resume la correlaci√≥n con todos los parametros estad√≠sticos necesarios para sacar una conclusi√≥n sobre la relaci√≥n. La estructura orginal es una lista de vectores. No obstante, la funci√≥n tidy() del paquete {broom} nos permite convertir el resultado en formato de tabla.\ntidy(cor_nao_bil)\r## # A tibble: 1 x 5\r## estimate statistic p.value method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.153 44372. 0.213 Spearman\u0026#39;s rank correlation rho two.sided\r\rAplicar el test de correlaci√≥n a m√∫ltiples variables\rEl objetivo es aplicar el test de correlaci√≥n a todas las estaciones meteorol√≥gicas e √≠ndices de teleconexi√≥n.\nPrimero, debemos pasar la tabla al formato largo, o sea, crear una columna de la ciudad y el valor de la precipitaci√≥n correspondiente. Despu√©s lo repetimos para las teleconexiones.\ndata \u0026lt;- gather(data_all, city, pr, Bilbao:Valencia) %\u0026gt;%\rgather(telecon, index, NAO:AO)\rdata\r## # A tibble: 2,720 x 5\r## yr city pr telecon index\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Bilbao 1342 NAO 0.49\r## 2 1951 Bilbao 1306. NAO -0.07\r## 3 1952 Bilbao 1355. NAO -0.37\r## 4 1953 Bilbao 1372. NAO 0.4 ## 5 1954 Bilbao 1428. NAO 0.51\r## 6 1955 Bilbao 1062. NAO -0.64\r## 7 1956 Bilbao 1254. NAO 0.17\r## 8 1957 Bilbao 968. NAO -0.02\r## 9 1958 Bilbao 1272. NAO 0.12\r## 10 1959 Bilbao 1450. NAO 0.49\r## # ... with 2,710 more rows\rPara poder aplicar el test a todas las ciudades, debemos tener las correspondientes agrupaciones. Por ello, usamos la funci√≥n group_by() indicando los dos grupos (city y telecon), y adem√°s, aplicamos la funci√≥n nest() del paquete {tidyr}, colecci√≥n {tidyverse}, con el objetivo de crear listas de tablas encajadas por fila. En otras palabras, en cada fila de cada ciudad y teleconexi√≥n tendremos una nueva tabla que contiene correspondientemente el a√±o, la precipitaci√≥n y el valor del √≠ndice.\ndata_nest \u0026lt;- group_by(data, city, telecon) %\u0026gt;% nest()\rhead(data_nest)\r## # A tibble: 6 x 3\r## # Groups: city, telecon [6]\r## city telecon data ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt;\rstr(head(slice(data_nest, 1)))\r## grouped_df [6 x 3] (S3: grouped_df/tbl_df/tbl/data.frame)\r## $ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data :List of 6\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## - attr(*, \u0026quot;groups\u0026quot;)= tibble [6 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows : list\u0026lt;int\u0026gt; [1:6] ## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..@ ptype: int(0) ## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi TRUE\rEl siguiente paso es crear una funci√≥n, en la que definimos el test de correlaci√≥n y lo pasamos al formato limpio, que aplicamos a cada agrupaci√≥n.\ncor_fun \u0026lt;- function(df) cor.test(df$pr, df$index, method=\u0026quot;spearman\u0026quot;) %\u0026gt;% tidy()\rAhora s√≥lo nos queda por aplicar nuestra funci√≥n a la columna que contiene las tablas por cada combinaci√≥n entre ciudad y teleconexi√≥n. Para ello, usamos la funci√≥n map() que aplica otra funci√≥n sobre un vector o lista. Lo que hacemos es crear una nueva columna que contiene el resultado, una tabla del resumen estad√≠stico, por cada fila de cada combinaci√≥n.\ndata_nest \u0026lt;- mutate(data_nest, model = map(data, cor_fun))\rhead(data_nest)\r## # A tibble: 6 x 4\r## # Groups: city, telecon [6]\r## city telecon data model ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\rstr(head(slice(data_nest, 1)))\r## grouped_df [6 x 4] (S3: grouped_df/tbl_df/tbl/data.frame)\r## $ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data :List of 6\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## $ model :List of 6\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.00989\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 52912\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.936\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.295\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 67832\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.161\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43966\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.19\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.255\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 65754\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0361\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.0203\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 53460\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.869\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.178\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43082\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## - attr(*, \u0026quot;groups\u0026quot;)= tibble [6 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows : list\u0026lt;int\u0026gt; [1:6] ## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..@ ptype: int(0) ## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi TRUE\r¬øC√≥mo podemos deshacer la lista de tablas en cada fila de nuestra tabla?\nPues bien, primero eliminamos la columna con los datos y despu√©s aplicamos simplemente la funci√≥n unnest().\ncorr_pr \u0026lt;- select(data_nest, -data) %\u0026gt;% unnest()\r## Warning: `cols` is now required when using unnest().\r## Please use `cols = c(model)`\rcorr_pr\r## # A tibble: 40 x 7\r## # Groups: city, telecon [40]\r## city telecon estimate statistic p.value method alternative\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Bilbao NAO 0.153 44372. 0.213 Spearman\u0026#39;s rank co~ two.sided ## 2 Santiago NAO -0.181 61902. 0.139 Spearman\u0026#39;s rank co~ two.sided ## 3 Barcelona NAO -0.0203 53460. 0.869 Spearman\u0026#39;s rank co~ two.sided ## 4 Madrid NAO -0.291 64692. 0.0169 Spearman\u0026#39;s rank co~ two.sided ## 5 Valencia NAO -0.113 27600. 0.422 Spearman\u0026#39;s rank co~ two.sided ## 6 Bilbao WeMO 0.404 31242 0.000706 Spearman\u0026#39;s rank co~ two.sided ## 7 Santiago WeMO 0.332 35014 0.00594 Spearman\u0026#39;s rank co~ two.sided ## 8 Barcelona WeMO 0.0292 50862 0.813 Spearman\u0026#39;s rank co~ two.sided ## 9 Madrid WeMO 0.109 44660 0.380 Spearman\u0026#39;s rank co~ two.sided ## 10 Valencia WeMO -0.252 31056 0.0688 Spearman\u0026#39;s rank co~ two.sided ## # ... with 30 more rows\rEl resultado es una tabla en la que podemos ver las correlaciones y su significaci√≥n estad√≠stica para cada ciudad y teleconexiones.\n\rHeatmap de los resultados\rFinalmente, hacemos un heatmap del resultado obtenido. Antes creamos una columna que indica si la correlaci√≥n es significativa con p-valor menor de 0,05.\ncorr_pr \u0026lt;- mutate(corr_pr, sig = ifelse(p.value \u0026lt; 0.05, \u0026quot;Sig.\u0026quot;, \u0026quot;Non Sig.\u0026quot;))\rggplot()+\rgeom_tile(data = corr_pr,\raes(city, telecon, fill = estimate),\rsize = 1,\rcolour = \u0026quot;white\u0026quot;)+\rgeom_tile(data = filter(corr_pr, sig == \u0026quot;Sig.\u0026quot;),\raes(city, telecon),\rsize = 1,\rcolour = \u0026quot;black\u0026quot;,\rfill = \u0026quot;transparent\u0026quot;)+\rgeom_text(data = corr_pr,\raes(city, telecon, label = round(estimate, 2),\rfontface = ifelse(sig == \u0026quot;Sig.\u0026quot;, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)))+\rscale_fill_gradient2(breaks = seq(-1, 1, 0.2))+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, p.value = \u0026quot;\u0026quot;)+\rtheme_minimal()+\rtheme(panel.grid.major = element_blank(),\rpanel.border = element_blank(),\rpanel.background = element_blank(),\raxis.ticks = element_blank())\r\n\r","date":1555459200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1555459200,"objectID":"6677a02cd4a14e7565e728f5a682f06e","permalink":"https://dominicroye.github.io/es/2019/resumir-tests-de-correlaciones-en-r/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/es/2019/resumir-tests-de-correlaciones-en-r/","section":"post","summary":"Cuando pretendemos estimar la correlaci√≥n entre m√∫ltiples variables, la tarea se complica para obtener un resultado simple y limpio. Una forma sencilla es usar la funci√≥n ``tidy()`` del paquete *{broom}*. Como ejemplo, en este post vamos a estimar la correlaci√≥n entre la precipitaci√≥n anual de varias ciudades espa√±olas y varios √≠ndices de teleconexiones clim√°ticas.","tags":["correlaci√≥n","variables","resumir","tests","limpio"],"title":"Resumir tests de correlaciones en R","type":"post"},{"authors":["M Lemus-Canovas","JA Lopez-Bustins","J Martin-Vide","D Roy√©"],"categories":null,"content":"","date":1555286400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1555286400,"objectID":"27583a24ed658f26c0b96abd4fe4a35a","permalink":"https://dominicroye.github.io/es/publication/2019-rpackage-synoptreg-environmental-modelling/","publishdate":"2019-04-15T00:00:00Z","relpermalink":"/es/publication/2019-rpackage-synoptreg-environmental-modelling/","section":"publication","summary":"Spatial knowledge of the climatic or environmental variables associated with the most frequent circulation types is essential with regard to developing strategies to address the risk of avalanches, floods, soil erosion, air pollution or other natural hazards. In order to derive an environmental regionalization, we present an Open Source R package known as synoptReg, which combines the spatialization of environmental variables based on the atmospheric circulation types. The synoptReg package contains a set of functions, which we will employ (1) to perform a PCA-based synoptic classification using an atmospheric variable; (2) to map the spatial distribution of the selected environmental variable based upon the circulation types; (3) to develop a spatial environmental regionalization based on the previous results. We illustrate the usefulness of the package for a case study in the Alps area.","tags":["Alpes","regionalizaci√≥n ambiental","paquete R","synoptReg","clasificaci√≥n sin√≥ptica"],"title":"synoptReg: An R package for computing a synoptic climate classification and a spatial regionalization of environmental data","type":"publication"},{"authors":["D Roy√©","Mar√≠a T Zarrabeitia","Javier Riancho","Ana Santurt√∫n"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1554076800,"objectID":"3f3a0498b1f87d0adea14706995bea90","permalink":"https://dominicroye.github.io/es/publication/2019-ictus-madrid-environmental-research/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/es/publication/2019-ictus-madrid-environmental-research/","section":"publication","summary":"The understanding of the role of environment on the pathogenesis of stroke is gaining importance in the context of climate change. This study analyzes the temporal pattern of ischemic stroke (IS) in Madrid, Spain, during a 13-year period (2001-2013), and the relationship between ischemic stroke (admissions and deaths) incidence and environmental factors on a daily scale by using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of air pollutants and Apparent Temperature (AT), a biometeorological index which represents human thermal comfort on IS, a lag non-linear model was fitted in a generalized additive model. The mortality rate followed a downward trend over the studied period, however admission rates progressively increased. Our results show that both increases and decreases in AT had a marked relationship with IS deaths, while hospital admissions were only associated with low AT. When analyzing the cumulative effects (for lag 0 to 14 days), with an AT of 1.7¬∞C (percentile 5%) a RR of 1.20 (95% CI, 1.05-1.37) for IS mortality and a RR of 1.09 (95% CI, 0.91-1.29) for morbidity is estimated. Concerning gender differences, men show higher risks of mortality in low temperatures and women in high temperatures. No significant relationship was found between air pollutant concentrations and IS morbi mortality, but this result must be interpreted with caution, since there are strong spatial fluctuations of the former between nearby geographical areas that make it difficult to perform correlation analyses.","tags":["efectos a corto plazo","Espa√±a","Madrid","ambiente t√©rmico","accidente cerebrovascular isqu√©mico","contaminantes del aire","temperatura aparente","mortalidad","ingresos hospitalarios"],"title":"A time series analysis of the relationship between Apparent Temperature, Air Pollutants and Ischemic Stroke in Madrid, Spain","type":"publication"},{"authors":null,"categories":["gesti√≥n","R","R:intermedio"],"content":"\r\rCuando trabajamos con diferentes fuentes de datos, nos podemos encontrar con tablas distrubidas sobre varias hojas de Excel. En este post vamos a importar la temperatura media diaria de Madrid y Berl√≠n que se encuentra en dos archvios de Excel con hojas para cada a√±o entre 2000 y 2005: descarga.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de paquetes (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, purrr,etc.\r\rfs\rProporciona una interfaz uniforme y multiplataforma para las operaciones del sistema de archivos\r\rreadxl\rImportar archivos Excel\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;readxl\u0026quot;)) install.packages(\u0026quot;readxl\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(fs)\rlibrary(readxl)\rPor defecto, la funci√≥n read_excel() importa la primera hoja. Para importar una hoja diferente es necesario indicarlo con el argumento sheet o bien el n√∫mero o el nombre (segundo argumento).\n#importar primera hoja\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;)\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#importar hoja 3\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;, 3)\r## # A tibble: 365 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2002-01-01 00:00:00 8.7 2002\r## 2 2002-01-02 00:00:00 7.4 2002\r## 3 2002-01-03 00:00:00 8.5 2002\r## 4 2002-01-04 00:00:00 9.2 2002\r## 5 2002-01-05 00:00:00 9.3 2002\r## 6 2002-01-06 00:00:00 7.3 2002\r## 7 2002-01-07 00:00:00 5.4 2002\r## 8 2002-01-08 00:00:00 5.6 2002\r## 9 2002-01-09 00:00:00 6.8 2002\r## 10 2002-01-10 00:00:00 6.1 2002\r## # ... with 355 more rows\rLa funci√≥n excel_sheets() permite extraer los nombres de las hojas.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rpath %\u0026gt;%\rexcel_sheets()\r## [1] \u0026quot;2000\u0026quot; \u0026quot;2001\u0026quot; \u0026quot;2002\u0026quot; \u0026quot;2003\u0026quot; \u0026quot;2004\u0026quot; \u0026quot;2005\u0026quot;\rEl resultado nos indica que en cada hoja encontramos un a√±o de los datos desde 2000 a 2005. La funci√≥n m√°s importante para leer m√∫ltiples hojas es map() del paquete {purrr} que forma parte de la colecci√≥n de paquetes {tidyverse}. map() permite aplicar una funci√≥n a cada elemento de un vector o lista.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap(read_excel,\rpath = path)\rstr(mad)\r## List of 6\r## $ 2000: tibble [366 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:366], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## ..$ ta : num [1:366] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:366] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ 2001: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2001-01-01\u0026quot; \u0026quot;2001-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 8.2 8.8 7.5 9.2 10 9 5.5 4.6 3 7.9 ...\r## ..$ yr : num [1:365] 2001 2001 2001 2001 2001 ...\r## $ 2002: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2002-01-01\u0026quot; \u0026quot;2002-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 8.7 7.4 8.5 9.2 9.3 7.3 5.4 5.6 6.8 6.1 ...\r## ..$ yr : num [1:365] 2002 2002 2002 2002 2002 ...\r## $ 2003: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2003-01-01\u0026quot; \u0026quot;2003-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 9.4 10.8 9.7 9.2 6.3 6.6 3.8 6.4 4.3 3.4 ...\r## ..$ yr : num [1:365] 2003 2003 2003 2003 2003 ...\r## $ 2004: tibble [366 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:366], format: \u0026quot;2004-01-01\u0026quot; \u0026quot;2004-01-02\u0026quot; ...\r## ..$ ta : num [1:366] 6.6 5.9 7.8 8.1 6.4 5.7 5.2 6.9 11.8 12.2 ...\r## ..$ yr : num [1:366] 2004 2004 2004 2004 2004 ...\r## $ 2005: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2005-01-01\u0026quot; \u0026quot;2005-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 7.1 7.8 6.4 5.6 4.4 6.8 7.4 6 5.2 4.2 ...\r## ..$ yr : num [1:365] 2005 2005 2005 2005 2005 ...\rEl resultado es una lista nombrada con el nombre de cada hoja que contiene el data.frame. Dado que se trata de la misma tabla en todas las hojas, podr√≠amos usar la funci√≥n bind_rows(), no obstante, existe una variante de map()que directamente nos une todas las tablas por fila: map_df(). Si fuese necesario unir por columna se deber√≠a usar map_dfc().\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path)\rmad\r## # A tibble: 2,192 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 2,182 more rows\rEn nuestro caso tenemos una columna en cada hoja (a√±o, pero tambi√©n la fecha) que diferencia cada tabla. Si no fuera el caso, deber√≠amos usar el nombre de las hojas como nueva columna al unir todas. En bind_rows() puede hacerse con el argumento .id asignando un nombre para la columna. Lo mismo valdr√≠a para map_df().\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path,\r.id = \u0026quot;yr2\u0026quot;)\rstr(mad)\r## tibble [2,192 x 4] (S3: tbl_df/tbl/data.frame)\r## $ yr2 : chr [1:2192] \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; ...\r## $ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num [1:2192] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## $ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r¬øPero c√≥mo importamos m√∫ltiples archivos de Excel?\nPara ello, primero debemos conocer la funci√≥n dir_ls() del paquete {fs}. Es cierto que existe la funci√≥n dir() de R Base, pero las ventajas del reciente paquete son varias, pero especialmente es la compatibilidad con la colecci√≥n de {tidyverse}.\ndir_ls()\r## berlin_temp.xlsx featured.png index.es.html index.es.Rmd ## index.es.Rmd.lock~ index.es_files madrid_temp.xlsx\r#podemos filtrar los archivos que queremos\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) \r## berlin_temp.xlsx madrid_temp.xlsx\rImportamos los dos archivos de Excel que tenemos.\n#sin unir\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;)%\u0026gt;%\rmap(read_excel)\r## $berlin_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 1.2 2000\r## 2 2000-01-02 00:00:00 3.6 2000\r## 3 2000-01-03 00:00:00 5.7 2000\r## 4 2000-01-04 00:00:00 5.1 2000\r## 5 2000-01-05 00:00:00 2.2 2000\r## 6 2000-01-06 00:00:00 1.8 2000\r## 7 2000-01-07 00:00:00 4.2 2000\r## 8 2000-01-08 00:00:00 4.2 2000\r## 9 2000-01-09 00:00:00 4.2 2000\r## 10 2000-01-10 00:00:00 1.7 2000\r## # ... with 356 more rows\r## ## $madrid_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#uniendo con una nueva columna\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;)%\u0026gt;%\rmap_df(read_excel, .id = \u0026quot;city\u0026quot;)\r## # A tibble: 732 x 4\r## city date ta yr\r## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 berlin_temp.xlsx 2000-01-01 00:00:00 1.2 2000\r## 2 berlin_temp.xlsx 2000-01-02 00:00:00 3.6 2000\r## 3 berlin_temp.xlsx 2000-01-03 00:00:00 5.7 2000\r## 4 berlin_temp.xlsx 2000-01-04 00:00:00 5.1 2000\r## 5 berlin_temp.xlsx 2000-01-05 00:00:00 2.2 2000\r## 6 berlin_temp.xlsx 2000-01-06 00:00:00 1.8 2000\r## 7 berlin_temp.xlsx 2000-01-07 00:00:00 4.2 2000\r## 8 berlin_temp.xlsx 2000-01-08 00:00:00 4.2 2000\r## 9 berlin_temp.xlsx 2000-01-09 00:00:00 4.2 2000\r## 10 berlin_temp.xlsx 2000-01-10 00:00:00 1.7 2000\r## # ... with 722 more rows\rAhora bien, en este caso s√≥lo importamos la primera hoja de cada archivo Excel. Para resolver este problema, debemos crear nuestra propia funci√≥n. En esta funci√≥n hacemos lo que hicimos previamente de forma individual.\nread_multiple_excel \u0026lt;- function(path) {\rpath %\u0026gt;%\rexcel_sheets() %\u0026gt;% set_names() %\u0026gt;% map_df(read_excel, path = path)\r}\rAplicamos nuestra funci√≥n creada para importar m√∫ltiples hojas de varios archivos Excel.\n#por separado\rdata \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map(read_multiple_excel)\rstr(data)\r## List of 2\r## $ berlin_temp.xlsx: tibble [2,192 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## ..$ ta : num [1:2192] 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ madrid_temp.xlsx: tibble [2,192 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## ..$ ta : num [1:2192] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r#unir todas\rdata_df \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map_df(read_multiple_excel,\r.id = \u0026quot;city\u0026quot;)\rstr(data_df)\r## tibble [4,384 x 4] (S3: tbl_df/tbl/data.frame)\r## $ city: chr [1:4384] \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; ...\r## $ date: POSIXct[1:4384], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num [1:4384] 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## $ yr : num [1:4384] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r\n\r","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1552176000,"objectID":"fbd88730df3fd2ffe2a25bcedf4ba028","permalink":"https://dominicroye.github.io/es/2019/importar-varias-hojas-excel-en-r/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/es/2019/importar-varias-hojas-excel-en-r/","section":"post","summary":"Cuando trabajamos con diferentes fuentes de datos, nos podemos encontrar con tablas distrubidas sobre varias hojas de Excel. En este post vamos a importar la temperatura media diaria de Madrid y Berl√≠n que se encuentran en dos archvios de Excel con hojas para cada a√±o entre 2000 y 2005.","tags":["excel","hojas","importar"],"title":"Importar varias hojas Excel en R","type":"post"},{"authors":["D Roy√©","N Lorenzo","D Rasilla","A Mart√≠"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1551398400,"objectID":"a886efbb0057c84bc3ecacb4e1f008aa","permalink":"https://dominicroye.github.io/es/publication/2018-cloudiness-peninsula-ij-climatology/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/es/publication/2018-cloudiness-peninsula-ij-climatology/","section":"publication","summary":"This paper presents the first systematic study of the relationships between atmospheric circulation types (CT) and cloud fraction (CF) over the whole Iberian Peninsula, using satellite data from the MODIS (MOD09GA and MYD09GA) cloud mask for the period 2001-2017. The high level of detail, in combination with a classification for circulation patterns, provides us with relevant information about the spatio-temporal variability of cloudiness and the main mechanisms affecting the genesis of clouds. The results show that westerly CTs are the most influential, followed by cyclonic types, in cloudiness in the west of the Iberian Peninsula. Westerly flows, however, do not affect the Mediterranean coastline, which is dominated by easterly CTs, suggesting that local factors such as convective processes, orography and proximity to a body of warm water could play a major role in cloudiness processes. The Cantabrian Coast also has a particularly characteristic cloudiness dominated by northerly CTs. In general, the results found in this study are in line with the few studies that exist on cloudiness in the Iberian Peninsula. Furthermore, the results are geographically consistent, showing links to synoptic forcing in terms of atmospheric circulation patterns and the impact of the Iberian Peninsulas complex orography upon this element of the climate system.","tags":["nubosidad","tipos de circulaci√≥n","pen√≠nsula ib√©rica","MODIS","clima","patrones espacio-temporales"],"title":"Spatio-temporal variations of cloud fraction based on circulation types in the Iberian Peninsula","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://dominicroye.github.io/es/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/es/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["sig","R","R:elemental"],"content":"\r\rEn geograf√≠a, la distancia al mar es una variable fundamental, especialmente relevante a la hora de modelizar. Por ejemplo, en interpolaciones de la temperatura del aire habitualmente se hace uso de la distancia al mar como variable predictora, ya que existe una relaci√≥n casual entre ambas que explica la variaci√≥n espacial. ¬øC√≥mo podemos estimar la distancia (m√°s corta) a la costa en R?\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripci√≥n\r\r\r\rtidyverse\rConjunto de librer√≠as (visualizaci√≥n y manipulaci√≥n de datos): ggplot2, dplyr, etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rraster\rImportar, exportar y manipular raster\r\rrnaturalearth\rConjunto de mapas vectoriales ‚Äònatural earth‚Äô\r\rRColorBrewer\rPaletas de colores\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r#paquetes\rlibrary(rnaturalearth)\rlibrary(sf)\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(RColorBrewer)\r\rLa costa de Islandia como ejemplo\rNuestro ejemplo en este post ser√° Islandia, como es un territorio insular facilitar√° el ensayo y de este modo es posible mostrar el proceso de forma sencilla. La librer√≠a rnaturalearth permite importar los l√≠mites de pa√≠ses (con diferentes niveles administrativos) de todo el mundo. Los datos vienen de la plataforma naturalearthdata.com. Recomiendo explorar la librer√≠a, m√°s info aqu√≠. La funci√≥n ne_countries( ) importa los l√≠mites de pa√≠ses. En este caso indicamos con el argumento scale la resoluci√≥n (10,50 o 110m), con country indicamos el pa√≠s concreto de inter√©s y con returnclass determinamos que clase queremos (sf o sp), en nuestro caso sf (simple feature).\nworld \u0026lt;- ne_countries(scale = 50) #mapamundi con 50m de resoluci√≥n\rplot(world) #tiene clase sp por defecto\r#importamos los l√≠mites de Islandia iceland \u0026lt;- ne_countries(scale = 10,country = \u0026quot;Iceland\u0026quot;, returnclass = \u0026quot;sf\u0026quot;)\r#info del objeto vectorial\riceland\r## Simple feature collection with 1 feature and 94 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: -24.53991 ymin: 63.39671 xmax: -13.50292 ymax: 66.56415\r## CRS: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\r## featurecla scalerank labelrank sovereignt sov_a3 adm0_dif level\r## 188 Admin-0 country 0 3 Iceland ISL 0 2\r## type admin adm0_a3 geou_dif geounit gu_a3 su_dif subunit\r## 188 Sovereign country Iceland ISL 0 Iceland ISL 0 Iceland\r## su_a3 brk_diff name name_long brk_a3 brk_name brk_group abbrev postal\r## 188 ISL 0 Iceland Iceland ISL Iceland \u0026lt;NA\u0026gt; Iceland IS\r## formal_en formal_fr name_ciawf note_adm0 note_brk name_sort\r## 188 Republic of Iceland \u0026lt;NA\u0026gt; Iceland \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Iceland\r## name_alt mapcolor7 mapcolor8 mapcolor9 mapcolor13 pop_est pop_rank\r## 188 \u0026lt;NA\u0026gt; 1 4 4 9 339747 10\r## gdp_md_est pop_year lastcensus gdp_year economy\r## 188 16150 2017 NA 2016 2. Developed region: nonG7\r## income_grp wikipedia fips_10_ iso_a2 iso_a3 iso_a3_eh iso_n3\r## 188 1. High income: OECD NA IC IS ISL ISL 352\r## un_a3 wb_a2 wb_a3 woe_id woe_id_eh woe_note adm0_a3_is\r## 188 352 IS ISL 23424845 23424845 Exact WOE match as country ISL\r## adm0_a3_us adm0_a3_un adm0_a3_wb continent region_un subregion\r## 188 ISL NA NA Europe Europe Northern Europe\r## region_wb name_len long_len abbrev_len tiny homepart min_zoom\r## 188 Europe \u0026amp; Central Asia 7 7 7 NA 1 0\r## min_label max_label ne_id wikidataid name_ar name_bn name_de name_en\r## 188 2 7 1159320917 Q189 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Island Iceland\r## name_es name_fr name_el name_hi name_hu name_id name_it name_ja name_ko\r## 188 Islandia Islande \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Izland Islandia Islanda \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt;\r## name_nl name_pl name_pt name_ru name_sv name_tr name_vi name_zh\r## 188 IJsland Islandia Isl√¢ndia \u0026lt;NA\u0026gt; Island Izlanda Iceland \u0026lt;NA\u0026gt;\r## geometry\r## 188 MULTIPOLYGON (((-14.56363 6...\r#aqu√≠ Islandia\rplot(iceland)\rPor defecto, la funci√≥n plot( ) con la clase sf nos crea tantas facetas del mapa como variables tiene. Para limitarlo podemos usar o bien con el nombre de una variable plot(iceland[\"admin\"]) o el argumento max.plot plot(iceland,max.plot=1). Con el argumento max.plot=1 la funci√≥n usa la primera variable disponible del mapa.\nAdem√°s, vemos en la informaci√≥n del objeto sf que la proyecci√≥n es WGS84 con grados decimales (c√≥digo EPSG:4326). Para el c√°lculo de distancias es m√°s conveniente usar metros en lugar de grados. Debido a ello, lo primero que hacemos es transformar el mapa de Islandia a UTM Zona 27 (c√≥digo EPSG:3055). M√°s informaci√≥n sobre EPSG y proyecciones aqu√≠. Con ese objetivo, usamos la funci√≥n st_transform( ). Simplemente indicamos el mapa y el c√≥digo EPSG.\n#transformamos a UTM\riceland \u0026lt;- st_transform(iceland, 3055)\r\rCrear una red de puntos\rTodav√≠a necesitamos los puntos donde queremos conocer la distancia. En nuestro caso ser√° una red regular de puntos en Islandia con una resoluci√≥n de 5km. Esa tarea la hacemos con la funci√≥n st_make_grid( ), indicando con el argumento cellsize la resoluci√≥n en la unidad del sistema de coordenadas (metros en nuestro caso) y qu√© geometr√≠a nos gustar√≠a crear what (poligonos, centros o esquinas).\n#crear red de puntos\rgrid \u0026lt;- st_make_grid(iceland,cellsize = 5000, what = \u0026quot;centers\u0026quot;)\r#nuestra red sobre la extensi√≥n de Islandia\rplot(grid)\r#exraemos s√≥lamente los puntos en los l√≠mites de Islandia\rgrid \u0026lt;- st_intersection(grid, iceland) #nuestra red ahora\rplot(grid)\r\rCalcular la distancia\rPara estimar la distancia usamos la funci√≥n st_distance( ) que nos devuelve un vector de distancias para todos nuestros puntos de la red. Pero antes es necesario transformar el mapa de Islandia de una forma de pol√≠gono (MULTIPOLYGON) a l√≠nea (MULTILINESTRING). M√°s detalles con ?st_cast.\n#convertimos Islandia de geometr√≠a poligono a l√≠nea\riceland \u0026lt;- st_cast(iceland, \u0026quot;MULTILINESTRING\u0026quot;)\r#c√°lculo de la distancia entre la costa y nuestros puntos\rdist \u0026lt;- st_distance(iceland, grid)\r#distancia con unidad en metros\rhead(dist[1,])\r## Units: [m]\r## [1] 790.7906 1151.4360 1270.7603 3128.9057 2428.5677 4197.7472\r\rVisualizar la distancia calculada\rUna vez obtenida la distancia para nuestros puntos, podemos combinarlos con las coordenadas y plotearlos en ggplot2. Para ello, creamos un data.frame. El objeto dist es una matriz de una columna, por eso, tenemos que convertirla a vector con la funci√≥n as.vector( ). Adem√°s, dividimos por 1000 para convertir la distancia en metros a km. La funci√≥n st_coordinates( ) extrae las coordenadas de nuestros puntos. Para la visualizaci√≥n usamos un vector de colores con la gama RdGy (m√°s aqu√≠).\n#creamos un data.frame con la distancia y las coorendas de los puntos\rdf \u0026lt;- data.frame(dist = as.vector(dist)/1000,\rst_coordinates(grid))\r#estructura\rstr(df)\r## \u0026#39;data.frame\u0026#39;: 4104 obs. of 3 variables:\r## $ dist: num 0.791 1.151 1.271 3.129 2.429 ...\r## $ X : num 608796 613796 583796 588796 593796 ...\r## $ Y : num 7033371 7033371 7038371 7038371 7038371 ...\r#colores col_dist \u0026lt;- brewer.pal(11, \u0026quot;RdGy\u0026quot;)\rggplot(df, aes(X, Y,fill = dist))+ #variables\rgeom_tile()+ #geometr√≠a\rscale_fill_gradientn(colours = rev(col_dist))+ #colores para la distancia\rlabs(fill = \u0026quot;Distance (km)\u0026quot;)+ #nombre de la leyenda\rtheme_void()+ #estilo del mapa\rtheme(legend.position = \u0026quot;bottom\u0026quot;) #posici√≥n de la leyenda\r\rExportar la distancia como raster\rPara poder exportar la distancia con respecto al mar de Islandia, debemos usar la funci√≥n rasterize( ) de la librer√≠a raster.\nPrimero, es necesario crear un raster vac√≠o. En este raster debemos indicar la resoluci√≥n, en nuestro caso es de 5000m, la proyecci√≥n y la extensi√≥n del raster.\nLa proyecci√≥n la podemos extraer de la informaci√≥n del mapa de Islandia.\n\rLa extensi√≥n la conseguimos extraer de nuestros puntos grid con la funci√≥n extent( ). No obstante, esta √∫ltima funci√≥n necesita la clase sp, por eso pasamos el objeto grid en formato sf, √∫nicamente para ello, a la clase sp usando la funci√≥n as( ) y el argumento ‚ÄúSpatial‚Äù.\n\r\rAdem√°s de lo anterior, el data.frame df que creamos antes debemos convertir en clase sf. Por eso, aplicamos la funci√≥n st_as_sf( ) con el argumento coords indicando los nombres de las coordenadas. Adicionalmente, tambi√©n definimos el sistema de coordenadas que conocemos.\n\r\r#obtenemos la extensi√≥n\rext \u0026lt;- extent(as(grid, \u0026quot;Spatial\u0026quot;))\r#objeto extent\rext\r## class : Extent ## xmin : 338795.6 ## xmax : 848795.6 ## ymin : 7033371 ## ymax : 7383371\r#raster destino\rr \u0026lt;- raster(resolution = 5000, ext = ext, crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\r#convertimos los puntos a un spatial object clase sf\rdist_sf \u0026lt;- st_as_sf(df, coords = c(\u0026quot;X\u0026quot;,\u0026quot;Y\u0026quot;)) %\u0026gt;%\rst_set_crs(3055)\r#creamos el raster de la distancia\rdist_raster \u0026lt;- rasterize(dist_sf, r, \u0026quot;dist\u0026quot;, fun = mean)\r#raster\rdist_raster\r## class : RasterLayer ## dimensions : 70, 102, 7140 (nrow, ncol, ncell)\r## resolution : 5000, 5000 (x, y)\r## extent : 338795.6, 848795.6, 7033371, 7383371 (xmin, xmax, ymin, ymax)\r## crs : +proj=utm +zone=27 +ellps=intl +units=m +no_defs ## source : memory\r## names : layer ## values : 0.006124901, 115.1712 (min, max)\r#plotear el raster\rplot(dist_raster)\r#exportamos el raster\rwriteRaster(dist_raster, file = \u0026quot;dist_islandia.tif\u0026quot;, format = \u0026quot;GTiff\u0026quot;, overwrite = TRUE)\rLa funci√≥n rasterize( ) est√° pensada para crear rasters a partir de un grid irregular. En caso que tengamos un grid regular, como este mismo, podemos usar una alternativa m√°s f√°cil. La funci√≥n rasterFromXYZ( ) convierte un data.frame con longitud, latitud y la variable Z en un raster. Es importante que el orden debe ser longitud, latitud, variables.\nr \u0026lt;- rasterFromXYZ(df[, c(2:3, 1)], crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\rplot(r)\rCon el c√°lculo de la distancia podemos llegar crear arte, como se ve en la cabezera de este post, que incluye un mapamundi √∫nicamente con la distancia al mar de todos los continentes. Una perspectiva diferente a nuestro mundo (aqu√≠ m√°s).\n\n\r","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1546905600,"objectID":"a9c7a1973a23786c34b109fed2a3286e","permalink":"https://dominicroye.github.io/es/2019/calcular-la-distancia-al-mar-en-r/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/es/2019/calcular-la-distancia-al-mar-en-r/","section":"post","summary":"En geograf√≠a, la distancia al mar es una variable fundamental, especialmente relevante a la hora de modelizar. Por ejemplo, en interpolaciones de la temperatura del aire habitualmente se hace uso de la distancia al mar como variable predictora, ya que existe una relaci√≥n casual entre ambas que explica la variaci√≥n espacial. ¬øC√≥mo podemos estimar la distancia (m√°s corta) a la costa en R?","tags":["distancia","raster","calculo","variable"],"title":"Calcular la distancia al mar en R","type":"post"},{"authors":["F Mori-Gamarra","L Moure-Rodr√≠guez","X Sureda","C Carbiae","D Roy√©","A Montes-Mart√≠nez","F Cadaveira","F Caama√±o-Isorna"],"categories":null,"content":"","date":1545350400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1545350400,"objectID":"efc75ad0020b6bd8f49ed175c631e04b","permalink":"https://dominicroye.github.io/es/publication/2018-alcohol-galicia-gaceta/","publishdate":"2020-12-21T00:00:00Z","relpermalink":"/es/publication/2018-alcohol-galicia-gaceta/","section":"publication","summary":"Objective: To assess the influence that alcohol outlet density, off- and on-alcohol premises, and alcohol consumption wield on the consumption patterns of young pre-university students in Galicia (Spain). Method: A cross-sectional analysis of a cohort of students of the University of Santiago de Compostela (Compostela Cohort 2016) was carried out. Consumption prevalence were calculated for each of the municipalities from the first-cycle students‚Äô home residence during the year prior to admission. The association with risky alcohol consumption (RC) and binge-drinking (BD) was assessed with a logistic model considering as independent variables the municipality population, alcohol outlet density of off- premises, density of off- and on- premises and total density of both types of premises in the municipality. Results: The prevalence of RC was 60.5% (95% confidence interval [95%CI]: 58.4-62.5) and the BD was 28.5% (95%CI: 26.7-30.2). A great variability was observed according to the municipality of provenance. The multivariate logistic model showed municipalities with a density of 8.42-9.34 of both types of premises per thousand inhabitants presented a higher risk of RC (odds ratio [OR]: 1,39; 95%CI: 1.09-1.78) and BD (OR: 1.29; 95%CI: 1.01-1.66). Conclusion: These data suggest the importance of including environmental information when studying alcohol consumption. Knowing our environment better could help plan policies that encourage healthier behaviour in the population.","tags":["densidad de venta de alcohol","alcohol","consumo de alcohol por menores de edad","adolescentes"],"title":"Alcohol outlet density and alcohol consumption in Galician youth","type":"publication"},{"authors":null,"categories":["visualizaci√≥n","R","R:elemental"],"content":"\r\rEste a√±o se hicieron muy famosos en todo el mundo los llamados warming stripes, las tiras del calentamiento global, que fueron creadas por el cient√≠fico Ed Hawkins de la Universidad de Reading. Estos gr√°ficos representan y comunican el cambio clim√°tico de una forma muy ilustrativa y eficaz.\nVisualising global temperature change since records began in 1850. Versions for USA, central England \u0026amp; Toronto available too: https://t.co/H5Hv9YgZ7v pic.twitter.com/YMzdySrr3A\n\u0026mdash; Ed Hawkins (@ed_hawkins) May 23, 2018  A partir de su idea, cre√© tiras para ejemplos de Espa√±a, como el siguiente de Madrid.\n#Temperatura anual en #MadridRetiro desde 1920 a 2017. #CambioClimatico #dataviz #ggplot2 (idea de @ed_hawkins üôè) @Divulgameteo @edupenabad @climayagua @ClimaGroupUB @4gotas_com pic.twitter.com/wmLb5uczpT\n\u0026mdash; Dr. Dominic Roy√© (@dr_xeo) June 2, 2018  En este post voy a ense√±ar c√≥mo se pueden crear estas tiras en R con el paquete ggplot2. Aunque debo decir que existen muchos caminos en R que nos pueden llevar al mismo resultado o a uno similar, incluso dentro de ggplot2.\nDatos\rEn este caso usaremos las temperaturas anuales de Lisboa del\rGISS Surface Temperature Analysis que comprenden el periodo 1880-2018. Se trata de series temporales homogeneizadas. Tambi√©n se podr√≠an usar temperaturas mensuales u otras series temporales. El archivo se puede descargar aqu√≠. Lo primero que debemos hacer, siempre y cuando no lo hayamos hecho, es instalar la colecci√≥n de paquetes tidyverse que incluyen tambi√©n ggplot2. Adem√°s, nos har√° falta el paquete lubridate para el tratamiento de fechas. Despu√©s, importamos los datos de Lisboa que est√°n en formato csv.\n#instalamos los paquetes lubridate y tidyverse\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(RColorBrewer)\r#importar las temperaturas anuales\rtemp_lisboa \u0026lt;- read_csv(\u0026quot;temp_lisboa.csv\u0026quot;)\rstr(temp_lisboa)\r## spec_tbl_df [139 x 18] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r## $ YEAR : num [1:139] 1880 1881 1882 1883 1884 ...\r## $ JAN : num [1:139] 9.17 11.37 10.07 10.86 11.16 ...\r## $ FEB : num [1:139] 12 11.8 11.9 11.5 10.6 ...\r## $ MAR : num [1:139] 13.6 14.1 13.5 10.5 12.4 ...\r## $ APR : num [1:139] 13.1 14.4 14 13.8 12.2 ...\r## $ MAY : num [1:139] 15.7 17.3 15.6 14.6 16.4 ...\r## $ JUN : num [1:139] 17 19.2 17.9 17.2 19.1 ...\r## $ JUL : num [1:139] 19.1 21.8 20.3 19.5 21.4 ...\r## $ AUG : num [1:139] 20.6 23.5 21 21.6 22.4 ...\r## $ SEP : num [1:139] 20.7 20 18 18.8 19.5 ...\r## $ OCT : num [1:139] 17.9 16.3 16.4 15.8 16.4 ...\r## $ NOV : num [1:139] 12.5 14.7 13.7 13.5 12.5 ...\r## $ DEC : num [1:139] 11.07 9.97 10.66 9.46 10.25 ...\r## $ D-J-F : num [1:139] 10.7 11.4 10.6 11 10.4 ...\r## $ M-A-M : num [1:139] 14.1 15.2 14.3 12.9 13.6 ...\r## $ J-J-A : num [1:139] 18.9 21.5 19.7 19.4 20.9 ...\r## $ S-O-N : num [1:139] 17 17 16 16 16.1 ...\r## $ metANN: num [1:139] 15.2 16.3 15.2 14.8 15.3 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. YEAR = col_double(),\r## .. JAN = col_double(),\r## .. FEB = col_double(),\r## .. MAR = col_double(),\r## .. APR = col_double(),\r## .. MAY = col_double(),\r## .. JUN = col_double(),\r## .. JUL = col_double(),\r## .. AUG = col_double(),\r## .. SEP = col_double(),\r## .. OCT = col_double(),\r## .. NOV = col_double(),\r## .. DEC = col_double(),\r## .. `D-J-F` = col_double(),\r## .. `M-A-M` = col_double(),\r## .. `J-J-A` = col_double(),\r## .. `S-O-N` = col_double(),\r## .. metANN = col_double()\r## .. )\r## - attr(*, \u0026quot;problems\u0026quot;)=\u0026lt;externalptr\u0026gt;\rVemos en las columnas que tenemos valores mensuales y estacionales, y el valor anual. Pero antes de proceder a visualizar la temperatura anual, debemos sustituir los valores ausentes 999.9 por NA, usando la funci√≥n ifelse( ) que lleva una condici√≥n y los argumentos correspondientes a verdadero y falso de la condici√≥n dada.\n#selecionamos la columna del a√±o y la temperatura anual\rtemp_lisboa_yr \u0026lt;- select(temp_lisboa, YEAR, metANN)\r#cambiamos el nombre de la columna temp_lisboa_yr \u0026lt;- rename(temp_lisboa_yr, ta = metANN)\r#valores ausentes 999.9\rsummary(temp_lisboa_yr) \r## YEAR ta ## Min. :1880 Min. : 14.53 ## 1st Qu.:1914 1st Qu.: 15.65 ## Median :1949 Median : 16.11 ## Mean :1949 Mean : 37.38 ## 3rd Qu.:1984 3rd Qu.: 16.70 ## Max. :2018 Max. :999.90\rtemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, ta = ifelse(ta == 999.9, NA, ta))\rCuando usamos el a√±o como variable, no solemos convertirlo en un objeto de fecha, no obstante es aconsejable. De este modo nos permite usar las funciones de fechas del paquete lubridate y las funciones de apoyo dentro de ggplot2. La funci√≥n str_c( ) del paquete stringr, forma parte de la colecci√≥n de tidyverse, es similar a paste( ) de R Base que nos permite combinar caracteres especificando un separador (sep=‚Äú-‚Äù). La funci√≥n ymd( ) (year month day) del paquete lubridate convierte una fecha en un objeto Date. Es posible combinar varias funciones\rhaciendo uso del pipe operator %\u0026gt;% que ayuda a encadenar sin asignar el resultado a un nuevo objeto. Su uso es muy extendido especialmente con el paquete tidyverse. Si quieres saber m√°s de su uso, aqu√≠ tienes un tutorial.\ntemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, date = str_c(YEAR, \u0026quot;01-01\u0026quot;, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd())\r\rCreando las tiras\rPrimero, creamos el estilo del gr√°fico, especificando todo los argumentos del aspecto que queremos ajustar. Partimos del estilo por defecto de theme_minimal( ). Adem√°s, asignamos\rlos colores procedientes de RColorBrewer a un objeto col_srip. M√°s informaci√≥n sobre los colores usados aqu√≠.\ntheme_strip \u0026lt;- theme_minimal()+\rtheme(axis.text.y = element_blank(),\raxis.line.y = element_blank(),\raxis.title = element_blank(),\rpanel.grid.major = element_blank(),\rlegend.title = element_blank(),\raxis.text.x = element_text(vjust = 3),\rpanel.grid.minor = element_blank(),\rplot.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;)\r)\rcol_strip \u0026lt;- brewer.pal(11, \u0026quot;RdBu\u0026quot;)\rbrewer.pal.info\r## maxcolors category colorblind\r## BrBG 11 div TRUE\r## PiYG 11 div TRUE\r## PRGn 11 div TRUE\r## PuOr 11 div TRUE\r## RdBu 11 div TRUE\r## RdGy 11 div FALSE\r## RdYlBu 11 div TRUE\r## RdYlGn 11 div FALSE\r## Spectral 11 div FALSE\r## Accent 8 qual FALSE\r## Dark2 8 qual TRUE\r## Paired 12 qual TRUE\r## Pastel1 9 qual FALSE\r## Pastel2 8 qual FALSE\r## Set1 9 qual FALSE\r## Set2 8 qual TRUE\r## Set3 12 qual FALSE\r## Blues 9 seq TRUE\r## BuGn 9 seq TRUE\r## BuPu 9 seq TRUE\r## GnBu 9 seq TRUE\r## Greens 9 seq TRUE\r## Greys 9 seq TRUE\r## Oranges 9 seq TRUE\r## OrRd 9 seq TRUE\r## PuBu 9 seq TRUE\r## PuBuGn 9 seq TRUE\r## PuRd 9 seq TRUE\r## Purples 9 seq TRUE\r## RdPu 9 seq TRUE\r## Reds 9 seq TRUE\r## YlGn 9 seq TRUE\r## YlGnBu 9 seq TRUE\r## YlOrBr 9 seq TRUE\r## YlOrRd 9 seq TRUE\rPara el gr√°fico final usamos la geometr√≠a geom_tile( ). Como los datos no tienen un valor espec√≠fico para el eje Y, usamos un valor dummy, aqu√≠ es 1. Adem√°s, ajusto el ancho de la barra de colores en la leyenda.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1,fill = ta))+\rgeom_tile()+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_continuous(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rguides(fill = guide_colorbar(barwidth = 1))+\rlabs(title = \u0026quot;LISBOA 1880-2018\u0026quot;,\rcaption = \u0026quot;Datos: GISS Surface Temperature Analysis\u0026quot;)+\rtheme_strip\rEn el caso de que quisieramos obtener √∫nicamente las tiras, podemos usar theme_void( ) y el argumento show.legend=FALSE en geom_tile( ) para eliminar todos los elementos de estilo. Tambi√©n podemos cambiar el color para los valores NA, incluyendo el argumento na.value=‚Äúgrey70‚Äù en la funci√≥n scale_fill_gradientn( ).\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile(show.legend = FALSE)+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_discrete(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rtheme_void()\r\n\r","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1543968000,"objectID":"abb18afbfdd83d022c5083620cc5688b","permalink":"https://dominicroye.github.io/es/2018/como-crear-warming-stripes-in-r/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/es/2018/como-crear-warming-stripes-in-r/","section":"post","summary":"Este a√±o se hicieron muy famosos en todo el mundo los llamados warming stripes, las tiras del calentamiento global, que fueron creados por el cient√≠fico Ed Hawkins de la Universidad de Reading. Estos gr√°ficos representan y comunican el cambio clim√°tico de una forma muy ilustrativa y eficaz.","tags":["ggplot2","warming stripes","calentamiento global"],"title":"C√≥mo crear 'Warming Stripes' in R","type":"post"},{"authors":null,"categories":["visualizaci√≥n","R:elemental","R","mapping"],"content":"\r\rLa base de datos de Open Street Maps\rRecientemente cre√© un mapa de la distribuci√≥n de gasolineras y estaciones de carga el√©ctrica en Europa.\nPopulation density through the number of gas stations in Europe. #dataviz @AGE_Oficial @mipazos @simongerman600 @openstreetmap pic.twitter.com/eIUx2yn7ej\n\u0026mdash; Dr. Dominic Roy√© (@dr_xeo) February 25, 2018  ¬øC√≥mo se puede obtener estos datos?\nPues, en este caso us√© puntos de inter√©s (PDIs) de la base de datos de Open Street Maps (OSM). Obviamente OSM no s√≥lo contiene las carreteras, sino tambi√©n informaci√≥n que nos puede ser √∫til a la hora de usar un mapa, como por ejemplo las ubicaciones de hospitales o gasolineras. Para evitar la descarga de todo el OSM y extraer la informaci√≥n requerida, se puede hacer uso de una overpass API, que nos permite hacer consultas a la base de datos de OSM con nuestros propios criterios.\nUna forma f√°cil de acceder a una overpass API es a trav√©s de overpass-turbo.eu, que incluso incluye un asistente para construir una consulta y muestra los resultados sobre un mapa interactivo. Una explicaci√≥n detallada de la p√°gina anterior la podemos encontrar aqu√≠.\rSin embargo, tenemos a nuestra disposic√≥n el paquete osmdata que nos permite crear y hacer las consultas directamente desde el entorno de R. A√∫n as√≠, el uso de la overpass-turbo.eu puede ser √∫til cuando no estamos seguros de lo que buscamos o tenemos alguna dificultad en construir la consulta.\n\rAcceso a la overpass API desde R\rEl primer paso, que debemos seguir, es instalar varios paquetes, en el caso de que no est√©n instaldos. En casi todos mis scripts hago uso de tidyverse que es una colecci√≥n fundamental de distintos paquetes, incluyendo dplyr (manipulaci√≥n de datos), ggplot2 (visualizaci√≥n), etc. El paquete sf es el nuevo est√°ndar para trabajar con datos espaciales y es compatible con ggplot2 y dplyr. Por √∫ltimo, ggmap nos facilita el trabajo para crear mapas.\n#instalamos los paquetes osmdata, sf, tidyverse y ggmap\rif(!require(\u0026quot;osmdata\u0026quot;)) install.packages(\u0026quot;osmdata\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggmap\u0026quot;)) install.packages(\u0026quot;ggmap\u0026quot;)\r#cargamos las librer√≠as\rlibrary(tidyverse)\rlibrary(osmdata)\rlibrary(sf)\rlibrary(ggmap)\r\rConstruir una consulta\rAntes de crear una consulta, debemos conocer qu√© podemos filtrar. La funci√≥n available_features( ) nos devuelve un listado amplio de las caracter√≠sticas disponibles en OSM que a su vez tienen diferentes categor√≠as (tags). Est√°n disponibles m√°s detalles en la wiki de OSM aqu√≠.\rPor ejemplo, la caracter√≠stica shop contiene como categor√≠a entre otros supermarket, fishing, books, etc.\n#las primeras cinco caracter√≠sticas head(available_features())\r## [1] \u0026quot;4wd_only\u0026quot; \u0026quot;abandoned\u0026quot; \u0026quot;abutters\u0026quot; \u0026quot;access\u0026quot; \u0026quot;addr\u0026quot; \u0026quot;addr:city\u0026quot;\r#instalaciones y establecimientos p√∫blicos\rhead(available_tags(\u0026quot;amenity\u0026quot;))\r## [1] \u0026quot;animal_boarding\u0026quot; \u0026quot;animal_breeding\u0026quot; \u0026quot;animal_shelter\u0026quot; \u0026quot;arts_centre\u0026quot; ## [5] \u0026quot;atm\u0026quot; \u0026quot;baby_hatch\u0026quot;\r#tiendas\rhead(available_tags(\u0026quot;shop\u0026quot;))\r## [1] \u0026quot;agrarian\u0026quot; \u0026quot;alcohol\u0026quot; \u0026quot;anime\u0026quot; \u0026quot;antiques\u0026quot; \u0026quot;appliance\u0026quot; \u0026quot;art\u0026quot;\rLa primera consulta: ¬øD√≥nde podemos encontrar cines en Madrid?\rPara construir la consulta se hace uso del pipe operator %\u0026gt;% que ayuda a encadenar varias funciones sin asignar el resultado a un nuevo objeto. Su uso es muy extendido especialmente con el paquete tidyverse. Si quieres saber m√°s de su uso, aqu√≠ tienes un tutorial.\nEn la primera parte de la consulta debemos indicar el lugar donde queremos extraer la informaci√≥n. La funci√≥n getbb( ) crea un rect√°ngulo de selecci√≥n para un lugar dado, buscando el nombre. La funci√≥n principal es opq( ) que construye la consulta final. A√±adimos con la funci√≥n add_osm_feature( ) nuestros criterios de filtro. En esta primera consulta buscaremos cines en Madrid. Por eso, usamos como clave amenity y como categor√≠a cinema. Existen varios formatos para obtener el resultado de la consulta. La funci√≥n osmdata_*( ) env√≠a la consulta al servidor y en funci√≥n del sufijo * sf/sp/xml nos devuelve el formato simple feature, spatial o XML.\n#construcci√≥n de la consulta\rq \u0026lt;- getbb(\u0026quot;Madrid\u0026quot;) %\u0026gt;%\ropq() %\u0026gt;%\radd_osm_feature(\u0026quot;amenity\u0026quot;, \u0026quot;cinema\u0026quot;)\rstr(q) #la estructura de la consulta\r## List of 4\r## $ bbox : chr \u0026quot;40.3119774,-3.8889539,40.6437293,-3.5179163\u0026quot;\r## $ prefix : chr \u0026quot;[out:xml][timeout:25];\\n(\\n\u0026quot;\r## $ suffix : chr \u0026quot;);\\n(._;\u0026gt;;);\\nout body;\u0026quot;\r## $ features: chr \u0026quot; [\\\u0026quot;amenity\\\u0026quot;=\\\u0026quot;cinema\\\u0026quot;]\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr [1:2] \u0026quot;list\u0026quot; \u0026quot;overpass_query\u0026quot;\r## - attr(*, \u0026quot;nodes_only\u0026quot;)= logi FALSE\rcinema \u0026lt;- osmdata_sf(q)\rcinema\r## Object of class \u0026#39;osmdata\u0026#39; with:\r## $bbox : 40.3119774,-3.8889539,40.6437293,-3.5179163\r## $overpass_call : The call submitted to the overpass API\r## $meta : metadata including timestamp and version numbers\r## $osm_points : \u0026#39;sf\u0026#39; Simple Features Collection with 220 points\r## $osm_lines : NULL\r## $osm_polygons : \u0026#39;sf\u0026#39; Simple Features Collection with 12 polygons\r## $osm_multilines : NULL\r## $osm_multipolygons : NULL\rVemos que el resultado es una lista de distintos objetos espaciales. En nuestro caso √∫nicamente nos interesar√≠a osm_points.\n¬øC√≥mo podemos visulizar estos puntos?\nLa ventaja de objetos sf es que para ggplot2 existe una geometr√≠a propia geom_sf( ). Adem√°s, haciendo uso de ggmap podemos incluir un mapa de fondo. La funci√≥n get_map( ) descarga el mapa para un lugar dado. En lugar puede ser una direcci√≥n, latitud/longitud o un rect√°ngulo de selecci√≥n. El argumento maptype nos permite indicar el estilo o tipo de mapa. Podemos consultar m√°s detalles en la ayuda de la funci√≥n ?get_map.\nCuando construimos un gr√°fico con ggplot habitualmente empezamos con ggplot( ). En este caso, se empieza por ggmap( ) que incluye el objeto con nuestro mapa de fondo. Despu√©s a√±adimos con geom_sf( ) los puntos de los cines en Madrid. Es importante indicar con el argumento inherit.aes=FALSE que debe usar aesthetic mappings del objeto espacial osm_points. Adem√°s, indicamos el color (colour, fill), transparencia (alpha), tipo y tama√±o (size) del c√≠rculo.\n#nuestro mapa de fondo\rmad_map \u0026lt;- get_map(getbb(\u0026quot;Madrid\u0026quot;), maptype = \u0026quot;toner-background\u0026quot;)\r#mapa final\rggmap(mad_map)+\rgeom_sf(data = cinema$osm_points,\rinherit.aes = FALSE,\rcolour = \u0026quot;#238443\u0026quot;,\rfill = \u0026quot;#004529\u0026quot;,\ralpha = .5,\rsize = 4,\rshape = 21)+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;)\r\r¬øD√≥nde est√°n los supermercados de Mercadona?\rEn lugar de obtener un rect√°ngulo de selecci√≥n con la funci√≥n getbb( ) podemos construir nuestro propio. Para ello, creamos un vector de cuatro elementos, siendo aqu√≠ el orden Oeste/Sur/Este/Norte. En la consulta usamos dos caracter√≠sticas: name y shop para poder filtrar supermercados que sean de esta marca en concreto. En funci√≥n del area o bien del volumen que tenga la consulta, es necesario ampliar el tiempo de espera. Por defecto, son 25 segundo (timeout).\nEl mapa que creamos en este caso se basa √∫nicamente en los puntos de supermercados. Por eso, usamos la gram√°tica habitual a√±adiendo la geometr√≠a geom_sf( ). La funci√≥n theme_void( ) elimina todo con excepci√≥n de los puntos.\n#rect√°ngulo de selecci√≥n para la Pen√≠nsula Ib√©rica\rm \u0026lt;- c(-10, 30, 5, 46)\r#construcci√≥n de la consulta\rq \u0026lt;- m %\u0026gt;% opq (timeout = 25*100) %\u0026gt;%\radd_osm_feature(\u0026quot;name\u0026quot;, \u0026quot;Mercadona\u0026quot;) %\u0026gt;%\radd_osm_feature(\u0026quot;shop\u0026quot;, \u0026quot;supermarket\u0026quot;)\rstr(q) #estructura de la consulta\r## List of 4\r## $ bbox : chr \u0026quot;30,-10,46,5\u0026quot;\r## $ prefix : chr \u0026quot;[out:xml][timeout:2500];\\n(\\n\u0026quot;\r## $ suffix : chr \u0026quot;);\\n(._;\u0026gt;;);\\nout body;\u0026quot;\r## $ features: chr \u0026quot; [\\\u0026quot;name\\\u0026quot;=\\\u0026quot;Mercadona\\\u0026quot;] [\\\u0026quot;shop\\\u0026quot;=\\\u0026quot;supermarket\\\u0026quot;]\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr [1:2] \u0026quot;list\u0026quot; \u0026quot;overpass_query\u0026quot;\r## - attr(*, \u0026quot;nodes_only\u0026quot;)= logi FALSE\r#consulta mercadona \u0026lt;- osmdata_sf(q)\r#mapa final del resultado\rggplot(mercadona$osm_points)+\rgeom_sf(colour = \u0026quot;#08519c\u0026quot;,\rfill = \u0026quot;#08306b\u0026quot;,\ralpha = .5,\rsize = 1,\rshape = 21)+\rtheme_void()\r\n\r\r","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1541203200,"objectID":"c83b921c6f624c47b8ccc546d8b00573","permalink":"https://dominicroye.github.io/es/2018/acceso-a-la-base-de-datos-de-openstreetmaps-desde-r/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/es/2018/acceso-a-la-base-de-datos-de-openstreetmaps-desde-r/","section":"post","summary":"Recientemente cre√© un mapa de la distribuci√≥n de las gasolineras y estaciones de carga el√©ctrica en Europa. ¬øC√≥mo se puede obtener estos datos? Pues, en este caso us√© puntos de inter√©s (PDIs) de la base de datos de *Open Street Maps* (OSM). Obviamente OSM no s√≥lo contiene las carreteras sino tambi√©n informaci√≥n que nos puede ser √∫til a la hora de usar un mapa como por ejemplo las ubicaciones de hospitales o gasolineras.","tags":["base de datos","overpass API","OSM","Puntos de inter√©s"],"title":"Acceso a la base de datos de OpenStreetMaps desde R","type":"post"},{"authors":["S Mathbout","JA Lopez-Bustins","D Roy√©","J Martin-Vide","J Bech","FS Rodrigo"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1541030400,"objectID":"691f8167a730e0b2b9d64edf96fb59d9","permalink":"https://dominicroye.github.io/es/publication/2017-precipitation-eastern-mediterranean-applied-geophysics/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/es/publication/2017-precipitation-eastern-mediterranean-applied-geophysics/","section":"publication","summary":"The Eastern Mediterranean is one of the most prominent hot spots of climate change in the world and extreme climatic phenomena in this region such as drought or extreme rainfall events are expected to become more frequent and intense. In this study climate extreme indices recommended by the joint World Meteorological Organization Expert Team on Climate Change Detection and Indices are calculated for daily precipitation data in 70 weather stations during 1961‚Äì2012. Observed trends and changes in daily precipitation extremes over the EM basin were analysed using the RClimDex package, which was developed by the Climate Research Branch of the Meteorological Service of Canada. Extreme and heavy precipitation events showed globally a statistically significant decrease in the Eastern Mediterranean and, in the southern parts, a significant decrease in total precipitation. The overall analysis of extreme precipitation indices reveals that decreasing trends are generally more frequent than increasing trends. We found statistically significant decreasing trends (reaching 74% of stations for extremely wet days) and increasing trends (reaching 36% of stations for number of very heavy precipitation days). Finally, most of the extreme precipitation indices have a statistically significant positive correlation with annual precipitation, particularly the number of heavy and very heavy precipitation days.","tags":["mediterr√°neo oriental","precipitaci√≥n extrema","tendencia","distribuci√≥n espacio-temporal"],"title":"Observed Changes in Daily Precipitation Extremes at Annual Timescale Over the Eastern Mediterranean During 1961‚Äì2012","type":"publication"},{"authors":null,"categories":["R","R:intermedio"],"content":"\r\r\r1 Introducci√≥n\r2 NCEP\r\r2.1 Paquetes\r2.2 Descarga de datos\r2.3 Promedio mensual\r2.4 Visualizaci√≥n\r\r3 ERA-Interim\r\r3.1 Instalaci√≥n\r3.2 Conexi√≥n y descarga con la ECMWF API\r3.3 Procesar ncdf\r\r4 Actualizaci√≥n para acceder ERA-5\r\r\rUn amigo me propuso que presentara los niveles de aprendizaje de R como categor√≠as. Una idea que ahora introduzco para cada entrada del blog. Hay tres niveles: elemental, intermedio y avanzado. Espero que ayude al lector y al usuario R.\n1 Introducci√≥n\rEn este post ense√±ar√© c√≥mo podemos descargar y trabajar directamente con datos provenientes de los rean√°lisis clim√°ticos en R. Se trata de sistemas de asimilaci√≥n de datos que combinan modelos de pron√≥stico meteorol√≥gico y observaciones de distintas fuentes de forma objetiva con el fin de sintetizar el estado actual y la evoluci√≥n de multiples variables de la atm√≥sfera, la superficie de la tierra y los oc√©anos. Los dos rean√°lisis m√°s usados son NCEP-DO (Reanalysis II) de la NOAA/OAR/ESRL, una versi√≥n mejorada de NCEP-NCAR (Reanalysis I), y ERA-Interim del ECMWF. Dado que NCEP-DO es de la primera generaci√≥m, se recomienda usar rean√°lisis de tercera generaci√≥n, especialmente ERA-Interim. Una visi√≥n general de los actuales rean√°lisis atmosf√©ricos la podemos encontrar aqu√≠. Primero vamos a ver c√≥mo acceder a los datos del NCEP a trav√©s de un paquete de R en CRAN que facilita la descarga y el manejo de los datos. Despu√©s haremos lo mismo con ERA-Interim, no obstante, para acceder a este √∫ltimo dataset de rean√°lisis es necesario usar python y la correspondiente API del ECMWF.\n\r2 NCEP\rPara acceder a los rean√°lisis del NCEP es necesario instalar el paquete correspondiente RNCEP. La funci√≥n principal es NCEP.gather( ). La resoluci√≥n del rean√°lisis del NCEP es de 2,5¬∫ X 2,5¬∫.\n2.1 Paquetes\r#instalamos los paquetes RNCEP, lubridate y tidyverse\rif(!require(\u0026quot;RNCEP\u0026quot;)) install.packages(\u0026quot;RNCEP\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\r#cargamos las librer√≠as\rlibrary(RNCEP)\rlibrary(lubridate) #la necesitamos para manipular fechas\rlibrary(tidyverse) #para visualizar y manipular library(RColorBrewer) #colores para la visualizaci√≥n\rlibrary(sf) #para importar un shapefile y trabajar con geom_sf\r\r2.2 Descarga de datos\rDescargaremos la temperatura del aire a la altura de 850haPa para el a√±o 2016. Las variables y niveles de presi√≥n pueden ser consultados en los detalles de la funci√≥n ?NCEP.gather. El argumento reanalysis2 nos permite descargar tanto la versi√≥n I como la versi√≥n II, siendo por defecto FALSE, o sea, se accede al rean√°lisis I. En todas las consultas obtendremos datos horarios de cada 6 horas (00:00, 06:00, 12:00 y 18:00). Esto supone un total de 1464 valores para el a√±o 2016.\n#definimos los argumentos necesarios\rmonth_range \u0026lt;- c(1,12) #per√≠odo de meses\ryear_range \u0026lt;- c(2016,2016) #per√≠odo de a√±os\rlat_range \u0026lt;- c(30,60) #rango de latitud\rlon_range \u0026lt;- c(-30,50) #rango de longitud\rdata \u0026lt;- NCEP.gather(\u0026quot;air\u0026quot;, #nombre de la variable\r850, #altura 850hPa\rmonth_range,year_range,\rlat_range,lon_range,\rreturn.units = TRUE,\rreanalysis2=TRUE)\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r#dimensiones dim(data) \r## [1] 13 33 1464\r#encontramos en dimnames( ) lon,lat y tiempo\r#fechas y horas date_time \u0026lt;- dimnames(data)[[3]]\rdate_time \u0026lt;- ymd_h(date_time)\rhead(date_time)\r## [1] \u0026quot;2016-01-01 00:00:00 UTC\u0026quot; \u0026quot;2016-01-01 06:00:00 UTC\u0026quot;\r## [3] \u0026quot;2016-01-01 12:00:00 UTC\u0026quot; \u0026quot;2016-01-01 18:00:00 UTC\u0026quot;\r## [5] \u0026quot;2016-01-02 00:00:00 UTC\u0026quot; \u0026quot;2016-01-02 06:00:00 UTC\u0026quot;\r#longitud y latitud\rlat \u0026lt;- dimnames(data)[[1]]\rlon \u0026lt;- dimnames(data)[[2]]\rhead(lon);head(lat)\r## [1] \u0026quot;-30\u0026quot; \u0026quot;-27.5\u0026quot; \u0026quot;-25\u0026quot; \u0026quot;-22.5\u0026quot; \u0026quot;-20\u0026quot; \u0026quot;-17.5\u0026quot;\r## [1] \u0026quot;60\u0026quot; \u0026quot;57.5\u0026quot; \u0026quot;55\u0026quot; \u0026quot;52.5\u0026quot; \u0026quot;50\u0026quot; \u0026quot;47.5\u0026quot;\r\r2.3 Promedio mensual\rVemos que se trata de un array de tres dimensiones con [lat,lon,tiempo]. Adem√°s, extraemos latitud, longitud y el tiempo. La temperatura est√° dada en Kelvin. El objetivo aqu√≠ ser√° mostrar dos mapas comparando enero y julio de 2016.\n#creamos nuestra variable de agrupaci√≥n group \u0026lt;- month(date_time) #estimamos el promedio por mes de la temperatura\rdata_month \u0026lt;- aperm(\rapply(\rdata, #nuestros datos\rc(1,2), #aplicamos a cada serie temporal 1:fila, 2:columna la funci√≥n mean( )\rby, #agrupar por group, #meses como agrupaci√≥n\rfunction(x)ifelse(all(is.na(x)),NA,mean(x))),\rc(2,3,1)) #reordenamos para obtener un array como el original\rdim(data_month) #temperatura 850haP por mes enero a diciembre\r## [1] 13 33 12\r\r2.4 Visualizaci√≥n\rAhora, podemos visualizar con ggplot2 la temperatura de enero y julio. En este ejemplo, uso geom_sf( ) del paquetes sf, que hace el trabajo m√°s f√°cil para visualizar en ggplot objetos espaciales (en el futuro har√© un post sobre sf y ggplot). En la dimensi√≥n de latitud y longitud vemos que √∫nicamente nos indica para cada fila y columna un valor. Pero necesitamos las coordenadas de todas las celdas de la matriz. Para crear todas las combinaciones entre dos variables usamos la funci√≥n expand.grid( ).\n#primero creamos todas las combinaciones de lonlat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#lonlat es car√°cter, ya que fue un nombre, por eso lo convertimos en n√∫merico\rlonlat \u0026lt;- apply(lonlat,2,as.numeric)\r#lon y lat no est√°n en el orden como lo esperamos\r#fila=lon; columna=lat\rdata_month \u0026lt;- aperm(data_month,c(2,1,3))\r#subtraemos 273.15K para convertir K a ¬∫C.\rdf \u0026lt;- data.frame(lonlat,\rTa01=as.vector(data_month[,,1])-273.15,\rTa07=as.vector(data_month[,,7])-273.15)\rAntes de visualizar los datos con ggplot2, tenemos que adpatar la tabla. El shapefile con los limites de los pa√≠ses se puede descargar aqu√≠.\n#convertimos la tabla ancha en una larga\rdf \u0026lt;- gather(df,month,Ta,Ta01:Ta07)%\u0026gt;%\rmutate(month=factor(month,unique(month),c(\u0026quot;Jan\u0026quot;,\u0026quot;Jul\u0026quot;)))\r#importamos el limite de pa√≠ses\rlimit \u0026lt;- st_read(\u0026quot;CNTR_RG_03M_2014.shp\u0026quot;)\r## Reading layer `CNTR_RG_03M_2014\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\es\\post\\2018-09-15-acceso-a-datos-de-los-reanalisis-desde-r\\CNTR_RG_03M_2014.shp\u0026#39; ## using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 256 features and 3 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: -180 ymin: -90 xmax: 180 ymax: 83.66068\r## Geodetic CRS: ETRS89\r#gama de colores\rcolbr \u0026lt;- brewer.pal(11,\u0026quot;RdBu\u0026quot;)\rggplot(df)+\rgeom_tile(aes(lon,lat,fill=Ta))+ #temperatura\rgeom_sf(data=limit,fill=NA,size=.5)+ #limite\rscale_fill_gradientn(colours=rev(colbr))+\rcoord_sf(ylim=c(30,60),xlim=c(-30,50))+\rscale_x_continuous(breaks=seq(-30,50,10),expand=c(0,0))+\rscale_y_continuous(breaks=seq(30,60,5),expand=c(0,0))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Ta 850hPa (¬∫C)\u0026quot;)+\rfacet_grid(month~.)+ #mapa por mes\rtheme_bw()\r\r\r3 ERA-Interim\rEl ECMWF ofrece acceso a sus bases de datos p√∫blicos a partir de una pyhton-API. Es necesario estar registrado en la web del ECMWF. Se puede darse de alta aqu√≠. Al tratarse de otro lenguaje de programaci√≥n, en R debemos usar un interfaz entre ambos lo que nos permite el paquete reticulate. Tambi√©n debemos que tener instalada una distribuci√≥n de pyhton (versi√≥n 2.x o 3.x). En el caso de Windows podemos usar anaconda.\n Recientemente se ha publicado un nuevo paquete ecmwfr que facilita el acceso a los APIs de Copernicus y ECMWF. La gran ventaja es que no hace falta instalar python. M√°s detalles aqu√≠. En 2022 publiqu√© un nuevo post actualizado.   3.1 Instalaci√≥n\rif(!require(\u0026quot;reticulate\u0026quot;)) install.packages(\u0026quot;reticulate\u0026quot;)\rif(!require(\u0026quot;ncdf4\u0026quot;)) install.packages(\u0026quot;ncdf4\u0026quot;) #para manejar formato netCDF\r#cargamos las librer√≠as\rlibrary(reticulate)\rlibrary(ncdf4)\rUna vez que tenemos instalado anaconda y el paquete reticulate, podemos instalar el paquete python ecmwfapi. La instalaci√≥n la podemos llevar a cabo, o bien atrav√©s del CMD de Windows usando el comando conda install -c conda-forge ecmwf-api-client, o bien con la funci√≥n py_install( ) del paquete reticulate. La misma funci√≥n permite instalar cualquier librer√≠a python desde R.\n#instalamos la API ECMWF\rpy_install(\u0026quot;ecmwf-api-client\u0026quot;)\r\r3.2 Conexi√≥n y descarga con la ECMWF API\rPara poder acceder a la API es requisito crear un archivo con la informaci√≥n del usuario.\nEl archivo ‚Äú.ecmwfapirc‚Äù debe contener la siguiente informaci√≥n:\n{\r\u0026quot;url\u0026quot; : \u0026quot;https://api.ecmwf.int/v1\u0026quot;,\r\u0026quot;key\u0026quot; : \u0026quot;XXXXXXXXXXXXXXXXXXXXXX\u0026quot;,\r\u0026quot;email\u0026quot; : \u0026quot;john.smith@example.com\u0026quot;\r}\rLa clave podemos obtenerla con la cuenta de usuario aqu√≠.\nEl archivo se puede crear con el bloc de notas de Windows.\nCreamos un documento ‚Äúecmwfapirc.txt‚Äù.\rRenombramos este archivo a ‚Äú.ecmwfapirc.‚Äù\r\rEl √∫ltimo punto desaparece de forma autom√°tica. Despu√©s guardamos este archivo en ‚ÄúC:/USERNAME/.ecmwfapirc‚Äù o ‚ÄúC:/USERNAME/Documents/.ecmwfapirc‚Äù.\n#importamos la librer√≠a python ecmwfapi\recmwf \u0026lt;- import(\u0026#39;ecmwfapi\u0026#39;)\r#en este paso debe existir el archivo .ecmwfapirc\rserver = ecmwf$ECMWFDataServer() #iniciamos la conexi√≥n\rLlegados a este punto, ¬øc√≥mo creamos una consulta? Lo m√°s f√°cil es ir a la web del ECMWF d√≥nde elegimos la base de datos, en este caso ERA-Interim en superficie, para crear un script con todos los datos necesarios. M√°s detalles sobre la sintaxis podemos encontrar aqu√≠. Cuando procedemos en la web s√≥lamente tenemos que hacer click en ‚ÄúView MARS Request‚Äù. Este paso nos lleva al script en python.\nCon la sintaxis del script que nos da la MARS Request podemos crear la consulta en R.\n#creamos la consulta\rquery \u0026lt;-r_to_py(list(\rclass=\u0026#39;ei\u0026#39;,\rdataset= \u0026quot;interim\u0026quot;, #base de datos\rdate= \u0026quot;2017-01-01/to/2017-12-31\u0026quot;, #periodo expver= \u0026quot;1\u0026quot;,\rgrid= \u0026quot;0.125/0.125\u0026quot;, #resoluci√≥n\rlevtype=\u0026quot;sfc\u0026quot;,\rparam= \u0026quot;167.128\u0026quot;, # temperatura del aire (2m)\rarea=\u0026quot;45/-10/30/5\u0026quot;, #N/W/S/E\rstep= \u0026quot;0\u0026quot;,\rstream=\u0026quot;oper\u0026quot;,\rtime=\u0026quot;00:00:00/06:00:00/12:00:00/18:00:00\u0026quot;, #paso de tiempo\rtype=\u0026quot;an\u0026quot;,\rformat= \u0026quot;netcdf\u0026quot;, #formato\rtarget=\u0026#39;ta2017.nc\u0026#39; #nombre del archivo\r))\rserver$retrieve(query)\rEl resultado es un archivo netCDF que podemos processar con el paquete ncdf4.\n\r3.3 Procesar ncdf\rA partir de aqu√≠, el objetivo ser√° la extracci√≥n de una serie temporal de una coordenada m√°s pr√≥xima a una dada. Usaremos las coordenadas de Madrid (40.418889, -3.691944).\n#cargamos las librer√≠as library(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\r#abrimos la conexi√≥n con el archivo\rnc \u0026lt;- nc_open(\u0026quot;ta2017.nc\u0026quot;)\r#extraemos lon y lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r#extraemos el tiempo\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#unidad del tiempo: horas desde 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r#convertimos las horas en fecha+hora #as_datetime de lubridate espera segundos\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#importamos los datos\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#cerramos la conexi√≥n\rnc_close(nc)\rM√°s detalles se pueden consultar en este breve manual sobre c√≥mo trabajar con netCDF aqui. En esta pr√≥xima secci√≥n hacemos uso del paquete sf la cu√°l est√° sustituyendo las m√°s conocidas sp y rgdal.\n#creamos todas las combinaciones\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#debemos convertir las coordenadas en objeto espacial sf\r#adem√°s indicamos el sistema de coordenadas en codigo EPSG\rcoord \u0026lt;- st_as_sf(lonlat,coords=c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;))%\u0026gt;%\rst_set_crs(4326)\r#lo mismo hacemos con nuestra coordenada de Madrid\rpsj \u0026lt;- st_point(c(-3.691944,40.418889))%\u0026gt;%\rst_sfc()%\u0026gt;%\rst_set_crs(4326)\r#plot de los puntos\rplot(st_geometry(coord))\rplot(psj,add=TRUE,pch = 3, col = \u0026#39;red\u0026#39;)\rEn los pr√≥ximos pasos calculamos la distancia de nuestro punto de referencia a todos los puntos del grid. Posteriormente, buscamos aquel con menos distancia.\n#a√±adimos la distancia a los puntos\rcoord \u0026lt;- mutate(coord,dist=st_distance(coord,psj))\r#creamos una matrix de distancia con las mismas dimensiones que nuestros datos\rdist_mat \u0026lt;- matrix(coord$dist,dim(data)[-3])\r#la funci√≥n arrayInd es √∫til para obtener los √≠ndices fila y columna en este caso\rmat_index \u0026lt;- as.vector(arrayInd(which.min(dist_mat), dim(dist_mat)))\r#extraemos la serie temporal y cambiamos la unidad de K a ¬∫C\r#convertimos el tiempo en fecha + hora\rdf \u0026lt;- data.frame(ta=data[mat_index[1],mat_index[2],],time=timestamp)%\u0026gt;%\rmutate(ta=ta-273.15,time=ymd_hms(time))\rPara terminar, visualizamos nuestra serie temporal.\nggplot(df,\raes(time,ta))+\rgeom_line()+\rlabs(y=\u0026quot;Temperatura (¬∫C)\u0026quot;,\rx=\u0026quot;\u0026quot;)+\rtheme_bw()\r\r\r4 Actualizaci√≥n para acceder ERA-5\rRecientemente el nuevo rean√°lisis ERA-5 con single level o pressure level fue puesto a disposici√≥n de los usarios. Es la quinta generaci√≥n del European Centre for Medium-Range Weather Forecasts (ECMWF) y accesible a trav√©s de una nueva API de Copernicus. El nuevo rean√°lisis ERA-5 tiene una cobertura temporal desde 1950 hasta la actualidad a una resoluci√≥n horizontal de 30km a nivel mundial, con 137 niveles desde la superficie hasta una altura de 80km. Una diferencia importante con respecto al ERA-Interim anterior es la resoluci√≥n temporal con datos horarios.\nEl acceso cambia a la infrastructura de Climate Data Store (CDS) con su propia API. Es posible descargar directamente desde la p√°gina o usando la Python API en una forma similar a la ya presentada en este post. Sin embargo, existen ligeras diferencias que voy a explicar a continuaci√≥n.\nEs necesario tener una cuenta en CDS de Copernicus link\rNuevamente, hace falta una clave link\rCambia la librer√≠a de Python y algo los argumentos en la consulta.\r\r#cargamos las librer√≠as library(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\rlibrary(reticulate)\r#instalamos la CDS API\rconda_install(\u0026quot;r-reticulate\u0026quot;,\u0026quot;cdsapi\u0026quot;,pip=TRUE)\rPara poder acceder a la API un requisito es crear un archivo con la informaci√≥n del usuario.\nEl archivo ‚Äú.cdsapirc‚Äù debe contener la siguiente informaci√≥n:\n\rurl: https://cds.climate.copernicus.eu/api/v2\rkey: {uid}:{api-key}\r\rLa clave la podemos obtener con la cuenta de usuario en el User profile.\nEl archivo se puede crear con el bloc de notas de Windows del mismo modo como ha sido explicado para ERA-Interim.\n#importamos la librer√≠a python CDS\rcdsapi \u0026lt;- import(\u0026#39;cdsapi\u0026#39;)\r#en este paso debe existir el archivo .cdsapirc\rserver = cdsapi$Client() #iniciamos la conexi√≥n\rCon la sintaxis del script que nos da la Show API request podemos crear la consulta en R.\n#creamos la consulta\rquery \u0026lt;- r_to_py(list(\rvariable= \u0026quot;2m_temperature\u0026quot;,\rproduct_type= \u0026quot;reanalysis\u0026quot;,\ryear= \u0026quot;2018\u0026quot;,\rmonth= \u0026quot;07\u0026quot;, #formato: \u0026quot;01\u0026quot;,\u0026quot;01\u0026quot;, etc.\rday= str_pad(1:31,2,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;), time= str_c(0:23,\u0026quot;00\u0026quot;,sep=\u0026quot;:\u0026quot;)%\u0026gt;%str_pad(5,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;),\rformat= \u0026quot;netcdf\u0026quot;,\rarea = \u0026quot;45/-20/35/5\u0026quot; # North, West, South, East\r))\rserver$retrieve(\u0026quot;reanalysis-era5-single-levels\u0026quot;,\rquery,\r\u0026quot;era5_ta_2018.nc\u0026quot;)\rEs posible que la primera vez se reciba un mensaje de error, dado que todav√≠a no se han aceptado los t√©rminos y las condiciones requeridas. √önicamente se debe seguir el enlace indicado.\nError in py_call_impl(callable, dots$args, dots$keywords) : Exception: Client has not agreed to the required terms and conditions.. To access this resource, you first need to accept the termsof \u0026#39;Licence to Use Copernicus Products\u0026#39; at https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products\rA partir de aqu√≠ podemos seguir los mismos pasos como los hechos con ERA-Interim.\n#abrimos la conexi√≥n con el archivo\rnc \u0026lt;- nc_open(\u0026quot;era5_ta_2018.nc\u0026quot;)\r#extraemos lon y lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r#extraemos el tiempo\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#unidad del tiempo: horas desde 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r#convertimos las horas en fecha+hora #as_datetime de lubridate espera segundos\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#temperatura en K de julio 2018\rhead(timestamp)\r#importamos los datos\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#plot\rfilled.contour(data[,,1])\rplot(data.frame(date=timestamp,ta=data[1,5,]),\rtype=\u0026quot;l\u0026quot;)\r#cerramos la conexi√≥n\rnc_close(nc)\r\n\r","date":1537005584,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1537005584,"objectID":"b8f9c2ee950e35ea3898cf650edb0066","permalink":"https://dominicroye.github.io/es/2018/acceso-a-datos-de-los-reanalisis-climaticos-desde-r/","publishdate":"2018-09-15T10:59:44+01:00","relpermalink":"/es/2018/acceso-a-datos-de-los-reanalisis-climaticos-desde-r/","section":"post","summary":"En este post ense√±ar√© c√≥mo podemos descargar y trabajar directamente con datos provinientes de los rean√°lisis clim√°ticos en R. Se trata de sistemas de asimilaci√≥n de datos que combinan modelos de pron√≥stico meteorol√≥gico y observaciones de distintas fuentes de forma objetiva con el fin de sintetizar el estado actual y la evoluci√≥n de multiples variables de la atm√≥sfera, la superficie de la tierra y los oc√©anos.","tags":["reanalisis","interim","NCEP/NCAR","era","descarga","ncdf","acceso","api","python","ECMWF"],"title":"Acceso a datos de los rean√°lisis clim√°ticos desde R","type":"post"},{"authors":null,"categories":["visualizaci√≥n","R","R:elemental"],"content":"\r\rBienvenido a mi blog! Soy Dominic Roy√©, investigador y docente de geograf√≠a f√≠sica en la Universidad de Santiago de Compostela. Una de mis pasiones es la programaci√≥n en R para visualizar y analizar cualquier tipo de datos. Por eso, mi idea de iniciar este blog tiene su origen en las publicaciones que he ido haciendo en el √∫timo a√±o en Twitter sobre diferentes temas visualizando datos que describen el mundo. Adem√°s, me gustar√≠a aprovechar la posibilidad del blog e ir publicando breves ensayos sobre visualizaci√≥n, gesti√≥n y manipulaci√≥n de datos en R. Espero que os guste. Cualquier sugerencia o idea, ser√° bienvenida.\nPre√°mbulo\rSiempre he querido escribir sobre el uso del gr√°fico de tarta. El gr√°fico circular es ampliamente usado en investigaci√≥n, docencia, periodismo o en informes t√©cnicos. Es m√°s, no s√© si es debido a Excel, pero todav√≠a peor que el mismo gr√°fico de tarta es su versi√≥n en 3D (tambi√©n para el gr√°fico de barras). Sobre las versiones 3D √∫nicamente debo decir que no es recomendable, ya que en estos casos la tercera dimensi√≥n no contiene ninguna informaci√≥n y por tanto no ayuda en leer correctamente la informaci√≥n del gr√°fico. Respecto al gr√°fico de tarta, entre muchos expertos no se aconseja claramente su uso. Pero, ¬øpor qu√©?\nYa en un estudio hecho por Simkin (1987) encontraron que la interpretaci√≥n y el procesamiento de √°ngulos es m√°s d√≠ficil que el de formas lineales. Mayormente es m√°s f√°cil leer un gr√°fico de barras que uno de tarta. Un problema que se hace muy visible cuando nos encontramos con; 1) demasiadas categor√≠as 2) pocas diferencias entre las categor√≠as 3) un mal uso de colores como leyenda √≥ 4) comparaciones entre varios gr√°ficos de tarta.\nPara decidir qu√© posibles representaciones gr√°ficas existen para nuestros datos, recomiendo ir a la p√°gina web www.data-to-viz.com o usar Financial Times Visual Vocabulary.\n\nAhora bien, ¬øqu√© alternativas podemos usar en R?\n\rAlternativas al gr√°fico de tarta\rLos datos sobre el estado de la enfermedad sarampi√≥n corresponden a junio de 2018 en Europa y vienen del ECDC.\n#librer√≠as\rlibrary(tidyverse)\rlibrary(scales)\rlibrary(RColorBrewer)\r#datos\rmeasles \u0026lt;- data.frame(\rvacc_status=c(\u0026quot;Unvaccinated\u0026quot;,\u0026quot;1 Dose\u0026quot;,\r\u0026quot;\u0026gt;= 2 Dose\u0026quot;,\u0026quot;Unkown Dose\u0026quot;,\u0026quot;Unkown\u0026quot;),\rprop=c(0.75,0.091,0.05,0.012,0.096)\r)\r#ordenamos de mayor a menor y lo fijamos con un factor measles \u0026lt;- arrange(measles,\rdesc(prop))%\u0026gt;%\rmutate(vacc_status=factor(vacc_status,vacc_status))\r\r\rvacc_status\rprop\r\r\r\rUnvaccinated\r0.750\r\rUnkown\r0.096\r\r1 Dose\r0.091\r\r\u0026gt;= 2 Dose\r0.050\r\rUnkown Dose\r0.012\r\r\r\rGr√°fico de barra o similar\rggplot(measles,aes(vacc_status,prop))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convertimos en %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\rggplot(measles,aes(x=vacc_status,prop,ymin=0,ymax=prop))+\rgeom_pointrange()+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convertimos en %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\r#definiciones sobre el theme que usamos\rtheme_singlebar \u0026lt;- theme_bw()+\rtheme(\rlegend.position = \u0026quot;bottom\u0026quot;,\raxis.title = element_blank(),\raxis.ticks.y = element_blank(),\raxis.text.y = element_blank(),\rpanel.border = element_blank(),\rpanel.grid=element_blank(),\rplot.title=element_text(size=14, face=\u0026quot;bold\u0026quot;)\r)\rmutate(measles,\rvacc_status=factor(vacc_status, #cambiamos el orden de las categor√≠as\rrev(levels(vacc_status))))%\u0026gt;%\rggplot(aes(1,prop,fill=vacc_status))+ #ponemos 1 en x para crear una barra √∫nica\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent,\rlimits=c(0,1),\rexpand=c(.01,.01))+\rscale_x_continuous(expand=c(0,0))+\rscale_fill_brewer(\u0026quot;\u0026quot;,palette=\u0026quot;Set1\u0026quot;)+\rcoord_flip()+\rtheme_singlebar\r#ampliamos los datos con las cifras de Italia\rmeasles2 \u0026lt;- mutate(measles,\ritaly=c(0.826,0.081,0.053,0.013,0.027),\rvacc_status=factor(vacc_status,rev(levels(vacc_status))))%\u0026gt;%\rrename(europe=\u0026quot;prop\u0026quot;)%\u0026gt;%\rgather(region,prop,europe:italy)\rggplot(measles2,aes(region,prop,fill=vacc_status))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,position=\u0026quot;stack\u0026quot;)+ #stack: columna 100%\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convertimos en %\rlimits=c(0,1),\rexpand=c(0,0))+\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;)+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Vaccination Status\u0026quot;)+\rtheme_minimal()\r\rGr√°fico de Waffle\r#liber√≠a\rlibrary(waffle)\r#la funci√≥n de waffle usa un vector con nombres\rval_measles \u0026lt;- round(measles$prop*100)\rnames(val_measles) \u0026lt;- measles$vacc_status\rwaffle(val_measles, #datos\rcolors=brewer.pal(5,\u0026quot;Set1\u0026quot;), #colores\rrows=5) #n√∫mero de filas \rEl gr√°fico de Waffle me parece muy interesante cuando queramos mostrar una proporci√≥n de una categor√≠a individual.\nmedida \u0026lt;- c(41,59) #datos de la OECD 2015\rnames(medida) \u0026lt;- c(\u0026quot;Estudios Superiores\u0026quot;,\u0026quot;Otros estudios\u0026quot;)\rwaffle(medida,\rcolors=c(\u0026quot;#377eb8\u0026quot;,\u0026quot;#bdbdbd\u0026quot;),\rrows=5)\r\rMapa de arbol (treemap)\r#librer√≠a\rlibrary(treemap)\rtreemap(measles,\rindex=\u0026quot;vacc_status\u0026quot;, #variable de categr√≠as\rvSize=\u0026quot;prop\u0026quot;, #valores\rtype=\u0026quot;index\u0026quot;, #estilo m√°s en ?treemap\rtitle=\u0026quot;\u0026quot;, palette = brewer.pal(5,\u0026quot;Set1\u0026quot;) #colores\r)\rPersonalmente, creo que todos los tipos de representaciones gr√°ficas tienen sus ventajas y desventajas. No obstante, en la actualidad tenemos una gran variedad de alternativas para evitar el uso del gr√°fico de tarta. Si a√∫n as√≠ se quiere hacer un gr√°fico de tarta, algo que tampoco descartar√≠a, recomiendo seguir ciertas reglas que ha resumido muy bien Lisa Charlotte Rost en un reciente post. Por ejemplo, debemos ordenar de mayor a menor a no ser que haya un orden natural o usar como m√°ximo cinco categor√≠as. Por √∫ltimo, os dejo un enlace a un cheatsheet de policyviz sobre normas b√°sicas de visualizaci√≥n de datos. Una buena referencia sobre gr√°ficos, usando diferentes programas desde Excel hasta R, pod√©is encontrar en Creating more effective graphs (Robbins 2013).\n\rReferencias\r\r\r","date":1534896000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1534896000,"objectID":"9dd4d903e1e176537fe595f97268bd28","permalink":"https://dominicroye.github.io/es/2018/grafico-de-tarta/","publishdate":"2018-08-22T00:00:00Z","relpermalink":"/es/2018/grafico-de-tarta/","section":"post","summary":"Bienvenido a mi blog! Soy Dominic Roy√©, investigador y docente de geograf√≠a f√≠sica en la Universidad de Santiago de Compostela. Una de mis pasiones es la programaci√≥n en R para visualizar y analizar cualquier tipo de datos.","tags":["gr√°fico de tarta","datos","circular","proporciones","primera entrada","treemap","waffle","barra","visualizaci√≥n"],"title":"gr√°fico de tarta","type":"post"},{"authors":["D Roy√©","A Figueiras","M Taracido-Trunk"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1522540800,"objectID":"ea6e53da0515d25a726b65d99f008856","permalink":"https://dominicroye.github.io/es/publication/2018-over-the-counter-drugs-pharmacoepidemiolgy/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/es/publication/2018-over-the-counter-drugs-pharmacoepidemiolgy/","section":"publication","summary":"The consumption of medication, especially over-the-counter (OTC) drugs, can reflect environmental exposure with a lesser degree of severity in terms of morbidity. The non-linear effects of maximum and minimum apparent temperature on respiratory drug sales in A Coru√±a from 2006 to 2010 were examined using a distributed lag non-linear model. In particular, low apparent temperatures proved to be associated with increased sales of respiratory drugs. The strongest consistent risk estimates were found for minimum apparent temperatures in respiratory drug sales with an increase of 33.4% (95% CI: 12.5-58.0%) when the temperature changed from 2.8 ¬∫C to ‚àí1.4 ¬∫C. These findings may serve to guide the planning of public health interventions in order to predict and manage the health effects of exposure to the thermal environment for lower degrees of morbidity. More precisely, significant increases in the use of measured OTC medication could be used to identify and anticipate influenza outbreaks due to a more sensitive degree of the data source.","tags":["medicamentos","farmacoepidemiolog√≠a","causas respiratorias","efectos a corto plazo","Espa√±a","ambiente t√©rmico"],"title":"Short-term effects of heat and cold on respiratory drug use. A time-series epidemiological study in A Coru√±a, Spain","type":"publication"},{"authors":["D Roy√©","N Lorenzo","J Martin-Vide"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1522540800,"objectID":"d2783bf176c9ad719df9b5bca1010728","permalink":"https://dominicroye.github.io/es/publication/2019-lightning-galicia-natural-hazards/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/es/publication/2019-lightning-galicia-natural-hazards/","section":"publication","summary":"The spatial-temporal patterns of cloud-to-ground (CG) lightning covering the period 2010-2015 over the northwest Iberian Peninsula were investigated. The analysis conducted employed three main methods: the circulation weather types developed by Jenkinson \u0026 Collison, the fit of a generalized additive model for geographic variables and the use of a concentration index for the ratio of lightning strikes and thunderstorm days. The main activity in the summer months can be attributed to situations with eastern or anticyclonic flow due to convection by insolation. In winter, lightning proves to have a frontal origin and is mainly associated with western or cyclonic flow situations which occur with advections of air masses of maritime origin. The largest number of CG discharges occurs under eastern flow and their hybrids with anticyclonic situations. Thunderstorms with greater CG lightning activity, highlighted by a higher Concentration Index, are located in areas with a higher density of lightning strikes, above all in mountainous areas away from the sea. The modeling of lightning density with geographic variables shows the positive influence of altitude and, particularly, distance to the sea, with nonlinear relationships due to the complex orography of the region. Likewise, areas with convex topography receive more lightning strikes than concave ones, a relation which has been demonstrated for the first time from a Generalized Additive Model (GAM).","tags":["tormenta","pen√≠nsula ib√©rica","√≠ndice de concentraci√≥n","tipos de tiempo","√≠ndice de convexidad","modelo aditivo generalizado","rel√°mpago de nube a tierra"],"title":"Spatial‚Äìtemporal patterns of cloud-to-ground lightning over the northwest Iberian Peninsula during the period 2010‚Äì2015","type":"publication"},{"authors":["D Roy√©"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1501545600,"objectID":"069710e0950378660e62ac25f43fac73","permalink":"https://dominicroye.github.io/es/publication/2017-hotnights-barcelona-ij-biometeo/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/es/publication/2017-hotnights-barcelona-ij-biometeo/","section":"publication","summary":"Heat-related effects on mortality have been widely analyzed using maximum and minimum temperatures as exposure variables. Nevertheless, the main focus is usually on the former with the minimum temperature being limited in use as far as human health effects are concerned. Therefore, new thermal indices were used in this research to describe the duration of night hours with air temperatures higher than the 95% percentile of the minimum temperature (Hot Night hours) and intensity as the summation of these air temperatures in degrees (Hot Night degrees). An exposure-response relationship between mortality due to natural, respiratory and cardiovascular causes and summer night temperatures was assessed using data from the Barcelona region between 2003 and 2013. The non-linear relationship between the exposure and response variables was modeled using a distributed lag non-linear model. The estimated associations for both exposure variables and mortality shows a relationship with high and medium values that persist significantly up to a lag of 1‚Äì2 days. In mortality due to natural causes an increase of 1.1% per 10% (CI95% 0.6‚Äì1.5) for Hot Night hours and 5.8% per each 10¬∫ (CI95% 3.5‚Äì8.2%) for Hot Night degrees is observed. The effects of Hot Night hours reach their maximum with 100% and leads to an increase by 9.2% (CI95% 5.3‚Äì13.1%). The hourly description of night heat effects reduced to a single indicator in duration and intensity is a new approach and shows a different perspective and significant heat-related effects on human health.","tags":["calor","mortalidad","noche tropical","noche calurosa","efectos","salud humana","cambio clim√°tico"],"title":"The effects of hot nights on mortality in Barcelona, Spain","type":"publication"},{"authors":["D Roy√©","J Martin-Vide"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1496275200,"objectID":"26c02ac889210ed94598f98e7006ee4b","permalink":"https://dominicroye.github.io/es/publication/2017-ci-usa-atmospheric-research/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/es/publication/2017-ci-usa-atmospheric-research/","section":"publication","summary":"The contiguous US exhibits a wide variety of precipitation regimes, first, because of the wide range of latitudes and altitudes. The physiographic units with a basic meridional configuration contribute to the differentiation between east and west in the country while generating some large interior continental spaces. The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. Positive exponential curves, which plot the cumulative percentages of days with precipitation against the cumulative percentage of the rainfall amounts that they contribute, can be evaluated through the Concentration Index. The Concentration Index has been applied to the contiguous United States using a gridded climate dataset of daily precipitation data, at a resolution of 0.25¬∞, provided by CPC/NOAA/OAR/Earth System Research Laboratory, for the period between 1956 and 2006. At the same time, other rainfall indices and variables such as the annual coefficient of variation, seasonal rainfall regimes and the probabilities of a day with precipitation have been presented with a view to explaining spatial CI patterns. The spatial distribution of the CI in the contiguous United States is geographically consistent, reflecting the principal physiographic and climatic units of the country. Likewise, linear correlations have been established between the CI and geographical factors such as latitude, longitude and altitude. In the latter case the Pearson correlation coefficient (r) between this factor and the CI is ‚àí0.51 (p-value ","tags":["√≠ndice de concentraci√≥n","Estados Unidos contiguos","precipitaci√≥n diaria","√≠ndices de precipitaci√≥n","patrones espacio-temporales"],"title":"Concentration of Daily Precipitation in the Contiguous United States","type":"publication"},{"authors":null,"categories":null,"content":"\n                                                                 \n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1493251200,"objectID":"f587a6ed3c666895dc27f4decefa8d5e","permalink":"https://dominicroye.github.io/es/project/climate/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/es/project/climate/","section":"project","summary":".","tags":["clima","meteo","clima","dataviz","atm√≥sfera","temperatura"],"title":"Clima y tiempo","type":"project"},{"authors":["P Fdez-Arroyabe","D Roy√©"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1483228800,"objectID":"f837334b8a2b59cb162956814d1fe5cb","permalink":"https://dominicroye.github.io/es/publication/2017-health-related-climate-services-springer/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/es/publication/2017-health-related-climate-services-springer/","section":"publication","summary":"Co-creation of scientific knowledge based on new technologies and big data sources is one of the main challenges for the digital society in the XXI century. Data management and the analysis of patterns among datasets based on machine learning and artificial intelligence has become essential for many sectors nowadays. The development of real time health-related climate services represents an example where abundant structured and unstructured information and transdisciplinary research are needed. The study of the interactions between atmospheric processes and human health through a big data approach can reveal the hidden value of data. The Oxyalert technological platform is presented as an example of a digital biometeorological infrastructure able to forecast, at an individual level, oxygen changes impacts on human health.","tags":["co-creaci√≥n","interdisciplinariedad","transdisciplinariedad","morbilidad","servicios clim√°ticos","brecha digital","big data","salud"],"title":"Co-creation and Participatory Design of Big Data Infrastructures on the Field of Human Health Related Climate Services","type":"publication"},{"authors":null,"categories":null,"content":"\n                                                           \n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1461715200,"objectID":"2af5c55ffb6a7c9711c91ff9da440e2f","permalink":"https://dominicroye.github.io/es/project/geography/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/es/project/geography/","section":"project","summary":".","tags":["geograf√≠a","dataviz"],"title":"Geograf√≠a","type":"project"},{"authors":["D Roy√©","J Taboada","A Ezpeleta-Mart√≠","N Lorenzo"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1459468800,"objectID":"9c734b1b35d39b36fa1e6438b90d6e59","permalink":"https://dominicroye.github.io/es/publication/2016-cwt-hospital-galicia-ij-biometeo/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/es/publication/2016-cwt-hospital-galicia-ij-biometeo/","section":"publication","summary":"The link between various pathologies and atmospheric conditions has been a constant topic of study over recent decades in many places across the world; knowing more about it enables us to pre-empt the worsening of certain diseases, thereby optimizing medical resources. This study looked specifically at the connections in winter between respiratory diseases and types of atmospheric weather conditions (Circulation Weather Types, CWT) in Galicia, a region in the north-western corner of the Iberian Peninsula. To do this, the study used hospital admission data associated with these pathologies as well as an automatic classification of weather types. The main result obtained was that weather types giving rise to an increase in admissions due to these diseases are those associated with cold, dry weather, such as those in the east and south-east, or anticyclonic types. A second peak was associated with humid, hotter weather, generally linked to south-west weather types. In the future, this result may help to forecast the increase in respiratory pathologies in the region some days in advance.","tags":["tipo de tiempo","enfermedades respiratorias","ingresos hospitalarios","salud humana","Espa√±a"],"title":"Winter circulation weather types and hospital admissions for respiratory diseases in Galicia, Spain","type":"publication"},{"authors":["D Roy√©","A Ezpeleta-Mart√≠"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1448928000,"objectID":"6cc0078c47ea592afe6336436ef9ed39","permalink":"https://dominicroye.github.io/es/publication/2015-hotnight-fachada-atlantica-bage/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/es/publication/2015-hotnight-fachada-atlantica-bage/","section":"publication","summary":"Analysis of tropical nights on the Atlantic coast of the Iberian peninsula. A proposed methodology. This paper presents a new methodology for the study of warm nights, also called ¬´tropical¬ª, in Galicia and Portugal in order to identify those nights where people can be affected by heat stress. The use of two indicators obtained through half-hourly data has allowed us to define in more detail the thermal characteristics of the nights between May and October, thereby being able to more accurately assess the risk to the health and well-being of the population. There is a significant increase in the frequency of tropical nights and warm nights on the Atlantic coast, from the north of Galicia to the south of Portugal. The lower latitude and proximity to the coastline are associated with greater persistence of heat and thermal stress during these nights. In inland areas the persistence is less. The warmest nights are more frequent and intense in centres of the cities, due to the effect of the urban heat island.","tags":["noche tropical","estr√©s t√©rmico","isla de calor","Galicia","Portugal"],"title":"Analysis of tropical nights on the atlantic coast of the Iberian Peninsula. A proposed methodology","type":"publication"},{"authors":["D Roy√©"],"categories":null,"content":"Used datasets are available for download here. Alternative datasets for Spain in ncdf format can be downloaded:\nAEMET\n  Gridded 20km and 50km (precipitation and temperature)\n  Gridded 5km (precipitation)\n  CSIC\n Gridded 5km (precipitation)  ","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1448928000,"objectID":"43da158fda5ff317611ce8996ecab175","permalink":"https://dominicroye.github.io/es/publication/2015-manual-ncdf-semata/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/es/publication/2015-manual-ncdf-semata/","section":"publication","summary":"A practical introduction in the use of netCDF in the environment of R Spatio-temporal data is currently key to many disciplines, especially to climatology and meteorology. A widespread format is netCDF allowing a multidimensional structure and an exchange of data machine independently. In this article, we introduce the use of these databases with the free software environment R. To do this, we will work with a grid of the maximum temperature of the Iberian Peninsula for the period 1971-2007. The goal is to read and visualize the netCDF format, and make some fist overall and specifi calculations. Finally the applicability is shown in a case study: the diurnal temperature variation in the Iberian Peninsula for January and August 2006. (Spanish)","tags":["netCDF","R","climatolog√≠a","temperatura","matriz","base de datos"],"title":"The use of climate databases netCDF with array structure in the environment of R","type":"publication"},{"authors":null,"categories":null,"content":"\n                        \n","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1430092800,"objectID":"60c1fbba20235e8b3ac453c8f914ad01","permalink":"https://dominicroye.github.io/es/project/population/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/es/project/population/","section":"project","summary":".","tags":["poblaci√≥n","dataviz"],"title":"Poblaci√≥n","type":"project"},{"authors":null,"categories":null,"content":"\n   Smoothed daily maximum temperature throughout the year in Europe. Data: ERA5-Land.\n   Smoothed daily maximum temperature throughout the year in China. Data: ERA5-Land.\n   Smoothed daily maximum temperature throughout the year in the Indian subcontinent. Data: ERA5-Land.\n   Smoothed daily precipitation throughout the year in Brazil. Data: ERA5-Land.\n   Smoothed daily maximum temperature throughout the year in Brazil. Data: ERA5-Land.\n   The average temperature of 24 hours in August 2020 for Europe. Data: ERA5-Land.\n   The average temperature of 24 hours in January 2020. Data: ERA5-Land.\n   Smoothed daily rainfall throughout the year in Australia. Data: SILO.\n   Smoothed daily maximum temperature throughout the year in Australia. Data: SILO.\n   How do the spatial patterns of daily precipitation change throughout the year in Europe? Data: E-OBS.\n   Smoothed daily maximum temperature throughout the year in the contiguous USA. Data: PRISM.\n   Smoothed daily maximum temperature throughout the year in Europe. Data: E-OBS.\n   How do the spatial patterns of daily precipitation change throughout the year in mainland Spain and the Balearic Islands? Data: SPREAD.\n   Smoothed daily sea surface temperature throughout the year for the Northeast Atlantic, the Mediterranean, North and Black Sea. Data: NOAA/NODC.\n   Probability of a summer day (maximum temperature greater than 25¬∫C/77¬∫F) through the year in Europe. Data: E-OBS.\n   Probability of a summer day (maximum temperature greater than 25¬∫C/77¬∫F) through the year in the Contiguous United States. Data: PRISM.\n   Probability of a summer day (maximum temperature greater than 25¬∫C) through the year in Australia. Data: SILO.\n   Probability of a frost day (minimum temperature less than 0¬∫C) through the year in Europe. Data: E-OBS 18e.\n   Probability of a frost day (minimum temperature less than 0¬∫C/32¬∫F) through the year in the Contiguous United States. Data: PRISM. Platform: Google Earth Engine.\n\n","date":1398556800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1398556800,"objectID":"46d95729e9233e3265f2537f46bb13f7","permalink":"https://dominicroye.github.io/es/project/animations/","publishdate":"2014-04-27T00:00:00Z","relpermalink":"/es/project/animations/","section":"project","summary":".","tags":["clima","meteo","mapas","gr√°ficos","clima","dataviz","atm√≥sfera","temperatura"],"title":"Animaciones","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://dominicroye.github.io/es/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/es/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]