[{"authors":["C Iñiguez","D Royé","A Tobías"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1609459200,"objectID":"e8e61e2800550109d5d2fffba2a02506","permalink":"/es/publication/morti-morbilidad_spain_2021/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/es/publication/morti-morbilidad_spain_2021/","section":"publication","summary":"Background: Climate change is a severe public health challenge. Understanding to what extent fatal and non-fatal consequences of specific diseases are associated with temperature may help to improve the effectiveness of preventive public health efforts. This study examines the effects of temperature on deaths and hospital admissions by cardiovascular and respiratory diseases, empathizing the difference between mortality and morbidity. Methods: Daily counts for mortality and hospital admissions by cardiovascular and respiratory diseases were collected for the 52 provincial capital cities in Spain, between 1990 and 2014. The association with temperature in each city was investigated by means of distributed lag non-linear models using quasiPoisson regression. City-specific exposure-response curves were pooled by multivariate random-effects meta-analysis to obtain countrywide risk estimates of mortality and hospitalizations due to heat and cold, and attributable fractions were computed. Results: Heat and cold exposure were identified to be associated with increased risk of cardiovascular and respiratory mortality. Heat was not found to have an impact on hospital admissions. The estimated fraction of mortality attributable to cold was of greater magnitude in hospitalizations (17.5% for cardiovascular and 12.5% for respiratory diseases) compared to deaths (9% and 2.7%, respectively). Conclusions: There were noteworthy differences between temperature-related mortality and hospital admissions regarding cardiovascular and respiratory diseases, hence reinforcing the convenience of cause-specific measures to prevent temperature-related deaths.","tags":["Temperatura","Mortalidad","Ingresos hospitalarios","Cardiovascular","Respiratorio","España","Modelos no lineales de retardo distribuido"],"title":"Contrasting patterns of temperature related mortality and hospitalization by cardiovascular and respiratory diseases in 52 Spanish cities","type":"publication"},{"authors":null,"categories":["visualización","R","R:avanzado"],"content":"\rEn el campo de la visualización de datos, la animación de datos espaciales en su dimensión temporal lleva a mostrar cambios y patrones fascinantes y muy visuales. A raíz de una de las últimas publicaciones que he realizado en los RRSS me pidieron que hiciera un post acerca de cómo lo creé. Pues bien, aquí vamos para empezar con datos de la España peninsular. Podéis encontrar más animaciones en la sección de gráficos de este mismo blog.\nI couldn\u0026#39;t resist to make another animation. Smoothed daily maximum temperature throughout the year in Europe. #rstats #ggplot2 #dataviz #climate pic.twitter.com/ZC9L0vh3vR\n\u0026mdash; Dominic Royé (@dr_xeo) May 9, 2020  Paquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rrnaturalearth\rMapas vectoriales del mundo ‘Natural Earth’\r\rlubridate\rFácil manipulación de fechas y tiempos\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rraster\rImportar, exportar y manipular raster\r\rggthemes\rEstilos para ggplot2\r\rgifski\rCrear gifs\r\rshowtext\rUsar fuentes más fácilmente en gráficos R\r\rsysfonts\rCargar fuentes del sistema y fuentes de Google\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;ggthemes\u0026quot;)\rif(!require(\u0026quot;gifski\u0026quot;)) install.packages(\u0026quot;gifski\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# paquetes\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggthemes)\rlibrary(sf)\rlibrary(rnaturalearth)\rlibrary(extrafont)\rlibrary(showtext)\rlibrary(RColorBrewer)\rlibrary(gifski)\rPara aquellos con menos experiencia con tidyverse, recomiendo la breve introducción en este blog post.\n\rPreparación\rDatos\rDescargamos los datos STEAD de la temperatura máxima (tmax_pen.nc) en formato netCDF desde el repositario del CSIC aquí (el tamaño de los datos es de 2 GB). Se trata de un conjunto de datos con una resolución espacial de 5 km y comprenden las temperaturas máximas diarias desde 1901 a 2014. En la climatología y la meteorología, un formato de uso muy extendido es el de las bases de datos netCDF, que permiten obtener una estructura multidimensional e intercambiar los datos de forma independiente al sistema operativo empleado. Se trata de un formato espacio-temporal con una cuadrícula regular o irregular. La estructura multidimensional en forma de matriz (array) permite usar no sólo datos espacio-temporales sino también multivariables. En nuestros datos tendremos un cubo de tres dimensiones: longitud, latitud y tiempo de la temperatura máxima.\nRoyé 2015. Sémata: Ciencias Sociais e Humanidades 27:11-37\n\r\rImportar los datos\rEl formato netCDF con extensión .nc lo podemos importar vía dos paquetes principales: 1) ncdf4 y 2) raster. Aunque el paquete raster realmente lo que hace es usar el primer paquete para importar los datos. En este post usaremos el paquete raster dado que es algo más fácil, con algunas funciones muy útiles y más universales para todo tipo de formato raster. Las funciones principales de importación son: raster(), stack() y brick(). La primera función sólo permite importar una única capa, en cambio, las últimas dos funciones se emplean para datos multidimensionales. En nuestro caso sólo tenemos una variable, por tanto no sería necesario hacer uso del argumento varname.\n# importamos los datos ncdf\rtmx \u0026lt;- brick(\u0026quot;tmax_pen.nc\u0026quot;, varname = \u0026quot;tx\u0026quot;)\r## Loading required namespace: ncdf4\rtmx # metadatos\r## class : RasterBrick ## dimensions : 190, 230, 43700, 41638 (nrow, ncol, ncell, nlayers)\r## resolution : 0.0585, 0.045 (x, y)\r## extent : -9.701833, 3.753167, 35.64247, 44.19247 (xmin, xmax, ymin, ymax)\r## crs : +proj=longlat +datum=WGS84 +no_defs ## source : C:/Users/xeo19/Documents/GitHub/blogR_update/content/post/es/2020-10-11-animacion-climatica-temperatura-maxima/tmax_pen.nc ## names : X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ... ## Time (days since 1901-01-01): 1, 41638 (min, max)\r## varname : tx\rEn la estructura del objeto RasterBrick vemos todos los metadatos necesarios: desde la resolución, las dimensiones o el tipo de proyección, hasta el nombre de la variable. Además nos indica que únicamente apunta a los datos (source) y no los ha importado a la memoria RAM lo que facilita el trabajo con grandes conjuntos de datos.\nPara acceder a cualquier capa hacemos uso de [[ ]] con el índice correspondiente. Así podemos plotear fácilmente cualquier día de los 41.638 días de los que disponemos.\n# mapear cualquier día\rplot(tmx[[200]], col = rev(heat.colors(7)))\r\rCalcular el promedio de la temperatura\rEn este paso el objetivo es calcular el promedio de la temperatura para cada día del año. Por eso, lo primero que hacemos es crear un vector, indicando el día del año para toda la serie temporal. En el paquete raster disponemos de la función stackApply() que permite aplicar una función sobre grupos de capas, o mejor dicho, índices. Dado que nuestro conjunto de datos es grande, incluimos esta función en las funciones de paralelización.\nEmpezamos con las funciones beginClusterr() y endCluster() que inician y finalizan la paralelización. En la primera debemos indicar el número de núcleos que queremos usar. En este caso uso 4 de 7 posibles núcleos, no obstante, se debe cambiar el número según las características de cada CPU, siendo la norma n-1. Entonces, la función clusterR permite ejecutar funciones en paralelo con múltiples núcleos. El primer argumento corresponde al objeto raster, el segundo a la función empleada, y en forma de lista pasamos los argumentos de la función stackApply(), los índices que crean los grupos y la función usada para cada uno de los grupos. Si añadimos el argumento progress = 'text' se muestra una barra de progreso del cálculo.\nPara el conjunto de datos de EEUU hice un preprocesamiento, el cálculo del promedio en la nube a través de Google Earth Engine lo que hace todo el proceso más rápido. En el caso de Australia, el preprocesamiento fue más complejo ya que el conjunto de datos esta en archivos netCDF para cada año.\n # convertimos las fechas entre 1901 y 2014 a días del año\rtime_days \u0026lt;- yday(seq(as_date(\u0026quot;1901-01-01\u0026quot;), as_date(\u0026quot;2014-12-31\u0026quot;), \u0026quot;day\u0026quot;))\r# calculamos el promedio beginCluster(4)\rtmx_mean \u0026lt;- clusterR(tmx, stackApply, args = list(indices = time_days, fun = mean))\rendCluster()\r\rSuavizar la variabilidad de las temperaturas\rAntes de pasar a suavizar las series temporales de nuestro RasterBrick, un ejemplo del por qué lo hacemos. Extraemos un píxel de nuestro conjunto de datos en las coordenadas -1º de longitud y 40º de latitud usando la función extract(). Dado que la función con el mismo nombre aparece en varios paquetes, debemos cambiar a la forma nombre_paquete::nombre_función. El resultado es una matriz con una fila correspondiente al píxel y 366 columnas de los días del año. El siguiente paso es la creación de un data.frame con una fecha dummy y la temperatura máxima extraída.\n# extraemos un píxel\rpoint_ts \u0026lt;- raster::extract(tmx_mean, matrix(c(-1, 40), nrow = 1))\rdim(point_ts) # dimensiones \r## [1] 1 366\r# creamos un data.frame\rdf \u0026lt;- data.frame(date = seq(as_date(\u0026quot;2000-01-01\u0026quot;), as_date(\u0026quot;2000-12-31\u0026quot;), \u0026quot;day\u0026quot;),\rtmx = point_ts[1,])\r# visualizamos la temperatura máxima ggplot(df, aes(date, tmx)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rlabs(y = \u0026quot;Temperatura máxima\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rEl gráfico muestra claramente la todavía existente variabilidad, lo que haría fluctuar bastante una animación. Por eso, creamos una función de suavizado basado en un ajuste de regresión polinomial local (LOESS), más detalles los encontráis en la ayuda de la función loess(). El argumento más importante es span que determina el grado de suavizado de la función, cuanto más pequeño el valor menos suave será la curva. El mejor resultado me ha dado un valor del 0,5.\ndaily_smooth \u0026lt;- function(x, span = 0.5){\rif(all(is.na(x))){\rreturn(x) } else {\rdf \u0026lt;- data.frame(yd = 1:366, ta = x)\rm \u0026lt;- loess(ta ~ yd, span = span, data = df)\rest \u0026lt;- predict(m, 1:366)\rreturn(est)\r}\r}\rAplicamos nuestra nueva función de suavizado a la serie temporal extraída y hacemos algunos cambios para poder visualizar la diferencia entre los datos originales y suavizados.\n# suavizamos la temperatura\rdf \u0026lt;- mutate(df, tmx_smoothed = daily_smooth(tmx)) %\u0026gt;% pivot_longer(2:3, names_to = \u0026quot;var\u0026quot;, values_to = \u0026quot;temp\u0026quot;)\r# visualizamos la diferencia ggplot(df, aes(date, temp, colour = var)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rscale_colour_manual(values = c(\u0026quot;#f4a582\u0026quot;, \u0026quot;#b2182b\u0026quot;)) +\rlabs(y = \u0026quot;Temperatura máxima\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rComo vemos en el gráfico la curva suavizada sigue muy bien la curva original. En el siguiente paso empleamos nuestra función sobre el RasterBrick usando la función calc(). La función devuelve tantas capas como las que devuelve la función empleada a cada de las series temporales.\n# suavizar el RasterBrick\rtmx_smooth \u0026lt;- calc(tmx_mean, fun = daily_smooth)\r\r\rVisualización\rPreparación\rPara visualizar las temperaturas máximas durante todo el año, primero, convertimos el RasterBrick a un data.frame, incluyendo longitud y latitud, pero eliminando todas las series temporales sin valores (NA).\n# convertir a data.frame\rtmx_mat \u0026lt;- as.data.frame(tmx_smooth, xy = TRUE, na.rm = TRUE)\r# renombrar las columnas\rtmx_mat \u0026lt;- set_names(tmx_mat, c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, str_c(\u0026quot;D\u0026quot;, 1:366)))\rstr(tmx_mat[, 1:10])\r## \u0026#39;data.frame\u0026#39;: 20676 obs. of 10 variables:\r## $ lon: num -8.03 -7.98 -7.92 -7.86 -7.8 ...\r## $ lat: num 43.8 43.8 43.8 43.8 43.8 ...\r## $ D1 : num 10.5 10.3 10 10.9 11.5 ...\r## $ D2 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D3 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D4 : num 10.6 10.4 10.1 10.9 11.5 ...\r## $ D5 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D6 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D7 : num 10.6 10.4 10.2 11 11.6 ...\r## $ D8 : num 10.6 10.4 10.2 11 11.6 ...\rSegundo, importamos los límites administrativos con la función ne_countries() del paquete rnaturalearth limitando la extensión a la región de la Península Ibérica, el sur de Francia y el norte de África.\n# importamos los límites globales\rmap \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;) %\u0026gt;% st_cast(\u0026quot;MULTILINESTRING\u0026quot;)\r# limitamos la extensión\rmap \u0026lt;- st_crop(map, xmin = -10, xmax = 5, ymin = 35, ymax = 44) \r## although coordinates are longitude/latitude, st_intersection assumes that they are planar\r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r# mapa de los límites\rplot(map)\r## Warning: plotting the first 9 out of 94 attributes; use max.plot = 94 to plot\r## all\rTercero, creamos un vector con etiquetas del día del año para incluirlas en la animación. Además, definimos los cortes de la temperatura máxima, adaptados a la distribución de nuestros datos, para obtener una categorización con un total de 20 clases.\nCuarto, aplicamos la función cut() con los cortes a todas las columnas con las temperaturas de cada día del año.\n# etiquetas de los días del año\rlab \u0026lt;- as_date(0:365, \u0026quot;2000-01-01\u0026quot;) %\u0026gt;% format(\u0026quot;%d %B\u0026quot;)\r# cortes para la temperatura\rct \u0026lt;- c(-5, 0, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 40, 45)\r# datos categorizados con los cortes fijados\rtmx_mat_cat \u0026lt;- mutate_at(tmx_mat, 3:368, cut, breaks = ct)\rQuinto, descargamos la fuente Montserrat y definimos los colores correspondientes a las clases creadas.\n# descarga de la fuente\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# uso de showtext con DPI 300\rshowtext_opts(dpi = 300)\rshowtext_auto()\r# definimos una rampa de colores\rcol_spec \u0026lt;- colorRampPalette(rev(brewer.pal(11, \u0026quot;Spectral\u0026quot;)))\r\rMapa estático\rEn esta primera visualización hacemos un mapa del 29 de mayo (día 150). No voy a explicar todos los detalles de la construcción con ggplot2, no obstante, es importante destacar que hago uso de la función aes_string() en lugar de aes() para poder usar los nombres de las columnas en formato de carácter. Con la función geom_raster() añadimos los datos en rejilla de temperatura como primera capa del gráfico y con geom_sf() los límites de clase sf. Por último, la función guide_colorsteps() permite crear una bonita leyenda basada en las clases creadas por la función cut().\nggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = \u0026quot;D150\u0026quot;)) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Promedio de la temperatura máxima durante el año en España\u0026quot;, subtitle = lab[150], caption = \u0026quot;Período de referencia 1901-2014. Datos: STEAD\u0026quot;,\rfill = \u0026quot;ºC\u0026quot;)\r\rAnimación de todo el año\rLa animación final consiste en crear un gif a partir de todas las imágenes de los 366 días, en principio, se podría usar el paquete gganimate, pero en mi experiencia es más lento, dado que requiere un data.frame en formato largo. En este ejemplo una tabla larga tendría más de siete millones de filas, por eso, lo que hacemos es usar un bucle sobre las columnas y unir todas las imágenes creadas con el paquete gifski que también usa gganimate para la reproducción en formato gif.\nAntes del bucle creamos un vector con los pasos temporales o nombres de las columnas y otro vector con el nombre de las imágenes, incluida el nombre de la carpeta. Con el objetivo de obtener una lista de imágenes ordenadas por su número debemos mantener tres cifras rellenando las posiciones a la izquierda con ceros.\ntime_step \u0026lt;- str_c(\u0026quot;D\u0026quot;, 1:366)\rfiles \u0026lt;- str_c(\u0026quot;./ta_anima/D\u0026quot;, str_pad(1:366, 3, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;), \u0026quot;.png\u0026quot;)\rPor último, incluimos la construcción anterior del gráfico en un bucle for.\nfor(i in 1:366){\rggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = time_step[i])) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Promedio de la temperatura máxima durante el año en España\u0026quot;, subtitle = lab[i], caption = \u0026quot;Período de referencia 1901-2014. Datos: STEAD\u0026quot;,\rfill = \u0026quot;ºC\u0026quot;)\rggsave(files[i], width = 8.28, height = 7.33, type = \u0026quot;cairo\u0026quot;)\r}\rDespués de haber creado imágenes para cada día del año, únicamente nos queda por crear el gif.\ngifski(files, \u0026quot;tmx_spain.gif\u0026quot;, width = 800, height = 700, loop = FALSE, delay = 0.05)\r\r\r","date":1602374400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1602374400,"objectID":"d60db0588841e28e4b2718e0f2f1f116","permalink":"/es/2020/animaci%C3%B3n-clim%C3%A1tica-de-la-temperatura-m%C3%A1xima/","publishdate":"2020-10-11T00:00:00Z","relpermalink":"/es/2020/animaci%C3%B3n-clim%C3%A1tica-de-la-temperatura-m%C3%A1xima/","section":"post","summary":"En el campo de la visualización de datos, la animación de datos espaciales en su dimensión temporal lleva a mostrar cambios y patrones fascinantes y muy visuales. A raíz de una de las últimas publicaciones que he realizado en los RRSS me pidieron que hiciera un post acerca de cómo lo creé. Pues bien, aquí vamos para empezar con datos de la España peninsular.","tags":["animación","temperatura","clima","SIG"],"title":"Animación climática de la temperatura máxima","type":"post"},{"authors":["P Fdez-Arroyabe","K Kourtidis","C Haldoupis","et al."],"categories":null,"content":"","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1602028800,"objectID":"13fa870fb210ed69eed9ee49781af86a","permalink":"/es/publication/glossary_electricity_2020/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/es/publication/glossary_electricity_2020/","section":"publication","summary":"There is an increasing interest to study the interactions between atmospheric electrical parameters and living organisms at multiple scales. So far, relatively few studies have been published that focus on possible biological effects of atmospheric electric and magnetic fields. To foster future work in this area of multidisciplinary research, here we present a glossary of relevant terms. Its main purpose is to facilitate the process of learning and communication among the different scientific disciplines working on this topic. While some definitions come from existing sources, other concepts have been re-defined to better reflect the existing and emerging scientific needs of this multidisciplinary and transdisciplinary area of research.","tags":["Fenómenos de la electricidad atmosférica","Campo eléctrico atmosférico","Efectos biologicos","Perfil biometeorológico","Glosario"],"title":"Glossary on atmospheric electricity and its effects on biology","type":"publication"},{"authors":null,"categories":["sig","R","R:avanzado"],"content":"\rRecientemente creé una visualización de la distribución de las direcciones del flujo fluvial y también de las orientaciones costeras. A raíz de su publicación en los RRSS (aquí) me pidieron que hiciera un post acerca de cómo lo hice. Pues bien, aquí vamos para empezar con un ejemplo de los ríos, la orientación costera es algo más compleja. Lo mismo hice para una selección de ríos europeos aquí en este tweet. No obstante, originalmente empecé con la orientación de las costas europeas.\nHave you ever wondered where the European #coasts are oriented? #rstats #ggplot2 #geography #dataviz pic.twitter.com/tpWVxSoHlw\n\u0026mdash; Dominic Royé (@dr_xeo) May 26, 2020  Paquetes\rEn este post usaremos los siguientes paquetes:\n\r\r\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rremotes\rInstalación desde repositorios remotos\r\rRQGIS3\rInterfaz entre R y QGIS3\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rggtext\rSoporte para la representación de texto mejorado con ggplot2\r\rsysfonts\rCargar fuentes en R\r\rshowtext\rUsar fuentes más fácilmente en gráficos R\r\rcircular\rFunciones para trabajar con datos circulares\r\rgeosphere\rTrigonometría esférica para aplicaciones geográficas\r\r\r\rEn el caso del paquete RQGIS3 es necesario instalar QGIS en OSGeo4W aquí. Más adelante explicaré la razón del uso de QGIS.\n# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;remotes\u0026quot;)) install.packages(\u0026quot;remotes\u0026quot;)\rif(!require(\u0026quot;RQGIS3\u0026quot;)) remotes::install_github(\u0026quot;jannes-m/RQGIS3\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggtext\u0026quot;)) install.packages(\u0026quot;ggtext\u0026quot;)\rif(!require(\u0026quot;circular\u0026quot;)) install.packages(\u0026quot;circular\u0026quot;)\rif(!require(\u0026quot;geosphere\u0026quot;)) install.packages(\u0026quot;geosphere\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# paquetes\rlibrary(sf)\rlibrary(tidyverse)\rlibrary(ggtext)\rlibrary(circular)\rlibrary(geosphere)\rlibrary(RQGIS3)\rlibrary(showtext)\rlibrary(sysfonts)\r\rConsideraciones iniciales\rLos ángulos en líneas vectoriales se basan en el ángulo entre dos vértices, y el número de vértices depende de la complejidad, y en consecuencia de la resolución, de los datos vectoriales. Por tanto, puede haber diferencias en usar distintas resoluciones de una línea vectorial, sea de la costa o del río como en este ejemplo. Una línea recta simplemente se construye con dos puntos de longitud y latitud.\nRelacionado con ello está la fractalidad, una estructura aparentemente irregular pero que se repite a diferentes escalas, de la línea de costa o también del río. La característica más paradójica es que la longitud de una línea costera depende de la escala de medida, cuanto menor es el incremento de medida, la longitud medida se incrementa.\nExisten dos posibiliades de obtener los ángulos de los vértices. En la primera calculamos el ángulo entre todos los vértices consecutivos.\nPor ejemplo, imaginémonos dos puntos, Madrid (-3.71, 40.43) y Barcelona (2.14, 41.4).\n¿Cuál es el ángulo de su línea recta?\nbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4))\r## [1] 77.62391\rVemos que es el de 77º, o sea, dirección noreste. Pero, ¿y si voy de Barcelona a Madrid?\nbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43))\r## [1] 257.6239\rEl angúlo es diferente porque nos movemos desde el noreste al suroeste. Podemos invertir fácilmente el ángulo para obtener el movimiento contrario.\n# ángulo contrario de Barcelona -\u0026gt; Madrid\rbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43)) - 180\r## [1] 77.62391\r# ángulo contrario de Madrid -\u0026gt; Barcelona\rbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4)) + 180\r## [1] 257.6239\rLa dirección en la que calculamos los ángulos es importante. En el caso de los ríos se espera que sea la dirección de flujo de origen a la desembocadura, ahora bien, un problema puede ser que los vértices, que construyen las líneas, no estén ordenados geográficamente en la tabla de atributos. Otro problema puede ser que los vértices empiecen en la desembocadura lo que daría al angúlo inverso como lo hemos visto antes.\nSin embargo, hay una forma más fácil. Podemos aprovechar los atributos de los sistemas de coordenadas proyectados (proyección Robinson, etc) que incluyen el ángulo entre los vértices. Este último enfoque lo vamos usar en este post. Aún así, debemos prestar mucha atención a los resultados según lo dicho anteriormente.\n\rPreparación\rDatos\rDescargamos las líneas centrales de los ríos más grandes del mundo (descarga), accesible también en Zeenatul Basher et al. 2018.\n\rImportar y proyectar\rLo primero que hacemos es importar, proyectar y eliminar la tercera dimensión Z, usando el encadenamiento de las siguientes functions: st_read() nos ayuda a importar cualquier formato vectorial, st_zm() elimina la dimensión Z o M de una geometría vectorial y st_transform() proyecta los datos vectoriales a la nueva proyección en formato proj4. La combinación de las funciones la realizamos con el famoso pipe (%\u0026gt;%) que facilita la aplicación de una secuencia de funciones sobre un conjunto de datos, más detalles en este post. Todas las funciones del paquete sf comienzan por st_* haciendo referencia al carácter espacial de su aplicación, similar a PostGIS. Igualmente, y al mismo estilo que PostGIS, se usan verbos como nombres de función.\nproj_rob \u0026lt;- \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m no_defs\u0026quot;\rriver_line \u0026lt;- st_read(\u0026quot;RiverHRCenterlinesCombo.shp\u0026quot;) %\u0026gt;% st_zm() %\u0026gt;% st_transform(proj_rob)\r## Reading layer `RiverHRCenterlinesCombo\u0026#39; from data source `C:\\Users\\xeo19\\Documents\\GitHub\\blogR_update\\content\\post\\es\\2020-07-24-direcciones-del-flujo-fluvial\\RiverHRCenterlinesCombo.shp\u0026#39; using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 78 features and 6 fields\r## geometry type: MULTILINESTRING\r## dimension: XYZ\r## bbox: xmin: -164.7059 ymin: -36.97094 xmax: 151.5931 ymax: 72.64474\r## z_range: zmin: 0 zmax: 0\r## geographic CRS: WGS 84\r\rExtraer los ángulos\rEn el siguiente paso debemos extraer los ángulos de los vértices. Desgraciadamente, hasta donde sepa, no es posible extraer los atributos con alguna función del paquete sf. Aunque la función st_coordinates() nos devuelve las coordenadas, no incluye otros atributos. Por eso, debemos usar otra forma, y es que el open software Quantum GIS extrae todos los atributos de los vértices. Podríamos importar los datos vectoriales en QGIS Desktop y exportar los vértices desde allí, pero también es posible acceder a las funciones de QGIS desde R directamente.\nPara ello, tenemos que tener instalado QGIS en OSGeo4W. El paquete RQGIS3 nos permite de forma muy fácil usar las funciones del programa en R. Primero empleamos la función set_env() para definir todas las rutas necesarias de QGIS e inciamos la API con open_app().\n# rutas a QGIS\rset_env()\r## Trying to find QGIS in C:/OSGEO4~1\r## $root\r## [1] \u0026quot;C:/OSGeo4W64\u0026quot;\r## ## $qgis_prefix_path\r## [1] \u0026quot;C:/OSGeo4W64/apps/qgis\u0026quot;\r## ## $python_plugins\r## [1] \u0026quot;C:/OSGeo4W64/apps/qgis/python/plugins\u0026quot;\r## ## $platform\r## [1] \u0026quot;Windows\u0026quot;\r# inicio de QGIS Python\ropen_app()\rLa función find_algorithms() nos ayuda a buscar diferentes herramientas de QGIS. Además la función get_usage() especifica la forma de uso con todos los argumentos requeridos.\n# buscar herramientas\rfind_algorithms(search_term = \u0026quot;vertices\u0026quot;, name_only = TRUE)\r## [1] \u0026quot;native:extractspecificvertices\u0026quot; ## [2] \u0026quot;native:extractvertices\u0026quot; ## [3] \u0026quot;native:filterverticesbym\u0026quot; ## [4] \u0026quot;native:filterverticesbyz\u0026quot; ## [5] \u0026quot;native:removeduplicatevertices\u0026quot; ## [6] \u0026quot;saga:convertpolygonlineverticestopoints\u0026quot;\r# uso de la herramienta\rget_usage(alg = \u0026quot;native:extractvertices\u0026quot;)\r## Extract vertices (native:extractvertices)\r## ## This algorithm takes a line or polygon layer and generates a point layer with points representing the vertices in the input lines or polygons. The attributes associated to each point are the same ones associated to the line or polygon that the point belongs to.\r## ## Additional fields are added to the point indicating the vertex index (beginning at 0)\r## the vertex’s part and its index within the part (as well as its ring for polygons)\r## distance along original geometry and bisector angle of vertex for original geometry.\r## ## ## ----------------\r## Input parameters\r## ----------------\r## ## INPUT: Input layer\r## ## Parameter type: QgsProcessingParameterFeatureSource\r## ## Accepted data types:\r## - str: layer ID\r## - str: layer name\r## - str: layer source\r## - QgsProcessingFeatureSourceDefinition\r## - QgsProperty\r## - QgsVectorLayer\r## ## OUTPUT: Vertices\r## ## Parameter type: QgsProcessingParameterFeatureSink\r## ## Accepted data types:\r## - str: destination vector file\r## e.g. d:/test.shp\r## - str: memory: to store result in temporary memory layer\r## - str: using vector provider ID prefix and destination URI\r## e.g. postgres:… to store result in PostGIS table\r## - QgsProcessingOutputLayerDefinition\r## - QgsProperty\r## ## ----------------\r## Outputs\r## ----------------\r## ## OUTPUT: \u0026lt;QgsProcessingOutputVectorLayer\u0026gt;\r## Vertices\rEn nuestro caso la herramienta para extraer los vértices es simple y sólo lleva una entrada y una salida. La función run_qgis() ejecuta una herramienta de QGIS indicando el algoritmo y sus argumentos. La ventaja de usar el algoritmo directamente desde R es que podemos pasar objetos de clase sf (o sp) y raster que tenemos importados o creados en R. Como salida creamos un geojson, también podría ser de otro formato vectorial, y lo guardamos en una carpeta temporal. Al mismo tiempo le indicamos que importe el resultado directamente a R (load_output = TRUE).\nriver_vertices \u0026lt;- run_qgis(alg = \u0026quot;native:extractvertices\u0026quot;,\rINPUT = river_line,\rOUTPUT = file.path(tempdir(), \u0026quot;rivers_world_vertices.geojson\u0026quot;),\rload_output = TRUE)\r## $OUTPUT\r## [1] \u0026quot;C:/Users/xeo19/AppData/Local/Temp/RtmpGurVm9/rivers_world_vertices.geojson\u0026quot;\rActualmente en Windows parece haber problemas con la librería de proj. En principio si termina creando el objeto river_vertices no debes preocuparte. En caso contrario, recomiendo mirar la discusión en el issue abierto en gitbub.\n \rSelección\rAntes de seguir con la estimación de la distribución de los ángulos, filtramos algunos ríos de interés. Las funciones de la colección tidyverse son compatibles con el paquete sf. En el último post hice una introducción a tidyverse aquí.\nriver_vertices \u0026lt;- filter(river_vertices, NAME %in% c(\u0026quot;Mississippi\u0026quot;, \u0026quot;Colorado\u0026quot;, \u0026quot;Amazon\u0026quot;, \u0026quot;Nile\u0026quot;, \u0026quot;Orange\u0026quot;, \u0026quot;Ganges\u0026quot;, \u0026quot;Yangtze\u0026quot;, \u0026quot;Danube\u0026quot;,\r\u0026quot;Mackenzie\u0026quot;, \u0026quot;Lena\u0026quot;, \u0026quot;Murray\u0026quot;, \u0026quot;Niger\u0026quot;)\r) river_vertices \r## Simple feature collection with 94702 features and 11 fields\r## geometry type: POINT\r## dimension: XY\r## bbox: xmin: -10377520 ymin: -3953778 xmax: 13124340 ymax: 7507359\r## geographic CRS: WGS 84\r## Warning in st_is_longlat(x): bounding box has potentially an invalid value range\r## for longlat data\r## Warning in st_is_longlat(x): bounding box has potentially an invalid value range\r## for longlat data\r## # A tibble: 94,702 x 12\r## NAME SYSTEM name_alt scalerank rivernum Length_km vertex_index vertex_part\r## * \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 0 0\r## 2 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 1 0\r## 3 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 2 0\r## 4 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 3 0\r## 5 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 4 0\r## 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 5 0\r## 7 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 6 0\r## 8 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 7 0\r## 9 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 8 0\r## 10 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 9 0\r## # ... with 94,692 more rows, and 4 more variables: vertex_part_index \u0026lt;int\u0026gt;,\r## # distance \u0026lt;dbl\u0026gt;, angle \u0026lt;dbl\u0026gt;, geometry \u0026lt;POINT [°]\u0026gt;\r\r\rEstimar la distribución\rPara visualizar la distribución podemos usar, o bien un histograma o un gráfico de densidad. Pero en el caso de estimar la función de densidad de probabilidad nos encontramos con un problema matemático a la hora de aplicarlo a datos circulares. No debemos usar la función estandar de R density() dado que en nuestros datos una dirección de 360º es la misma a 0º, lo que provocaría errores en este rango de valores. Es un problema general para diferentes métricas estadísticas. Más detalles estadísticos se explican en el paquete circular. Este paquete permite definir las características de los datos circulares (unidad, tipo de datos, rotación, etc.) como una clase de objeto en R.\nPor tanto, lo que hacemos es construir una función que estime la densidad y devuelva una tabla con los ángulos (x) y las estimaciones de densidad (y). Dado que los ríos tienen diferentes longitudes, y queremos ver diferencias independientemente de ello, normalizamos las estimaciones usando el valor máximo. A diferencia de la función density(), en la que el ancho de banda de suavizado bw es optimizado, aquí es requerido indicarlo. Es similar a definir el ancho de barra en un histograma. Existe una función de optimización para la banda, bw.nrd.circular() que se podría emplear aquí.\ndens_circ \u0026lt;- function(x){\rdens \u0026lt;- density.circular(circular(x$angle, units = \u0026quot;degrees\u0026quot;),\rbw = 70, kernel = \u0026quot;vonmises\u0026quot;,\rcontrol.circular = list(units = \u0026quot;degrees\u0026quot;))\rdf \u0026lt;- data.frame(x = dens$x, y = dens$y/max(dens$y))\rreturn(df)\r}\rPara finalizar, estimamos la densidad de cada río de nuestra selección. Empleamos la función split() de R Base para obtener una tabla de cada río en una lista. Después aplicamos con la función map_df() del paquete purrr nuestra función de estimación de densidad a la lista. El sufijo _df permite que obtengamos una tabla unida, en lugar de una lista con los resultados de cada río. No obstante, es necesario indicar el nombre de la columna con el argumento .id, la que contendrá el nombre de cada río. En caso contrario no sabríamos diferenciar los resultados. También aquí recomiendo leer más detalles en el último post sobre tidyverse aquí.\ndens_river \u0026lt;- split(river_vertices, river_vertices$NAME) %\u0026gt;% map_df(dens_circ, .id = \u0026quot;river\u0026quot;)\r# resultado\rhead(dens_river)\r## river x y\r## 1 Amazon 0.000000 0.2399907\r## 2 Amazon 0.704501 0.2492548\r## 3 Amazon 1.409002 0.2585758\r## 4 Amazon 2.113503 0.2679779\r## 5 Amazon 2.818004 0.2774859\r## 6 Amazon 3.522505 0.2871232\r\rVisualización\rAhora ya sólo nos queda la visualización mediante el famoso paquete ggplot. Primero añadimos una nueva fuente Montserrat para usarla en este gráfico.\n# descarga de fuente\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# usar showtext para fuentes\rshowtext_opts(dpi = 200)\rshowtext_auto() \rEn el siguiente paso creamos dos objetos con el título y con una nota de pie. En el título estamos usando un código html para dar color a una parte de texto en sustitución de una leyenda. Se puede usar html de forma muy fácil con el paquete ggtext.\n# título con html title \u0026lt;- \u0026quot;Relative distribution of river \u0026lt;span style=\u0026#39;color:#011FFD;\u0026#39;\u0026gt;\u0026lt;strong\u0026gt;flow direction\u0026lt;/strong\u0026gt;\u0026lt;/span\u0026gt; in the world\u0026quot;\rcaption \u0026lt;- \u0026quot;Based on data from Zeenatul Basher, 20180215\u0026quot;\rLa cuadrícula de fondo que crea ggplot por defecto para coordenadas polares no me convenció, por eso creamos una tabla con las líneas de fondo del eje x.\ngrid_x \u0026lt;- tibble(x = seq(0, 360 - 22.5, by = 22.5), y = rep(0, 16), xend = seq(0, 360 - 22.5, by = 22.5), yend = rep(Inf, 16))\rA continuación definimos todos los estilos del gráfico. Lo más importante en este paso es la función element_textbox() del paquete ggtext para poder interpretar nuestro código html incorporado al título.\ntheme_polar \u0026lt;- theme_minimal() +\rtheme(axis.title.y = element_blank(),\raxis.text.y = element_blank(),\rlegend.title = element_blank(),\rplot.title = element_textbox(family = \u0026quot;Montserrat\u0026quot;, hjust = 0.5, colour = \u0026quot;white\u0026quot;, size = 15),\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\raxis.text.x = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\rstrip.text = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rpanel.grid = element_blank()\r)\rPara terminar construimos el gráfico: 1) Usamos la función geom_hline() con diferentes puntos de intersección en y para crear la cuadrícula de fondo. La función geom_segment() crea la cuadrícula en x. 2) El área de densidad la creamos usando la función geom_area(). 3) En scale_x_continous() definimos un límite inferior\rnegativo para que no colapse en un punto pequeño. Las etiquetas de las ocho direcciones principales las indicamos en la función scale_y_continous(), y 4) Por último, cambiamos a un sistema de coordenadas polar y fijamos la variable para crear facetas.\nggplot() +\rgeom_hline(yintercept = c(0, .2, .6, .8, 1), colour = \u0026quot;white\u0026quot;) +\rgeom_segment(data = grid_x , aes(x = x, y = y, xend = xend, yend = yend), linetype = \u0026quot;dashed\u0026quot;, col = \u0026quot;white\u0026quot;) +\rgeom_area(data = dens_river, aes(x = x, y = y, ymin = 0, ymax = y), alpha = .7, colour = NA, show.legend = FALSE,\rfill = \u0026quot;#011FFD\u0026quot;) + scale_y_continuous(limits = c(-.2, 1), expand = c(0, 0)) +\rscale_x_continuous(limits = c(0, 360), breaks = seq(0, 360 - 22.5, by = 22.5),\rminor_breaks = NULL,\rlabels = c(\u0026quot;N\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NE\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;E\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SE\u0026quot;, \u0026quot;\u0026quot;,\r\u0026quot;S\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SW\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NW\u0026quot;, \u0026quot;\u0026quot;)) +\rcoord_polar() + facet_wrap(river ~ ., ncol = 4) +\rlabs(title = title, caption = caption, x = \u0026quot;\u0026quot;) +\rtheme_polar\r## Warning: Ignoring unknown aesthetics: ymin, ymax\r\r","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1595548800,"objectID":"1c52dd7c2bd4340ad38c4ddb961c5de6","permalink":"/es/2020/direcciones-del-flujo-fluvial/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/es/2020/direcciones-del-flujo-fluvial/","section":"post","summary":"Recientemente creé una visualización de la distribución de las direcciones del flujo fluvial y  también de las orientaciones costeras. A raíz de su publicación en los RRSS me pidieron que hiciera un post acerca de cómo lo hice. Pues bien, aquí vamos para empezar con un ejemplo de los ríos, la orientación costera es algo más compleja.","tags":["direcciones","rios","fluvial","orientación","distribución"],"title":"Direcciones del flujo fluvial","type":"post"},{"authors":null,"categories":["tidyverse","R","R:principante"],"content":"\r\r1 Tidyverse\r2 Guía de estilo\r3 Pipe %\u0026gt;%\r4 Paquetes de Tidyverse\r4.1 Lectura y escritura\r4.2 Manipulación de caracteres\r4.3 Manejo de fechas y horas\r4.4 Manipulación de tablas y vectores\r4.4.1 Selecionar y renombrar\r4.4.2 Filtrar y ordenar\r4.4.3 Agrupar y resumir\r4.4.4 Unir tablas\r4.4.5 Tablas largas y anchas\r\r4.5 Visualizar datos\r4.5.1 Gráfico de linea y puntos\r4.5.2 Boxplot\r4.5.3 Heatmap\r\r4.6 Aplicar funciones sobre vectores o listas\r\r\r\r1 Tidyverse\rEl universo de los paquetes de tidyverse, una colección de paquetes de funciones para un uso especialmente enfocado en la ciencia de datos, abrió un antes y después en la programación de R. En este post voy a resumir muy brevemente lo más esencial para inicarse en este mundo. La gramática sigue en todas las funciones una estructura común. Lo más esencial es que el primer argumento es el objeto y a continuación viene el resto de argumentos. Además, se proporciona un conjunto de verbos que facilitan el uso de las funciones. En la actualidad, la filosofía de las funciones también se refleja en otros paquetes que hacen compatible su uso con la colección de tidyverse. Por ejemplo, el paquete sf (simple feature) para el tratamiento de datos vectoriales, permite el uso de múltiples funciones que encontramos en el paquete dplyr.\nEl núcleo de la colección lo constituyen los siguientes paquetes:\n\r\rPaquete\rDescripción\r\r\r\rggplot2\rGramática para la creación de gráficos\r\rpurrr\rProgramación funcional de R\r\rtibble\rSistema moderno y efectivo de tablas\r\rdplyr\rGramatica para la manipulación de datos\r\rtidyr\rConjunto de funciones para ordenar datos\r\rstringr\rConjunto de funciones para trabajar con caracteres\r\rreadr\rUna forma fácil y rápida para importar datos\r\rforcats\rHerramientas y funciones para trabajar fácilmente con factores\r\r\r\rAdemás de los paquetes menciondos, también se usa muy frecuentemente lubridate para trabajar con fechas y horas, y también readxl que nos permite importar archivos en formato Excel. Para conocer todos los paquetes disponibles podemos emplear la función tidyverse_packages().\n## [1] \u0026quot;broom\u0026quot; \u0026quot;cli\u0026quot; \u0026quot;crayon\u0026quot; \u0026quot;dbplyr\u0026quot; \u0026quot;dplyr\u0026quot; ## [6] \u0026quot;forcats\u0026quot; \u0026quot;ggplot2\u0026quot; \u0026quot;haven\u0026quot; \u0026quot;hms\u0026quot; \u0026quot;httr\u0026quot; ## [11] \u0026quot;jsonlite\u0026quot; \u0026quot;lubridate\u0026quot; \u0026quot;magrittr\u0026quot; \u0026quot;modelr\u0026quot; \u0026quot;pillar\u0026quot; ## [16] \u0026quot;purrr\u0026quot; \u0026quot;readr\u0026quot; \u0026quot;readxl\u0026quot; \u0026quot;reprex\u0026quot; \u0026quot;rlang\u0026quot; ## [21] \u0026quot;rstudioapi\u0026quot; \u0026quot;rvest\u0026quot; \u0026quot;stringr\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;tidyr\u0026quot; ## [26] \u0026quot;xml2\u0026quot; \u0026quot;tidyverse\u0026quot;\rEs muy fácil encontrarnos con conflicos de funciones, o sea, que el mismo nombre de función exista en varios paquetes. Para evitarlo, podemos escribir el nombre del paquete delante de la función que queremos usar, separados por el símbolo de dos puntos escrito dos veces (package_name::function_name).\nAntes de empezar con los paquetes, espero que sea verdaderamente una breve introducción, algunos comentarios sobre el estilo al programar en R.\n\r2 Guía de estilo\rEn R no existe una guía de estilo universal, o sea, en la sintaxis de R no es necesario seguir normas concretas para nuestros scripts. Es recomendable trabajar de forma homogénea y clara a la hora de escribir con un estilo uniforme y legible. La colección de tidyverse tiene una guia propia (https://style.tidyverse.org/).\nLas recomendaciones más importes son:\n\rEvitar usar más de 80 caracteres por línea para permitir leer el código completo.\rUsar siempre un espacio después de una coma, nunca antes.\rLos operadores (==, +, -, \u0026lt;-, %\u0026gt;%, etc.) deben tener un espacio antes y después.\rNo hay espacio entre el nombre de una función y el primer paréntesis, ni entre el último arguemento y el paréntesis final de una función.\rEvitar reutilizar nombres de funciones y variables comunes (c \u0026lt;- 5 vs. c())\rOrdenar el script separando las partes con la forma de comentario # Importar datos -----\rSe deben evitar tildes o símbolos especiales en nombres, archivos, rutas, etc.\rNombres de los objetos deben seguir una estructura constante: day_one, day_1.\r\rEs aconsejable usar una correcta indentación para multiples argumentos de una función o funciones encadenadas por el operador pipe (%\u0026gt;%).\n\r3 Pipe %\u0026gt;%\rPara facilitar el trabajo en la gestión, manipulación y visualización de datos, el paquete magrittr introduce el operador llamado pipe en la forma %\u0026gt;% con el objetivo de combinar varias funciones sin la necesidad de asignar el resultado a un nuevo objeto. El operador pipe pasa a la salida de una función aplicada al primer argumento de la siguiente función. Esta forma de combinar funciones permite encadenar varios pasos de forma simultánea. En el siguiente ejemplo, muy sencillo, pasamos el vector 1:5 a la función mean() para calcular el promedio.\n1:5 %\u0026gt;% mean()\r## [1] 3\r\r4 Paquetes de Tidyverse\r4.1 Lectura y escritura\rEl paquete readr facilita la lectura o escritura de múltiples formatos de archivo usando funciones que comienzan por read_* o write_*.\rEn comparación con R Base las funciones son más rápidas, ayudan a limpiar los nombres de las columnas y las fechas son convertidas automáticamente. Las tablas importadas son de clase tibble (tbl_df), una versión moderna de data.frame del paquete tibble. En el mismo sentido se puede usar la función read_excel() del paquete readxl para importar datos de hojas de Excel (más detalles también en esta entrada de mi blog). En el siguiente ejemplo importamos los datos de la movilidad registrada por Google (enlace) durante los últimos meses a causa de la pandemia COVID-19 (descarga).\n\r\rFunción lectura\rDescripción\r\r\r\rread_csv() o read_csv2()\rcoma o punto-coma (CSV)\r\rread_delim()\rseparador general\r\rread_table()\respacio blanco\r\r\r\r# cargar el paquete\rlibrary(tidyverse)\rgoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Parsed with column specification:\r## cols(\r## country_region_code = col_character(),\r## country_region = col_character(),\r## sub_region_1 = col_character(),\r## sub_region_2 = col_logical(),\r## iso_3166_2_code = col_character(),\r## census_fips_code = col_logical(),\r## date = col_date(format = \u0026quot;\u0026quot;),\r## retail_and_recreation_percent_change_from_baseline = col_double(),\r## grocery_and_pharmacy_percent_change_from_baseline = col_double(),\r## parks_percent_change_from_baseline = col_double(),\r## transit_stations_percent_change_from_baseline = col_double(),\r## workplaces_percent_change_from_baseline = col_double(),\r## residential_percent_change_from_baseline = col_double()\r## )\r## Warning: 597554 parsing failures.\r## row col expected actual file\r## 200119 sub_region_2 1/0/T/F/TRUE/FALSE Autauga County \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200119 census_fips_code 1/0/T/F/TRUE/FALSE 01001 \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200120 sub_region_2 1/0/T/F/TRUE/FALSE Autauga County \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200120 census_fips_code 1/0/T/F/TRUE/FALSE 01001 \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200121 sub_region_2 1/0/T/F/TRUE/FALSE Autauga County \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## ...... ................ .................. .............. ............................\r## See problems(...) for more details.\rgoogle_mobility\r## # A tibble: 516,697 x 13\r## country_region_~ country_region sub_region_1 sub_region_2 iso_3166_2_code\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; ## 1 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 2 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 3 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 4 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 5 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 6 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 7 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 8 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 9 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## 10 AE United Arab E~ \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; ## # ... with 516,687 more rows, and 8 more variables: census_fips_code \u0026lt;lgl\u0026gt;,\r## # date \u0026lt;date\u0026gt;, retail_and_recreation_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # grocery_and_pharmacy_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # parks_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # transit_stations_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # workplaces_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # residential_percent_change_from_baseline \u0026lt;dbl\u0026gt;\rDebemos prestar atención a los nombres de los argumentos, ya que cambian en las funciones de readr. Por ejemplo, el argumento conocido header = TRUE de read.csv() es en este caso col_names = TRUE. Podemos encontrar más detalles en el Cheat-Sheet de readr .\n\r4.2 Manipulación de caracteres\rCuando se requiere manipular cadenas de texto usamos el paquete stringr, cuyas funciones siempre empiezan por str_* seguidas por un verbo y el primer argumento.\nAlgunas de estas funciones son las siguientes:\n\r\rFunción\rDescripción\r\r\r\rstr_replace()\rreemplazar patrones\r\rstr_c()\rcombinar characteres\r\rstr_detect()\rdetectar patrones\r\rstr_extract()\rextraer patrones\r\rstr_sub()\rextraer por posición\r\rstr_length()\rlongitud de la cadena de caracteres\r\r\r\rSe suelen usar expresiones regulares para patrones de caracteres. Por ejemplo, la expresión regular [aeiou] coincide con cualquier caracter único que sea una vocal. El uso de corchetes [] corresponde a clases de caracteres. Por ejemplo, [abc] corresponde a cada letra independientemente de la posición. [a-z] o [A-Z] o [0-9] cada uno entre a y z ó 0 y 9. Y por último, [:punct:] puntuación, etc. Con llaves “{}” podemos indicar el número del elemento anterior {2} sería dos veces, {1,2} entre una y dos, etc. Además con $o ^ podemos indicar si el patrón empieza al principio o termina al final. Podemos encontrar más detalles y patrones en el Cheat-Sheet de stringr.\n# reemplazamos \u0026#39;er\u0026#39; al final por vacío\rstr_replace(month.name, \u0026quot;er$\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;March\u0026quot; \u0026quot;April\u0026quot; \u0026quot;May\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;Septemb\u0026quot; \u0026quot;Octob\u0026quot; \u0026quot;Novemb\u0026quot; \u0026quot;Decemb\u0026quot;\rstr_replace(month.name, \u0026quot;^Ma\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;rch\u0026quot; \u0026quot;April\u0026quot; \u0026quot;y\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;September\u0026quot; \u0026quot;October\u0026quot; \u0026quot;November\u0026quot; \u0026quot;December\u0026quot;\r# combinar caracteres\ra \u0026lt;- str_c(month.name, 1:12, sep = \u0026quot;_\u0026quot;)\ra\r## [1] \u0026quot;January_1\u0026quot; \u0026quot;February_2\u0026quot; \u0026quot;March_3\u0026quot; \u0026quot;April_4\u0026quot; \u0026quot;May_5\u0026quot; ## [6] \u0026quot;June_6\u0026quot; \u0026quot;July_7\u0026quot; \u0026quot;August_8\u0026quot; \u0026quot;September_9\u0026quot; \u0026quot;October_10\u0026quot; ## [11] \u0026quot;November_11\u0026quot; \u0026quot;December_12\u0026quot;\r# colapsar combinación\rstr_c(month.name, collapse = \u0026quot;, \u0026quot;)\r## [1] \u0026quot;January, February, March, April, May, June, July, August, September, October, November, December\u0026quot;\r# dedectamos patrones\rstr_detect(a, \u0026quot;_[1-5]{1}\u0026quot;)\r## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE\r# extraemos patrones\rstr_extract(a, \u0026quot;_[1-9]{1,2}\u0026quot;)\r## [1] \u0026quot;_1\u0026quot; \u0026quot;_2\u0026quot; \u0026quot;_3\u0026quot; \u0026quot;_4\u0026quot; \u0026quot;_5\u0026quot; \u0026quot;_6\u0026quot; \u0026quot;_7\u0026quot; \u0026quot;_8\u0026quot; \u0026quot;_9\u0026quot; \u0026quot;_1\u0026quot; \u0026quot;_11\u0026quot; \u0026quot;_12\u0026quot;\r# extraermos los caracteres en las posiciones entre 1 y 2\rstr_sub(month.name, 1, 2)\r## [1] \u0026quot;Ja\u0026quot; \u0026quot;Fe\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ap\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Au\u0026quot; \u0026quot;Se\u0026quot; \u0026quot;Oc\u0026quot; \u0026quot;No\u0026quot; \u0026quot;De\u0026quot;\r# longitud de cada mes\rstr_length(month.name)\r## [1] 7 8 5 5 3 4 4 6 9 7 8 8\r# con pipe, el \u0026#39;.\u0026#39; representa al objeto que pasa el operador %\u0026gt;%\rstr_length(month.name) %\u0026gt;% str_c(month.name, ., sep = \u0026quot;.\u0026quot;)\r## [1] \u0026quot;January.7\u0026quot; \u0026quot;February.8\u0026quot; \u0026quot;March.5\u0026quot; \u0026quot;April.5\u0026quot; \u0026quot;May.3\u0026quot; ## [6] \u0026quot;June.4\u0026quot; \u0026quot;July.4\u0026quot; \u0026quot;August.6\u0026quot; \u0026quot;September.9\u0026quot; \u0026quot;October.7\u0026quot; ## [11] \u0026quot;November.8\u0026quot; \u0026quot;December.8\u0026quot;\rUna función muy útil es str_glue() para interpolar caracteres.\nname \u0026lt;- c(\u0026quot;Juan\u0026quot;, \u0026quot;Michael\u0026quot;)\rage \u0026lt;- c(50, 80) date_today \u0026lt;- Sys.Date()\rstr_glue(\r\u0026quot;My name is {name}, \u0026quot;,\r\u0026quot;I\u0026#39;am {age}, \u0026quot;,\r\u0026quot;and my birth year is {format(date_today-age*365, \u0026#39;%Y\u0026#39;)}.\u0026quot;\r)\r## My name is Juan, I\u0026#39;am 50, and my birth year is 1970.\r## My name is Michael, I\u0026#39;am 80, and my birth year is 1940.\r\r4.3 Manejo de fechas y horas\rEl paquete lubridate ayuda en el manejo de fechas y horas. Nos permite crear los objetos reconocidos por R con funciones (como ymd() ó ymd_hms()) y hacer cálculos.\nDebemos conocer las siguientes abreviaturas:\n\rymd: representa y:year, m:month, d:day\rhms: representa h:hour, m:minutes, s:seconds\r\r# paquete\rlibrary(lubridate)\r## ## Attaching package: \u0026#39;lubridate\u0026#39;\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## date, intersect, setdiff, union\r# vector de fechas\rdat \u0026lt;- c(\u0026quot;1999/12/31\u0026quot;, \u0026quot;2000/01/07\u0026quot;, \u0026quot;2005/05/20\u0026quot;,\u0026quot;2010/03/25\u0026quot;)\r# vector de fechas y horas\rdat_time \u0026lt;- c(\u0026quot;1988-08-01 05:00\u0026quot;, \u0026quot;2000-02-01 22:00\u0026quot;)\r# convertir a clase date\rdat \u0026lt;- ymd(dat) dat\r## [1] \u0026quot;1999-12-31\u0026quot; \u0026quot;2000-01-07\u0026quot; \u0026quot;2005-05-20\u0026quot; \u0026quot;2010-03-25\u0026quot;\r# otras formatos\rdmy(\u0026quot;05-02-2000\u0026quot;)\r## [1] \u0026quot;2000-02-05\u0026quot;\rymd(\u0026quot;20000506\u0026quot;)\r## [1] \u0026quot;2000-05-06\u0026quot;\r# convertir a POSIXct\rdat_time \u0026lt;- ymd_hm(dat_time)\rdat_time\r## [1] \u0026quot;1988-08-01 05:00:00 UTC\u0026quot; \u0026quot;2000-02-01 22:00:00 UTC\u0026quot;\r# diferentes formatos en un vector dat_mix \u0026lt;- c(\u0026quot;1999/12/05\u0026quot;, \u0026quot;05-09-2008\u0026quot;, \u0026quot;2000/08/09\u0026quot;, \u0026quot;25-10-2019\u0026quot;)\r# indicar formato con la convención conocida en ?strptime\rparse_date_time(dat_mix, order = c(\u0026quot;%Y/%m/%d\u0026quot;, \u0026quot;%d-%m-%Y\u0026quot;))\r## [1] \u0026quot;1999-12-05 UTC\u0026quot; \u0026quot;2008-09-05 UTC\u0026quot; \u0026quot;2000-08-09 UTC\u0026quot; \u0026quot;2019-10-25 UTC\u0026quot;\rMás funciones útiles:\n# extraer el año\ryear(dat)\r## [1] 1999 2000 2005 2010\r# el mes\rmonth(dat)\r## [1] 12 1 5 3\rmonth(dat, label = TRUE) # como etiqueta\r## [1] dic ene may mar\r## 12 Levels: ene \u0026lt; feb \u0026lt; mar \u0026lt; abr \u0026lt; may \u0026lt; jun \u0026lt; jul \u0026lt; ago \u0026lt; sep \u0026lt; ... \u0026lt; dic\r# el día de la semana\rwday(dat)\r## [1] 6 6 6 5\rwday(dat, label = TRUE) # como etiqueta\r## [1] vi\\\\. vi\\\\. vi\\\\. ju\\\\.\r## Levels: do\\\\. \u0026lt; lu\\\\. \u0026lt; ma\\\\. \u0026lt; mi\\\\. \u0026lt; ju\\\\. \u0026lt; vi\\\\. \u0026lt; sá\\\\.\r# la hora\rhour(dat_time)\r## [1] 5 22\r# sumar 10 días\rdat + days(10)\r## [1] \u0026quot;2000-01-10\u0026quot; \u0026quot;2000-01-17\u0026quot; \u0026quot;2005-05-30\u0026quot; \u0026quot;2010-04-04\u0026quot;\r# sumar 1 mes\rdat + months(1)\r## [1] \u0026quot;2000-01-31\u0026quot; \u0026quot;2000-02-07\u0026quot; \u0026quot;2005-06-20\u0026quot; \u0026quot;2010-04-25\u0026quot;\rPor último, la función make_date() es muy útil en crear fechas a partir de diferentes partes de las mismas como puede ser el año, mes, etc.\n# crear fecha a partir de sus elementos, aquí con año y mes\rmake_date(2000, 5)\r## [1] \u0026quot;2000-05-01\u0026quot;\r# crear fecha con hora make_datetime(2005, 5, 23, 5)\r## [1] \u0026quot;2005-05-23 05:00:00 UTC\u0026quot;\rPodemos encontrar más detalles en el Cheat-Sheet de lubridate.\n\r4.4 Manipulación de tablas y vectores\rLos paquetes dplyr y tidyr nos proporciona una gramática de manipulación de datos con un conjunto de verbos útiles para resolver los problemas más comunes. Las funciones más importantes son:\n\r\rFunción\rDescripción\r\r\r\rmutate()\rañadir nuevas variables o modificar existentes\r\rselect()\rseleccionar variables\r\rfilter()\rfiltrar\r\rsummarise()\rresumir/reducir\r\rarrange()\rordenar\r\rgroup_by()\ragrupar\r\rrename()\rrenombrar columnas\r\r\r\rEn caso de que no lo hayas hecho antes, importamos los datos de movilidad.\ngoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Parsed with column specification:\r## cols(\r## country_region_code = col_character(),\r## country_region = col_character(),\r## sub_region_1 = col_character(),\r## sub_region_2 = col_logical(),\r## iso_3166_2_code = col_character(),\r## census_fips_code = col_logical(),\r## date = col_date(format = \u0026quot;\u0026quot;),\r## retail_and_recreation_percent_change_from_baseline = col_double(),\r## grocery_and_pharmacy_percent_change_from_baseline = col_double(),\r## parks_percent_change_from_baseline = col_double(),\r## transit_stations_percent_change_from_baseline = col_double(),\r## workplaces_percent_change_from_baseline = col_double(),\r## residential_percent_change_from_baseline = col_double()\r## )\r## Warning: 597554 parsing failures.\r## row col expected actual file\r## 200119 sub_region_2 1/0/T/F/TRUE/FALSE Autauga County \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200119 census_fips_code 1/0/T/F/TRUE/FALSE 01001 \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200120 sub_region_2 1/0/T/F/TRUE/FALSE Autauga County \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200120 census_fips_code 1/0/T/F/TRUE/FALSE 01001 \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## 200121 sub_region_2 1/0/T/F/TRUE/FALSE Autauga County \u0026#39;Global_Mobility_Report.csv\u0026#39;\r## ...... ................ .................. .............. ............................\r## See problems(...) for more details.\r4.4.1 Selecionar y renombrar\rPodemos selecionar o eliminar columnas con la función select(), usando el nombre o índice de la(s) columna(s). Para suprimir columnas hacemos uso del signo negativo. La función rename ayuda en renombrar columnas o bien con el mismo nombre o con su índice.\nresidential_mobility \u0026lt;- select(google_mobility, country_region_code:sub_region_1, date, residential_percent_change_from_baseline) %\u0026gt;% rename(resi = 5)\r\r4.4.2 Filtrar y ordenar\rPara filtrar datos, empleamos filter() con operadores lógicos (|, ==, \u0026gt;, etc) o funciones que devuelven un valor lógico (str_detect(), is.na(), etc.). La función arrange() ordena de menor a mayor por una o múltiples variables (con el signo negativo - se invierte el orden de mayor a menor).\nfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;)\r## # A tibble: 304,648 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States \u0026lt;NA\u0026gt; 2020-02-15 -1\r## 2 US United States \u0026lt;NA\u0026gt; 2020-02-16 -1\r## 3 US United States \u0026lt;NA\u0026gt; 2020-02-17 5\r## 4 US United States \u0026lt;NA\u0026gt; 2020-02-18 1\r## 5 US United States \u0026lt;NA\u0026gt; 2020-02-19 0\r## 6 US United States \u0026lt;NA\u0026gt; 2020-02-20 1\r## 7 US United States \u0026lt;NA\u0026gt; 2020-02-21 0\r## 8 US United States \u0026lt;NA\u0026gt; 2020-02-22 -1\r## 9 US United States \u0026lt;NA\u0026gt; 2020-02-23 -1\r## 10 US United States \u0026lt;NA\u0026gt; 2020-02-24 0\r## # ... with 304,638 more rows\rfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;, sub_region_1 == \u0026quot;New York\u0026quot;)\r## # A tibble: 7,068 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States New York 2020-02-15 0\r## 2 US United States New York 2020-02-16 -1\r## 3 US United States New York 2020-02-17 9\r## 4 US United States New York 2020-02-18 3\r## 5 US United States New York 2020-02-19 2\r## 6 US United States New York 2020-02-20 2\r## 7 US United States New York 2020-02-21 3\r## 8 US United States New York 2020-02-22 -1\r## 9 US United States New York 2020-02-23 -1\r## 10 US United States New York 2020-02-24 0\r## # ... with 7,058 more rows\rfilter(residential_mobility, resi \u0026gt; 50) %\u0026gt;% arrange(-resi)\r## # A tibble: 32 x 5\r## country_region_co~ country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 KW Kuwait Al Farwaniyah Governorate 2020-05-14 56\r## 2 KW Kuwait Al Farwaniyah Governorate 2020-05-21 55\r## 3 SG Singapore \u0026lt;NA\u0026gt; 2020-05-01 55\r## 4 KW Kuwait Al Farwaniyah Governorate 2020-05-28 54\r## 5 PE Peru Metropolitan Municipality~ 2020-04-10 54\r## 6 EC Ecuador Pichincha 2020-03-27 53\r## 7 KW Kuwait Al Farwaniyah Governorate 2020-05-11 53\r## 8 KW Kuwait Al Farwaniyah Governorate 2020-05-13 53\r## 9 KW Kuwait Al Farwaniyah Governorate 2020-05-20 53\r## 10 SG Singapore \u0026lt;NA\u0026gt; 2020-04-10 53\r## # ... with 22 more rows\r\r4.4.3 Agrupar y resumir\r¿Dónde encontramos mayor variabilidad entre regiones en cada país el día 1 de abril de 2020?\nPara responder a esta pregunta, primero filtramos los datos y después agrupamos por la columna de país. Cuando empleamos la función summarise() posterior a la agrupación, nos permite resumir por estos grupos. Incluso, la combinación del group_by() con la función mutate() permite modificar columnas por grupos. En summarise() calculamos el valor máximo, mínimo y la diferencia entre ambos extremos creando nuevas columnas.\nresi_variability \u0026lt;- residential_mobility %\u0026gt;% filter(date == ymd(\u0026quot;2020-04-01\u0026quot;),\r!is.na(sub_region_1)) %\u0026gt;% group_by(country_region) %\u0026gt;% summarise(mx = max(resi, na.rm = TRUE), min = min(resi, na.rm = TRUE),\rrange = abs(mx)-abs(min))\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in max(resi, na.rm = TRUE): ningun argumento finito para max; retornando\r## -Inf\r## Warning in min(resi, na.rm = TRUE): ningún argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ningún argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ningún argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ningún argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ningún argumento finito para min; retornando\r## Inf\r## Warning in min(resi, na.rm = TRUE): ningún argumento finito para min; retornando\r## Inf\r## `summarise()` ungrouping output (override with `.groups` argument)\rarrange(resi_variability, -range)\r## # A tibble: 94 x 4\r## country_region mx min range\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Nigeria 43 6 37\r## 2 United States 35 6 29\r## 3 India 36 15 21\r## 4 Malaysia 45 26 19\r## 5 Philippines 40 21 19\r## 6 Vietnam 28 9 19\r## 7 Colombia 41 24 17\r## 8 Ecuador 44 27 17\r## 9 Argentina 35 19 16\r## 10 Chile 30 14 16\r## # ... with 84 more rows\r\r4.4.4 Unir tablas\r¿Cómo podemos filtrar los datos para obtener un subconjunto de Europa?\nPara ello, importamos datos espaciales con el código de país y una columna de las regiones. Explicaciones detalladas sobre el paquete sf (simple feature) para trabajar con datos vectoriales, lo dejaremos para otro post.\nlibrary(rnaturalearth) # paquete de datos vectoriales\r# datos de países\rwld \u0026lt;- ne_countries(returnclass = \u0026quot;sf\u0026quot;)\r# filtramos los países con código y seleccionamos las dos columnas de interés\rwld \u0026lt;- filter(wld, !is.na(iso_a2)) %\u0026gt;% select(iso_a2, subregion)\r# plot\rplot(wld)\rOtras funciones de dplyr nos permiten unir tablas: *_join(). Según hacia qué tabla (izquierda o derecha) se quiere unir, cambia la función : left_join(), right_join() o incluso full_join(). El argumento by no es necesario siempre y cuando ambas tablas tienen una columna en común. No obstante, en este caso la columna de fusión es diferente, por eso, usamos el modo c(\"country_region_code\"=\"iso_a2\"). El paquete forcats de tidyverse tiene muchas funciones útiles para manejar variables categóricas (factors), variables que tienen un conjunto fijo y conocido de valores posibles. Todas las funciones de forcats tienen el prefijo fct_*. Por ejemplo, en este caso usamos fct_reorder() para reordenar las etiquetas de los países en orden de la máxima basada en los registros de movibilidad residencial. Finalmente, creamos una nueva columna ‘resi_real’ para cambiar el valor de referencia, el promedio (baseline), fijado en 0 a 100.\nsubset_europe \u0026lt;- filter(residential_mobility, is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;%\rmutate(resi_real = resi + 100,\rregion = fct_reorder(country_region, resi, .fun = \u0026quot;max\u0026quot;, .desc = FALSE)) %\u0026gt;% select(-geometry, -sub_region_1)\rstr(subset_europe)\r## tibble [3,988 x 7] (S3: tbl_df/tbl/data.frame)\r## $ country_region_code: chr [1:3988] \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; ...\r## $ country_region : chr [1:3988] \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; ...\r## $ date : Date[1:3988], format: \u0026quot;2020-02-15\u0026quot; \u0026quot;2020-02-16\u0026quot; ...\r## $ resi : num [1:3988] -2 -2 0 0 1 0 1 -2 0 -1 ...\r## $ subregion : chr [1:3988] \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; ...\r## $ resi_real : num [1:3988] 98 98 100 100 101 100 101 98 100 99 ...\r## $ region : Factor w/ 35 levels \u0026quot;Belarus\u0026quot;,\u0026quot;Ukraine\u0026quot;,..: 18 18 18 18 18 18 18 18 18 18 ...\r## - attr(*, \u0026quot;problems\u0026quot;)= tibble [597,554 x 5] (S3: tbl_df/tbl/data.frame)\r## ..$ row : int [1:597554] 200119 200119 200120 200120 200121 200121 200122 200122 200123 200123 ...\r## ..$ col : chr [1:597554] \u0026quot;sub_region_2\u0026quot; \u0026quot;census_fips_code\u0026quot; \u0026quot;sub_region_2\u0026quot; \u0026quot;census_fips_code\u0026quot; ...\r## ..$ expected: chr [1:597554] \u0026quot;1/0/T/F/TRUE/FALSE\u0026quot; \u0026quot;1/0/T/F/TRUE/FALSE\u0026quot; \u0026quot;1/0/T/F/TRUE/FALSE\u0026quot; \u0026quot;1/0/T/F/TRUE/FALSE\u0026quot; ...\r## ..$ actual : chr [1:597554] \u0026quot;Autauga County\u0026quot; \u0026quot;01001\u0026quot; \u0026quot;Autauga County\u0026quot; \u0026quot;01001\u0026quot; ...\r## ..$ file : chr [1:597554] \u0026quot;\u0026#39;Global_Mobility_Report.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;Global_Mobility_Report.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;Global_Mobility_Report.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;Global_Mobility_Report.csv\u0026#39;\u0026quot; ...\r\r4.4.5 Tablas largas y anchas\rAntes de pasar a la visualización con ggplot2. Es muy habitual modificar la tabla entre dos formatos principales. Una tabla es tidy cuando 1) cada variable es una columna 2) cada observación/caso es una fila y 3) cada tipo de unidad observacional forma una tabla.\n# subconjunto mobility_selection \u0026lt;- select(subset_europe, country_region_code, date:resi)\rmobility_selection\r## # A tibble: 3,988 x 3\r## country_region_code date resi\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 AT 2020-02-15 -2\r## 2 AT 2020-02-16 -2\r## 3 AT 2020-02-17 0\r## 4 AT 2020-02-18 0\r## 5 AT 2020-02-19 1\r## 6 AT 2020-02-20 0\r## 7 AT 2020-02-21 1\r## 8 AT 2020-02-22 -2\r## 9 AT 2020-02-23 0\r## 10 AT 2020-02-24 -1\r## # ... with 3,978 more rows\r# tabla ancha\rmobi_wide \u0026lt;- pivot_wider(mobility_selection, names_from = country_region_code,\rvalues_from = resi)\rmobi_wide\r## # A tibble: 114 x 36\r## date AT BA BE BG BY CH CZ DE DK EE ES\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 -2 -1 -1 0 -1 -1 -2 -1 0 0 -2\r## 2 2020-02-16 -2 -1 1 -3 0 -1 -1 0 1 0 -2\r## 3 2020-02-17 0 -1 0 -2 0 1 0 0 1 1 -1\r## 4 2020-02-18 0 -1 0 -2 0 1 0 1 1 1 0\r## 5 2020-02-19 1 -1 0 -1 -1 1 0 1 1 0 -1\r## 6 2020-02-20 0 -1 0 0 -1 0 0 1 1 0 -1\r## 7 2020-02-21 1 -2 0 -1 -1 1 0 2 1 1 -2\r## 8 2020-02-22 -2 -1 0 0 -2 -2 -3 0 1 0 -2\r## 9 2020-02-23 0 -1 0 -3 -1 -1 0 0 0 -2 -3\r## 10 2020-02-24 -1 -1 4 -1 0 0 0 4 0 16 0\r## # ... with 104 more rows, and 24 more variables: FI \u0026lt;dbl\u0026gt;, FR \u0026lt;dbl\u0026gt;, GB \u0026lt;dbl\u0026gt;,\r## # GR \u0026lt;dbl\u0026gt;, HR \u0026lt;dbl\u0026gt;, HU \u0026lt;dbl\u0026gt;, IE \u0026lt;dbl\u0026gt;, IT \u0026lt;dbl\u0026gt;, LT \u0026lt;dbl\u0026gt;, LU \u0026lt;dbl\u0026gt;,\r## # LV \u0026lt;dbl\u0026gt;, MD \u0026lt;dbl\u0026gt;, MK \u0026lt;dbl\u0026gt;, NL \u0026lt;dbl\u0026gt;, NO \u0026lt;dbl\u0026gt;, PL \u0026lt;dbl\u0026gt;, PT \u0026lt;dbl\u0026gt;,\r## # RO \u0026lt;dbl\u0026gt;, RS \u0026lt;dbl\u0026gt;, RU \u0026lt;dbl\u0026gt;, SE \u0026lt;dbl\u0026gt;, SI \u0026lt;dbl\u0026gt;, SK \u0026lt;dbl\u0026gt;, UA \u0026lt;dbl\u0026gt;\r# tabla larga\rpivot_longer(mobi_wide,\r2:36,\rnames_to = \u0026quot;country_code\u0026quot;,\rvalues_to = \u0026quot;resi\u0026quot;)\r## # A tibble: 3,990 x 3\r## date country_code resi\r## \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 AT -2\r## 2 2020-02-15 BA -1\r## 3 2020-02-15 BE -1\r## 4 2020-02-15 BG 0\r## 5 2020-02-15 BY -1\r## 6 2020-02-15 CH -1\r## 7 2020-02-15 CZ -2\r## 8 2020-02-15 DE -1\r## 9 2020-02-15 DK 0\r## 10 2020-02-15 EE 0\r## # ... with 3,980 more rows\rOtro grupo de funciones a las que deberías echar un vistazo son: separate(), case_when(), complete(). Podemos encontrar más detalles en el Cheat-Sheet de dplyr\n\r\r4.5 Visualizar datos\rggplot2 es un sistema moderno, y con una enorme variedad de opciones, para visualización de datos. A diferencia del sistema gráfico de R Base se utiliza una gramática diferente.\rLa gramática de los gráficos (grammar of graphics, de allí “gg”) consiste en la suma de varias capas u objetos independientes que se combinan usando + para construir el gráfico final. ggplot diferencia entre los datos, lo que se visualiza y la forma en que se visualiza.\n\rdata: nuestro conjunto de datos (data.frame o tibble)\n\raesthetics: con la función aes() indicamos las variables que corresponden a los ejes x, y, z,… o, cuando se pretende aplicar parámetros gráficos (color, size, shape) según una variable. Es posible incluir aes() en ggplot() o en la función correspondiente a una geometría geom_*.\n\rgeometries: son objetos geom_* que indican la geometría a usar, (p. ej.: geom_point(), geom_line(), geom_boxplot(), etc.).\n\rscales: son objetos de tipo scales_* (p. ej.: scale_x_continous(), scale_colour_manual()) para manipular las ejes, definir colores, etc.\n\rstatistics: son objetos stat_* (p.ej.: stat_density()) que permiten aplicar transformaciones estadísticas.\n\r\rPodemos encontrar más detalles en el Cheat-Sheet de ggplot2. ggplot es complementado constantemente con extensiones para geometrías u otras opciones gráficas (https://exts.ggplot2.tidyverse.org/ggiraph.html), para obtener ideas gráficas, debes echarle un vistazo a la Galería de Gráficos R (https://www.r-graph-gallery.com/).\n4.5.1 Gráfico de linea y puntos\rCreamos un subconjunto de nuestros datos de movilidad para residencias y parques, filtrando los registros de regiones italianas. Además, dividimos los valores de movilidad en porcentaje por 100 para obtener la fracción, ya que ggplot2 nos permite indicar la unidad de porcentaje en el argumento de las etiquetas (último gráfico de esta sección).\n# creamos el subconjunto\rit \u0026lt;- filter(google_mobility, country_region == \u0026quot;Italy\u0026quot;, is.na(sub_region_1)) %\u0026gt;% mutate(resi = residential_percent_change_from_baseline/100, parks = parks_percent_change_from_baseline/100)\r# gráfico de línea ggplot(it, aes(date, resi)) + geom_line()\r# gráfico de dispersión con línea de correlación\rggplot(it, aes(parks, resi)) + geom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rPara modificar los ejes, empleamos las diferentes funciones de scale_* que debemos adpatar a las escalas de medición (date, discrete, continuous, etc.). La función labs() nos ayuda en definir los títulos de ejes, del gráfico y de la leyenda. Por último, añadimos con theme_light() el estilo del gráfico (otros son theme_bw(), theme_minimal(), etc.). También podríamos hacer cambios de todos los elementos gráficos a través de theme().\n# time serie plot\rggplot(it, aes(date, resi)) + geom_line(colour = \u0026quot;#560A86\u0026quot;, size = 0.8) +\rscale_x_date(date_breaks = \u0026quot;10 days\u0026quot;, date_labels = \u0026quot;%d %b\u0026quot;) +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r# scatter plot\rggplot(it, aes(parks, resi)) + geom_point(alpha = .4, size = 2) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rscale_x_continuous(breaks = seq(-1, 1.4, 0.2), labels = scales::percent) +\rscale_y_continuous(breaks = seq(-1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;Park mobility\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\r4.5.2 Boxplot\rPodemos visualizar diferentes aspectos de los datos de movilidad con otras geometrías. Aquí creamos boxplots por cada país europeo representando la variabilidad de movilidad entre y en los países durante la pandemia del COVID-19.\n# subconjunto\rsubset_europe_reg \u0026lt;- filter(residential_mobility, !is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;% mutate(resi = resi/100, country_region = fct_reorder(country_region, resi))\r# boxplot\rggplot(subset_europe_reg, aes(country_region, resi, fill = subregion)) + geom_boxplot() +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;) +\rcoord_flip() +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;, fill = \u0026quot;\u0026quot;) +\rtheme_minimal()\r\r4.5.3 Heatmap\rPara visualizar la tendencia de todos los países europeos es recomendable usar un heatmap en lugar de un bulto de líneas. Antes de constuir el gráfico, creamos un vector de fechas para las etiquetas con los domingos en el período de registros.\n# secuencia de fechas\rdf \u0026lt;- data.frame(d = seq(ymd(\u0026quot;2020-02-15\u0026quot;), ymd(\u0026quot;2020-06-07\u0026quot;), \u0026quot;day\u0026quot;))\r# filtramos los domingos creando el día de la semana\rsundays \u0026lt;- df %\u0026gt;% mutate(wd = wday(d, week_start = 1)) %\u0026gt;% filter(wd == 7) %\u0026gt;% pull(d)\rSi queremos usar etiquetas en otras lenguas, es necesario cambiar la configuración regional del sistema.\nSys.setlocale(\u0026quot;LC_TIME\u0026quot;, \u0026quot;English\u0026quot;)\r## [1] \u0026quot;English_United States.1252\u0026quot;\rEl relleno de color para los boxplots lo dibujamos por cada región de los países europeos. Podemos fijar el tipo de color con scale_fill_*, en este caso, de las gamas viridis.\nAdemás, la función guides() nos permite modificar la barra de color de la leyenda. Por último, aquí vemos el uso de theme() con cambios adicionales a theme_minimal().\n# headmap\rggplot(subset_europe, aes(date, region, fill = resi_real)) +\rgeom_tile() +\rscale_x_date(breaks = sundays,\rdate_labels = \u0026quot;%d %b\u0026quot;) +\rscale_fill_viridis_c(option = \u0026quot;A\u0026quot;, breaks = c(91, 146),\rlabels = c(\u0026quot;Less\u0026quot;, \u0026quot;More\u0026quot;), direction = -1) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;top\u0026quot;, title = element_text(size = 14),\rpanel.grid.major.x = element_line(colour = \u0026quot;white\u0026quot;, linetype = \u0026quot;dashed\u0026quot;),\rpanel.grid.minor.x = element_blank(),\rpanel.grid.major.y = element_blank(),\rpanel.ontop = TRUE,\rplot.margin = margin(r = 1, unit = \u0026quot;cm\u0026quot;)) +\rlabs(y = \u0026quot;\u0026quot;, x = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, title = \u0026quot;Mobility trends for places of residence\u0026quot;,\rcaption = \u0026quot;Data: google.com/covid19/mobility/\u0026quot;) +\rguides(fill = guide_colorbar(barwidth = 10, barheight = .5,\rlabel.position = \u0026quot;top\u0026quot;, ticks = FALSE)) +\rcoord_cartesian(expand = FALSE)\r\r\r4.6 Aplicar funciones sobre vectores o listas\rEl paquete purrr contiene un conjunto de funciones avanzadas de programación funcional para trabajar con funciones y vectores. La familia de funciones lapply() conocido de R Basecorresponde a las funciones de map() en este paquete. Una de las mayores ventajas es poder reducir el uso de bucles (for, etc.).\n# lista con dos vectores\rvec_list \u0026lt;- list(x = 1:10, y = 50:70)\r# calculamos el promedio para cada uno\rmap(vec_list, mean)\r## $x\r## [1] 5.5\r## ## $y\r## [1] 60\r# podemos cambiar tipo de salida map_* (dbl, chr, lgl, etc.)\rmap_dbl(vec_list, mean)\r## x y ## 5.5 60.0\rUn ejemplo más complejo. Calculamos el coeficiente de correlación entre la movilidad residencial y la de los parques en todos los países europeos. Para obtener un resumen tidy de un modelo o un test usamos la función tidy() del paquete broom.\nlibrary(broom) # tidy outputs\r# función adaptada cor_test \u0026lt;- function(x, formula) { df \u0026lt;- cor.test(as.formula(formula), data = x) %\u0026gt;% tidy()\rreturn(df)\r}\r# preparamos los datos\reurope_reg \u0026lt;- filter(google_mobility, !is.na(sub_region_1),\r!is.na(residential_percent_change_from_baseline)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;))\r# aplicamos la función a cada país creando una lista\reurope_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;) \r## $AT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.360 -12.3 2.68e-32 1009 -0.413 -0.305 Pearson\u0026#39;~ two.sided ## ## $BE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.312 -6.06 3.67e-9 340 -0.405 -0.213 Pearson\u0026#39;~ two.sided ## ## $BG\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pearson~ two.sided ## ## $CH\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0786 -2.91 0.00370 1360 -0.131 -0.0256 Pearson\u0026#39;s~ two.sided ## ## $CZ\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0837 -3.35 0.000824 1593 -0.132 -0.0347 Pearson\u0026#39;~ two.sided ## ## $DE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.00239 0.102 0.919 1814 -0.0436 0.0484 Pearson\u0026#39;s~ two.sided ## ## $DK\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.237 5.81 1.04e-8 567 0.158 0.313 Pearson\u0026#39;~ two.sided ## ## $EE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.235 -2.88 0.00462 142 -0.384 -0.0740 Pearson\u0026#39;s~ two.sided ## ## $ES\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.825 -65.4 0 2005 -0.839 -0.811 Pearson\u0026#39;s~ two.sided ## ## $FI\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.0427 1.42 0.155 1106 -0.0162 0.101 Pearson\u0026#39;s~ two.sided ## ## $FR\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.698 -37.4 3.29e-216 1474 -0.723 -0.671 Pearson~ two.sided ## ## $GB\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.105 -11.0 9.19e-28 10712 -0.124 -0.0865 Pearson\u0026#39;~ two.sided ## ## $GR\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.692 -27.0 1.03e-114 796 -0.726 -0.654 Pearson~ two.sided ## ## $HR\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.579 -21.9 9.32e-87 954 -0.620 -0.536 Pearson\u0026#39;~ two.sided ## ## $HU\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.342 -15.6 6.71e-52 1843 -0.382 -0.301 Pearson\u0026#39;~ two.sided ## ## $IE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.222 -8.45 7.49e-17 1378 -0.271 -0.171 Pearson\u0026#39;~ two.sided ## ## $IT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.831 -71.0 0 2250 -0.844 -0.818 Pearson\u0026#39;s~ two.sided ## ## $LT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.204 -5.45 7.17e-8 686 -0.274 -0.131 Pearson\u0026#39;~ two.sided ## ## $LV\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.544 -6.87 3.84e-10 112 -0.662 -0.401 Pearson\u0026#39;~ two.sided ## ## $NL\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.143 5.31 1.25e-7 1356 0.0903 0.195 Pearson\u0026#39;~ two.sided ## ## $NO\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.0483 1.69 0.0911 1221 -0.00774 0.104 Pearson\u0026#39;s~ two.sided ## ## $PL\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.531 -26.7 6.08e-133 1815 -0.564 -0.498 Pearson~ two.sided ## ## $PT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.729 -46.9 2.12e-321 1938 -0.749 -0.707 Pearson~ two.sided ## ## $RO\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.640 -56.0 0 4517 -0.657 -0.623 Pearson\u0026#39;s~ two.sided ## ## $SE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.106 3.93 9.09e-5 1367 0.0529 0.158 Pearson\u0026#39;~ two.sided ## ## $SI\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.627 -11.4 1.98e-23 200 -0.704 -0.535 Pearson\u0026#39;~ two.sided ## ## $SK\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.196 -5.70 1.65e-8 810 -0.262 -0.129 Pearson\u0026#39;~ two.sided\rComo ya hemos visto anteriormente, existen subfunciones de map_* para obtener en lugar de una lista un objeto de otra clase, aquí de data.frame.\ncor_mobility \u0026lt;- europe_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map_df(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;, .id = \u0026quot;country_code\u0026quot;)\rarrange(cor_mobility, estimate)\r## # A tibble: 27 x 9\r## country_code estimate statistic p.value parameter conf.low conf.high method\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 IT -0.831 -71.0 0. 2250 -0.844 -0.818 Pears~\r## 2 ES -0.825 -65.4 0. 2005 -0.839 -0.811 Pears~\r## 3 PT -0.729 -46.9 2.12e-321 1938 -0.749 -0.707 Pears~\r## 4 FR -0.698 -37.4 3.29e-216 1474 -0.723 -0.671 Pears~\r## 5 GR -0.692 -27.0 1.03e-114 796 -0.726 -0.654 Pears~\r## 6 BG -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pears~\r## 7 RO -0.640 -56.0 0. 4517 -0.657 -0.623 Pears~\r## 8 SI -0.627 -11.4 1.98e- 23 200 -0.704 -0.535 Pears~\r## 9 HR -0.579 -21.9 9.32e- 87 954 -0.620 -0.536 Pears~\r## 10 LV -0.544 -6.87 3.84e- 10 112 -0.662 -0.401 Pears~\r## # ... with 17 more rows, and 1 more variable: alternative \u0026lt;chr\u0026gt;\rOtros ejemplos prácticos aquí en este post or este otro. Podemos encontrar más detalles en el Cheat-Sheet de purrr.\n\r\r","date":1591401600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1591401600,"objectID":"7fe4ed9e6a779c5b03e0c4f34abccba5","permalink":"/es/2020/una-muy-breve-introducci%C3%B3n-a-tidyverse/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/es/2020/una-muy-breve-introducci%C3%B3n-a-tidyverse/","section":"post","summary":"El universo de los paquetes de tidyverse, una colección de paquetes de funciones para un uso especialmente enfocado en la ciencia de datos, abrió un antes y después en la programación de R. En este post voy a resumir muy brevemente lo más esencial para inicarse en este mundo. La gramática sigue en todas las funciones una estructura común. Lo más esencial es que el primer argumento es el objeto y a continuación viene el resto de argumentos. Además, se proporciona un conjunto de verbos que facilitan el uso de las funciones. En la actualidad, la filosofía de las funciones también se refleja en otros paquetes que hacen compatible su uso con la colección de tidyverse.","tags":["introducción","visualización","gestión","datos","COVID-19"],"title":"Una muy breve introducción a Tidyverse","type":"post"},{"authors":["A Santurtún","R Almendra","P Fdez-Arroyabe","A Sanchez-Lorenzo","D Royé","MT Zarrabeitia","P Santana"],"categories":null,"content":"","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1588032000,"objectID":"ef7f8ab1bcf9f689d6abe4667c466c63","permalink":"/es/publication/ingresos_cardio_2020/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/es/publication/ingresos_cardio_2020/","section":"publication","summary":"The natural environment has been considered an important determinant of cardiovascular morbidity. This work seeks to assess the impact of the winter thermal environment on hospital admissions from diseases of the circulatory system by using three biometeorological indices in five regions of the Iberian Peninsula. A theoretical index based on a thermophysiological model (Universal Thermal Climate Index [UTCI]) and two experimental biometeorological ones (Net Effective Temperature [NET] and Apparent Temperature [AT]) were estimated in two metropolitan areas of Portugal (Porto and Lisbon) and in three provinces of Spain (Madrid, Barcelona and Valencia). Subsequently, their relationship with hospital admissions, adjusted by NO2 concentration, time, and day of the week, was analyzed using a Generalized Additive Model. As the estimation method, a semi-parametric quasi-Poisson regression was used. Around 53% of the hospitalizations occurred during the cold periods. The admissions rate followed an upward trend over the 9-year period in both capitals (Madrid and Lisbon) as well as in Barcelona. An inverse and statistically significant relationship was found between thermal comfort and hospital admissions in the five regions (p","tags":["Enfermedades del sistema circulatorio","Temperatura del aire","Temperatura efectiva neta","Temperatura aparente","Universal thermal climate index"],"title":"Predictive value of three thermal confort indices in low temperatures on cardiovascular morbidity in the Iberian Peninsula","type":"publication"},{"authors":null,"categories":["visualización","R","R:intermedio"],"content":"\rCuando visualizamos anomalías de precipitación y temperatura, simplemente usamos series temporales en un gráfico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, para tener una imagen global necesitamos ambas anomalías en un único gráfico. Así podríamos responder directamente a la pregunta de si una estación del año o un mes en concreto fue seco-cálido o húmedo-frío, e incluso comparar estas anomalías en el contexto de años anteriores.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rlubridate\rFácil manipulación de fechas y tiempos\r\rggrepel\rEtiquetas sin superposición con ggplot2\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggrepel\u0026quot;)) install.packages(\u0026quot;ggrepel\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggrepel)\r\rPreparar los datos\rPrimero importamos la precipitación y temperatura diaria de la estación meteorológica seleccionada (descarga). Usaremos los datos de Tenerife Sur (España) [1981-2020] accesible a través de Open Data AEMET. En R existe un paquete meteoland que facilita la descarga con funciones específicas para acceder a los datos de AEMET, Meteogalicia y Meteocat.\nPaso 1: importar los datos\rImportamos los datos en formato csv, siendo la primera columna la fecha, la segunda la precipitación (pr) y la última la temperatura media diaria (ta).\ndata \u0026lt;- read_csv(\u0026quot;meteo_tenerife.csv\u0026quot;) \r## Parsed with column specification:\r## cols(\r## date = col_date(format = \u0026quot;\u0026quot;),\r## pr = col_double(),\r## ta = col_double()\r## )\rdata\r## # A tibble: 14,303 x 3\r## date pr ta\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1981-01-02 0 17.6\r## 2 1981-01-03 0 16.8\r## 3 1981-01-04 0 17.4\r## 4 1981-01-05 0 17.6\r## 5 1981-01-06 0 17 ## 6 1981-01-07 0 17.6\r## 7 1981-01-08 0 18.6\r## 8 1981-01-09 0 19.8\r## 9 1981-01-10 0 21.5\r## 10 1981-01-11 3.8 17.6\r## # ... with 14,293 more rows\r\rPaso 2: preparar los datos\rEn el segundo paso preparamos los datos para calcular las anomalías. Para ello, creamos tres nuevas columnas: el mes, el año y la estación del año. Como nuestro objetivo son las anomalías invernales no podemos usar el año natural, ya que el invierno comprende el mes de diciembre de un año y los meses de enero y febrero del siguiente. La función personalizada meteo_yr() extrae el año de una fecha indicando el mes inicial. El concepto es similar al año hidrológico en el que se empieza en octubre.\nmeteo_yr \u0026lt;- function(dates, start_month = NULL) {\r# convertir a POSIXlt\rdates.posix \u0026lt;- as.POSIXlt(dates)\r# la compensación\roffset \u0026lt;- ifelse(dates.posix$mon \u0026gt;= start_month - 1, 1, 0)\r# nuevo año\radj.year = dates.posix$year + 1900 + offset\rreturn(adj.year)\r}\rUsaremos las funciones de la colección de paquetes tidyverse (https://www.tidyverse.org/). La función mutate() ayuda a añadir nuevas columnas o a cambiar otras existentes. Para definir las estaciones del año, empleamos la función case_when() del paquete dplyr lo que tiene muchas ventajas en comparación con una cadena de ifelse(). En la función case_when() usamos fórmulas en dos tiempos, por un lado la condición y por otro la acción cuando se cumpla esa condición. En R una fórmula de dos tiempos o dos lados se consistuye con el operador ~. El operador binario %in% nos permite filtrar varios valores en un conjunto.\ndata \u0026lt;- mutate(data, winter_yr = meteo_yr(date, 12),\rmonth = month(date), season = case_when(month %in% c(12,1:2) ~ \u0026quot;Winter\u0026quot;,\rmonth %in% 3:5 ~ \u0026quot;Spring\u0026quot;,\rmonth %in% 6:8 ~ \u0026quot;Summer\u0026quot;,\rmonth %in% 9:11 ~ \u0026quot;Autum\u0026quot;))\rdata\r## # A tibble: 14,303 x 6\r## date pr ta winter_yr month season\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1981-01-02 0 17.6 1981 1 Winter\r## 2 1981-01-03 0 16.8 1981 1 Winter\r## 3 1981-01-04 0 17.4 1981 1 Winter\r## 4 1981-01-05 0 17.6 1981 1 Winter\r## 5 1981-01-06 0 17 1981 1 Winter\r## 6 1981-01-07 0 17.6 1981 1 Winter\r## 7 1981-01-08 0 18.6 1981 1 Winter\r## 8 1981-01-09 0 19.8 1981 1 Winter\r## 9 1981-01-10 0 21.5 1981 1 Winter\r## 10 1981-01-11 3.8 17.6 1981 1 Winter\r## # ... with 14,293 more rows\r\rPaso 3: estimar las anomalías invernales\rEn el siguiente paso creamos un subset del invierno. Después agrupamos por el año meteorológico y calculamos la suma y el promedio para la precipitación y la temperatura, respectivamente. Para facilitar el trabajo el paquete magrittr introduce el operador llamado pipe en la forma %\u0026gt;% con el objetivo de combinar varias funciones sin la necesidad de asignar el resultado a un nuevo objeto. El operador pipe pasa la salida de una función aplicada al primer argumento de la siguiente función. Esta forma de combinar funciones permite encadenar varios pasos de forma simultánea. Se debe entender y pronunciar el %\u0026gt;% como “luego” (then).\ndata_inv \u0026lt;- filter(data, season == \u0026quot;Winter\u0026quot;) %\u0026gt;% group_by(winter_yr) %\u0026gt;%\rsummarise(pr = sum(pr, na.rm = TRUE),\rta = mean(ta, na.rm = TRUE))\rSólo nos quedan por calcular las anomalías de precipitación y temperatura. Las columnas pr_mean y ta_mean contendrán el promedio climatico, la referencia de las anomalías respecto al periodo normal 1981-2010. Por eso debemos filtrar los valores al periodo antes de 2010, lo que haremos de la forma habitual de filtrado de vectores en R. Una vez que tenemos las referencias estimamos las anomalías pr_anom y ta_anom. Para facilitar la interpretación, en el caso de la precipitación lo expresamos en porcentaje, pero poniendo el promedio en 0% en lugar del 100%.\nAdemás, añadimmos tres columnas con información necesaria en la creación del gráfico. 1) labyr contiene el año de cada anomalía siempre y cuando haya sido mayor/menor del -+10% o -+0,5ºC, respectivamente (lo hago para que no haya demasiadas etiquetas), 2) symb_point una variable dummy para poder crear un simbolo diferencial entre los casos de (1), y 3) lab_font destacaremos en negrita el año 2020.\ndata_inv \u0026lt;- mutate(data_inv, pr_mean = mean(pr[winter_yr \u0026lt;= 2010]), ta_mean = mean(ta[winter_yr \u0026lt;= 2010]),\rpr_anom = (pr*100/pr_mean)-100, ta_anom = ta-ta_mean,\rlabyr = case_when(pr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr),\rsymb_point = ifelse(!is.na(labyr), \u0026quot;yes\u0026quot;, \u0026quot;no\u0026quot;),\rlab_font = ifelse(labyr == 2020, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)\r)\r\r\rCrear el gráfico\rEl gráfico lo construiremos añadiendo capa por capa los diferentes elementos: 1) el fondo con las diferentes cuadrículas (Seco-Cálido, Seco-Frío, etc.) 2) los puntos y etiquetas, y 3) los últimos ajustes de estilo.\nParte 1\rLa idea es que tengamos los puntos con anomalías seco-cálido en el cuadrante I (arriba-derecha) y los de húmedo-frío en el cuadrante III (abajo-izquierda). Por eso, debemos invertir el signo en las anomalías de precipitación. Después creamos un data.frame con las posiciones de las etiquetas de los cuatro cuadrantes. Para las posiciones en x y y se usan Inf y -Inf lo que equivale al punto máximo dentro del panel. No obstante, es necesario ajustar la posición hacia los puntos extremos dentro del marco gráfico con los argumentos conocidos de ggplot2: hjust y vjust.\ndata_inv_p \u0026lt;- mutate(data_inv, pr_anom = pr_anom * -1)\rbglab \u0026lt;- data.frame(x = c(-Inf, Inf, -Inf, Inf), y = c(Inf, Inf, -Inf, -Inf),\rhjust = c(1, 1, 0, 0), vjust = c(1, 0, 1, 0),\rlab = c(\u0026quot;Húmedo-Cálido\u0026quot;, \u0026quot;Seco-Cálido\u0026quot;,\r\u0026quot;Húmedo-Frío\u0026quot;, \u0026quot;Seco-Frío\u0026quot;))\rbglab\r## x y hjust vjust lab\r## 1 -Inf Inf 1 1 Húmedo-Cálido\r## 2 Inf Inf 1 0 Seco-Cálido\r## 3 -Inf -Inf 0 1 Húmedo-Frío\r## 4 Inf -Inf 0 0 Seco-Frío\r\rParte 2\rEn la segunda podemos empezar a construir el gráfico añadiendo todos los elementos gráficos. En esta parte creamos el fondo con los diferentes colores de cada cuadrante. La función annotate() permite añadir capas de geometría sin el uso de variables dentro de un data.frame. Con la función geom_hline() y geom_vline() marcamos los cuadrantes en horizontal y vertical usando una linea discontinua. Por último, dibujamos las etiquetas de cada cuadrante, empleando la función geom_text(). Cuando usamos diferentes fuentes de data.frames, uno diferente al principal usado en ggplot(), debemos indicarlo con el argumento data en la función de geomtría correspondiente.\ng1 \u0026lt;- ggplot(data_inv_p, aes(pr_anom, ta_anom)) +\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = 0, ymax = Inf, fill = \u0026quot;#fc9272\u0026quot;, alpha = .6) + #humedo-calido\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = 0, ymax = Inf, fill = \u0026quot;#cb181d\u0026quot;, alpha = .6) + #seco-calido\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = -Inf, ymax = 0, fill = \u0026quot;#2171b5\u0026quot;, alpha = .6) + #humedo-frio\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = -Inf, ymax = 0, fill = \u0026quot;#c6dbef\u0026quot;, alpha = .6) + #seco-frio\rgeom_hline(yintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_vline(xintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = bglab, aes(x, y, label = lab, hjust = hjust, vjust = vjust),\rfontface = \u0026quot;italic\u0026quot;, size = 5, angle = 90, colour = \u0026quot;white\u0026quot;)\rg1\r\rParte 3\rEn la tercera parte simplemente añadimos los puntos de las anomalías y las etiquetas de los años. La función geom_text_repel() es similar a la que conocemos por defecto en ggplot2, geom_text(), pero evita el sobrlapso entre sí.\ng2 \u0026lt;- g1 + geom_point(aes(fill = symb_point, colour = symb_point),\rsize = 2.8, shape = 21, show.legend = FALSE) +\rgeom_text_repel(aes(label = labyr, fontface = lab_font),\rmax.iter = 5000, size = 3.5) g2\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\rParte 4\rEn la última parte ajustamos, además del estilo general, los ejes, el tipo de color y el (sub)título. Recuerda que cambiamos el signo en las anomalías de precipitación. Por eso, debemos usar los argumentos breaks y labels en la función scale_x_continouous() para revertir el signo en las etiquetas correspondientes a los cortes.\ng3 \u0026lt;- g2 + scale_x_continuous(\u0026quot;Anomalía de precipitación en %\u0026quot;,\rbreaks = seq(-100, 250, 10) * -1,\rlabels = seq(-100, 250, 10),\rlimits = c(min(data_inv_p$pr_anom), 100)) +\rscale_y_continuous(\u0026quot;Anomalía de temperatura media en ºC\u0026quot;,\rbreaks = seq(-2, 2, 0.5)) +\rscale_fill_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;)) +\rscale_colour_manual(values = rev(c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;))) +\rlabs(title = \u0026quot;Anomalías invernales en Tenerife Sur\u0026quot;, caption = \u0026quot;Datos: AEMET\\nPeriodo normal 1981-2010\u0026quot;) +\rtheme_bw()\rg3\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\r\r","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1585440000,"objectID":"693df593ec8364cdc9ed36839f78a168","permalink":"/es/2020/visualizar-anomal%C3%ADas-clim%C3%A1ticas/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/es/2020/visualizar-anomal%C3%ADas-clim%C3%A1ticas/","section":"post","summary":"Cuando visualizamos anomalías de precipitación y temperatura, simplemente usamos series temporales en un gráfico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, para tener una imagen global necesitamos ambas anomalías en un único gráfico. Así podríamos responder directamente a la pregunta de si una estación del año o un mes en concreto fue seco-cálido o húmedo-frío, e incluso comparar estas anomalías en el contexto de años anteriores.","tags":["anomalía","precipitación","temperatura","clima","puntos"],"title":"Visualizar anomalías climáticas","type":"post"},{"authors":["R Monjo","D Royé","J Martin-Vide"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1581379200,"objectID":"46d5fc034ed4c860511941136abc95da","permalink":"/es/publication/drought_class_2020/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/es/publication/drought_class_2020/","section":"publication","summary":"Drought duration strongly depends on the definition thereof. In meteorology, dryness is habitually measured by means of fixed thresholds (e.g. 0.1 or 1 mm usually define dry spells) or climatic mean values (as is the case of the Standard-ised Precipitation Index), but this also depends on the aggregation time interval considered. However, robust measurements of drought duration are required for analysing the statistical significance of possible changes. Herein we have climatically classified the drought duration around the world according to their similarity to the voids of the Cantor set. Dryness time structure 5 can be concisely measured by the n-index (from the regular/irregular alternation of dry/wet spells), which is closely related to the Gini index and to a Cantor-based exponent. This enables the world's climates to be classified into six large types based upon a new measure of drought duration. We performed the dry-spell analysis using the full global gridded daily Multi-Source Weighted-Ensemble Precipitation (MSWEP) dataset. The MSWEP combines gauge-, satellite-, and reanalysis-based data to provide reliable precipitation estimates. The study period comprises the years 1979-2016 (total of 45165 days), and a spatial 10 resolution of 0.5°, with a total of 259,197 grid points. Data set is publicly available at https://doi.org/10.5281/zenodo.3247041 (Monjo et al., 2019).","tags":["sequía","clasificación","mundo","lacunaridad","patrones espacio-temporales","rachas secas"],"title":"Meteorological drought lacunarity around the world and its classification","type":"publication"},{"authors":["D Royé","A Tobías","C Iñiguez"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1581033600,"objectID":"13c4997c47732f75f46b8651ecc10fae","permalink":"/es/publication/era5_esp_2020/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/es/publication/era5_esp_2020/","section":"publication","summary":"Background: Most studies use temperature observation data from weather stations near the analyzed region or city as the reference point for the exposure-response association. Climatic reanalysis data sets have already been used for climate studies, but are not yet used routinely in environmental epidemiology. Methods: We compared the mortality-temperature association using weather station temperature and ERA-5 reanalysis data for the 52 provincial capital cities in Spain, using time-series regression with distributed lag non-linear models. Results: The shape of temperature distribution is very close between the weather station and ERA-5 reanalysis data (correlation from 0.90 to 0.99). The overall cumulative exposure-response curves are very similar in their shape and risks estimates for cold and heat effects, although risk estimates for ERA-5 were slightly lower than for weather station temperature. Conclusions: Reanalysis data allow the estimation of the health effects of temperature, even in areas located far from weather stations or without any available.","tags":["España","temperature","reanálisis","ERA-5","mortalidad","Estaciones meteorológicas"],"title":"Comparison of temperature-mortality associations using observed weather station and reanalysis data in 52 Spanish cities","type":"publication"},{"authors":null,"categories":["análisis espacial","R","R:principante","gis"],"content":"\rEl primer post del año 2020, lo dedicaré a una consulta que me hicieron recientemente. Me plantearon la pregunta de cómo se podría calcular la distancia más corta entre diferentes puntos y cómo saber cúal es el punto más próximo a uno dado. Cuando trabajamos con datos espaciales en R, en la actualidad lo más fácil es usar el paquete sf en combinación con la colección de paquetes tidyverse. Además usamos el paquete units que es muy útil para trabajar con unidades de medida.\nPaquetes\r\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\runits\rProporciona unidades de medida para vectores R: conversión, derivación, simplificación\r\rmaps\rMapas geográficos y conjuntos de datos\r\rrnaturalearth\rMapas vectoriales del mundo ‘Natural Earth’\r\r\r\r# instalamos los paquetes necesarios\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;units\u0026quot;)) install.packages(\u0026quot;units\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;maps\u0026quot;)) install.packages(\u0026quot;maps\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r# cargamos los paquetes\rlibrary(maps)\rlibrary(sf) library(tidyverse)\rlibrary(units)\rlibrary(rnaturalearth)\r\rUnidades de medida\rEl uso de vectores y matrices de clase units nos permite calcular y transformar unidades de medida.\n# longitud\rl \u0026lt;- set_units(1:10, m)\rl\r## Units: [m]\r## [1] 1 2 3 4 5 6 7 8 9 10\r# convertir a otras unidades\rset_units(l, cm)\r## Units: [cm]\r## [1] 100 200 300 400 500 600 700 800 900 1000\r# sumar diferentes unidades\rset_units(l, cm) + l\r## Units: [cm]\r## [1] 200 400 600 800 1000 1200 1400 1600 1800 2000\r# area\ra \u0026lt;- set_units(355, ha)\rset_units(a, km2)\r## 3.55 [km2]\r# velocidad\rvel \u0026lt;- set_units(seq(20, 50, 10), km/h)\rset_units(vel, m/s)\r## Units: [m/s]\r## [1] 5.555556 8.333333 11.111111 13.888889\r\rCapitales del mundo\rVamos a usar las capitales de todo el mundo con el objetivo de calcular la distancia a la capital más próxima y indicar el nombre de la ciudad.\n# conjunto de ciudades del mundo con coordenadas\rhead(world.cities) # proviene del paquete maps\r## name country.etc pop lat long capital\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 31.31 34.34 0\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 31.32 34.35 0\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 30.55 72.11 0\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 29.36 47.98 0\r## 5 \u0026#39;Abud Palestine 2456 32.03 35.07 0\r## 6 \u0026#39;Abwein Palestine 3434 32.03 35.20 0\rPara convertir puntos con longitud y latitud en un objeto espacial de clase sf, empleamos la función st_as_sf(), indicando las columnas de las coordenadas y el sistema de referencia de coordenadas (WSG84, epsg:4326).\n# convertimos los puntos en un objeto sf con CRS WSG84\rcities \u0026lt;- st_as_sf(world.cities, coords = c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;), crs = 4326)\rcities\r## Simple feature collection with 43645 features and 4 fields\r## geometry type: POINT\r## dimension: XY\r## bbox: xmin: -178.8 ymin: -54.79 xmax: 179.81 ymax: 78.93\r## CRS: EPSG:4326\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 0 POINT (34.34 31.31)\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 0 POINT (34.35 31.32)\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 0 POINT (72.11 30.55)\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 0 POINT (47.98 29.36)\r## 5 \u0026#39;Abud Palestine 2456 0 POINT (35.07 32.03)\r## 6 \u0026#39;Abwein Palestine 3434 0 POINT (35.2 32.03)\r## 7 \u0026#39;Adadlay Somalia 9198 0 POINT (44.65 9.77)\r## 8 \u0026#39;Adale Somalia 5492 0 POINT (46.3 2.75)\r## 9 \u0026#39;Afak Iraq 22706 0 POINT (45.26 32.07)\r## 10 \u0026#39;Afif Saudi Arabia 41731 0 POINT (42.93 23.92)\rEn el próximo paso, simplemente filtramos por las capitales codificadas en la columna capital con 1. La ventaja del paquete sf es la posibilidad de aplicar funciones de la colección tidyverse para manipular los atributos. Además, añadimos una columna con nuevas etiquetas usando la función str_c() del paquete stringr, la cúal es similar a la de R Base paste().\n# filtramos por las capitales\rcapitals \u0026lt;- filter(cities, capital == 1)\r# creamos una nueva etiqueta combinando nombre y país\rcapitals \u0026lt;- mutate(capitals, city_country = str_c(name, \u0026quot; (\u0026quot;, country.etc, \u0026quot;)\u0026quot;))\rcapitals \r## Simple feature collection with 230 features and 5 fields\r## geometry type: POINT\r## dimension: XY\r## bbox: xmin: -176.13 ymin: -51.7 xmax: 179.2 ymax: 78.21\r## CRS: EPSG:4326\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Amman Jordan 1303197 1 POINT (35.93 31.95)\r## 2 Abu Dhabi United Arab Emirates 619316 1 POINT (54.37 24.48)\r## 3 Abuja Nigeria 178462 1 POINT (7.17 9.18)\r## 4 Accra Ghana 2029143 1 POINT (-0.2 5.56)\r## 5 Adamstown Pitcairn 51 1 POINT (-130.1 -25.05)\r## 6 Addis Abeba Ethiopia 2823167 1 POINT (38.74 9.03)\r## 7 Agana Guam 1041 1 POINT (144.75 13.47)\r## 8 Algiers Algeria 2029936 1 POINT (3.04 36.77)\r## 9 Alofi Niue 627 1 POINT (-169.92 -19.05)\r## 10 Amsterdam Netherlands 744159 1 POINT (4.89 52.37)\r## city_country\r## 1 \u0026#39;Amman (Jordan)\r## 2 Abu Dhabi (United Arab Emirates)\r## 3 Abuja (Nigeria)\r## 4 Accra (Ghana)\r## 5 Adamstown (Pitcairn)\r## 6 Addis Abeba (Ethiopia)\r## 7 Agana (Guam)\r## 8 Algiers (Algeria)\r## 9 Alofi (Niue)\r## 10 Amsterdam (Netherlands)\r\rCalcular distancias\rLa distancia geográfica (euclidiana o de gran círculo) se calcula con la función st_distance(), o bien entre dos puntos, entre un punto y otros múltiples o entre todos. En el último caso obtenemos una matriz simétrica de distancias (NxN), tomados por pares de un conjunto. En la diagonal encontramos las combinaciones entre los mismos puntos dando todas nulas.\n\r\r\rA\rB\rC\r\rA\r0\r336\r384\r\rB\r336\r0\r374\r\rC\r384\r374\r0\r\r\r\rCuando queremos saber, por ejemplo, la distancia de Amsterdam a Abu Dhabi, Washington y Tokyo pasamos dos objetos espaciales.\n# calcular la distancia\rdist_amsterdam \u0026lt;- st_distance(slice(capitals, 10), slice(capitals, c(2, 220, 205)))\rdist_amsterdam # distancia en metros\r## Units: [m]\r## [,1] [,2] [,3]\r## [1,] 5167859 6203802 9316790\rEl resultado es una matriz de una fila o de una columna (en función del orden de los objetos) con clase de units. Así es posible cambiar fácilmente a otra unidad de medida. Si queremos obtener un vector sin clase units, únicamente aplicamos la función as.vector().\n# cambiamos de m a km\rset_units(dist_amsterdam, \u0026quot;km\u0026quot;)\r## Units: [km]\r## [,1] [,2] [,3]\r## [1,] 5167.859 6203.802 9316.79\r# units class a vector\ras.vector(dist_amsterdam)\r## [1] 5167859 6203802 9316790\rA continuación estimamos la matriz de distancia entre todas las capitales. Es importante convertir los valores nulos a NA para obtener posteriormente el índice correcto de la matriz.\n# calcular la distancia\rm_distance \u0026lt;- st_distance(capitals)\r# matriz\rdim(m_distance)\r## [1] 230 230\r# cambiamos de m a km\rm_distance_km \u0026lt;- set_units(m_distance, km)\r# reemplazamos la distance de 0 con NA\rm_distance_km[m_distance_km == set_units(0, km)] \u0026lt;- NA\rCuando el resultado es de clase units es necesario usar la misma clase para poder hacer consultas logicas. Por ejemplo, set_units(1, m) == set_units(1, m) vs. set_units(1, m) == 1.\n Con el objetivo de obtener la distancia más corta, además de la posición de la misma, usamos la función apply() que a su vez nos permite aplicar la función which.min() y min() sobre cada fila. También sería posible emplear la función sobre columnas que daría el mismo resulado. Para finalizar, añadimos los resultados como nuevas columnas con la función mutate(). Las posiciones en pos nos permiten obtener los nombres de las ciudades más próximas.\n# obtenemos la posición de la ciudad y la distancia\rpos \u0026lt;- apply(m_distance_km, 1, which.min)\rdist \u0026lt;- apply(m_distance_km, 1, min, na.rm = TRUE)\r# añadimos la distancia y obtenemos el nombre de la ciudad\rcapitals \u0026lt;- mutate(capitals, nearest_city = city_country[pos], geometry_nearest = geometry[pos],\rdistance_city = dist)\r\rMapa de distancias a la próxima capital\rPor último, construimos un mapa representando la distancia en circulos proporcionales. Para ello, usamos la gramática habitual de ggplot() añadiendo la geometría geom_sf(), primero para el mapamundi de fondo y después para los circulos de las ciudades. En aes() indicamos con el argumento size = distance_city la variable que debe ser mapeado proporcionalmente. La función theme_void() elimina todos los elementos de estilo. Además, definimos con la función coord_sf() una nueva proyección indicando el formato proj4.\n# mapamundi\rworld \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;)\r# mapa ggplot(world) +\rgeom_sf(fill = \u0026quot;black\u0026quot;, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = capitals, aes(size = distance_city),\ralpha = 0.7,\rfill = \u0026quot;#bd0026\u0026quot;,\rshape = 21,\rshow.legend = \u0026#39;point\u0026#39;) +\rcoord_sf(crs = \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\u0026quot;) +\rlabs(size = \u0026quot;Distance (km)\u0026quot;, title = \u0026quot;Distance to the next capital\u0026quot;) +\rtheme_void()\r\r","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1579392000,"objectID":"65592ccfd94552a47a08dce147dde63c","permalink":"/es/2020/distancias-geogr%C3%A1ficas/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/es/2020/distancias-geogr%C3%A1ficas/","section":"post","summary":"El primer post del año 2020, lo dedicaré a una consulta que me hicieron recientemente. Me plantearon la pregunta de cómo se podría calcular la distancia más corta entre diferentes puntos y cómo saber cúal es el punto más próximo a uno dado. Cuando trabajamos con datos espaciales en R, en la actualidad lo más fácil es usar el paquete ``sf`` en combinación con la colección de paquetes ``tidyverse``. Además usamos el paquete ``units`` que es muy útil para trabajar con unidades de medida.","tags":["distancia","puntos","ciudades"],"title":"Distancias geográficas","type":"post"},{"authors":["F Tedim","V Leone","M Coughlan","C Bouillon","G Xanthopoulos","D Royé","F.J.M. Correia","C Ferreira"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1577836800,"objectID":"8d22fdb676cf9ec56f36bd6fbda99f45","permalink":"/es/publication/chapter_elsevier_2019/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/es/publication/chapter_elsevier_2019/","section":"publication","summary":"Extreme wildfires events (EWEs) represent a minority among all wildfires but are a true challenge for societies as they exceed the current control capacity even in the best prepared regions of the world and they create destruction and a disproportionately number of fatalities. Recent events in Portugal, Chile, Greece, Australia, Canada, and the USA provide evidence that EWEs are an escalating worldwide problem, exceeding all previous records. Despite the challenges put by climate change, the occurrence of EWEs and disasters is not an ecological inevitability. In this chapter the rationale of the definition of EWEs and the integration of potential consequences on people and assets in a novel wildfire classification scheme are proposed and discussed. They are excellent instruments to enhance wildfire risk and crisis communication programs and to define appropriate prevention, mitigation, and response measures which are crucial to build up citizens' safety.","tags":["Control capacity","Disaster Extreme wildfire event (EWE)","Fire intensity","Mitigation","Preparedness","Prevention","Rate of spread","Socioeconomic system (SES)","Wildfire classification"],"title":"Extreme wildfire events: the definition","type":"publication"},{"authors":["D Royé","R Codesido","A Tobías","M Taracido"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1577836800,"objectID":"802fbbdd8ca0d0f684bf48f4acf5db25","permalink":"/es/publication/ehf_esp_2020/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/es/publication/ehf_esp_2020/","section":"publication","summary":"In the current context of climate change, heat waves have become a significant problem for human health. This study assesses the effects of heat wave intensity on mortality (natural, respiratory and cardiovascular causes) in four of the largest cities of Spain (Barcelona, Bilbao, Madrid and Seville) during the period between 1990 and 2014. To model the heat wave severity the Excess Heat Factor (EHF) was used. The EHF is a two-component index. The first is the comparison of the three-day average daily mean temperature with the 95th percentile. The second component is a measure of the temperatures reached during the three-day period compared with the recent past (the previous 30 days). The city-specific exposure-response curves showed a non-linear J-shaped relationship between mortality and the EHF. Overall city-specific mortality risk estimates for 1th vs. 99th percentile increases range from the highest mortality risk with 2.73 (95% CI: 2.34-3.18) in Seville to a risk of 1.78 (95% CI: 1.62-1.97) and 1.78 (95% CI: 1.45-2.19) in Barcelona and Bilbao, respectively. When we compare our results with risk estimates for the analyzed Spanish cities in other studies, the heat wave related mortality risks seem to be clearly higher. Furthermore, it has been demonstrated that different heat wave days of the same event do not present the same degree of severity/intensity. Thus, the intensity of a heat wave is an important mortality risk indicator during heat wave days. Due to the low number of studies on the EHF as a heat wave intensity indicator and heat-related mortality and morbidity, further research is required to validate its application in other geographic areas and focus populations.","tags":["España","ola de calor","Heat Excess Factor","mortalidad"],"title":"Heat wave intensity and daily mortality in four of the largest cities of Spain","type":"publication"},{"authors":null,"categories":["visualización","R","R:principante","gis"],"content":"\rLa Dirección General del Catastro de España dispone de información espacial de toda la edificación a excepción del País Vasco y Navarra. Este conjunto de datos forma parte de la implantación de INSPIRE, la Infraestructura de Información Espacial en Europa. Más información podemos encontrar aquí. Utilizaremos los enlaces (urls) en formato ATOM, que es un formato de redifusión de tipo RSS, permitiéndonos obtener el enlace de descarga para cada municipio.\nEsta entrada de blog es una versión reducida del caso práctico que podéis encontrar en nuestra reciente publicación - Introducción a los SIG con R - publicado por Dominic Royé y Roberto Serrano-Notivoli.\n Paquetes\r\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rfs\rProporciona una interfaz uniforme y multiplataforma para las operaciones del sistema de archivos\r\rlubridate\rFácil manipulación de fechas y tiempos\r\rfeedeR\rImportar formatos de redifusión RSS\r\rtmap\rFácil creación de mapas temáticos\r\rclassInt\rPara crear intervalos de clase univariantes\r\rsysfonts\rCarga familias tipográficas del sistema y de Google\r\rshowtext\rUsar familias tipográficas más fácilmente en gráficos R\r\r\r\r# instalamos los paquetes necesarios\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;feedeR\u0026quot;)) install.packages(\u0026quot;feedeR\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;tmap\u0026quot;)) install.packages(\u0026quot;tmap\u0026quot;)\rif(!require(\u0026quot;classInt\u0026quot;)) install.packages(\u0026quot;classInt\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;rvest\u0026quot;)) install.packages(\u0026quot;rvest\u0026quot;)\r# cargamos los paquetes\rlibrary(feedeR)\rlibrary(sf) library(fs)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(classInt)\rlibrary(tmap)\rlibrary(rvest)\r\rEnlaces de descarga\rLa primera url nos dará acceso a un listado de provincias, sedes territoriales (no siempre coinciden con la provincia), con nuevos enlaces RSS los cuales incluyen los enlaces finales de descarga para cada municipio. En este caso, descargaremos el edificado de Valencia. Los datos del Catastro se actualizan cada seis meses.\nurl \u0026lt;- \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.bu.atom.xml\u0026quot;\r# importamos los RSS con enlaces de provincias\rprov_enlaces \u0026lt;- feed.extract(url)\rstr(prov_enlaces) #estructura es lista\r## List of 4\r## $ title : chr \u0026quot;Download service of Buildings. Territorial Office\u0026quot;\r## $ link : chr \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.BU.atom.xml\u0026quot;\r## $ updated: POSIXct[1:1], format: \u0026quot;2019-10-26\u0026quot;\r## $ items :\u0026#39;data.frame\u0026#39;: 52 obs. of 4 variables:\r## ..$ title: chr [1:52] \u0026quot;Territorial office 02 Albacete\u0026quot; \u0026quot;Territorial office 03 Alicante\u0026quot; \u0026quot;Territorial office 04 AlmerÃ­a\u0026quot; \u0026quot;Territorial office 05 Avila\u0026quot; ...\r## ..$ date : POSIXct[1:52], format: \u0026quot;2019-10-26\u0026quot; \u0026quot;2019-10-26\u0026quot; ...\r## ..$ link : chr [1:52] \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/02/ES.SDGC.bu.atom_02.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/03/ES.SDGC.bu.atom_03.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/04/ES.SDGC.bu.atom_04.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/05/ES.SDGC.bu.atom_05.xml\u0026quot; ...\r## ..$ hash : chr [1:52] \u0026quot;d21ebb7975e59937\u0026quot; \u0026quot;bdba5e149f09e9d8\u0026quot; \u0026quot;03bcbcc7c5be2e17\u0026quot; \u0026quot;8a154202dd778143\u0026quot; ...\r# extraemos la tabla con los enlaces\rprov_enlaces_tab \u0026lt;- as_tibble(prov_enlaces$items) %\u0026gt;% mutate(title = repair_encoding(title))\r## Best guess: UTF-8 (100% confident)\rprov_enlaces_tab\r## # A tibble: 52 x 4\r## title date link hash ## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ d21ebb79~\r## 2 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ bdba5e14~\r## 3 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ 03bcbcc7~\r## 4 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ 8a154202~\r## 5 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ 7d3fd376~\r## 6 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ 9c08741f~\r## 7 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ ff722b15~\r## 8 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ b431aa61~\r## 9 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ f79c6562~\r## 10 Territorial of~ 2019-10-26 00:00:00 http://www.catastro.minhap.es/~ d702a6a8~\r## # ... with 42 more rows\rAccedemos y descargamos los datos de Valencia. Para encontrar el enlace final de descarga usamos la función filter() del paquete dplyr buscando el nombre de la sede territorial y posteriormente el nombre del municipio en mayúsculas con la función str_detect() de stringr. La función pull() nos permite extraer una columna de un data.frame.\nActualmente la función feed.extract() no importa correctamente en el encoding UTF-8 (Windows). Por eso, en algunas ciudades pueden aparecer una mala codificación de caracteres especiales “CÃ¡diz”. Para subsanar este problema aplicamos la función repair_encoding() del paquete rvest.\n # filtramos la provincia y obtenemos la url RSS\rval_atom \u0026lt;- filter(prov_enlaces_tab, str_detect(title, \u0026quot;Valencia\u0026quot;)) %\u0026gt;% pull(link)\r# importamos la RSS\rval_enlaces \u0026lt;- feed.extract(val_atom)\r# obtenemos la tabla con los enlaces de descarga\rval_enlaces_tab \u0026lt;- val_enlaces$items\rval_enlaces_tab \u0026lt;- mutate(val_enlaces_tab, title = repair_encoding(title),\rlink = repair_encoding(link)) \r## Best guess: UTF-8 (100% confident)\r## Best guess: UTF-8 (100% confident)\r# filtramos la tabla con el nombre de la ciudad\rval_link \u0026lt;- filter(val_enlaces_tab, str_detect(title, \u0026quot;VALENCIA\u0026quot;)) %\u0026gt;% pull(link)\rval_link\r## [1] \u0026quot;http://www.catastro.minhap.es/INSPIRE/Buildings/46/46900-VALENCIA/A.ES.SDGC.BU.46900.zip\u0026quot;\r\rDescarga de datos\rLa descarga se realiza con la función download.file() que únicamente tiene dos argumentos principales, el enlace de descarga y la ruta con el nombre del archivo. En este caso hacemos uso de la función tempfile(), que nos es útil para crear archivos temporales, es decir, archivos que únicamente existen en la memoría RAM por un tiempo determinado.\rEl archivo que descargamos tiene extensión *.zip, por lo que debemos descomprimirlo con otra función (unzip()), que requiere el nombre del archivo y el nombre de la carpeta donde lo queremos descomprimir. Por último, la función URLencode() codifica una dirección URL que contiene caracteres especiales.\n# creamos un archivo temporal temp \u0026lt;- tempfile()\r# descargamos los datos\rdownload.file(URLencode(val_link), temp)\r# descomprimimos a una carpeta llamda buildings\runzip(temp, exdir = \u0026quot;buildings\u0026quot;)\r\rImportar los datos\rPara importar los datos utilizamos la función dir_ls() del paquete fs, que nos permite obtener los archivos y carpetas de una ruta concreta al mismo tiempo que filtramos por un patrón de texto (regexp: expresión regular). Aplicamos la función st_read() del paquete sf al archivo espacial de formato Geography Markup Language (GML).\n# obtenemos la ruta con el archivo\rfile_val \u0026lt;- dir_ls(\u0026quot;buildings\u0026quot;, regexp = \u0026quot;building.gml\u0026quot;)\r# importamos los datos\rbuildings_val \u0026lt;- st_read(file_val)\r## Reading layer `Building\u0026#39; from data source `C:\\Users\\xeo19\\Documents\\GitHub\\blogR_update\\content\\post\\es\\2019-11-01-visualizar-crecimiento-urbano\\buildings\\A.ES.SDGC.BU.46900.building.gml\u0026#39; using driver `GML\u0026#39;\r## Simple feature collection with 36296 features and 24 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: 720608 ymin: 4351287 xmax: 734982.5 ymax: 4382906\r## CRS: 25830\r\rPreparación de los datos\rÚnicamente convertimos la columna de la edad del edificio (beginning) en clase Date. La columna de la fecha contiene algunas fechas en formato --01-01 lo que no corresponde a ninguna fecha reconocible. Por eso, reemplazamos el primer - por 0000.\n# buildings_val \u0026lt;- mutate(buildings_val, beginning = str_replace(beginning, \u0026quot;^-\u0026quot;, \u0026quot;0000\u0026quot;) %\u0026gt;% ymd_hms() %\u0026gt;% as_date()\r)\r## Warning: 4 failed to parse.\r\rGráfico de distribución\rAntes de crear el mapa de la edad del edificado, lo que reflejará el crecimiento urbano, haremos un gráfico de distribución de la fecha de construcción de los edificios. Podremos identificar claramente períodos de expansión urbana. Usaremos el paquete ggplot2 con la geometría de geom_density() para este objetivo. La función font_add_google() del paquete sysfonts nos permite descargar e incluir familias tipográficas de Google.\n#descarga de familia tipográfica\rsysfonts::font_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r#usar showtext para familias tipográficas\rshowtext::showtext_auto() \r#limitamos al periodo posterior a 1750\rfilter(buildings_val, beginning \u0026gt;= \u0026quot;1750-01-01\u0026quot;) %\u0026gt;%\rggplot(aes(beginning)) + geom_density(fill = \u0026quot;#2166ac\u0026quot;, alpha = 0.7) +\rscale_x_date(date_breaks = \u0026quot;20 year\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) +\rtheme_minimal() +\rtheme(title = element_text(family = \u0026quot;Montserrat\u0026quot;),\raxis.text = element_text(family = \u0026quot;Montserrat\u0026quot;)) +\rlabs(y = \u0026quot;\u0026quot;,x = \u0026quot;\u0026quot;, title = \u0026quot;Evolución del desarrollo urbano\u0026quot;)\r\rBuffer de 2,5 km de Valencia\rPara poder visualizar bien la distribución del crecimiento, limitamos el mapa a un radio de 2,5 km desde el centro de la ciudad. Usamos la función geocode_OSM() del paquete tmaptools para obtener las coordenadas de Valencia en clase sf. Después proyectamos los puntos al sistema que usamos para el edificado (EPSG:25830). Como último paso creamos con la función st_buffer() un buffer con 2500 m y la intersección con nuestros datos de los edificios. También es posible crear un buffer en forma de un rectángulo indicando el tipo de estilo con el argumento endCapStyle = \"SQUARE\".\n# obtenemos las coordinadas de Valencia\rciudad_point \u0026lt;- tmaptools::geocode_OSM(\u0026quot;Valencia\u0026quot;, as.sf = TRUE)\r# proyectamos los datos\rciudad_point \u0026lt;- st_transform(ciudad_point, 25830)\r# creamos un buffer\rpoint_bf \u0026lt;- st_buffer(ciudad_point, 2500)\r# obtenemos la intersección entre el buffer y la edificación\rbuildings_val25 \u0026lt;- st_intersection(buildings_val, point_bf)\r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r\rPreparar los datos para el mapas\rPara poder visualizar bien las diferentes épocas de crecimiento, categorizamos el año en 15 grupos empleando cuartiles.\n#encontrar 15 clases\rbr \u0026lt;- classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;)\r## Warning in classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;): var\r## has missing values, omitted in finding classes\r#crear etiquetas\rlab \u0026lt;- names(print(br, under = \u0026quot;\u0026lt;\u0026quot;, over = \u0026quot;\u0026gt;\u0026quot;, cutlabels = FALSE))\r## style: quantile\r## \u0026lt; 1890 1890 - 1912 1912 - 1925 1925 - 1930 1930 - 1940 1940 - 1950 ## 940 1369 971 596 1719 1080 ## 1950 - 1957 1957 - 1962 1962 - 1966 1966 - 1970 1970 - 1973 1973 - 1977 ## 1227 1266 1233 1165 1161 932 ## 1977 - 1987 1987 - 1999 \u0026gt; 1999 ## 1337 1197 1190\r#categorizar el año\rbuildings_val25 \u0026lt;- mutate(buildings_val25, yr_cl = cut(year(beginning), br$brks, labels = lab, include.lowest = TRUE))\r\rMapa de Valencia\rEl mapa creamos con el paquete tmap. Es una interesante alternativa a ggplot2. Se trata de un paquete de funciones especializadas en crear mapas temáticos. La filosofía del paquete sigue a la de ggplot2, creando multiples capas con diferentes funciones, que siempre empiezan con tm_* y se combinan con +. La construcción de un mapa con tmap siempre comienza con tm_shape(), donde se definen los datos que queremos dibujar. Luego agregamos la geometría correspondiente al tipo de datos (tm_polygon(), tm_border(), tm_dots() o incluso tm_raster()). La función tm_layout() ayuda a configurar el estilo del mapa.\nCuando necesitamos más colores del máximo permitido por RColorBrewer podemos pasar los colores a la función colorRampPalette(). Esta función interpola para un mayor número más colores de la gama.\n#colores\rcol_spec \u0026lt;- RColorBrewer::brewer.pal(11, \u0026quot;Spectral\u0026quot;)\r#función de una gama de colores\rcol_spec_fun \u0026lt;- colorRampPalette(col_spec)\r#crear los mapas\rtm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15),\rtextNA = \u0026quot;Sin dato\u0026quot;,\rtitle = \u0026quot;\u0026quot;) +\rtm_layout(bg.color = \u0026quot;black\u0026quot;,\router.bg.color = \u0026quot;black\u0026quot;,\rlegend.outside = TRUE,\rlegend.text.color = \u0026quot;white\u0026quot;,\rlegend.text.fontfamily = \u0026quot;Montserrat\u0026quot;, panel.label.fontfamily = \u0026quot;Montserrat\u0026quot;,\rpanel.label.color = \u0026quot;white\u0026quot;,\rpanel.label.bg.color = \u0026quot;black\u0026quot;,\rpanel.label.size = 5,\rpanel.label.fontface = \u0026quot;bold\u0026quot;)\r\rMapa dinámico en leaflet\rUna ventaja muy interesante es la función tmap_leaflet() del paquete tmap para pasar de forma sencilla un mapa creado en el mismo marco a leaflet.\n#mapa tmap de Santiago\rm \u0026lt;- tm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15),\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;)\r#mapa dinámico\rtmap_leaflet(m)\r\r\r","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1572566400,"objectID":"fd57e91534c1b429d9f9f4d872a7aaaa","permalink":"/es/2019/visualizar-el-crecimiento-urbano/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/es/2019/visualizar-el-crecimiento-urbano/","section":"post","summary":"La Dirección General del Catastro de España dispone de información espacial de toda la edificación a excepción del País Vasco y Navarra. Este conjunto de datos forma parte de la implantación de [INSPIRE](https://inspire.ec.europa.eu/), la Infraestructura de Información Espacial en Europa. Más información podemos encontrar [aquí](http://www.catastro.meh.es/webinspire/index.html). Utilizaremos los enlaces (*urls*) en formato *ATOM*, que es un formato de redifusión de tipo RSS, permitiéndonos obtener el enlace de descarga para cada municipio.","tags":["crecimiento urbano","ciudad","geografia urbana"],"title":"Visualizar el crecimiento urbano","type":"post"},{"authors":["D Royé","R Serrano-Notivoli"],"categories":null,"content":"","date":1570406400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1570406400,"objectID":"180687303cb05fbd636f8a768b099dff","permalink":"/es/publication/manual_rgis_2019/","publishdate":"2019-10-07T00:00:00Z","relpermalink":"/es/publication/manual_rgis_2019/","section":"publication","summary":"R tiene, como lenguaje de programación enfocado al análisis estadístico, todos los ingredientes para ser usado como herramienta de análisis espacial y representación cartográﬁca: es gratuito, permite personalizar, replicar y compartir los análisis de cualquier nivel de diﬁcultad y no tiene ninguna limitación en cuanto a cantidad de información a procesar o tipos de formato diferentes para gestionar. Esto le sitúa en una situación de ventaja que mejora día a día, gracias a su amplia comunidad de usuarios, respecto a un SIG (Sistema de Información Geográﬁca) convencional. Este manual explica, sin necesidad de conocimientos previos, cómo desarrollar con R todos los análisis disponibles en un SIG, con ejemplos sencillos y multitud de casos prácticos. Además, se muestran las enormes posibilidades de representación cartográﬁca, que van mucho más allá de la simple creación de mapas. R permite, desde exportar a cualquier formato de archivo, hasta crear mapas dinámicos para supublicación en Internet.","tags":["R","manual","visualización","GIS"],"title":"Introducción a los SIG con R","type":"publication"},{"authors":["D Royé","F Tedim","J Martin-Vide","M Salis","J Vendrell","R Lovreglio","C Bouillon","V Leone"],"categories":null,"content":"","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1569196800,"objectID":"1ec62542da400e4bf5becbb36fb5f898","permalink":"/es/publication/incendios_ci_2019/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/es/publication/incendios_ci_2019/","section":"publication","summary":"The most widely used metrics to characterize wildfire regime and estimate the impact of wildfires are total burnt area (BA) and the number of fire events (FE). However, these are insufficient to analyse the threat to society of a new fire regime characterized by a higher occurrence of very large events. To overcome this weakness, we propose the use of a Concentration Index (CIB) which makes it possible to identify spatio-temporal patterns. The frequency distribution of BA follows a negative exponential distribution almost everywhere, in which a small minority of FE are responsible of the majority of BA. In this article, the spatio-temporal behaviour of BA is analysed in Western Mediterranean Europe, with particular focus on Portugal, Spain, France and Italy, using data from the European Forest Fire Information System and national wildfire databases. This is the first time that the CI has been applied to wildfire events. This research shows that, in most Mediterranean European countries, the amount of BA is increasingly related with a lower number of fires. The spatio-temporal distribution of CIB shows high variability in all of the countries analysed in Europe. Portugal and Spain show increasing significant trends of CIB +7.6% (p-value = 0.001) and +1.3% per decade (p-value = 0.003). Statistically significant correlations for Portugal, Spain and Italy are also found between the annual CIB and several teleconnection indices. The application of the CIB demonstrates its discriminatory ability, which is a key point in detecting vulnerable areas and temporal trends under climate change.","tags":["incendios forestales","indice de concentración","Europa","teleconexión","patrones espacio-temporales"],"title":"Wildfire burnt area patterns and trends in Western Mediterranean Europe via the application of a concentration index","type":"publication"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","A Benhamrouche"],"categories":null,"content":"","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1565827200,"objectID":"c46bc37d93fb232cf7591da4d0da2d6b","permalink":"/es/publication/ijc_teleconection_medit_2019/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/es/publication/ijc_teleconection_medit_2019/","section":"publication","summary":"This study has addressed the spatiotemporal distribution of the daily rainfall concentration and its relation to the teleconnection patterns across the Mediterranean (MR). Daily Concentration Index (CI) and the ordered n index () are used at annual time scale to reveal the statistical structure of precipitation across the MR based on 233 daily rainfall series for the period 1975–2015. Eight teleconnection patterns ,North Atlantic Oscillation (NAO), Mediterranean Oscillation (MO), Western Mediterranean Oscillation (WeMO), Upper Level Mediterranean Oscillation index (ULMO), East Atlantic (EA) pattern, East Atlantic/West Russia (EATL/WRUS) pattern, Scandinavia (SCAND) pattern and Southern Oscillation (SO) at annual time scale are selected. The spatiotemporal patterns in precipitation concentration indices, annual precipitation and their teleconnections with previous large-scale circulations are investigated. Results show a strong connection between the CI and the (r = 0.70, p","tags":["Mediterráneo","índice n","índice de concentración","patrones de teleconexión","precipitación diaria"],"title":"Spatiotemporal variability of daily precipitation concentration and its relationship to teleconnection patterns over the Mediterranean during 1975-2015","type":"publication"},{"authors":["D Royé","MT Zarrabeitia","P Fdez-Arroyabe","A Álvarez-Gutiérrez","A Santurtún"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1564617600,"objectID":"7e652e401d5f5ce407570be119eaf98a","permalink":"/es/publication/iam_cantabria_2018/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/es/publication/iam_cantabria_2018/","section":"publication","summary":"Introducción y objetivos. El papel del entorno en la salud cardiovascular ha ganado protagonismo en el contexto del cambio global. Este trabajo persigue analizar la relación de la temperatura aparente (TA) y los contaminantes atmosféricos con los ingresos por infarto agudo de miocardio (IAM) y realizar un análisis temporal de la enfermedad y la mortalidad asociada. Métodos. Se desarrolló un estudio de serie temporal de los ingresos por IAM en Cantabria entre 2001 y 2015. La asociación entre las variables ambientales (entre ellas, se estimó un índice biometeorológico, la TA) y los ingresos por IAM se analizó mediante una regresión de cuasi-Poisson, y se creó un modelo no lineal de retardo distribuido dentro de un modelo generalizado aditivo, con el fin de atender el efecto retardado y la presencia de relaciones no lineales de las variables ambientales. Resultados. La tasa de incidencia y la mortalidad por IAM siguieron una tendencia descendente durante el periodo de estudio (CC=–0,714; p=0,0002). Los ingresos por IAM tenían un patrón anual con máximos en invierno (p=0,005); había diferencias intrasemanales, y los mínimos se registraron durante el fin de semana (p=0,000005). Se encontró una asociación inversa entre la TA y el número de ingresos por IAM y una relación directa y estadísticamente significativa con las concentraciones de partículas de diámetro","tags":["Infarto agudo de miocardio","Temperatura aparente","Contaminantes atmosféricos","Material particulado"],"title":"Papel de la temperatura aparente y de los contaminantes atmosféricos en los ingresos por infarto agudo de miocardio en el norte de España","type":"publication"},{"authors":null,"categories":["visualización","R","R:intermedio"],"content":"\rNormalmente cuando visualizamos anomalías de precipitación mensual, simplemente usamos un gráfico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, no nos explica el contexto general de estas mismas anomalías. Por ejemplo, ¿cuál fue la anomalía más alta o más baja en cada mes? En principio, podríamos usar un boxplot para visualizar la distribución de las anomalías, pero en este caso concreto no encajarían bien estéticamente, por lo que debemos buscar una alternativa. Aquí os presento una forma gráfica muy útil.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rreadr\rImportar datos\r\rggthemes\rEstilos para ggplot2\r\rlubridate\rFácil manipulación de fechas y tiempos\r\rcowplot\rFácil creación de múltiples gráficos con ggplot2\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;cowplot\u0026quot;)) install.packages(\u0026quot;cowplot\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#paquetes\rlibrary(tidyverse) #contiene readr\rlibrary(ggthemes)\rlibrary(cowplot)\rlibrary(lubridate)\r\rPreparar los datos\rPrimero importamos la precipitación diaria de la estación meteorológica seleccionada (descarga). Usaremos datos de Santiago de Compostela (España) accesible a través de ECA\u0026amp;D.\nPaso 1: importar los datos\rNo sólo importamos los datos en formato csv, sino también hacemos los primeros cambios. Saltamos las primeras 21 filas que contienen información sobre la estación meteorológica. Además, convertimos la fecha a clase date y reemplazamos valores ausentes (-9999) por NA. La precipitación está en 0.1 mm, por tanto, debemos dividir los valores por 10. Después seleccionamos las columnas DATE y RR, y las renombramos.\ndata \u0026lt;- read_csv(\u0026quot;RR_STAID001394.txt\u0026quot;, skip = 21) %\u0026gt;%\rmutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10)) %\u0026gt;%\rselect(DATE:RR) %\u0026gt;% rename(date = DATE, pr = RR)\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\rdata\r## # A tibble: 27,606 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1943-11-01 0.6\r## 2 1943-11-02 0 ## 3 1943-11-03 0 ## 4 1943-11-04 0 ## 5 1943-11-05 0 ## 6 1943-11-06 0 ## 7 1943-11-07 0 ## 8 1943-11-08 0 ## 9 1943-11-09 0 ## 10 1943-11-10 0 ## # ... with 27,596 more rows\r\rPaso 2: crear valores menusales\rEn el segundo paso calculamos las cantidades mensuales de precipitación. Para ello, a) limitamos el período a los años posteriores a 1950, b) añadimos como variable el mes con etiqueta y el año.\ndata \u0026lt;- mutate(data, mo = month(date, label = TRUE), yr = year(date)) %\u0026gt;%\rfilter(date \u0026gt;= \u0026quot;1950-01-01\u0026quot;) %\u0026gt;%\rgroup_by(yr, mo) %\u0026gt;% summarise(prs = sum(pr, na.rm = TRUE))\rdata\r## # A tibble: 833 x 3\r## # Groups: yr [70]\r## yr mo prs\r## \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 ene 55.6\r## 2 1950 feb 349. ## 3 1950 mar 85.8\r## 4 1950 abr 33.4\r## 5 1950 may 272. ## 6 1950 jun 111. ## 7 1950 jul 35.4\r## 8 1950 ago 76.4\r## 9 1950 sep 85 ## 10 1950 oct 53 ## # ... with 823 more rows\r\rPaso 3: estimar las anomalías\rAhora debemos estimar los valores normales de cada mes y unir esta tabla a nuestros datos principales para posteriormente poder calcular la anomalía mensual. Expresamos la anomalía en porcentaje y restamos 100 para fijar el promedio en 0. Además, creamos una variable que nos indica si la anomalía es negativa o positiva, y otra con la fecha.\npr_ref \u0026lt;- filter(data, yr \u0026gt; 1981, yr \u0026lt;= 2010) %\u0026gt;%\rgroup_by(mo) %\u0026gt;%\rsummarise(pr_ref = mean(prs))\rdata \u0026lt;- left_join(data, pr_ref, by = \u0026quot;mo\u0026quot;)\rdata \u0026lt;- mutate(data, anom = (prs*100/pr_ref)-100, date = str_c(yr, as.numeric(mo), 1, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd(),\rsign= ifelse(anom \u0026gt; 0, \u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;))\rYa podemos hacer un primer ensayo de un gráfico de anomalías (la versión clásica), para ello filtramos el año 2018. En este caso usamos la geometría de barras, recuerda que por defecto la función geom_bar() aplica el conteo a la variable. No obstante, en este caso conocemos y, por tanto indicamos con el argumento stat = \"identity\" que debe usar el valor dado en aes().\nfilter(data, yr == 2018) %\u0026gt;%\rggplot(aes(date, anom, fill = sign)) + geom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(-100, 100, 20)) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rlabs(y = \u0026quot;Anomalía de precipitación (%)\u0026quot;, x = \u0026quot;\u0026quot;) +\rtheme_hc()\r\rPaso 4: calcular las medidas estadísticas\rEn este último paso estimamos el valor máximo, mínimo, el cuantil 25%/75% y el rango intercuartil por mes de toda la serie temporal.\ndata_norm \u0026lt;- group_by(data, mo) %\u0026gt;%\rsummarise(mx = max(anom),\rmin = min(anom),\rq25 = quantile(anom, .25),\rq75 = quantile(anom, .75),\riqr = q75-q25)\rdata_norm\r## # A tibble: 12 x 6\r## mo mx min q25 q75 iqr\r## \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 ene 193. -89.6 -43.6 56.3 99.9\r## 2 feb 320. -96.5 -51.2 77.7 129. ## 3 mar 381. -100 -40.6 88.2 129. ## 4 abr 198. -93.6 -51.2 17.1 68.3\r## 5 may 141. -90.1 -45.2 17.0 62.2\r## 6 jun 419. -99.3 -58.2 50.0 108. ## 7 jul 311. -98.2 -77.3 27.1 104. ## 8 ago 264. -100 -68.2 39.8 108. ## 9 sep 241. -99.2 -64.9 48.6 113. ## 10 oct 220. -99.0 -54.5 4.69 59.2\r## 11 nov 137. -98.8 -44.0 39.7 83.7\r## 12 dic 245. -91.8 -49.8 36.0 85.8\r\r\rCrear el gráfico\rPara crear el gráfico de anomalías con leyenda es necesario separar el gráfico principal de las leyendas.\nParte 1\rEn esta primera parte vamos añadiendo capa por capa los diferentes elementos: 1) el rango de anomalías máximo-mínimo 2) el rango intercuartil 3) las anomalías del año 2018.\n#rango de anomalías máximo-mínimo g1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#añadinos el rango intercuartil\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#añadimos las anomalías del año 2018\rg1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr == 2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE)\rg1.3\rFinalmente añadimos unos últimos ajustes de estilo.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Anomalía de precipitación (%)\u0026quot;,\rbreaks = seq(-100, 500, 25),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Anomalía de precipitación en Santiago de Compostela 2018\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Datos: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\r\rParte 2\rTodavía nos falta una leyenda. Primero la creamos para los valores normales.\n#datos de la leyenda\rlegend \u0026lt;- filter(data_norm, mo == \u0026quot;ene\u0026quot;)\rlegend_lab \u0026lt;- gather(legend, stat, y, mx:q75) %\u0026gt;%\rmutate(stat = factor(stat, stat, c(\u0026quot;máximo\u0026quot;,\r\u0026quot;mínimo\u0026quot;,\r\u0026quot;Cuantil 25%\u0026quot;,\r\u0026quot;Cuantil 75%\u0026quot;)) %\u0026gt;%\ras.character())\r#gráfico de la leyenda\rg2 \u0026lt;- legend %\u0026gt;% ggplot()+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;, width = 0.2) +\rgeom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;, width = 0.2) +\rgeom_text(data = legend_lab, aes(x = mo, y = y+c(12,-8,-10,12), label = stat), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.18, y = 40, label = \u0026quot;Período 1950-2018\u0026quot;, angle = 90, size = 3) +\rtheme_void() + theme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg2\rSegundo, creamos otra leyenda para las anomalías actuales.\nlegend2 \u0026lt;- filter(data, yr == 1950, mo %in% c(\u0026quot;ene\u0026quot;,\u0026quot;feb\u0026quot;)) %\u0026gt;% ungroup() %\u0026gt;% select(mo, anom, sign)\rlegend2[2,1] \u0026lt;- \u0026quot;ene\u0026quot;\rlegend_lab2 \u0026lt;- data.frame(mo = rep(\u0026quot;ene\u0026quot;, 3), anom= c(110, 3, -70), label = c(\u0026quot;Anomalía positiva\u0026quot;, \u0026quot;Promedio\u0026quot;, \u0026quot;Anomalía negativa\u0026quot;))\rg3 \u0026lt;- ggplot() + geom_bar(data = legend2,\raes(x = mo, y = anom, fill = sign),\ralpha = .6, colour = \u0026quot;NA\u0026quot;, stat = \u0026quot;identity\u0026quot;, show.legend = FALSE, width = 0.2) +\rgeom_segment(aes(x = .85, y = 0, xend = 1.15, yend = 0), linetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = legend_lab2, aes(x = mo, y = anom+c(10,5,-13), label = label), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.25, y = 20, label =\u0026quot;Referencia 1971-2010\u0026quot;, angle = 90, size = 3) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rtheme_void() +\rtheme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg3\r\rParte 3\rPara finalizar sólo debemos unir el gráfico y las leyendas con ayuda del paquete cowplot. La función princial de cowplot es plot_grid() que ayuda en combinar diferentes gráficos. No obstante, en este caso se hace necesario usar unas funciones más flexibles para crear formatos menos habituales. La función ggdraw() configura la capa básica del gráfico, y las funciones que están destinadas a operar en esta capa comienzan con draw_*.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .3, width = 1, height = 0.6) +\rdraw_plot(g2, x = 0, y = .15, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .15, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomalia2016_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r\rMúltiples facetas\rEn este apartado haremos el mismo gráfico como en el anterior, pero para varios años.\nParte 1\rÚnicamente debemos filtrar por un conjunto de años, en este caso de 2016 a 2018, usando el operador %in%, además añadimos la función facet_grid() a ggplot, lo que nos permite plotear los gráficos según una variable. La formula usada para la función de facetas es similar al uso en modelos: variable_por_fila ~ variable_por_columna. Cuando no tenemos una variable en columna debemos usar el ..\n#rango de anomalias maximo-minimo g1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#añadinos el rango intercuartil\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#añadimos las anomalías del año 2016-2018\rg1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr %in% 2016:2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE) +\rfacet_grid(yr ~ .)\rg1.3\rFinalmente, añadimos unos últimos ajustes de estilo.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Anomalía de precipitación (%)\u0026quot;,\rbreaks = seq(-100, 500, 50),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Anomalía de precipitación en Santiago de Compostela\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Datos: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\rUsamos la misma leyenda universal creada para el gráfico anterior.\n\r\rParte 2\rPara finalizar, sólo unimos el gráfico y las leyendas con ayuda del paquete cowplot. Lo único que debemos ajustar aquí son los argumentos en la función draw_plot() para colocar correctamente las diferentes partes.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .18, width = 1, height = 0.8) +\rdraw_plot(g2, x = 0, y = .08, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .08, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomalia20162018_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1562457600,"objectID":"be30c41c4386d74a34a277fed282f41a","permalink":"/es/2019/visualizar-anomal%C3%ADas-de-precipitaci%C3%B3n-mensual/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/es/2019/visualizar-anomal%C3%ADas-de-precipitaci%C3%B3n-mensual/","section":"post","summary":"Normalmente cuando visualizamos anomalías de precipitación mensual, simplemente usamos un gráfico de barras indicando con color rojo y azul valores negativos y positivos. No obstante, no nos explica el contexto general de estas mismas anomalías. Por ejemplo, ¿cuál fue la anomalía más alta o más baja en cada mes? En principio, podríamos usar un *boxplot* para visualizar la distribución de las anomalías, pero en este caso concreto no encajarían bien estéticamente, por lo que debemos buscar una alternativa. Aquí os presento una forma gráfica muy útil.","tags":["anomalía","precipitación","clima","boxplot"],"title":"Visualizar anomalías de precipitación mensual","type":"post"},{"authors":["A Martí","J Taboada","D Royé","X Fonseca"],"categories":null,"content":"","date":1560384000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1560384000,"objectID":"c6bc4640ca939d8e5a93397b0b45ec2d","permalink":"/es/publication/os_tempos_2019/","publishdate":"2019-06-13T00:00:00Z","relpermalink":"/es/publication/os_tempos_2019/","section":"publication","summary":"Que récords climáticos se alcanzaron en Galicia? Cales son os lugares máis calorosos? E os máis fríos? Onde chove máis? Onde se rexistran máis días de precipitación? Que zonas gozan dun maior número de horas de sol? Cales teñen maior nebulosidade? Que lugares son os máis ventosos? Como está a afectar o cambio climático a Galicia? Neste libro atoparás as respostas a estas e a outras preguntas relacionadas co clima de Galicia e os diversos tipos de tempo que o caracterizan. Nas súas páxinas explícase como se producen os fenómenos meteorolóxicos máis habituais no noso territorio: as inversións térmicas, as néboas costeiras e orográficas, as illas de calor urbanas, os tipos de precipitación, o efecto foehn, as brisas mariñas, o arco da vella etc. A través de exemplos concretos, analízanse tamén os riscos climáticos que afectan regularmente a Galicia, como vagas de calor, temporais de neve, cicloxéneses explosivas e temporais de choiva e vento, tormentas, secas, tornados... Tamén poderás coñecer como está a cambiar o clima da nosa comunidade debido ao quecemento global e cales son os escenarios de futuro.","tags":["tempo","Galicia","clima","divulgación","gallego"],"title":"Os tempos e o clima de Galicia","type":"publication"},{"authors":["A Vélez","J Martin-Vide","D Royé","O Santaella"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1556668800,"objectID":"610f81be23fa73e2a79cec7ae772bd48","permalink":"/es/publication/ci_pr_2018/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/es/publication/ci_pr_2018/","section":"publication","summary":"The present study analyzes spatial patterns of precipitation Concentration Index (CI) in Puerto Rico considering the daily precipitation data of precipitation-gauging stations during 1971-2010. The South and East interior parts of Puerto Rico are characterized by higher CI and the West and North-West parts show lower CI. The annual CI and the rainy season CI show a gradient from South-East to North-West and the dry season CI shows a gradient from South to North. Another difference between the rainy season CI and dry season CI is that the former shows the lowest values of CI while the latter shows the highest values of CI. The different types of seasonal precipitation seem to play a major role on the spatial CI distribution. However, the local relief plays a major role in the spatial patterns due to the effect of the air circulation by the mountains. These findings can contribute to basin-scale water resource management (ooding, soil erosion, etc.) and conservation of the ecological environment.","tags":["Concentration Index","Puerto Rico","precipitation","spatial–temporal patterns"],"title":"Spatial Analysis of Daily Precipitation Concentration in Puerto Rico","type":"publication"},{"authors":null,"categories":["estadistica","R","R:avanzado"],"content":"\rCuando pretendemos estimar la correlación entre múltiples variables, la tarea se complica para obtener un resultado simple y limpio. Una forma sencilla es usar la función tidy() del paquete {broom}. Como ejemplo, en este post vamos a estimar la correlación entre la precipitación anual de varias ciudades españolas y varios índices de teleconexiones climáticas: descarga. Los datos de las teleconexiones están preprocesados, pero pueden ser descargados directamente desde crudata.uea.ac.uk. La preciptiación diaria proviene de ECA\u0026amp;D.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rbroom\rConvierte resultados de funciones estadísticas (lm, t.test, cor.test, etc.) en bonitas tablas\r\rfs\rProporciona una interfaz uniforme y multiplataforma para las operaciones del sistema de archivos\r\rlubridate\rFácil manipulación de fechas y tiempos\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;broom\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(broom)\rlibrary(fs)\rlibrary(lubridate)\r\rImportar datos\rPrimero debemos importar la precipitación diaria de las estaciones meteorológicas seleccionadas.\nCreamos un vector con todos los archivos de precipitación con la función dir_ls() del paquete {fs}.\rImportamos los datos con ayuda de la función map_df() del paquete {purrr} que aplica otra función a un vector o lista, y los une en una única tabla.\rSeleccionamos únicamente las columnas que nos interesan, b) Convertimos la fecha en objeto date con la función ymd() del paquete {lubridate}, c) Creamos una nueva columna yr con el año, d) Dividimos la precipitación entre 10 y reclasificamos valores ausentes -9999 por NA, e) Por último, reclasificamos la ID de cada estación meteorológica, creando un factor con nuevas etiquetas.\r\r\rMás detalles sobre el uso de las funciones dir_ls() y map_df() en este último post.\n#archivos de la precipitación\rfiles \u0026lt;- dir_ls(regexp = \u0026quot;txt\u0026quot;)\rfiles\r## RR_STAID001393.txt RR_STAID001394.txt RR_STAID002969.txt ## RR_STAID003946.txt RR_STAID003969.txt\r#importamos todos, uniéndolos en una única tabla\rpr \u0026lt;- files %\u0026gt;% map_df(read_csv, skip = 20)\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\rpr\r## # A tibble: 133,343 x 5\r## STAID SOUID DATE RR Q_RR\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1393 20611 19470301 0 0\r## 2 1393 20611 19470302 5 0\r## 3 1393 20611 19470303 0 0\r## 4 1393 20611 19470304 33 0\r## 5 1393 20611 19470305 15 0\r## 6 1393 20611 19470306 0 0\r## 7 1393 20611 19470307 85 0\r## 8 1393 20611 19470308 3 0\r## 9 1393 20611 19470309 0 0\r## 10 1393 20611 19470310 0 0\r## # ... with 133,333 more rows\r#creamos los niveles del factor id \u0026lt;- unique(pr$STAID)\r#las etiquetas correspondientes\rlab \u0026lt;- c(\u0026quot;Bilbao\u0026quot;, \u0026quot;Santiago\u0026quot;, \u0026quot;Barcelona\u0026quot;, \u0026quot;Madrid\u0026quot;, \u0026quot;Valencia\u0026quot;)\r#primeros cambios\rpr \u0026lt;- select(pr, STAID, DATE, RR)%\u0026gt;% mutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10), STAID = factor(STAID, id, lab), yr = year(DATE)) pr\r## # A tibble: 133,343 x 4\r## STAID DATE RR yr\r## \u0026lt;fct\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1947-03-01 0 1947\r## 2 Bilbao 1947-03-02 0.5 1947\r## 3 Bilbao 1947-03-03 0 1947\r## 4 Bilbao 1947-03-04 3.3 1947\r## 5 Bilbao 1947-03-05 1.5 1947\r## 6 Bilbao 1947-03-06 0 1947\r## 7 Bilbao 1947-03-07 8.5 1947\r## 8 Bilbao 1947-03-08 0.3 1947\r## 9 Bilbao 1947-03-09 0 1947\r## 10 Bilbao 1947-03-10 0 1947\r## # ... with 133,333 more rows\rLo que todavía nos hace falta es filtrar y calcular la suma anual de precipitación. En principio, no es lo más correcto sumar la precipitación sin tener en cuenta que haya valores ausentes, pero nos sirve igualmente para este ensayo. Después, cambiamos el formato de la tabla con la función spread(), pasando de una tabla larga a una ancha, es decir, queremos obtener una columna por estación meteorológica.\npr_yr \u0026lt;- filter(pr, DATE \u0026gt;= \u0026quot;1950-01-01\u0026quot;, DATE \u0026lt; \u0026quot;2018-01-01\u0026quot;) %\u0026gt;%\rgroup_by(STAID, yr) %\u0026gt;%\rsummarise(pr = sum(RR, na.rm = TRUE))\rpr_yr\r## # A tibble: 324 x 3\r## # Groups: STAID [5]\r## STAID yr pr\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1950 1342 ## 2 Bilbao 1951 1306.\r## 3 Bilbao 1952 1355.\r## 4 Bilbao 1953 1372.\r## 5 Bilbao 1954 1428.\r## 6 Bilbao 1955 1062.\r## 7 Bilbao 1956 1254.\r## 8 Bilbao 1957 968.\r## 9 Bilbao 1958 1272.\r## 10 Bilbao 1959 1450.\r## # ... with 314 more rows\rpr_yr \u0026lt;- spread(pr_yr, STAID, pr)\rpr_yr\r## # A tibble: 68 x 6\r## yr Bilbao Santiago Barcelona Madrid Valencia\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA\r## 2 1951 1306. 2344. 1072. 798. NA\r## 3 1952 1355. 1973. 415. 524. NA\r## 4 1953 1372. 973. 683. 365. NA\r## 5 1954 1428. 1348. 581. 246. NA\r## 6 1955 1062. 1769. 530. 473. NA\r## 7 1956 1254. 1533. 695. 480. NA\r## 8 1957 968. 1599. 635. 424. NA\r## 9 1958 1272. 2658. 479. 482. NA\r## 10 1959 1450. 2847. 1006 665. NA\r## # ... with 58 more rows\rEl siguiente paso es importar los índices de las teleconexiones.\n#teleconexiones\rtelecon \u0026lt;- read_csv(\u0026quot;teleconnections_indices.csv\u0026quot;)\r## Parsed with column specification:\r## cols(\r## yr = col_double(),\r## NAO = col_double(),\r## WeMO = col_double(),\r## EA = col_double(),\r## `POL-EUAS` = col_double(),\r## `EATL/WRUS` = col_double(),\r## MO = col_double(),\r## SCAND = col_double(),\r## AO = col_double()\r## )\rtelecon\r## # A tibble: 68 x 9\r## yr NAO WeMO EA `POL-EUAS` `EATL/WRUS` MO SCAND AO\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 0.49 0.555 -0.332 0.0217 -0.0567 0.335 0.301 -1.99e-1\r## 2 1951 -0.07 0.379 -0.372 0.402 -0.419 0.149 -0.00667 -3.65e-1\r## 3 1952 -0.37 0.693 -0.688 -0.0117 -0.711 0.282 0.0642 -6.75e-1\r## 4 1953 0.4 -0.213 -0.727 -0.0567 -0.0508 0.216 0.0233 -1.64e-2\r## 5 1954 0.51 1.20 -0.912 0.142 -0.318 0.386 0.458 -5.83e-4\r## 6 1955 -0.64 0.138 -0.824 -0.0267 0.154 0.134 0.0392 -3.62e-1\r## 7 1956 0.17 0.617 -1.29 -0.197 0.0617 0.256 0.302 -1.63e-1\r## 8 1957 -0.02 0.321 -0.952 -0.638 -0.167 0.322 -0.134 -3.42e-1\r## 9 1958 0.12 0.941 -0.243 0.138 0.661 0.296 0.279 -8.68e-1\r## 10 1959 0.49 -0.055 -0.23 -0.0142 0.631 0.316 0.725 -7.62e-2\r## # ... with 58 more rows\rPor último nos falta unir ambas tablas por año.\ndata_all \u0026lt;- left_join(pr_yr, telecon, by = \u0026quot;yr\u0026quot;)\rdata_all\r## # A tibble: 68 x 14\r## yr Bilbao Santiago Barcelona Madrid Valencia NAO WeMO EA\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA 0.49 0.555 -0.332\r## 2 1951 1306. 2344. 1072. 798. NA -0.07 0.379 -0.372\r## 3 1952 1355. 1973. 415. 524. NA -0.37 0.693 -0.688\r## 4 1953 1372. 973. 683. 365. NA 0.4 -0.213 -0.727\r## 5 1954 1428. 1348. 581. 246. NA 0.51 1.20 -0.912\r## 6 1955 1062. 1769. 530. 473. NA -0.64 0.138 -0.824\r## 7 1956 1254. 1533. 695. 480. NA 0.17 0.617 -1.29 ## 8 1957 968. 1599. 635. 424. NA -0.02 0.321 -0.952\r## 9 1958 1272. 2658. 479. 482. NA 0.12 0.941 -0.243\r## 10 1959 1450. 2847. 1006 665. NA 0.49 -0.055 -0.23 ## # ... with 58 more rows, and 5 more variables: `POL-EUAS` \u0026lt;dbl\u0026gt;,\r## # `EATL/WRUS` \u0026lt;dbl\u0026gt;, MO \u0026lt;dbl\u0026gt;, SCAND \u0026lt;dbl\u0026gt;, AO \u0026lt;dbl\u0026gt;\r\rTest de correlación\rUn test de correlación lo podemos hacer con la función cor.test() de R Base. En este caso entre la precipitación anual de Bilbao y el índice de NAO.\ncor_nao_bil \u0026lt;- cor.test(data_all$Bilbao, data_all$NAO,\rmethod=\u0026quot;spearman\u0026quot;)\r## Warning in cor.test.default(data_all$Bilbao, data_all$NAO, method =\r## \u0026quot;spearman\u0026quot;): Cannot compute exact p-value with ties\rcor_nao_bil\r## ## Spearman\u0026#39;s rank correlation rho\r## ## data: data_all$Bilbao and data_all$NAO\r## S = 44372, p-value = 0.2126\r## alternative hypothesis: true rho is not equal to 0\r## sample estimates:\r## rho ## 0.1531149\rstr(cor_nao_bil)\r## List of 8\r## $ statistic : Named num 44372\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## $ parameter : NULL\r## $ p.value : num 0.213\r## $ estimate : Named num 0.153\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ null.value : Named num 0\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ alternative: chr \u0026quot;two.sided\u0026quot;\r## $ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## $ data.name : chr \u0026quot;data_all$Bilbao and data_all$NAO\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;htest\u0026quot;\rVemos que el resultado está en un formato poco manejable. Nos resume la correlación con todos los parametros estadísticos necesarios para sacar una conclusión sobre la relación. La estructura orginal es una lista de vectores. No obstante, la función tidy() del paquete {broom} nos permite convertir el resultado en formato de tabla.\ntidy(cor_nao_bil)\r## # A tibble: 1 x 5\r## estimate statistic p.value method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.153 44372. 0.213 Spearman\u0026#39;s rank correlation rho two.sided\r\rAplicar el test de correlación a múltiples variables\rEl objetivo es aplicar el test de correlación a todas las estaciones meteorológicas e índices de teleconexión.\nPrimero, debemos pasar la tabla al formato largo, o sea, crear una columna de la ciudad y el valor de la precipitación correspondiente. Después lo repetimos para las teleconexiones.\ndata \u0026lt;- gather(data_all, city, pr, Bilbao:Valencia) %\u0026gt;%\rgather(telecon, index, NAO:AO)\rdata\r## # A tibble: 2,720 x 5\r## yr city pr telecon index\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Bilbao 1342 NAO 0.49\r## 2 1951 Bilbao 1306. NAO -0.07\r## 3 1952 Bilbao 1355. NAO -0.37\r## 4 1953 Bilbao 1372. NAO 0.4 ## 5 1954 Bilbao 1428. NAO 0.51\r## 6 1955 Bilbao 1062. NAO -0.64\r## 7 1956 Bilbao 1254. NAO 0.17\r## 8 1957 Bilbao 968. NAO -0.02\r## 9 1958 Bilbao 1272. NAO 0.12\r## 10 1959 Bilbao 1450. NAO 0.49\r## # ... with 2,710 more rows\rPara poder aplicar el test a todas las ciudades, debemos tener las correspondientes agrupaciones. Por ello, usamos la función group_by() indicando los dos grupos (city y telecon), y además, aplicamos la función nest() del paquete {tidyr}, colección {tidyverse}, con el objetivo de crear listas de tablas encajadas por fila. En otras palabras, en cada fila de cada ciudad y teleconexión tendremos una nueva tabla que contiene correspondientemente el año, la precipitación y el valor del índice.\ndata_nest \u0026lt;- group_by(data, city, telecon) %\u0026gt;% nest()\rdata_nest\r## # A tibble: 40 x 3\r## city telecon data ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 7 Santiago WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 8 Barcelona WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 9 Madrid WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 10 Valencia WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## # ... with 30 more rows\rstr(slice(data_nest, 1))\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 3 variables:\r## $ city : chr \u0026quot;Bilbao\u0026quot;\r## $ telecon: chr \u0026quot;NAO\u0026quot;\r## $ data :List of 1\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\rEl siguiente paso es crear una función, en la que definimos el test de correlación y lo pasamos al formato limpio, que aplicamos a cada agrupación.\ncor_fun \u0026lt;- function(df) cor.test(df$pr, df$index, method=\u0026quot;spearman\u0026quot;) %\u0026gt;% tidy()\rAhora sólo nos queda por aplicar nuestra función a la columna que contiene las tablas por cada combinación entre ciudad y teleconexión. Para ello, usamos la función map() que aplica otra función sobre un vector o lista. Lo que hacemos es crear una nueva columna que contiene el resultado, una tabla del resumen estadístico, por cada fila de cada combinación.\ndata_nest \u0026lt;- mutate(data_nest, model = map(data, cor_fun))\rdata_nest\r## # A tibble: 40 x 4\r## city telecon data model ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 7 Santiago WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 8 Barcelona WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 9 Madrid WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 10 Valencia WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## # ... with 30 more rows\rstr(slice(data_nest, 1))\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 4 variables:\r## $ city : chr \u0026quot;Bilbao\u0026quot;\r## $ telecon: chr \u0026quot;NAO\u0026quot;\r## $ data :List of 1\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## $ model :List of 1\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.153\r## .. ..$ statistic : num 44372\r## .. ..$ p.value : num 0.213\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r¿Cómo podemos deshacer la lista de tablas en cada fila de nuestra tabla?\nPues bien, primero eliminamos la columna con los datos y después aplicamos simplemente la función unnest().\ncorr_pr \u0026lt;- select(data_nest, -data) %\u0026gt;% unnest()\rcorr_pr\r## # A tibble: 40 x 7\r## city telecon estimate statistic p.value method alternative\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Bilbao NAO 0.153 44372. 0.213 Spearman\u0026#39;s rank~ two.sided ## 2 Santia~ NAO -0.181 61902. 0.139 Spearman\u0026#39;s rank~ two.sided ## 3 Barcel~ NAO -0.0203 53460. 0.869 Spearman\u0026#39;s rank~ two.sided ## 4 Madrid NAO -0.291 64692. 0.0169 Spearman\u0026#39;s rank~ two.sided ## 5 Valenc~ NAO -0.113 27600. 0.422 Spearman\u0026#39;s rank~ two.sided ## 6 Bilbao WeMO 0.404 31242. 0.000706 Spearman\u0026#39;s rank~ two.sided ## 7 Santia~ WeMO 0.332 35014. 0.00594 Spearman\u0026#39;s rank~ two.sided ## 8 Barcel~ WeMO 0.0292 50862 0.813 Spearman\u0026#39;s rank~ two.sided ## 9 Madrid WeMO 0.109 44660 0.380 Spearman\u0026#39;s rank~ two.sided ## 10 Valenc~ WeMO -0.252 31056. 0.0688 Spearman\u0026#39;s rank~ two.sided ## # ... with 30 more rows\rEl resultado es una tabla en la que podemos ver las correlaciones y su significación estadística para cada ciudad y teleconexiones.\n\rHeatmap de los resultados\rFinalmente, hacemos un heatmap del resultado obtenido. Antes creamos una columna que indica si la correlación es significativa con p-valor menor de 0,05.\ncorr_pr \u0026lt;- mutate(corr_pr, sig = ifelse(p.value \u0026lt; 0.05, \u0026quot;Sig.\u0026quot;, \u0026quot;Non Sig.\u0026quot;))\rggplot()+\rgeom_tile(data = corr_pr,\raes(city, telecon, fill = estimate),\rsize = 1,\rcolour = \u0026quot;white\u0026quot;)+\rgeom_tile(data = filter(corr_pr, sig == \u0026quot;Sig.\u0026quot;),\raes(city, telecon),\rsize = 1,\rcolour = \u0026quot;black\u0026quot;,\rfill = \u0026quot;transparent\u0026quot;)+\rgeom_text(data = corr_pr,\raes(city, telecon, label = round(estimate, 2),\rfontface = ifelse(sig == \u0026quot;Sig.\u0026quot;, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)))+\rscale_fill_gradient2(breaks = seq(-1, 1, 0.2))+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, p.value = \u0026quot;\u0026quot;)+\rtheme_minimal()+\rtheme(panel.grid.major = element_blank(),\rpanel.border = element_blank(),\rpanel.background = element_blank(),\raxis.ticks = element_blank())\r\r","date":1555459200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1555459200,"objectID":"0a0047b8339dc032811f6bb1cd55f01b","permalink":"/es/2019/resumir-tests-de-correlaciones-en-r/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/es/2019/resumir-tests-de-correlaciones-en-r/","section":"post","summary":"Cuando pretendemos estimar la correlación entre múltiples variables, la tarea se complica para obtener un resultado simple y limpio. Una forma sencilla es usar la función ``tidy()`` del paquete *{broom}*. Como ejemplo, en este post vamos a estimar la correlación entre la precipitación anual de varias ciudades españolas y varios índices de teleconexiones climáticas.","tags":["correlación","variables","resumir","tests","limpio"],"title":"Resumir tests de correlaciones en R","type":"post"},{"authors":["M Lemus-Canovas","JA Lopez-Bustins","J Martin-Vide","D Royé"],"categories":null,"content":"","date":1555286400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1555286400,"objectID":"6f67cefd8fe5457da992aab82661b0f2","permalink":"/es/publication/synoptreg_2019/","publishdate":"2019-04-15T00:00:00Z","relpermalink":"/es/publication/synoptreg_2019/","section":"publication","summary":"Spatial knowledge of the climatic or environmental variables associated with the most frequent circulation types is essential with regard to developing strategies to address the risk of avalanches, floods, soil erosion, air pollution or other natural hazards. In order to derive an environmental regionalization, we present an Open Source R package known as synoptReg, which combines the spatialization of environmental variables based on the atmospheric circulation types. The synoptReg package contains a set of functions, which we will employ (1) to perform a PCA-based synoptic classification using an atmospheric variable; (2) to map the spatial distribution of the selected environmental variable based upon the circulation types; (3) to develop a spatial environmental regionalization based on the previous results. We illustrate the usefulness of the package for a case study in the Alps area.","tags":["Alps","Environmental regionalization","R package","synoptReg","Synoptic classification"],"title":"synoptReg: An R package for computing a synoptic climate classification and a spatial regionalization of environmental data","type":"publication"},{"authors":["D Royé","María T Zarrabeitia","Javier Riancho","Ana Santurtún"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1554076800,"objectID":"1bb4954049456a115c245d50b0442dcc","permalink":"/es/publication/ictus_madrid_2019/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/es/publication/ictus_madrid_2019/","section":"publication","summary":"The understanding of the role of environment on the pathogenesis of stroke is gaining importance in the context of climate change. This study analyzes the temporal pattern of ischemic stroke (IS) in Madrid, Spain, during a 13-year period (2001-2013), and the relationship between ischemic stroke (admissions and deaths) incidence and environmental factors on a daily scale by using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of air pollutants and Apparent Temperature (AT), a biometeorological index which represents human thermal comfort on IS, a lag non-linear model was fitted in a generalized additive model. The mortality rate followed a downward trend over the studied period, however admission rates progressively increased. Our results show that both increases and decreases in AT had a marked relationship with IS deaths, while hospital admissions were only associated with low AT. When analyzing the cumulative effects (for lag 0 to 14 days), with an AT of 1.7°C (percentile 5%) a RR of 1.20 (95% CI, 1.05-1.37) for IS mortality and a RR of 1.09 (95% CI, 0.91-1.29) for morbidity is estimated. Concerning gender differences, men show higher risks of mortality in low temperatures and women in high temperatures. No significant relationship was found between air pollutant concentrations and IS morbi mortality, but this result must be interpreted with caution, since there are strong spatial fluctuations of the former between nearby geographical areas that make it difficult to perform correlation analyses.","tags":["efectos de corto plazo","España","Madrid","ambiente térmico","ictus isquémico","contaminación atmosferica","temperatura aparente","mortalidad","ingresos hospitalarios"],"title":"A time series analysis of the relationship between Apparent Temperature, Air Pollutants and Ischemic Stroke in Madrid, Spain","type":"publication"},{"authors":null,"categories":["gestión","R","R:intermedio"],"content":"\rCuando trabajamos con diferentes fuentes de datos, nos podemos encontrar con tablas distrubidas sobre varias hojas de Excel. En este post vamos a importar la temperatura media diaria de Madrid y Berlín que se encuentra en dos archvios de Excel con hojas para cada año entre 2000 y 2005: descarga.\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de paquetes (visualización y manipulación de datos): ggplot2, dplyr, purrr,etc.\r\rfs\rProporciona una interfaz uniforme y multiplataforma para las operaciones del sistema de archivos\r\rreadxl\rImportar archivos Excel\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;readxl\u0026quot;)) install.packages(\u0026quot;readxl\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(fs)\rlibrary(readxl)\rPor defecto, la función read_excel() importa la primera hoja. Para importar una hoja diferente es necesario indicarlo con el argumento sheet o bien el número o el nombre (segundo argumento).\n#importar primera hoja\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;)\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#importar hoja 3\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;, 3)\r## # A tibble: 365 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2002-01-01 00:00:00 8.7 2002\r## 2 2002-01-02 00:00:00 7.4 2002\r## 3 2002-01-03 00:00:00 8.5 2002\r## 4 2002-01-04 00:00:00 9.2 2002\r## 5 2002-01-05 00:00:00 9.3 2002\r## 6 2002-01-06 00:00:00 7.3 2002\r## 7 2002-01-07 00:00:00 5.4 2002\r## 8 2002-01-08 00:00:00 5.6 2002\r## 9 2002-01-09 00:00:00 6.8 2002\r## 10 2002-01-10 00:00:00 6.1 2002\r## # ... with 355 more rows\rLa función excel_sheets() permite extraer los nombres de las hojas.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rpath %\u0026gt;%\rexcel_sheets()\r## [1] \u0026quot;2000\u0026quot; \u0026quot;2001\u0026quot; \u0026quot;2002\u0026quot; \u0026quot;2003\u0026quot; \u0026quot;2004\u0026quot; \u0026quot;2005\u0026quot;\rEl resultado nos indica que en cada hoja encontramos un año de los datos desde 2000 a 2005. La función más importante para leer múltiples hojas es map() del paquete {purrr} que forma parte de la colección de paquetes {tidyverse}. map() permite aplicar una función a cada elemento de un vector o lista.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap(read_excel,\rpath = path)\rstr(mad)\r## List of 6\r## $ 2000:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 366 obs. of 3 variables:\r## ..$ date: POSIXct[1:366], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:366] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:366] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ 2001:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2001-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 8.2 8.8 7.5 9.2 10 9 5.5 4.6 3 7.9 ...\r## ..$ yr : num [1:365] 2001 2001 2001 2001 2001 ...\r## $ 2002:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2002-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 8.7 7.4 8.5 9.2 9.3 7.3 5.4 5.6 6.8 6.1 ...\r## ..$ yr : num [1:365] 2002 2002 2002 2002 2002 ...\r## $ 2003:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2003-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 9.4 10.8 9.7 9.2 6.3 6.6 3.8 6.4 4.3 3.4 ...\r## ..$ yr : num [1:365] 2003 2003 2003 2003 2003 ...\r## $ 2004:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 366 obs. of 3 variables:\r## ..$ date: POSIXct[1:366], format: \u0026quot;2004-01-01\u0026quot; ...\r## ..$ ta : num [1:366] 6.6 5.9 7.8 8.1 6.4 5.7 5.2 6.9 11.8 12.2 ...\r## ..$ yr : num [1:366] 2004 2004 2004 2004 2004 ...\r## $ 2005:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2005-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 7.1 7.8 6.4 5.6 4.4 6.8 7.4 6 5.2 4.2 ...\r## ..$ yr : num [1:365] 2005 2005 2005 2005 2005 ...\rEl resultado es una lista nombrada con el nombre de cada hoja que contiene el data.frame. Dado que se trata de la misma tabla en todas las hojas, podríamos usar la función bind_rows(), no obstante, existe una variante de map()que directamente nos une todas las tablas por fila: map_df(). Si fuese necesario unir por columna se debería usar map_dfc().\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path)\rmad\r## # A tibble: 2,192 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 2,182 more rows\rEn nuestro caso tenemos una columna en cada hoja (año, pero también la fecha) que diferencia cada tabla. Si no fuera el caso, deberíamos usar el nombre de las hojas como nueva columna al unir todas. En bind_rows() puede hacerse con el argumento .id asignando un nombre para la columna. Lo mismo valdría para map_df().\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path,\r.id = \u0026quot;yr2\u0026quot;)\rstr(mad)\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 4 variables:\r## $ yr2 : chr \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; ...\r## $ date: POSIXct, format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## $ yr : num 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r¿Pero cómo importamos múltiples archivos de Excel?\nPara ello, primero debemos conocer la función dir_ls() del paquete {fs}. Es cierto que existe la función dir() de R Base, pero las ventajas del reciente paquete son varias, pero especialmente es la compatibilidad con la colección de {tidyverse}.\ndir_ls()\r## berlin_temp.xlsx featured.png index.es.html index.es.Rmd ## madrid_temp.xlsx\r#podemos filtrar los archivos que queremos\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) \r## berlin_temp.xlsx madrid_temp.xlsx\rImportamos los dos archivos de Excel que tenemos.\n#sin unir\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;)%\u0026gt;%\rmap(read_excel)\r## $berlin_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 1.2 2000\r## 2 2000-01-02 00:00:00 3.6 2000\r## 3 2000-01-03 00:00:00 5.7 2000\r## 4 2000-01-04 00:00:00 5.1 2000\r## 5 2000-01-05 00:00:00 2.2 2000\r## 6 2000-01-06 00:00:00 1.8 2000\r## 7 2000-01-07 00:00:00 4.2 2000\r## 8 2000-01-08 00:00:00 4.2 2000\r## 9 2000-01-09 00:00:00 4.2 2000\r## 10 2000-01-10 00:00:00 1.7 2000\r## # ... with 356 more rows\r## ## $madrid_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#uniendo con una nueva columna\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;)%\u0026gt;%\rmap_df(read_excel, .id = \u0026quot;city\u0026quot;)\r## # A tibble: 732 x 4\r## city date ta yr\r## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 berlin_temp.xlsx 2000-01-01 00:00:00 1.2 2000\r## 2 berlin_temp.xlsx 2000-01-02 00:00:00 3.6 2000\r## 3 berlin_temp.xlsx 2000-01-03 00:00:00 5.7 2000\r## 4 berlin_temp.xlsx 2000-01-04 00:00:00 5.1 2000\r## 5 berlin_temp.xlsx 2000-01-05 00:00:00 2.2 2000\r## 6 berlin_temp.xlsx 2000-01-06 00:00:00 1.8 2000\r## 7 berlin_temp.xlsx 2000-01-07 00:00:00 4.2 2000\r## 8 berlin_temp.xlsx 2000-01-08 00:00:00 4.2 2000\r## 9 berlin_temp.xlsx 2000-01-09 00:00:00 4.2 2000\r## 10 berlin_temp.xlsx 2000-01-10 00:00:00 1.7 2000\r## # ... with 722 more rows\rAhora bien, en este caso sólo importamos la primera hoja de cada archivo Excel. Para resolver este problema, debemos crear nuestra propia función. En esta función hacemos lo que hicimos previamente de forma individual.\nread_multiple_excel \u0026lt;- function(path) {\rpath %\u0026gt;%\rexcel_sheets() %\u0026gt;% set_names() %\u0026gt;% map_df(read_excel, path = path)\r}\rAplicamos nuestra función creada para importar múltiples hojas de varios archivos Excel.\n#por separado\rdata \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map(read_multiple_excel)\rstr(data)\r## List of 2\r## $ berlin_temp.xlsx:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 3 variables:\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:2192] 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ madrid_temp.xlsx:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 3 variables:\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:2192] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r#unir todas\rdata_df \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map_df(read_multiple_excel,\r.id = \u0026quot;city\u0026quot;)\rstr(data_df)\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 4384 obs. of 4 variables:\r## $ city: chr \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; ...\r## $ date: POSIXct, format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## $ yr : num 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r\r","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1552176000,"objectID":"d08ba3e4444285e38a94522cbea8f517","permalink":"/es/2019/importar-varias-hojas-excel-en-r/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/es/2019/importar-varias-hojas-excel-en-r/","section":"post","summary":"Cuando trabajamos con diferentes fuentes de datos, nos podemos encontrar con tablas distrubidas sobre varias hojas de Excel. En este post vamos a importar la temperatura media diaria de Madrid y Berlín que se encuentran en dos archvios de Excel con hojas para cada año entre 2000 y 2005.","tags":["excel","hojas","importar"],"title":"Importar varias hojas Excel en R","type":"post"},{"authors":["D Royé","N Lorenzo","D Rasilla","A Martí"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1551398400,"objectID":"ae246d77b9b92c3f6254230def92ce47","permalink":"/es/publication/cloudiness_ip_2018/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/es/publication/cloudiness_ip_2018/","section":"publication","summary":"This paper presents the first systematic study of the relationships between atmospheric circulation types (CT) and cloud fraction (CF) over the whole Iberian Peninsula, using satellite data from the MODIS (MOD09GA and MYD09GA) cloud mask for the period 2001--2017. The high level of detail, in combination with a classification for circulation patterns, provides us with relevant information about the spatio-temporal variability of cloudiness and the main mechanisms affecting the genesis of clouds. The results show that westerly CTs are the most influential, followed by cyclonic types, in cloudiness in the west of the Iberian Peninsula. Westerly flows, however, do not affect the Mediterranean coastline, which is dominated by easterly CTs, suggesting that local factors such as convective processes, orography and proximity to a body of warm water could play a major role in cloudiness processes. The Cantabrian Coast also has a particularly characteristic cloudiness dominated by northerly CTs. In general, the results found in this study are in line with the few studies that exist on cloudiness in the Iberian Peninsula. Furthermore, the results are geographically consistent, showing links to synoptic forcing in terms of atmospheric circulation patterns and the impact of the Iberian Peninsula's complex orography upon this element of the climate system.","tags":["nubosidad","tipos de circulación","Península Ibérica","MODIS","tiempo","patrones espacio-temporales"],"title":"Spatio-temporal variations of cloud fraction based on circulation types in the Iberian Peninsula","type":"publication"},{"authors":null,"categories":["sig","R","R:elemental"],"content":"\rEn geografía, la distancia al mar es una variable fundamental, especialmente relevante a la hora de modelizar. Por ejemplo, en interpolaciones de la temperatura del aire habitualmente se hace uso de la distancia al mar como variable predictora, ya que existe una relación casual entre ambas que explica la variación espacial. ¿Cómo podemos estimar la distancia (más corta) a la costa en R?\nPaquetes\rEn este post usaremos los siguientes paquetes:\n\r\rPaquete\rDescripción\r\r\r\rtidyverse\rConjunto de librerías (visualización y manipulación de datos): ggplot2, dplyr, etc.\r\rsf\rSimple Feature: importar, exportar y manipular datos vectoriales\r\rraster\rImportar, exportar y manipular raster\r\rrnaturalearth\rConjunto de mapas vectoriales ‘natural earth’\r\rRColorBrewer\rPaletas de colores\r\r\r\r#instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r#paquetes\rlibrary(rnaturalearth)\rlibrary(sf)\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(RColorBrewer)\r\rLa costa de Islandia como ejemplo\rNuestro ejemplo en este post será Islandia, como es un territorio insular facilitará el ensayo y de este modo es posible mostrar el proceso de forma sencilla. La librería rnaturalearth permite importar los límites de países (con diferentes niveles administrativos) de todo el mundo. Los datos vienen de la plataforma naturalearthdata.com. Recomiendo explorar la librería, más info aquí. La función ne_countries( ) importa los límites de países. En este caso indicamos con el argumento scale la resolución (10,50 o 110m), con country indicamos el país concreto de interés y con returnclass determinamos que clase queremos (sf o sp), en nuestro caso sf (simple feature).\nworld \u0026lt;- ne_countries(scale = 50) #mapamundi con 50m de resolución\rplot(world) #tiene clase sp por defecto\r#importamos los límites de Islandia iceland \u0026lt;- ne_countries(scale = 10,country = \u0026quot;Iceland\u0026quot;, returnclass = \u0026quot;sf\u0026quot;)\r#info del objeto vectorial\riceland\r## Simple feature collection with 1 feature and 94 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: -24.53991 ymin: 63.39671 xmax: -13.50292 ymax: 66.56415\r## epsg (SRID): 4326\r## proj4string: +proj=longlat +datum=WGS84 +no_defs\r## featurecla scalerank labelrank sovereignt sov_a3 adm0_dif level\r## 188 Admin-0 country 0 3 Iceland ISL 0 2\r## type admin adm0_a3 geou_dif geounit gu_a3 su_dif\r## 188 Sovereign country Iceland ISL 0 Iceland ISL 0\r## subunit su_a3 brk_diff name name_long brk_a3 brk_name brk_group\r## 188 Iceland ISL 0 Iceland Iceland ISL Iceland \u0026lt;NA\u0026gt;\r## abbrev postal formal_en formal_fr name_ciawf note_adm0\r## 188 Iceland IS Republic of Iceland \u0026lt;NA\u0026gt; Iceland \u0026lt;NA\u0026gt;\r## note_brk name_sort name_alt mapcolor7 mapcolor8 mapcolor9 mapcolor13\r## 188 \u0026lt;NA\u0026gt; Iceland \u0026lt;NA\u0026gt; 1 4 4 9\r## pop_est pop_rank gdp_md_est pop_year lastcensus gdp_year\r## 188 339747 10 16150 2017 NA 2016\r## economy income_grp wikipedia fips_10_\r## 188 2. Developed region: nonG7 1. High income: OECD NA IC\r## iso_a2 iso_a3 iso_a3_eh iso_n3 un_a3 wb_a2 wb_a3 woe_id woe_id_eh\r## 188 IS ISL ISL 352 352 IS ISL 23424845 23424845\r## woe_note adm0_a3_is adm0_a3_us adm0_a3_un adm0_a3_wb\r## 188 Exact WOE match as country ISL ISL NA NA\r## continent region_un subregion region_wb name_len\r## 188 Europe Europe Northern Europe Europe \u0026amp; Central Asia 7\r## long_len abbrev_len tiny homepart min_zoom min_label max_label\r## 188 7 7 NA 1 0 2 7\r## ne_id wikidataid name_ar name_bn name_de name_en name_es name_fr\r## 188 1159320917 Q189 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Island Iceland Islandia Islande\r## name_el name_hi name_hu name_id name_it name_ja name_ko name_nl\r## 188 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Izland Islandia Islanda \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; IJsland\r## name_pl name_pt name_ru name_sv name_tr name_vi name_zh\r## 188 Islandia Islândia \u0026lt;NA\u0026gt; Island Izlanda Iceland \u0026lt;NA\u0026gt;\r## geometry\r## 188 MULTIPOLYGON (((-14.56363 6...\r#aquí Islandia\rplot(iceland)\rPor defecto, la función plot( ) con la clase sf nos crea tantas facetas del mapa como variables tiene. Para limitarlo podemos usar o bien con el nombre de una variable plot(iceland[\u0026quot;admin\u0026quot;]) o el argumento max.plot plot(iceland,max.plot=1). Con el argumento max.plot=1 la función usa la primera variable disponible del mapa.\nAdemás, vemos en la información del objeto sf que la proyección es WGS84 con grados decimales (código EPSG:4326). Para el cálculo de distancias es más conveniente usar metros en lugar de grados. Debido a ello, lo primero que hacemos es transformar el mapa de Islandia a UTM Zona 27 (código EPSG:3055). Más información sobre EPSG y proyecciones aquí. Con ese objetivo, usamos la función st_transform( ). Simplemente indicamos el mapa y el código EPSG.\n#transformamos a UTM\riceland \u0026lt;- st_transform(iceland, 3055)\r\rCrear una red de puntos\rTodavía necesitamos los puntos donde queremos conocer la distancia. En nuestro caso será una red regular de puntos en Islandia con una resolución de 5km. Esa tarea la hacemos con la función st_make_grid( ), indicando con el argumento cellsize la resolución en la unidad del sistema de coordenadas (metros en nuestro caso) y qué geometría nos gustaría crear what (poligonos, centros o esquinas).\n#crear red de puntos\rgrid \u0026lt;- st_make_grid(iceland,cellsize = 5000, what = \u0026quot;centers\u0026quot;)\r#nuestra red sobre la extensión de Islandia\rplot(grid)\r#exraemos sólamente los puntos en los límites de Islandia\rgrid \u0026lt;- st_intersection(grid, iceland) #nuestra red ahora\rplot(grid)\r\rCalcular la distancia\rPara estimar la distancia usamos la función st_distance( ) que nos devuelve un vector de distancias para todos nuestros puntos de la red. Pero antes es necesario transformar el mapa de Islandia de una forma de polígono (MULTIPOLYGON) a línea (MULTILINESTRING). Más detalles con ?st_cast.\n#convertimos Islandia de geometría poligono a línea\riceland \u0026lt;- st_cast(iceland, \u0026quot;MULTILINESTRING\u0026quot;)\r#cálculo de la distancia entre la costa y nuestros puntos\rdist \u0026lt;- st_distance(iceland, grid)\r#distancia con unidad en metros\rhead(dist)\r## Units: [m]\r## [1] 790.7906 1151.4360 1270.7603 3128.9057 2428.5677 4197.7472\r\rVisualizar la distancia calculada\rUna vez obtenida la distancia para nuestros puntos, podemos combinarlos con las coordenadas y plotearlos en ggplot2. Para ello, creamos un data.frame. El objeto dist es una matriz de una columna, por eso, tenemos que convertirla a vector con la función as.vector( ). Además, dividimos por 1000 para convertir la distancia en metros a km. La función st_coordinates( ) extrae las coordenadas de nuestros puntos. Para la visualización usamos un vector de colores con la gama RdGy (más aquí).\n#creamos un data.frame con la distancia y las coorendas de los puntos\rdf \u0026lt;- data.frame(dist = as.vector(dist)/1000,\rst_coordinates(grid))\r#estructura\rstr(df)\r## \u0026#39;data.frame\u0026#39;: 4104 obs. of 3 variables:\r## $ dist: num 0.791 1.151 1.271 3.129 2.429 ...\r## $ X : num 608796 613796 583796 588796 593796 ...\r## $ Y : num 7033371 7033371 7038371 7038371 7038371 ...\r#colores col_dist \u0026lt;- brewer.pal(11, \u0026quot;RdGy\u0026quot;)\rggplot(df, aes(X, Y,fill = dist))+ #variables\rgeom_tile()+ #geometría\rscale_fill_gradientn(colours = rev(col_dist))+ #colores para la distancia\rlabs(fill = \u0026quot;Distance (km)\u0026quot;)+ #nombre de la leyenda\rtheme_void()+ #estilo del mapa\rtheme(legend.position = \u0026quot;bottom\u0026quot;) #posición de la leyenda\r\rExportar la distancia como raster\rPara poder exportar la distancia con respecto al mar de Islandia, debemos usar la función rasterize( ) de la librería raster.\nPrimero, es necesario crear un raster vacío. En este raster debemos indicar la resolución, en nuestro caso es de 5000m, la proyección y la extensión del raster.\nLa proyección la podemos extraer de la información del mapa de Islandia.\n\rLa extensión la conseguimos extraer de nuestros puntos grid con la función extent( ). No obstante, esta última función necesita la clase sp, por eso pasamos el objeto grid en formato sf, únicamente para ello, a la clase sp usando la función as( ) y el argumento “Spatial”.\n\r\rAdemás de lo anterior, el data.frame df que creamos antes debemos convertir en clase sf. Por eso, aplicamos la función st_as_sf( ) con el argumento coords indicando los nombres de las coordenadas. Adicionalmente, también definimos el sistema de coordenadas que conocemos.\n\r\r#obtenemos la extensión\rext \u0026lt;- extent(as(grid, \u0026quot;Spatial\u0026quot;))\r#objeto extent\rext\r## class : Extent ## xmin : 338795.6 ## xmax : 848795.6 ## ymin : 7033371 ## ymax : 7383371\r#raster destino\rr \u0026lt;- raster(resolution = 5000, ext = ext, crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\r#convertimos los puntos a un spatial object clase sf\rdist_sf \u0026lt;- st_as_sf(df, coords = c(\u0026quot;X\u0026quot;,\u0026quot;Y\u0026quot;)) %\u0026gt;%\rst_set_crs(3055)\r#creamos el raster de la distancia\rdist_raster \u0026lt;- rasterize(dist_sf, r, \u0026quot;dist\u0026quot;, fun = mean)\r#raster\rdist_raster\r## class : RasterLayer ## dimensions : 70, 102, 7140 (nrow, ncol, ncell)\r## resolution : 5000, 5000 (x, y)\r## extent : 338795.6, 848795.6, 7033371, 7383371 (xmin, xmax, ymin, ymax)\r## crs : +proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs ## source : memory\r## names : layer ## values : 0.006124901, 115.1712 (min, max)\r#plotear el raster\rplot(dist_raster)\r#exportamos el raster\rwriteRaster(dist_raster, file = \u0026quot;dist_islandia.tif\u0026quot;, format = \u0026quot;GTiff\u0026quot;, overwrite = TRUE)\rLa función rasterize( ) está pensada para crear rasters a partir de un grid irregular. En caso que tengamos un grid regular, como este mismo, podemos usar una alternativa más fácil. La función rasterFromXYZ( ) convierte un data.frame con longitud, latitud y la variable Z en un raster. Es importante que el orden debe ser longitud, latitud, variables.\nr \u0026lt;- rasterFromXYZ(df[, c(2:3, 1)], crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\rplot(r)\rCon el cálculo de la distancia podemos llegar crear arte, como se ve en la cabezera de este post, que incluye un mapamundi únicamente con la distancia al mar de todos los continentes. Una perspectiva diferente a nuestro mundo (aquí más).\n\r","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1546905600,"objectID":"f87c54a1462617a9bb20bc83c72f22e4","permalink":"/es/2019/calcular-la-distancia-al-mar-en-r/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/es/2019/calcular-la-distancia-al-mar-en-r/","section":"post","summary":"En geografía, la distancia al mar es una variable fundamental, especialmente relevante a la hora de modelizar. Por ejemplo, en interpolaciones de la temperatura del aire habitualmente se hace uso de la distancia al mar como variable predictora, ya que existe una relación casual entre ambas que explica la variación espacial. ¿Cómo podemos estimar la distancia (más corta) a la costa en R?","tags":["distancia","raster","calculo","variable"],"title":"Calcular la distancia al mar en R","type":"post"},{"authors":["F Mori-Gamarra","L Moure-Rodríguez","X Sureda","C Carbiae","D Royé","A Montes-Martínez","F Cadaveira","F Caamaño-Isorna"],"categories":null,"content":"","date":1545350400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1545350400,"objectID":"73e424dbc4f01c179ac85fe184da7c84","permalink":"/es/publication/alcohol_galicia2018/","publishdate":"2018-12-21T00:00:00Z","relpermalink":"/es/publication/alcohol_galicia2018/","section":"publication","summary":"Objetivo: Valorar la influencia que la densidad de los puntos de venta y los de venta y consumo de alcohol ejercen sobre los patrones de consumo de los/las jóvenes preuniversitarios/as de Galicia. Métodos: Se ha llevado a cabo un análisis transversal de la cohorte de estudiantes de la Universidad de Santiago de Compostela (Cohorte Compostela 2016). Se calcularon las prevalencias de consumo para cada uno de los municipios de procedencia de los/las estudiantes de primer ciclo durante el año anterior al ingreso. Se valoró la asociación del consumo de riesgo de alcohol (CRA) y consumo intensivo de alcohol (CIA) con un modelo logístico, considerando como variables independientes la población del municipio, la densidad de locales de venta, la densidad de locales de venta y consumo de alcohol, y la densidad de ambos tipos de locales en el municipio. Resultados: La prevalencia de CRA fue del 60,5% (interval de confianza del 95% [IC95%]: 58,4-62,5) y la de CIA de 28,5% (IC95%: 26,7-30,2). Se observó una gran variabilidad según el municipio de procedencia. El modelo logístico multivariante mostró que los municipios con una densidad de 8,42-9,34 de ambos tipos de locales por mil habitantes presentaban mayor riesgo de CRA (odds ratio [OR]:1.39; IC95%: 1,09-1,78) y de CIA (OR= 1,29; IC95%: 1,01-1,66). Conclusión: Estos datos sugieren la importancia de incluir la información del entorno al estudiar el consumo de alcohol. Conocer mejor el entorno podría ayudar a plantear políticas que fomenten en la población conductas más saludables.","tags":["Densidad de puntos de venta de alcohol","Alcohol","Consumo de menores de edad","Adolescentes"],"title":"Densidad de los puntos de venta de alcohol y su consumo en jóvenes de Galicia","type":"publication"},{"authors":[],"categories":null,"content":" \nVideos \n   Smoothed daily rainfall throughout the year in Australia. Data: SILO.\n   Smoothed daily maximum temperature throughout the year in Australia. Data: SILO.\n   How do the spatial patterns of daily precipitation change throughout the year in Europe? Data: E-OBS.\n   Smoothed daily maximum temperature throughout the year in the contiguous USA. Data: PRISM.\n   Smoothed daily maximum temperature throughout the year in Europe. Data: E-OBS.\n   How do the spatial patterns of daily precipitation change throughout the year in mainland Spain and the Balearic Islands? Data: SPREAD.\n   Smoothed daily sea surface temperature throughout the year for the Northeast Atlantic, the Mediterranean, North and Black Sea. Data: NOAA/NODC.\n   Probability of a summer day (maximum temperature greater than 25ºC/77ºF) through the year in Europe. Data: E-OBS.\n   Probability of a summer day (maximum temperature greater than 25ºC/77ºF) through the year in the Contiguous United States. Data: PRISM.\n   Probability of a summer day (maximum temperature greater than 25ºC) through the year in Australia. Data: SILO.\n   Probability of a frost day (minimum temperature less than 0ºC) through the year in Europe. Data: E-OBS 18e.\n   Probability of a frost day (minimum temperature less than 0ºC/32ºF) through the year in the Contiguous United States. Data: PRISM. Platform: Google Earth Engine.\n\nGráficos \n\r¿Cómo fue el año 2019? Calendario de viento para el año 2019 en Santiago de Compostela. Applicación: MeteoExtremos Galicia.\r\r\r¿Cómo fue el año 2019? Calendario de la temperatura máxima para el año 2019 en Santiago de Compostela. Applicación: MeteoExtremos Galicia.\r\r\r¿Cómo fue el año 2019? Calendario de precipìtación diaria para el año 2019 en Santiago de Compostela. Applicación: MeteoExtremos Galicia.\r\r\rAnomalías de horas de sol entre 1983-2019 para la Península Ibérica. Datos: EUMETSAT.\r\r\rAnomalías de horas de sol entre 1983-2017 para Francia. Datos: EUMETSAT.\r\r\rAnomalías de horas de sol entre 1983-2017 para Alemania. Datos: EUMETSAT.\r\r\rAnomalías de horas de sol en 2017 para Santiago de Compostela. Datos: EUMETSAT.\r\r\r¿A qué hora se suelen alcanzar las temperaturas máximas o mínimas? Aquí la distribución para Santiago de Compostela. Datos: Meteogalicia.\r\r\r¿A qué hora se suelen alcanzar las temperaturas máximas o mínimas? Aquí la distribución para Vigo. Datos: Meteogalicia.\r\r\rComparison between the period 1950-1979 and 1980-2019 for the average number of days with minimum temperature above 20ºC in Europe. Data: ECA\u0026amp;D.\r\r\rComparison between the period 1950-1979 and 1980-2019 for the average number of days with maximum temperature above 40ºC. Data: ECA\u0026amp;D.\r\r\rComparison between the period 1950-1979 and 1980-2019 for the average number of days with minimum temperature above 20ºC. Data: ECA\u0026amp;D.\r\r\rLa canícula empieza de media a partir del 24 de julio. Es interesante que en la fachada atlántica las temperaturas más altas son habituales a partir del 8 de agosto, un retraso debido al efecto del océano atlántico. Datos: ECA\u0026amp;D.\r\r\rHurricane Irma and Jose. Surface wind conditions on September 6, 2017 at 23:00 PM. Data: ERA-5/Copernicus\r\r\rEx-Hurrican Leslie. Surface wind conditions on October 14, 2018 at 00:00 AM. Data: ERA-5/Copernicus\r\r\rComparison of extreme temperatures 2011-2019 in Spain. Data: AEMET OPEN\r\r\rProbability of a summer day (maximum temperature \u0026gt; 25ºC) throughout the year for three cities in Spain. Data: AEMET OPEN\r\r\rThis April 2020 has been extraordinarily cloudy. 90% of the Iberian Peninsula has experienced a cloudiness greater than 86% (normal would be 65%). Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rEste noviembre 2019 ha sido extraordinariamente nublado. El 90% de la Península Ibérica ha vivido una nubosidad superior al 90% (lo normal serían 70%). Datos: NASA/MODIS. Plataforma: Google Earth Engine.\r\r\rÍndice de Concentración de la precipitación diaria en EEUU. Datos: PRISM. Más detalles aquí.\r\r\rPromedio de Grados Día de Calefacción y Refrigeración (1950-2018) en España. Como valor de referencia he usado 15.5ºC y 22ºC de temperatura media diaria, respectivamente. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Bilbao. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Zaragoza. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Santander. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Málaga. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Sevilla. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Santiago. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Valencia. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Vigo. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en A Coruña. Datos: ECA\u0026amp;D.\r\r\rAnomalías de la temperatura media diaria en invierno de Madrid. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Santander. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Barcelona. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Santiago. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Bilbao. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Sevilla. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de A Coruña. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Málaga. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rPromedio del primer día de verano en Europa (temperatura máxima \u0026gt;25ºC). Datos: E-OSB 18e.\r\r\rDistribución del promedio del primer día de verano en Europa (temperatura máxima \u0026gt;25ºC). Datos: E-OSB 18e.\r\r\rWarming stripes for several Spanish cities. These graphs represent and communicate climate change in a very illustrative and effective way. Data: ECA\u0026amp;D. Time series were homogenized with climatol. More: post.\r\r\r62 years of annual anomalies of precipitation (%) in peninsular Spain in a single graphic. Data: SPREAD.\r\r\rSummer anomaly of temperature and precipitation in Barcelona 1914-2018. Data: ECA\u0026amp;D.\r\r\rSummer anomaly of temperature and precipitation in Madrid 1920-2018. Data: ECA\u0026amp;D.\r\r\rSummer anomaly of temperature and precipitation in Santiago de Compostela 1950-2018. Data: ECA\u0026amp;D.\r\r\rMonthly precipitation anomaly registered in Santiago de Compostela in 2017. Data: ECA\u0026amp;D.\r\r\rMonthly precipitation anomaly registered in Barcelona in 2018. Data: ECA\u0026amp;D, opendata.aemet.es.\r\r\rTrends of first frost days in Barcelona 1950-2016. 12.1 days later each decade. Data: ECA\u0026amp;D.\r\r\rTrends of last frost days in Madrid 1920-2016. Data: ECA\u0026amp;D.\r\r\rTrends of first tropical nights in Barcelona 1950-2016. Data: ECA\u0026amp;D.\r\r\rTrends of last tropical nights in Barcelona 1950-2016. Data: ECA\u0026amp;D.\r\r\rTrends of first tropical night (minimum temperature \u0026gt; 20ºC) in Madrid 1920-2016. -2.3 days earlier each decade. Data: ECA\u0026amp;D.\r\r\rTrends of last tropical night (minimum temperature \u0026gt; 20ºC) in Madrid 1920-2016. 1.9 days later each decade. Data: ECA\u0026amp;D.\r\r\rTrends of frist days with more than 30ºC in several Spanish cities. Data: ECA\u0026amp;D.\r\r\rTrends of last days with more than 30ºC in several Spanish cities. Data: ECA\u0026amp;D.\r\r\rAverage of consecutive days without rainfall 1950-2012. Data: SPREAD.\r\r\rAverage of consecutive days without rainfall by seasons 1950-2012 in the Iberian Peninsula. Data: SPREAD.\r\r\rAnnual precipitation per inhabitant in Europe based on gridded population (1km resolution) and precipitation (0.25º) data. Data: ECA\u0026amp;D, SEDAC.\r\r\rAverage cloud fraction for summer 2018 and normal 2001-2018 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rLand Surface Temperature anomaly for summer 2018 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rAverage Land Surface Temperature for summer 2018 and normal 2001-2018 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rDistribution of temperature anomalies in autumn according to different decades in Barcelona. You can clearly see how the autumn is getting warmer due to global warming. Data: ECA\u0026amp;D.\r\r\rDistribution of temperature anomalies in autumn according to different decades in Santiago de Compostela. You can clearly see how the autumn is getting warmer due to global warming. Data: ECA\u0026amp;D.\r\r\rClimate circles for several Spanish cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: ECA\u0026amp;D.\r\r\rClimate circles for several European cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: ECA\u0026amp;D.\r\r\rGraphic definition of climate and weather. The difference between weather and climate is particularly a scale of time. Single atmospheric conditions over a short period of time is weather, and climate is the statistical description of all these single condicions over a relatively long period of time.\r\r\rClimate circles for several Chilean cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: explorador.cr2.cl.\r\r\rSummer months, mild winters, lot of sun and little wind, the climatic preferences for the Galician population. Map is based on survey results. More: article (galician).\r\r\rWhere is the lightning activity concentrated in a few days in Galicia? Values toward 1 indicate that a few days contribute much of all the lightning; instead, values toward 0 are places where more regularity is observed. More: article.\r\r\rHow is the lightning activity distributed annual and by seasons in Galicia? Data: meteogalicia. More: article.\r\r\rHow is the lightning activity distributed by month and hour in Galicia? Data: meteogalicia. More: article.\r\r\rSeasonal rainfall regime, i.e. ranking seasons according to average precipitation (1950-2017) in descending order in Europe. (P, spring; S, summer; A, autumn; W, winter). Data: ECA\u0026amp;D.\r\r\rSeasonal rainfall regime, i.e. ranking seasons according to average precipitation (1956-2006) in descending order in the contiguous United States. (P, spring; S, summer; A, autumn; W, winter). More: article, dataset.\r\r\rConcentration of Daily Precipitation (1956-2006) in the contiguous United States. (P, spring; S, summer; A, autumn; W, winter). The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. More: article, dataset.\r\r\rAverage cloud fraction for summer 2001-2018 in the contiguous United States. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for winter 2001-2018 in the contiguous United States. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) for different dates in Europe. Data: ECA\u0026amp;D.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) through the year in the pensinular Spain. Data: STEAD from Research Group climayagua.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) for different dates in the pensinular Spain. Data: STEAD from Research Group climayagua.\r\r\rAnnual sunhours for Germany in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: ECA\u0026amp;D, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rWarming stripes for Lisboa. These graphs represent and communicate climate change in a very illustrative and effective way. Data: GISTEMP. More: post.\r\r\rWhere do we observe the trajectories of extratropical cyclones in Europe? Here the frequency for the months October to March between 1979-2010. Data: extra-tropical cyclone tracks.\r\r\rWarming stripes for Madrid. These graphs represent and communicate climate change in a very illustrative and effective way. Data: ECA\u0026amp;D. More: post.\r\r\rAverage cloud fraction for summer 2001-2018 in Germany. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for winter 2001-2018 in Germany. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAnnual sunhours for Galicia (Spain) in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: Meteogalicia, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rAnnual sunhours for Spain in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: ECA\u0026amp;D, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rPotential insolation (sun hours) in Barcelona\u0026rsquo;s city center for July 21 and December 22. The calculation is based on the Sky View Factor. Estimation made using SAGA-GIS. Data: LiDAR-IGN.\r\r\rAverage Diurnal Land Surface Temperature for July 2017 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rAverage Night Land Surface Temperature for July 2017 in Europe. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rNumber of snow days on the ground in the Iberian Peninsula (2002-2017). The daily images with a binary code (condition: Snow_Cover_Daily_Tile == 200, and Fractional_Snow_Cover \u0026gt; 90) have been reclassified and than summed up and divided by the number of years. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rClimate circles for several American cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: NCDC-CDO/NOAA.\r\r\rAverage cloud fraction for summer 2018 and normal 2001-2017 in the Iberian Peninsular. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for march 2018 and march 2001-2018 in the Iberian Peninsular. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAccumulated precipitation of 2017 compared to other years in Valladolid. Data: ECA\u0026amp;D.\r\r\rAccumulated precipitation of 2017 compared to other years in Vigo. Data: ECA\u0026amp;D.\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1544828400,"objectID":"75473ca83a36f50bff87cff6d89ab60a","permalink":"/es/graphs/climate/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/es/graphs/climate/","section":"graphs","summary":" ","tags":["clima","tiempo","visualización","atmosfera","temperatura"],"title":"Clima y tiempo","type":"graphs"},{"authors":[],"categories":null,"content":" \nVideos \n   3d model of Nazaré Canyon off the coast of #Nazare, Portugal. Well known as hotspot for big wave surfing.\n\nGráficos \n\rLas venas azules de la Tierra para la Península Ibérica. La ciencia es arte. El ancho de la línea refleja el tamaño del río. Datos: https://hydrosheds.org/\r\r\rRiver flow directions were estimated using vectorial data.\r\r\rRiver flow directions were estimated using vectorial data.\r\r\rOrientations of coast segements were estimated using vectorial data with a resolución of 9 vertices to 1 km.\r\r\rOrientations of coast segements were estimated using vectorial data with a resolución of 9 vertices to 1 km.\r\r\rMobility trends for places of residence during COVID-19 epidemy in Europe based on data from Google. Data: Google COVID19\r\r\rDriving mobility during COVID-19 epidemy by country based data from Apple. Data: Apple COVID19\r\r\rSignificant wave height with a return period of 10 years. Particularly in the Costa da Morte there is a possibility of waves exceeding 10 meters once every 10 years. The Rias are highly protected. Data: Wave Atlas of Meteogalicia\r\r\rOur research found that, in most Mediterranean European countries, the amount of burnt area is increasingly related with a lower number of #wildfires. I estimated ad hoc the same index for NSW Australia with similar results. More: article.\r\r\rOur research found that, in most Mediterranean European countries, the amount of burnt area is increasingly related with a lower number of #wildfires. I estimated ad hoc the same index for California with similar results. More: article.\r\r\rSpatial patterns of cemeteries in Northwest Spain (number per 1,000 inhabitants) based on OpenStreetMaps. Data: OpenStreetMap, IGE More: post.\r\r\rSpatial patterns of cemeteries in Spain (number per 10,000 inhabitants) based on OpenStreetMaps. Data: OpenStreetMap, INE More: post.\r\r\rUrban growth seen by the year of construction for the six largest Spanish cities. Data: Catastro.\r\r\rAnimation of urban growth seen by the year of construction in Valencia, Spain. Data: Catastro.\r\r\rUrban growth seen by the year of construction in Valencia, Spain. Data: Catastro.\r\r\rDistribution of the year of building construction in Spanish provincial capitals from 1850. Data: Catastro.\r\r\rDistribution of travel time to high-density urban centers via surface transport in Europe by NUTS-1 in 2015. Data: resourcewatch.org.\r\r\rDistribution of travel time to high-density urban centers via surface transport in Spain by province in 2015. Data: resourcewatch.org.\r\r\rIndice de Precios al Consumidor: Alquiler de viviendas (variación anual) 2002-2019 en España por provincia. Datos: INE.\r\r\rTiempo de viaje a centros urbanos de alta densidad a través del transporte de superficie en España. Datos: resourcewatch.org.\r\r\rRanking del tiempo de viaje a centros urbanos de alta densidad a través del transporte de superficie según provincias españolas. Datos: resourcewatch.org.\r\r\rInspirado por el gran trabajo de @geo_coe, he creado un mapa de elevación para el río serpenteante Ebro, tramo medio entre Logroño y Zaragoza en España. Datos: Modelo Digital del Terreno - MDT05.\r\r\rInspirado por el gran trabajo de @geo_coe, he creado un mapa de elevación para el río serpenteante Alagón, afluente más largo del río Tajo en España. Datos: Modelo Digital del Terreno - MDT05.\r\r\rUrban growth of Santiag de Compostela from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rFlight routes of the ten busiest airports by passenger traffic in Europe based on 24 hour data for each airport. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rThe Sky View Factor is very useful urban spatial indicator for radiation and thermal environmental assessment. SVF describes how visible is the sky (0: the entire sky is blocked from view; 1: free view on the whole sky). Estimation made using SAGA-GIS. Data: LiDAR-IGN.\r\r\rEuropean flight density based on 24 hour data for each of the ten busiest airports by passenger. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of Frankfurt airport based on 24 hour data. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of the busiest airports in the Iberian Peninsula based on 24 hour data. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of Paris Charles de Gaulle airport based on 24 hour data. Data: https://www.flightradar24.com/. More: link\r\r\rUrban growth of Madrid from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rTotal hours of fishing activity per km2 for the year 2016 in the Iberian Peninsula. Data: https://globalfishingwatch.org/\r\r\rTotal hours of fishing activity per km2 for the year 2016 in the Mediterranean. Data: https://globalfishingwatch.org/\r\r\rDistribution of gas stations (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of drinking water (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (spanish),script\r\r\rDistribution of gas and charging stations (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of drinking water (Point Of Interest) in the World, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (Spanish),script\r\r\rAnother perspective on the world. Distance to the sea (the more black, the further away is the sea). Euclidean distance estimation made with R. More: article (Spanish)\r\r\rNumber of bars per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (Spanish),script\r\r\rNumber of Cafes per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of building heights at 10 meter resolution in European capitals. Data: COPERNICUS\r\r\rDifferences of building heights at 10 meter resolution in European capitals. Data: COPERNICUS\r\r\rUrban growth of Barcelona from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rNumber of published articles in ElPaís by year for the term \u0026lsquo;wildfire\u0026rsquo;. Data: elpais\r\r\rDistribution of fastfood restaurants in the contiguous United States, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of fastfood restaurants per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of pharmacies per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rSpain leads access to Open Data in the EU. Data: europendataportal\r\r\rNumber of pubs per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of fastfood restaurants in Europe., extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of restaurants per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of Kebab restaurants per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1515193200,"objectID":"50475b73777bcefbc7c28a465867fada","permalink":"/es/graphs/geography/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/es/graphs/geography/","section":"graphs","summary":" ","tags":["visualización","geografía","distribución","humana","física"],"title":"Geografía","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rDaily contribution of coal to electricity generation in Germany. Data: energy-charts.de\r\r\rLa contribución diaria del carbón a la generación eléctrica en España desde 2011. Hasta el 17 de agosto de 2019 no ha habido ningún día con menos del 1%. Data: REData\r\r\rLight pollution by municipality in 2015. What is the municipality that emits the most artificial light? The map shows the Coefficient of Variation (standard deviation/mean) of each municipalities. Data: VIIRS Nighttime Lights NOAA (2015).\r\r\rWhich coast has more light pollution in Peninsular Spain? The coast margins include a buffer of 5km. Data: VIIRS Nighttime Lights NOAA (2016).\r\r\rLight pollution by municipality in 2015. What is the municipality that emits the most artificial light? The map shows the Coefficient of Variation (standard deviation/mean) of each municipalities. Data: VIIRS Nighttime Lights NOAA (2015).\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in the Iberian Peninsula. Clearly stand out Barcelona and Madrid. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in Germany. Clearly stands out the Rhine-Ruhr metropolitan region. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in the contiguous United States. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAir pollution in the Iberian Peninsula. Annual average of PM2.5 for 2016 seen by MODIS/MISR/SeaWiFS. Clearly visible are Madrid and Barcelona. Data: SEDAC.\r\r\rOur human footprint in the Iberian Peninsula, or rather, the pressure we exert on the terrestrial ecosystem. Data: Global terrestrial Human Footprint .\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1544050800,"objectID":"0a4eb2a3127a9eac4b4ad1d7d8244d3c","permalink":"/es/graphs/environment/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/es/graphs/environment/","section":"graphs","summary":" ","tags":["visualización","ambiente","natura","contaminación","ecosistema"],"title":"Medio ambiente","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rPopulation movement registered with mobile phones at 12:00 in the morning compared to 20:00 in Spain. Data: INE\r\r\rPopulation movements registered with mobile phones in the COVID19 pandemic in Spain. Data: INE\r\r\rPopulation movements registered with mobile phones in november 2019 in Spain. Data: INE\r\r\rPopulation point clouds of Ourense since 1975. Data: IGE\r\r\rPopulation pyramid of Galicia since 1975. Data: IGE\r\r\rPopulation pyramid of Galician provinces since 1975. Data: IGE\r\r\rPopulation pyramid of Spanish autonomous community since 1998. Data: INE\r\r\rMigration flows between Spanish autonomous communities in 2008. Data: INE\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1544050800,"objectID":"3110f041ae5e0db11189de7b98b09047","permalink":"/es/graphs/population/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/es/graphs/population/","section":"graphs","summary":" ","tags":["visualización","demografía","humana","envejecimiento"],"title":"Populación","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rEvolution of adult obesity in Europe between 1975 and 2016. Data: ourwoldindata, script.\r\r\rNational Overdose Death in the US by different drugs since 1999, showing a horrifying trend. Data: ourwoldindata.\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1515193200,"objectID":"311610b04f97c3287614999a5b749d99","permalink":"/es/graphs/health/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/es/graphs/health/","section":"graphs","summary":" ","tags":["visualización","salud","población","enfermedad","global"],"title":"Salud","type":"graphs"},{"authors":null,"categories":["visualización","R","R:elemental"],"content":"\rEste año se hicieron muy famosos en todo el mundo los llamados warming stripes, las tiras del calentamiento global, que fueron creadas por el científico Ed Hawkins de la Universidad de Reading. Estos gráficos representan y comunican el cambio climático de una forma muy ilustrativa y eficaz.\nVisualising global temperature change since records began in 1850. Versions for USA, central England \u0026amp; Toronto available too: https://t.co/H5Hv9YgZ7v pic.twitter.com/YMzdySrr3A\n\u0026mdash; Ed Hawkins (@ed_hawkins) May 23, 2018  A partir de su idea, creé tiras para ejemplos de España, como el siguiente de Madrid.\n#Temperatura anual en #MadridRetiro desde 1920 a 2017. #CambioClimatico #dataviz #ggplot2 (idea de @ed_hawkins 🙏) @Divulgameteo @edupenabad @climayagua @ClimaGroupUB @4gotas_com pic.twitter.com/wmLb5uczpT\n\u0026mdash; Dominic Royé (@dr_xeo) June 2, 2018  En este post voy a enseñar cómo se pueden crear estas tiras en R con el paquete ggplot2. Aunque debo decir que existen muchos caminos en R que nos pueden llevar al mismo resultado o a uno similar, incluso dentro de ggplot2.\nDatos\rEn este caso usaremos las temperaturas anuales de Lisboa del GISS Surface Temperature Analysis que comprenden el periodo 1880-2018. Se trata de series temporales homogeneizadas. También se podrían usar temperaturas mensuales u otras series temporales. El archivo se puede descargar aquí. Lo primero que debemos hacer, siempre y cuando no lo hayamos hecho, es instalar la colección de paquetes tidyverse que incluyen también ggplot2. Además, nos hará falta el paquete lubridate para el tratamiento de fechas. Después, importamos los datos de Lisboa que están en formato csv.\n#instalamos los paquetes lubridate y tidyverse\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\r#paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(RColorBrewer)\r#importar las temperaturas anuales\rtemp_lisboa \u0026lt;- read_csv(\u0026quot;temp_lisboa.csv\u0026quot;)\rstr(temp_lisboa)\r## Classes \u0026#39;spec_tbl_df\u0026#39;, \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 139 obs. of 18 variables:\r## $ YEAR : num 1880 1881 1882 1883 1884 ...\r## $ JAN : num 9.17 11.37 10.07 10.86 11.16 ...\r## $ FEB : num 12 11.8 11.9 11.5 10.6 ...\r## $ MAR : num 13.6 14.1 13.5 10.5 12.4 ...\r## $ APR : num 13.1 14.4 14 13.8 12.2 ...\r## $ MAY : num 15.7 17.3 15.6 14.6 16.4 ...\r## $ JUN : num 17 19.2 17.9 17.2 19.1 ...\r## $ JUL : num 19.1 21.8 20.3 19.5 21.4 ...\r## $ AUG : num 20.6 23.5 21 21.6 22.4 ...\r## $ SEP : num 20.7 20 18 18.8 19.5 ...\r## $ OCT : num 17.9 16.3 16.4 15.8 16.4 ...\r## $ NOV : num 12.5 14.7 13.7 13.5 12.5 ...\r## $ DEC : num 11.07 9.97 10.66 9.46 10.25 ...\r## $ D-J-F : num 10.7 11.4 10.6 11 10.4 ...\r## $ M-A-M : num 14.1 15.2 14.3 12.9 13.6 ...\r## $ J-J-A : num 18.9 21.5 19.7 19.4 20.9 ...\r## $ S-O-N : num 17 17 16 16 16.1 ...\r## $ metANN: num 15.2 16.3 15.2 14.8 15.3 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. YEAR = col_double(),\r## .. JAN = col_double(),\r## .. FEB = col_double(),\r## .. MAR = col_double(),\r## .. APR = col_double(),\r## .. MAY = col_double(),\r## .. JUN = col_double(),\r## .. JUL = col_double(),\r## .. AUG = col_double(),\r## .. SEP = col_double(),\r## .. OCT = col_double(),\r## .. NOV = col_double(),\r## .. DEC = col_double(),\r## .. `D-J-F` = col_double(),\r## .. `M-A-M` = col_double(),\r## .. `J-J-A` = col_double(),\r## .. `S-O-N` = col_double(),\r## .. metANN = col_double()\r## .. )\rVemos en las columnas que tenemos valores mensuales y estacionales, y el valor anual. Pero antes de proceder a visualizar la temperatura anual, debemos sustituir los valores ausentes 999.9 por NA, usando la función ifelse( ) que lleva una condición y los argumentos correspondientes a verdadero y falso de la condición dada.\n#selecionamos la columna del año y la temperatura anual\rtemp_lisboa_yr \u0026lt;- select(temp_lisboa, YEAR, metANN)\r#cambiamos el nombre de la columna temp_lisboa_yr \u0026lt;- rename(temp_lisboa_yr, ta = metANN)\r#valores ausentes 999.9\rsummary(temp_lisboa_yr) \r## YEAR ta ## Min. :1880 Min. : 14.53 ## 1st Qu.:1914 1st Qu.: 15.65 ## Median :1949 Median : 16.11 ## Mean :1949 Mean : 37.38 ## 3rd Qu.:1984 3rd Qu.: 16.70 ## Max. :2018 Max. :999.90\rtemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, ta = ifelse(ta == 999.9, NA, ta))\rCuando usamos el año como variable, no solemos convertirlo en un objeto de fecha, no obstante es aconsejable. De este modo nos permite usar las funciones de fechas del paquete lubridate y las funciones de apoyo dentro de ggplot2. La función str_c( ) del paquete stringr, forma parte de la colección de tidyverse, es similar a paste( ) de R Base que nos permite combinar caracteres especificando un separador (sep=“-”). La función ymd( ) (year month day) del paquete lubridate convierte una fecha en un objeto Date. Es posible combinar varias funciones haciendo uso del pipe operator %\u0026gt;% que ayuda a encadenar sin asignar el resultado a un nuevo objeto. Su uso es muy extendido especialmente con el paquete tidyverse. Si quieres saber más de su uso, aquí tienes un tutorial.\ntemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, date = str_c(YEAR, \u0026quot;01-01\u0026quot;, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd())\r\rCreando las tiras\rPrimero, creamos el estilo del gráfico, especificando todo los argumentos del aspecto que queremos ajustar. Partimos del estilo por defecto de theme_minimal( ). Además, asignamos los colores procedientes de RColorBrewer a un objeto col_srip. Más información sobre los colores usados aquí.\ntheme_strip \u0026lt;- theme_minimal()+\rtheme(axis.text.y = element_blank(),\raxis.line.y = element_blank(),\raxis.title = element_blank(),\rpanel.grid.major = element_blank(),\rlegend.title = element_blank(),\raxis.text.x = element_text(vjust = 3),\rpanel.grid.minor = element_blank(),\rplot.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;)\r)\rcol_strip \u0026lt;- brewer.pal(11, \u0026quot;RdBu\u0026quot;)\rbrewer.pal.info\r## maxcolors category colorblind\r## BrBG 11 div TRUE\r## PiYG 11 div TRUE\r## PRGn 11 div TRUE\r## PuOr 11 div TRUE\r## RdBu 11 div TRUE\r## RdGy 11 div FALSE\r## RdYlBu 11 div TRUE\r## RdYlGn 11 div FALSE\r## Spectral 11 div FALSE\r## Accent 8 qual FALSE\r## Dark2 8 qual TRUE\r## Paired 12 qual TRUE\r## Pastel1 9 qual FALSE\r## Pastel2 8 qual FALSE\r## Set1 9 qual FALSE\r## Set2 8 qual TRUE\r## Set3 12 qual FALSE\r## Blues 9 seq TRUE\r## BuGn 9 seq TRUE\r## BuPu 9 seq TRUE\r## GnBu 9 seq TRUE\r## Greens 9 seq TRUE\r## Greys 9 seq TRUE\r## Oranges 9 seq TRUE\r## OrRd 9 seq TRUE\r## PuBu 9 seq TRUE\r## PuBuGn 9 seq TRUE\r## PuRd 9 seq TRUE\r## Purples 9 seq TRUE\r## RdPu 9 seq TRUE\r## Reds 9 seq TRUE\r## YlGn 9 seq TRUE\r## YlGnBu 9 seq TRUE\r## YlOrBr 9 seq TRUE\r## YlOrRd 9 seq TRUE\rPara el gráfico final usamos la geometría geom_tile( ). Como los datos no tienen un valor específico para el eje Y, usamos un valor dummy, aquí es 1. Además, ajusto el ancho de la barra de colores en la leyenda.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1,fill = ta))+\rgeom_tile()+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_continuous(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rguides(fill = guide_colorbar(barwidth = 1))+\rlabs(title = \u0026quot;LISBOA 1880-2018\u0026quot;,\rcaption = \u0026quot;Datos: GISS Surface Temperature Analysis\u0026quot;)+\rtheme_strip\rEn el caso de que quisieramos obtener únicamente las tiras, podemos usar theme_void( ) y el argumento show.legend=FALSE en geom_tile( ) para eliminar todos los elementos de estilo. También podemos cambiar el color para los valores NA, incluyendo el argumento na.value=“grey70” en la función scale_fill_gradientn( ).\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile(show.legend = FALSE)+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_discrete(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rtheme_void()\r\r","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1543968000,"objectID":"1586fbefaa1189d4ab4003f1df033d79","permalink":"/es/2018/c%C3%B3mo-crear-warming-stripes-in-r/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/es/2018/c%C3%B3mo-crear-warming-stripes-in-r/","section":"post","summary":"Este año se hicieron muy famosos en todo el mundo los llamados warming stripes, las tiras del calentamiento global, que fueron creados por el científico Ed Hawkins de la Universidad de Reading. Estos gráficos representan y comunican el cambio climático de una forma muy ilustrativa y eficaz.","tags":["ggplot2","warming stripes","calentamiento global"],"title":"Cómo crear 'Warming Stripes' in R","type":"post"},{"authors":null,"categories":["visualización","R:elemental","R","mapping"],"content":"\rLa base de datos de Open Street Maps\rRecientemente creé un mapa de la distribución de gasolineras y estaciones de carga eléctrica en Europa.\nPopulation density through the number of gas stations in Europe. #dataviz @AGE_Oficial @mipazos @simongerman600 @openstreetmap pic.twitter.com/eIUx2yn7ej\n\u0026mdash; Dominic Royé (@dr_xeo) February 25, 2018  ¿Cómo se puede obtener estos datos?\nPues, en este caso usé puntos de interés (PDIs) de la base de datos de Open Street Maps (OSM). Obviamente OSM no sólo contiene las carreteras, sino también información que nos puede ser útil a la hora de usar un mapa, como por ejemplo las ubicaciones de hospitales o gasolineras. Para evitar la descarga de todo el OSM y extraer la información requerida, se puede hacer uso de una overpass API, que nos permite hacer consultas a la base de datos de OSM con nuestros propios criterios.\nUna forma fácil de acceder a una overpass API es a través de overpass-turbo.eu, que incluso incluye un asistente para construir una consulta y muestra los resultados sobre un mapa interactivo. Una explicación detallada de la página anterior la podemos encontrar aquí.\rSin embargo, tenemos a nuestra disposicón el paquete osmdata que nos permite crear y hacer las consultas directamente desde el entorno de R. Aún así, el uso de la overpass-turbo.eu puede ser útil cuando no estamos seguros de lo que buscamos o tenemos alguna dificultad en construir la consulta.\n\rAcceso a la overpass API desde R\rEl primer paso, que debemos seguir, es instalar varios paquetes, en el caso de que no estén instaldos. En casi todos mis scripts hago uso de tidyverse que es una colección fundamental de distintos paquetes, incluyendo dplyr (manipulación de datos), ggplot2 (visualización), etc. El paquete sf es el nuevo estándar para trabajar con datos espaciales y es compatible con ggplot2 y dplyr. Por último, ggmap nos facilita el trabajo para crear mapas.\n#instalamos los paquetes osmdata, sf, tidyverse y ggmap\rif(!require(\u0026quot;osmdata\u0026quot;)) install.packages(\u0026quot;osmdata\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggmap\u0026quot;)) install.packages(\u0026quot;ggmap\u0026quot;)\r#cargamos las librerías\rlibrary(tidyverse)\rlibrary(osmdata)\rlibrary(sf)\rlibrary(ggmap)\r\rConstruir una consulta\rAntes de crear una consulta, debemos conocer qué podemos filtrar. La función available_features( ) nos devuelve un listado amplio de las características disponibles en OSM que a su vez tienen diferentes categorías (tags). Están disponibles más detalles en la wiki de OSM aquí.\rPor ejemplo, la característica shop contiene como categoría entre otros supermarket, fishing, books, etc.\n#las primeras cinco características head(available_features())\r## [1] \u0026quot;4wd only\u0026quot; \u0026quot;abandoned\u0026quot; \u0026quot;abutters\u0026quot; \u0026quot;access\u0026quot; \u0026quot;addr\u0026quot; \u0026quot;addr:city\u0026quot;\r#instalaciones y establecimientos públicos\rhead(available_tags(\u0026quot;amenity\u0026quot;))\r## [1] \u0026quot;animal_boarding\u0026quot; \u0026quot;animal_shelter\u0026quot; \u0026quot;arts_centre\u0026quot; \u0026quot;atm\u0026quot; ## [5] \u0026quot;baby_hatch\u0026quot; \u0026quot;baking_oven\u0026quot;\r#tiendas\rhead(available_tags(\u0026quot;shop\u0026quot;))\r## [1] \u0026quot;agrarian\u0026quot; \u0026quot;alcohol\u0026quot; \u0026quot;anime\u0026quot; \u0026quot;antiques\u0026quot; \u0026quot;appliance\u0026quot; \u0026quot;art\u0026quot;\rLa primera consulta: ¿Dónde podemos encontrar cines en Madrid?\rPara construir la consulta se hace uso del pipe operator %\u0026gt;% que ayuda a encadenar varias funciones sin asignar el resultado a un nuevo objeto. Su uso es muy extendido especialmente con el paquete tidyverse. Si quieres saber más de su uso, aquí tienes un tutorial.\nEn la primera parte de la consulta debemos indicar el lugar donde queremos extraer la información. La función getbb( ) crea un rectángulo de selección para un lugar dado, buscando el nombre. La función principal es opq( ) que construye la consulta final. Añadimos con la función add_osm_feature( ) nuestros criterios de filtro. En esta primera consulta buscaremos cines en Madrid. Por eso, usamos como clave amenity y como categoría cinema. Existen varios formatos para obtener el resultado de la consulta. La función osmdata_*( ) envía la consulta al servidor y en función del sufijo * sf/sp/xml nos devuelve el formato simple feature, spatial o XML.\n#construcción de la consulta\rq \u0026lt;- getbb(\u0026quot;Madrid\u0026quot;) %\u0026gt;%\ropq() %\u0026gt;%\radd_osm_feature(\u0026quot;amenity\u0026quot;, \u0026quot;cinema\u0026quot;)\rstr(q) #la estructura de la consulta\r## List of 4\r## $ bbox : chr \u0026quot;40.3119774,-3.8889539,40.6437293,-3.5179163\u0026quot;\r## $ prefix : chr \u0026quot;[out:xml][timeout:25];\\n(\\n\u0026quot;\r## $ suffix : chr \u0026quot;);\\n(._;\u0026gt;;);\\nout body;\u0026quot;\r## $ features: chr \u0026quot; [\\\u0026quot;amenity\\\u0026quot;=\\\u0026quot;cinema\\\u0026quot;]\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr [1:2] \u0026quot;list\u0026quot; \u0026quot;overpass_query\u0026quot;\rcinema \u0026lt;- osmdata_sf(q)\rcinema\r## Object of class \u0026#39;osmdata\u0026#39; with:\r## $bbox : 40.3119774,-3.8889539,40.6437293,-3.5179163\r## $overpass_call : The call submitted to the overpass API\r## $meta : metadata including timestamp and version numbers\r## $osm_points : \u0026#39;sf\u0026#39; Simple Features Collection with 197 points\r## $osm_lines : NULL\r## $osm_polygons : \u0026#39;sf\u0026#39; Simple Features Collection with 12 polygons\r## $osm_multilines : NULL\r## $osm_multipolygons : NULL\rVemos que el resultado es una lista de distintos objetos espaciales. En nuestro caso únicamente nos interesaría osm_points.\n¿Cómo podemos visulizar estos puntos?\nLa ventaja de objetos sf es que para ggplot2 existe una geometría propia geom_sf( ). Además, haciendo uso de ggmap podemos incluir un mapa de fondo. La función get_map( ) descarga el mapa para un lugar dado. En lugar puede ser una dirección, latitud/longitud o un rectángulo de selección. El argumento maptype nos permite indicar el estilo o tipo de mapa. Podemos consultar más detalles en la ayuda de la función ?get_map.\nCuando construimos un gráfico con ggplot habitualmente empezamos con ggplot( ). En este caso, se empieza por ggmap( ) que incluye el objeto con nuestro mapa de fondo. Después añadimos con geom_sf( ) los puntos de los cines en Madrid. Es importante indicar con el argumento inherit.aes=FALSE que debe usar aesthetic mappings del objeto espacial osm_points. Además, indicamos el color (colour, fill), transparencia (alpha), tipo y tamaño (size) del círculo.\n#nuestro mapa de fondo\rmad_map \u0026lt;- get_map(getbb(\u0026quot;Madrid\u0026quot;), maptype = \u0026quot;toner-background\u0026quot;)\r#mapa final\rggmap(mad_map)+\rgeom_sf(data = cinema$osm_points,\rinherit.aes = FALSE,\rcolour = \u0026quot;#238443\u0026quot;,\rfill = \u0026quot;#004529\u0026quot;,\ralpha = .5,\rsize = 4,\rshape = 21)+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;)\r\r¿Dónde están los supermercados de Mercadona?\rEn lugar de obtener un rectángulo de selección con la función getbb( ) podemos construir nuestro propio. Para ello, creamos un vector de cuatro elementos, siendo aquí el orden Oeste/Sur/Este/Norte. En la consulta usamos dos características: name y shop para poder filtrar supermercados que sean de esta marca en concreto. En función del area o bien del volumen que tenga la consulta, es necesario ampliar el tiempo de espera. Por defecto, son 25 segundo (timeout).\nEl mapa que creamos en este caso se basa únicamente en los puntos de supermercados. Por eso, usamos la gramática habitual añadiendo la geometría geom_sf( ). La función theme_void( ) elimina todo con excepción de los puntos.\n#rectángulo de selección para la Península Ibérica\rm \u0026lt;- c(-10, 30, 5, 46)\r#construcción de la consulta\rq \u0026lt;- m %\u0026gt;% opq (timeout = 25*100) %\u0026gt;%\radd_osm_feature(\u0026quot;name\u0026quot;, \u0026quot;Mercadona\u0026quot;) %\u0026gt;%\radd_osm_feature(\u0026quot;shop\u0026quot;, \u0026quot;supermarket\u0026quot;)\rstr(q) #estructura de la consulta\r## List of 4\r## $ bbox : chr \u0026quot;30,-10,46,5\u0026quot;\r## $ prefix : chr \u0026quot;[out:xml][timeout:2500];\\n(\\n\u0026quot;\r## $ suffix : chr \u0026quot;);\\n(._;\u0026gt;;);\\nout body;\u0026quot;\r## $ features: chr [1:2] \u0026quot; [\\\u0026quot;name\\\u0026quot;=\\\u0026quot;Mercadona\\\u0026quot;]\u0026quot; \u0026quot; [\\\u0026quot;shop\\\u0026quot;=\\\u0026quot;supermarket\\\u0026quot;]\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr [1:2] \u0026quot;list\u0026quot; \u0026quot;overpass_query\u0026quot;\r#consulta mercadona \u0026lt;- osmdata_sf(q)\r#mapa final del resultado\rggplot(mercadona$osm_points)+\rgeom_sf(colour = \u0026quot;#08519c\u0026quot;,\rfill = \u0026quot;#08306b\u0026quot;,\ralpha = .5,\rsize = 1,\rshape = 21)+\rtheme_void()\r\r\r","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1541203200,"objectID":"ce0faf06485edd221cd537202535fa95","permalink":"/es/2018/acceso-a-la-base-de-datos-de-openstreetmaps-desde-r/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/es/2018/acceso-a-la-base-de-datos-de-openstreetmaps-desde-r/","section":"post","summary":"Recientemente creé un mapa de la distribución de las gasolineras y estaciones de carga eléctrica en Europa. ¿Cómo se puede obtener estos datos? Pues, en este caso usé puntos de interés (PDIs) de la base de datos de *Open Street Maps* (OSM). Obviamente OSM no sólo contiene las carreteras sino también información que nos puede ser útil a la hora de usar un mapa como por ejemplo las ubicaciones de hospitales o gasolineras.","tags":["base de datos","overpass API","OSM","Puntos de interés"],"title":"Acceso a la base de datos de OpenStreetMaps desde R","type":"post"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","J Bech","FS Rodrigo"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1541030400,"objectID":"1e0e817f6e2eb2b129a09511287060fa","permalink":"/es/publication/appliedgeophysics_2017/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/es/publication/appliedgeophysics_2017/","section":"publication","summary":"The Eastern Mediterranean is one of the most prominent hot spots of climate change in the world and extreme climatic phenomena in this region such as drought or extreme rainfall events are expected to become more frequent and intense. In this study climate extreme indices recommended by the joint World Meteorological Organization Expert Team on Climate Change Detection and Indices are calculated for daily precipitation data in 70 weather stations during 1961–2012. Observed trends and changes in daily precipitation extremes over the EM basin were analysed using the RClimDex package, which was developed by the Climate Research Branch of the Meteorological Service of Canada. Extreme and heavy precipitation events showed globally a statistically significant decrease in the Eastern Mediterranean and, in the southern parts, a significant decrease in total precipitation. The overall analysis of extreme precipitation indices reveals that decreasing trends are generally more frequent than increasing trends. We found statistically significant decreasing trends (reaching 74% of stations for extremely wet days) and increasing trends (reaching 36% of stations for number of very heavy precipitation days). Finally, most of the extreme precipitation indices have a statistically significant positive correlation with annual precipitation, particularly the number of heavy and very heavy precipitation days.","tags":["Eastern Mediterranean","extreme precipitation","trend","spatial temporal distribution"],"title":"Observed Changes in Daily Precipitation Extremes at Annual Timescale Over the Eastern Mediterranean During 1961–2012","type":"publication"},{"authors":null,"categories":["R","R:intermedio"],"content":"\r\r1 Introducción\r2 NCEP\r2.1 Paquetes\r2.2 Descarga de datos\r2.3 Promedio mensual\r2.4 Visualización\r\r3 ERA-Interim\r3.1 Instalación\r3.2 Conexión y descarga con la ECMWF API\r3.3 Procesar ncdf\r\r4 Actualización para acceder ERA-5\r\r\rUn amigo me propuso que presentara los niveles de aprendizaje de R como categorías. Una idea que ahora introduzco para cada entrada del blog. Hay tres niveles: elemental, intermedio y avanzado. Espero que ayude al lector y al usuario R.\n1 Introducción\rEn este post enseñaré cómo podemos descargar y trabajar directamente con datos provenientes de los reanálisis climáticos en R. Se trata de sistemas de asimilación de datos que combinan modelos de pronóstico meteorológico y observaciones de distintas fuentes de forma objetiva con el fin de sintetizar el estado actual y la evolución de multiples variables de la atmósfera, la superficie de la tierra y los océanos. Los dos reanálisis más usados son NCEP-DO (Reanalysis II) de la NOAA/OAR/ESRL, una versión mejorada de NCEP-NCAR (Reanalysis I), y ERA-Interim del ECMWF. Dado que NCEP-DO es de la primera generacióm, se recomienda usar reanálisis de tercera generación, especialmente ERA-Interim. Una visión general de los actuales reanálisis atmosféricos la podemos encontrar aquí. Primero vamos a ver cómo acceder a los datos del NCEP a través de un paquete de R en CRAN que facilita la descarga y el manejo de los datos. Después haremos lo mismo con ERA-Interim, no obstante, para acceder a este último dataset de reanálisis es necesario usar python y la correspondiente API del ECMWF.\n\r2 NCEP\rPara acceder a los reanálisis del NCEP es necesario instalar el paquete correspondiente RNCEP. La función principal es NCEP.gather( ). La resolución del reanálisis del NCEP es de 2,5º X 2,5º.\n2.1 Paquetes\r#instalamos los paquetes RNCEP, lubridate y tidyverse\rif(!require(\u0026quot;RNCEP\u0026quot;)) install.packages(\u0026quot;RNCEP\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\r#cargamos las librerías\rlibrary(RNCEP)\rlibrary(lubridate) #la necesitamos para manipular fechas\rlibrary(tidyverse) #para visualizar y manipular library(RColorBrewer) #colores para la visualización\rlibrary(sf) #para importar un shapefile y trabajar con geom_sf\r\r2.2 Descarga de datos\rDescargaremos la temperatura del aire a la altura de 850haPa para el año 2016. Las variables y niveles de presión pueden ser consultados en los detalles de la función ?NCEP.gather. El argumento reanalysis2 nos permite descargar tanto la versión I como la versión II, siendo por defecto FALSE, o sea, se accede al reanálisis I. En todas las consultas obtendremos datos horarios de cada 6 horas (00:00, 06:00, 12:00 y 18:00). Esto supone un total de 1464 valores para el año 2016.\n#definimos los argumentos necesarios\rmonth_range \u0026lt;- c(1,12) #período de meses\ryear_range \u0026lt;- c(2016,2016) #período de años\rlat_range \u0026lt;- c(30,60) #rango de latitud\rlon_range \u0026lt;- c(-30,50) #rango de longitud\rdata \u0026lt;- NCEP.gather(\u0026quot;air\u0026quot;, #nombre de la variable\r850, #altura 850hPa\rmonth_range,year_range,\rlat_range,lon_range,\rreturn.units = TRUE,\rreanalysis2=TRUE)\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r#dimensiones dim(data) \r## [1] 13 33 1464\r#encontramos en dimnames( ) lon,lat y tiempo\r#fechas y horas date_time \u0026lt;- dimnames(data)[[3]]\rdate_time \u0026lt;- ymd_h(date_time)\rhead(date_time)\r## [1] \u0026quot;2016-01-01 00:00:00 UTC\u0026quot; \u0026quot;2016-01-01 06:00:00 UTC\u0026quot;\r## [3] \u0026quot;2016-01-01 12:00:00 UTC\u0026quot; \u0026quot;2016-01-01 18:00:00 UTC\u0026quot;\r## [5] \u0026quot;2016-01-02 00:00:00 UTC\u0026quot; \u0026quot;2016-01-02 06:00:00 UTC\u0026quot;\r#longitud y latitud\rlat \u0026lt;- dimnames(data)[[1]]\rlon \u0026lt;- dimnames(data)[[2]]\rhead(lon);head(lat)\r## [1] \u0026quot;-30\u0026quot; \u0026quot;-27.5\u0026quot; \u0026quot;-25\u0026quot; \u0026quot;-22.5\u0026quot; \u0026quot;-20\u0026quot; \u0026quot;-17.5\u0026quot;\r## [1] \u0026quot;60\u0026quot; \u0026quot;57.5\u0026quot; \u0026quot;55\u0026quot; \u0026quot;52.5\u0026quot; \u0026quot;50\u0026quot; \u0026quot;47.5\u0026quot;\r\r2.3 Promedio mensual\rVemos que se trata de un array de tres dimensiones con [lat,lon,tiempo]. Además, extraemos latitud, longitud y el tiempo. La temperatura está dada en Kelvin. El objetivo aquí será mostrar dos mapas comparando enero y julio de 2016.\n#creamos nuestra variable de agrupación group \u0026lt;- month(date_time) #estimamos el promedio por mes de la temperatura\rdata_month \u0026lt;- aperm(\rapply(\rdata, #nuestros datos\rc(1,2), #aplicamos a cada serie temporal 1:fila, 2:columna la función mean( )\rby, #agrupar por group, #meses como agrupación\rfunction(x)ifelse(all(is.na(x)),NA,mean(x))),\rc(2,3,1)) #reordenamos para obtener un array como el original\rdim(data_month) #temperatura 850haP por mes enero a diciembre\r## [1] 13 33 12\r\r2.4 Visualización\rAhora, podemos visualizar con ggplot2 la temperatura de enero y julio. En este ejemplo, uso geom_sf( ) del paquetes sf, que hace el trabajo más fácil para visualizar en ggplot objetos espaciales (en el futuro haré un post sobre sf y ggplot). En la dimensión de latitud y longitud vemos que únicamente nos indica para cada fila y columna un valor. Pero necesitamos las coordenadas de todas las celdas de la matriz. Para crear todas las combinaciones entre dos variables usamos la función expand.grid( ).\n#primero creamos todas las combinaciones de lonlat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#lonlat es carácter, ya que fue un nombre, por eso lo convertimos en númerico\rlonlat \u0026lt;- apply(lonlat,2,as.numeric)\r#lon y lat no están en el orden como lo esperamos\r#fila=lon; columna=lat\rdata_month \u0026lt;- aperm(data_month,c(2,1,3))\r#subtraemos 273.15K para convertir K a ºC.\rdf \u0026lt;- data.frame(lonlat,\rTa01=as.vector(data_month[,,1])-273.15,\rTa07=as.vector(data_month[,,7])-273.15)\rAntes de visualizar los datos con ggplot2, tenemos que adpatar la tabla. El shapefile con los limites de los países se puede descargar aquí.\n#convertimos la tabla ancha en una larga\rdf \u0026lt;- gather(df,month,Ta,Ta01:Ta07)%\u0026gt;%\rmutate(month=factor(month,unique(month),c(\u0026quot;Jan\u0026quot;,\u0026quot;Jul\u0026quot;)))\r#importamos el limite de países\rlimit \u0026lt;- st_read(\u0026quot;CNTR_RG_03M_2014.shp\u0026quot;)\r## Reading layer `CNTR_RG_03M_2014\u0026#39; from data source `C:\\Users\\xeo19\\Documents\\GitHub\\blogR_update\\content\\post\\es\\2018-09-15-acceso-a-datos-de-los-reanalisis-desde-r\\CNTR_RG_03M_2014.shp\u0026#39; using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 256 features and 3 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.66068\r## epsg (SRID): NA\r## proj4string: +proj=longlat +ellps=GRS80 +no_defs\r#gama de colores\rcolbr \u0026lt;- brewer.pal(11,\u0026quot;RdBu\u0026quot;)\rggplot(df)+\rgeom_tile(aes(lon,lat,fill=Ta))+ #temperatura\rgeom_sf(data=limit,fill=NA,size=.5)+ #limite\rscale_fill_gradientn(colours=rev(colbr))+\rcoord_sf(ylim=c(30,60),xlim=c(-30,50))+\rscale_x_continuous(breaks=seq(-30,50,10),expand=c(0,0))+\rscale_y_continuous(breaks=seq(30,60,5),expand=c(0,0))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Ta 850hPa (ºC)\u0026quot;)+\rfacet_grid(month~.)+ #mapa por mes\rtheme_bw()\r\r\r3 ERA-Interim\rEl ECMWF ofrece acceso a sus bases de datos públicos a partir de una pyhton-API. Es necesario estar registrado en la web del ECMWF. Se puede darse de alta aquí. Al tratarse de otro lenguaje de programación, en R debemos usar un interfaz entre ambos lo que nos permite el paquete reticulate. También debemos que tener instalada una distribución de pyhton (versión 2.x o 3.x). En el caso de Windows podemos usar anaconda.\nRecientemente se ha publicado un nuevo paquete ecmwfr que facilita el acceso a los APIs de Copernicus y ECMWF. La gran ventaja es que no hace falta instalar python. Más detalles aquí.\n 3.1 Instalación\rif(!require(\u0026quot;reticulate\u0026quot;)) install.packages(\u0026quot;reticulate\u0026quot;)\rif(!require(\u0026quot;ncdf4\u0026quot;)) install.packages(\u0026quot;ncdf4\u0026quot;) #para manejar formato netCDF\r#cargamos las librerías\rlibrary(reticulate)\rlibrary(ncdf4)\rUna vez que tenemos instalado anaconda y el paquete reticulate, podemos instalar el paquete python ecmwfapi. La instalación la podemos llevar a cabo, o bien através del CMD de Windows usando el comando conda install -c conda-forge ecmwf-api-client, o bien con la función py_install( ) del paquete reticulate. La misma función permite instalar cualquier librería python desde R.\n#instalamos la API ECMWF\rpy_install(\u0026quot;ecmwf-api-client\u0026quot;)\r\r3.2 Conexión y descarga con la ECMWF API\rPara poder acceder a la API es requisito crear un archivo con la información del usuario.\nEl archivo “.ecmwfapirc” debe contener la siguiente información:\n{\r\u0026quot;url\u0026quot; : \u0026quot;https://api.ecmwf.int/v1\u0026quot;,\r\u0026quot;key\u0026quot; : \u0026quot;XXXXXXXXXXXXXXXXXXXXXX\u0026quot;,\r\u0026quot;email\u0026quot; : \u0026quot;john.smith@example.com\u0026quot;\r}\rLa clave podemos obtenerla con la cuenta de usuario aquí.\nEl archivo se puede crear con el bloc de notas de Windows.\nCreamos un documento “ecmwfapirc.txt”.\rRenombramos este archivo a “.ecmwfapirc.”\r\rEl último punto desaparece de forma automática. Después guardamos este archivo en “C:/USERNAME/.ecmwfapirc” o “C:/USERNAME/Documents/.ecmwfapirc”.\n#importamos la librería python ecmwfapi\recmwf \u0026lt;- import(\u0026#39;ecmwfapi\u0026#39;)\r#en este paso debe existir el archivo .ecmwfapirc\rserver = ecmwf$ECMWFDataServer() #iniciamos la conexión\rLlegados a este punto, ¿cómo creamos una consulta? Lo más fácil es ir a la web del ECMWF dónde elegimos la base de datos, en este caso ERA-Interim en superficie, para crear un script con todos los datos necesarios. Más detalles sobre la sintaxis podemos encontrar aquí. Cuando procedemos en la web sólamente tenemos que hacer click en “View MARS Request”. Este paso nos lleva al script en python.\nCon la sintaxis del script que nos da la MARS Request podemos crear la consulta en R.\n#creamos la consulta\rquery \u0026lt;-r_to_py(list(\rclass=\u0026#39;ei\u0026#39;,\rdataset= \u0026quot;interim\u0026quot;, #base de datos\rdate= \u0026quot;2017-01-01/to/2017-12-31\u0026quot;, #periodo expver= \u0026quot;1\u0026quot;,\rgrid= \u0026quot;0.125/0.125\u0026quot;, #resolución\rlevtype=\u0026quot;sfc\u0026quot;,\rparam= \u0026quot;167.128\u0026quot;, # temperatura del aire (2m)\rarea=\u0026quot;45/-10/30/5\u0026quot;, #N/W/S/E\rstep= \u0026quot;0\u0026quot;,\rstream=\u0026quot;oper\u0026quot;,\rtime=\u0026quot;00:00:00/06:00:00/12:00:00/18:00:00\u0026quot;, #paso de tiempo\rtype=\u0026quot;an\u0026quot;,\rformat= \u0026quot;netcdf\u0026quot;, #formato\rtarget=\u0026#39;ta2017.nc\u0026#39; #nombre del archivo\r))\rserver$retrieve(query)\rEl resultado es un archivo netCDF que podemos processar con el paquete ncdf4.\n\r3.3 Procesar ncdf\rA partir de aquí, el objetivo será la extracción de una serie temporal de una coordenada más próxima a una dada. Usaremos las coordenadas de Madrid (40.418889, -3.691944).\n#cargamos las librerías library(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\r#abrimos la conexión con el archivo\rnc \u0026lt;- nc_open(\u0026quot;ta2017.nc\u0026quot;)\r#extraemos lon y lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r## [1] 121\r## [1] 121\r#extraemos el tiempo\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#unidad del tiempo: horas desde 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r## $units\r## [1] \u0026quot;hours since 1900-01-01 00:00:00.0\u0026quot;\r## ## $long_name\r## [1] \u0026quot;time\u0026quot;\r## ## $calendar\r## [1] \u0026quot;gregorian\u0026quot;\r#convertimos las horas en fecha+hora #as_datetime de lubridate espera segundos\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#importamos los datos\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#cerramos la conexión\rnc_close(nc)\rMás detalles se pueden consultar en este breve manual sobre cómo trabajar con netCDF aqui. En esta próxima sección hacemos uso del paquete sf la cuál está sustituyendo las más conocidas sp y rgdal.\n#creamos todas las combinaciones\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#debemos convertir las coordenadas en objeto espacial sf\r#además indicamos el sistema de coordenadas en codigo EPSG\rcoord \u0026lt;- st_as_sf(lonlat,coords=c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;))%\u0026gt;%\rst_set_crs(4326)\r#lo mismo hacemos con nuestra coordenada de Madrid\rpsj \u0026lt;- st_point(c(-3.691944,40.418889))%\u0026gt;%\rst_sfc()%\u0026gt;%\rst_set_crs(4326)\r#plot de los puntos\rplot(st_geometry(coord))\rplot(psj,add=TRUE,pch = 3, col = \u0026#39;red\u0026#39;)\rEn los próximos pasos calculamos la distancia de nuestro punto de referencia a todos los puntos del grid. Posteriormente, buscamos aquel con menos distancia.\n#añadimos la distancia a los puntos\rcoord \u0026lt;- mutate(coord,dist=st_distance(coord,psj))\r#creamos una matrix de distancia con las mismas dimensiones que nuestros datos\rdist_mat \u0026lt;- matrix(coord$dist,dim(data)[-3])\r#la función arrayInd es útil para obtener los índices fila y columna en este caso\rmat_index \u0026lt;- as.vector(arrayInd(which.min(dist_mat), dim(dist_mat)))\r#extraemos la serie temporal y cambiamos la unidad de K a ºC\r#convertimos el tiempo en fecha + hora\rdf \u0026lt;- data.frame(ta=data[mat_index[1],mat_index[2],],time=timestamp)%\u0026gt;%\rmutate(ta=ta-273.15,time=ymd_hms(time))\rPara terminar, visualizamos nuestra serie temporal.\nggplot(df,\raes(time,ta))+\rgeom_line()+\rlabs(y=\u0026quot;Temperatura (ºC)\u0026quot;,\rx=\u0026quot;\u0026quot;)+\rtheme_bw()\r\r\r4 Actualización para acceder ERA-5\rRecientemente el nuevo reanálisis ERA-5 con single level o pressure level fue puesto a disposición de los usarios. Es la quinta generación del European Centre for Medium-Range Weather Forecasts (ECMWF) y accesible a través de una nueva API de Copernicus. El nuevo reanálisis ERA-5 tiene una cobertura temporal desde 1950 hasta la actualidad a una resolución horizontal de 30km a nivel mundial, con 137 niveles desde la superficie hasta una altura de 80km. Una diferencia importante con respecto al ERA-Interim anterior es la resolución temporal con datos horarios.\nEl acceso cambia a la infrastructura de Climate Data Store (CDS) con su propia API. Es posible descargar directamente desde la página o usando la Python API en una forma similar a la ya presentada en este post. Sin embargo, existen ligeras diferencias que voy a explicar a continuación.\nEs necesario tener una cuenta en CDS de Copernicus link\rNuevamente, hace falta una clave link\rCambia la librería de Python y algo los argumentos en la consulta.\r\r#cargamos las librerías library(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\rlibrary(reticulate)\r#instalamos la CDS API\rconda_install(\u0026quot;r-reticulate\u0026quot;,\u0026quot;cdsapi\u0026quot;,pip=TRUE)\rPara poder acceder a la API un requisito es crear un archivo con la información del usuario.\nEl archivo “.cdsapirc” debe contener la siguiente información:\n\rurl: https://cds.climate.copernicus.eu/api/v2\rkey: {uid}:{api-key}\r\rLa clave la podemos obtener con la cuenta de usuario en el User profile.\nEl archivo se puede crear con el bloc de notas de Windows del mismo modo como ha sido explicado para ERA-Interim.\n#importamos la librería python CDS\rcdsapi \u0026lt;- import(\u0026#39;cdsapi\u0026#39;)\r#en este paso debe existir el archivo .cdsapirc\rserver = cdsapi$Client() #iniciamos la conexión\rCon la sintaxis del script que nos da la Show API request podemos crear la consulta en R.\n#creamos la consulta\rquery \u0026lt;- r_to_py(list(\rvariable= \u0026quot;2m_temperature\u0026quot;,\rproduct_type= \u0026quot;reanalysis\u0026quot;,\ryear= \u0026quot;2018\u0026quot;,\rmonth= \u0026quot;07\u0026quot;, #formato: \u0026quot;01\u0026quot;,\u0026quot;01\u0026quot;, etc.\rday= str_pad(1:31,2,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;), time= str_c(0:23,\u0026quot;00\u0026quot;,sep=\u0026quot;:\u0026quot;)%\u0026gt;%str_pad(5,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;),\rformat= \u0026quot;netcdf\u0026quot;,\rarea = \u0026quot;45/-20/35/5\u0026quot; # North, West, South, East\r))\rserver$retrieve(\u0026quot;reanalysis-era5-single-levels\u0026quot;,\rquery,\r\u0026quot;era5_ta_2018.nc\u0026quot;)\rEs posible que la primera vez se reciba un mensaje de error, dado que todavía no se han aceptado los términos y las condiciones requeridas. Únicamente se debe seguir el enlace indicado.\nError in py_call_impl(callable, dots$args, dots$keywords) : Exception: Client has not agreed to the required terms and conditions.. To access this resource, you first need to accept the termsof \u0026#39;Licence to Use Copernicus Products\u0026#39; at https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products\rA partir de aquí podemos seguir los mismos pasos como los hechos con ERA-Interim.\n#abrimos la conexión con el archivo\rnc \u0026lt;- nc_open(\u0026quot;era5_ta_2018.nc\u0026quot;)\r#extraemos lon y lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r## [1] 41\r## [1] 101\r#extraemos el tiempo\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#unidad del tiempo: horas desde 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r## $units\r## [1] \u0026quot;hours since 1900-01-01 00:00:00.0\u0026quot;\r## ## $long_name\r## [1] \u0026quot;time\u0026quot;\r## ## $calendar\r## [1] \u0026quot;gregorian\u0026quot;\r#convertimos las horas en fecha+hora #as_datetime de lubridate espera segundos\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#temperatura en K de julio 2018\rhead(timestamp)\r## [1] \u0026quot;2018-07-01 00:00:00 UTC\u0026quot; \u0026quot;2018-07-01 01:00:00 UTC\u0026quot;\r## [3] \u0026quot;2018-07-01 02:00:00 UTC\u0026quot; \u0026quot;2018-07-01 03:00:00 UTC\u0026quot;\r## [5] \u0026quot;2018-07-01 04:00:00 UTC\u0026quot; \u0026quot;2018-07-01 05:00:00 UTC\u0026quot;\r#importamos los datos\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#plot\rfilled.contour(data[,,1])\rplot(data.frame(date=timestamp,ta=data[1,5,]),\rtype=\u0026quot;l\u0026quot;)\r#cerramos la conexión\rnc_close(nc)\r\r","date":1537005584,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1537005584,"objectID":"8f8afa591ee1e2a60bf271add5ba14b3","permalink":"/es/2018/acceso-a-datos-de-los-rean%C3%A1lisis-clim%C3%A1ticos-desde-r/","publishdate":"2018-09-15T10:59:44+01:00","relpermalink":"/es/2018/acceso-a-datos-de-los-rean%C3%A1lisis-clim%C3%A1ticos-desde-r/","section":"post","summary":"En este post enseñaré cómo podemos descargar y trabajar directamente con datos provinientes de los reanálisis climáticos en R. Se trata de sistemas de asimilación de datos que combinan modelos de pronóstico meteorológico y observaciones de distintas fuentes de forma objetiva con el fin de sintetizar el estado actual y la evolución de multiples variables de la atmósfera, la superficie de la tierra y los océanos.","tags":["reanalisis","interim","NCEP/NCAR","era","descarga","ncdf","acceso","api","python","ECMWF"],"title":"Acceso a datos de los reanálisis climáticos desde R","type":"post"},{"authors":null,"categories":["visualización","R","R:elemental"],"content":"\rBienvenido a mi blog! Soy Dominic Royé, investigador y docente de geografía física en la Universidad de Santiago de Compostela. Una de mis pasiones es la programación en R para visualizar y analizar cualquier tipo de datos. Por eso, mi idea de iniciar este blog tiene su origen en las publicaciones que he ido haciendo en el útimo año en Twitter sobre diferentes temas visualizando datos que describen el mundo. Además, me gustaría aprovechar la posibilidad del blog e ir publicando breves ensayos sobre visualización, gestión y manipulación de datos en R. Espero que os guste. Cualquier sugerencia o idea, será bienvenida.\nPreámbulo\rSiempre he querido escribir sobre el uso del gráfico de tarta. El gráfico circular es ampliamente usado en investigación, docencia, periodismo o en informes técnicos. Es más, no sé si es debido a Excel, pero todavía peor que el mismo gráfico de tarta es su versión en 3D (también para el gráfico de barras). Sobre las versiones 3D únicamente debo decir que no es recomendable, ya que en estos casos la tercera dimensión no contiene ninguna información y por tanto no ayuda en leer correctamente la información del gráfico. Respecto al gráfico de tarta, entre muchos expertos no se aconseja claramente su uso. Pero, ¿por qué?\nYa en un estudio hecho por (???) encontraron que la interpretación y el procesamiento de ángulos es más díficil que el de formas lineales. Mayormente es más fácil leer un gráfico de barras que uno de tarta. Un problema que se hace muy visible cuando nos encontramos con; 1) demasiadas categorías 2) pocas diferencias entre las categorías 3) un mal uso de colores como leyenda ó 4) comparaciones entre varios gráficos de tarta.\nPara decidir qué posibles representaciones gráficas existen para nuestros datos, recomiendo ir a la página web www.data-to-viz.com o usar Financial Times Visual Vocabulary.\n\nAhora bien, ¿qué alternativas podemos usar en R?\n\rAlternativas al gráfico de tarta\rLos datos sobre el estado de la enfermedad sarampión corresponden a junio de 2018 en Europa y vienen del ECDC.\n#librerías\rlibrary(tidyverse)\rlibrary(scales)\rlibrary(RColorBrewer)\r#datos\rmeasles \u0026lt;- data.frame(\rvacc_status=c(\u0026quot;Unvaccinated\u0026quot;,\u0026quot;1 Dose\u0026quot;,\r\u0026quot;\u0026gt;= 2 Dose\u0026quot;,\u0026quot;Unkown Dose\u0026quot;,\u0026quot;Unkown\u0026quot;),\rprop=c(0.75,0.091,0.05,0.012,0.096)\r)\r#ordenamos de mayor a menor y lo fijamos con un factor measles \u0026lt;- arrange(measles,\rdesc(prop))%\u0026gt;%\rmutate(vacc_status=factor(vacc_status,vacc_status))\r\r\rvacc_status\rprop\r\r\r\rUnvaccinated\r0.750\r\rUnkown\r0.096\r\r1 Dose\r0.091\r\r\u0026gt;= 2 Dose\r0.050\r\rUnkown Dose\r0.012\r\r\r\rGráfico de barra o similar\rggplot(measles,aes(vacc_status,prop))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convertimos en %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\rggplot(measles,aes(x=vacc_status,prop,ymin=0,ymax=prop))+\rgeom_pointrange()+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convertimos en %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\r#definiciones sobre el theme que usamos\rtheme_singlebar \u0026lt;- theme_bw()+\rtheme(\rlegend.position = \u0026quot;bottom\u0026quot;,\raxis.title = element_blank(),\raxis.ticks.y = element_blank(),\raxis.text.y = element_blank(),\rpanel.border = element_blank(),\rpanel.grid=element_blank(),\rplot.title=element_text(size=14, face=\u0026quot;bold\u0026quot;)\r)\rmutate(measles,\rvacc_status=factor(vacc_status, #cambiamos el orden de las categorías\rrev(levels(vacc_status))))%\u0026gt;%\rggplot(aes(1,prop,fill=vacc_status))+ #ponemos 1 en x para crear una barra única\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent,\rlimits=c(0,1),\rexpand=c(.01,.01))+\rscale_x_continuous(expand=c(0,0))+\rscale_fill_brewer(\u0026quot;\u0026quot;,palette=\u0026quot;Set1\u0026quot;)+\rcoord_flip()+\rtheme_singlebar\r#ampliamos los datos con las cifras de Italia\rmeasles2 \u0026lt;- mutate(measles,\ritaly=c(0.826,0.081,0.053,0.013,0.027),\rvacc_status=factor(vacc_status,rev(levels(vacc_status))))%\u0026gt;%\rrename(europe=\u0026quot;prop\u0026quot;)%\u0026gt;%\rgather(region,prop,europe:italy)\rggplot(measles2,aes(region,prop,fill=vacc_status))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,position=\u0026quot;stack\u0026quot;)+ #stack: columna 100%\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convertimos en %\rlimits=c(0,1),\rexpand=c(0,0))+\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;)+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Vaccination Status\u0026quot;)+\rtheme_minimal()\r\rGráfico de Waffle\r#libería\rlibrary(waffle)\r#la función de waffle usa un vector con nombres\rval_measles \u0026lt;- round(measles$prop*100)\rnames(val_measles) \u0026lt;- measles$vacc_status\rwaffle(val_measles, #datos\rcolors=brewer.pal(5,\u0026quot;Set1\u0026quot;), #colores\rrows=5) #número de filas \rEl gráfico de Waffle me parece muy interesante cuando queramos mostrar una proporción de una categoría individual.\nmedida \u0026lt;- c(41,59) #datos de la OECD 2015\rnames(medida) \u0026lt;- c(\u0026quot;Estudios Superiores\u0026quot;,\u0026quot;Otros estudios\u0026quot;)\rwaffle(medida,\rcolors=c(\u0026quot;#377eb8\u0026quot;,\u0026quot;#bdbdbd\u0026quot;),\rrows=5)\r\rMapa de arbol (treemap)\r#librería\rlibrary(treemap)\rtreemap(measles,\rindex=\u0026quot;vacc_status\u0026quot;, #variable de categrías\rvSize=\u0026quot;prop\u0026quot;, #valores\rtype=\u0026quot;index\u0026quot;, #estilo más en ?treemap\rtitle=\u0026quot;\u0026quot;, palette = brewer.pal(5,\u0026quot;Set1\u0026quot;) #colores\r)\rPersonalmente, creo que todos los tipos de representaciones gráficas tienen sus ventajas y desventajas. No obstante, en la actualidad tenemos una gran variedad de alternativas para evitar el uso del gráfico de tarta. Si aún así se quiere hacer un gráfico de tarta, algo que tampoco descartaría, recomiendo seguir ciertas reglas que ha resumido muy bien Lisa Charlotte Rost en un reciente post. Por ejemplo, debemos ordenar de mayor a menor a no ser que haya un orden natural o usar como máximo cinco categorías. Por último, os dejo un enlace a un cheatsheet de policyviz sobre normas básicas de visualización de datos. Una buena referencia sobre gráficos, usando diferentes programas desde Excel hasta R, podéis encontrar en Creating more effective graphs (???).\n\rReferencias\r\r\r","date":1534933412,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1534933412,"objectID":"f21d950903f35ea0334f02bebc23ea63","permalink":"/es/2018/el-gr%C3%A1fico-de-tarta/","publishdate":"2018-08-22T11:23:32+01:00","relpermalink":"/es/2018/el-gr%C3%A1fico-de-tarta/","section":"post","summary":"Bienvenido a mi blog! Soy Dominic Royé, investigador y docente de geografía física en la Universidad de Santiago de Compostela. Una de mis pasiones es la programación en R para visualizar y analizar cualquier tipo de datos. Por eso, mi idea de iniciar este blog tiene su origen en las publicaciones que he ido haciendo en el útimo año en Twitter sobre diferentes temas visualizando datos que describen el mundo.","tags":["gráfico de tarta","datos","circular","proporciones","primera entrada","treemap","waffle","barra","visualización"],"title":"El gráfico de tarta","type":"post"},{"authors":["D Royé","A Figueiras","M Taracido-Trunk"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1522540800,"objectID":"0548c7a9a3b7d133740e95d174a65d2c","permalink":"/es/publication/pharma_coru%C3%B1a_2018/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/es/publication/pharma_coru%C3%B1a_2018/","section":"publication","summary":"The consumption of medication, especially over-the-counter (OTC) drugs, can reflect environmental exposure with a lesser degree of severity in terms of morbidity. The non-linear effects of maximum and minimum apparent temperature on respiratory drug sales in A Coruña from 2006 to 2010 were examined using a distributed lag non-linear model. In particular, low apparent temperatures proved to be associated with increased sales of respiratory drugs. The strongest consistent risk estimates were found for minimum apparent temperatures in respiratory drug sales with an increase of 33.4% (95% CI: 12.5-58.0%) when the temperature changed from 2.8 ºC to −1.4 ºC. These findings may serve to guide the planning of public health interventions in order to predict and manage the health effects of exposure to the thermal environment for lower degrees of morbidity. More precisely, significant increases in the use of measured OTC medication could be used to identify and anticipate influenza outbreaks due to a more sensitive degree of the data source.","tags":["drug sales","pharmacoepidemiology","respiratory cause","short‐term effects","Spain","thermal environment"],"title":"Short-term effects of heat and cold on respiratory drug use. A time-series epidemiological study in A Coruña, Spain","type":"publication"},{"authors":["D Royé","N Lorenzo","J Martin-Vide"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1522540800,"objectID":"1aa69b47bc265ad54507411d0e9866f7","permalink":"/es/publication/lightning_galicia_2018/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/es/publication/lightning_galicia_2018/","section":"publication","summary":"The spatial-temporal patterns of cloud-to-ground (CG) lightning covering the period 2010-2015 over the northwest Iberian Peninsula were investigated. The analysis conducted employed three main methods: the circulation weather types developed by Jenkinson \u0026 Collison, the fit of a generalized additive model for geographic variables and the use of a concentration index for the ratio of lightning strikes and thunderstorm days. The main activity in the summer months can be attributed to situations with eastern or anticyclonic flow due to convection by insolation. In winter, lightning proves to have a frontal origin and is mainly associated with western or cyclonic flow situations which occur with advections of air masses of maritime origin. The largest number of CG discharges occurs under eastern flow and their hybrids with anticyclonic situations. Thunderstorms with greater CG lightning activity, highlighted by a higher Concentration Index, are located in areas with a higher density of lightning strikes, above all in mountainous areas away from the sea. The modeling of lightning density with geographic variables shows the positive influence of altitude and, particularly, distance to the sea, with nonlinear relationships due to the complex orography of the region. Likewise, areas with convex topography receive more lightning strikes than concave ones, a relation which has been demonstrated for the first time from a Generalized Additive Model (GAM).","tags":["thunderstorm","Iberian Peninsula","Concentration Index","weather types","Convexity Index","Generalized Additive Model","cloud-to-ground lightning"],"title":"Spatial–temporal patterns of cloud-to-ground lightning over the northwest Iberian Peninsula during the period 2010–2015","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1501545600,"objectID":"d09c5cdabc94931312d002ad366170e8","permalink":"/es/publication/hotnights_bcn_2017/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/es/publication/hotnights_bcn_2017/","section":"publication","summary":"Heat-related effects on mortality have been widely analyzed using maximum and minimum temperatures as exposure variables. Nevertheless, the main focus is usually on the former with the minimum temperature being limited in use as far as human health effects are concerned. Therefore, new thermal indices were used in this research to describe the duration of night hours with air temperatures higher than the 95% percentile of the minimum temperature (Hot Night hours) and intensity as the summation of these air temperatures in degrees (Hot Night degrees). An exposure-response relationship between mortality due to natural, respiratory and cardiovascular causes and summer night temperatures was assessed using data from the Barcelona region between 2003 and 2013. The non-linear relationship between the exposure and response variables was modeled using a distributed lag non-linear model. The estimated associations for both exposure variables and mortality shows a relationship with high and medium values that persist significantly up to a lag of 1–2 days. In mortality due to natural causes an increase of 1.1% per 10% (CI95% 0.6–1.5) for Hot Night hours and 5.8% per each 10º (CI95% 3.5–8.2%) for Hot Night degrees is observed. The effects of Hot Night hours reach their maximum with 100% and leads to an increase by 9.2% (CI95% 5.3–13.1%). The hourly description of night heat effects reduced to a single indicator in duration and intensity is a new approach and shows a different perspective and significant heat-related effects on human health.","tags":["heat","mortality","tropical night","hot night","effects","human health","climate change"],"title":"The effects of hot nights on mortality in Barcelona, Spain","type":"publication"},{"authors":["D Royé","J Martin-Vide"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1496275200,"objectID":"ef78992172b280763b5273081ec3915b","permalink":"/es/publication/usa_ci_2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/es/publication/usa_ci_2017/","section":"publication","summary":"The contiguous US exhibits a wide variety of precipitation regimes, first, because of the wide range of latitudes and altitudes. The physiographic units with a basic meridional configuration contribute to the differentiation between east and west in the country while generating some large interior continental spaces. The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. Positive exponential curves, which plot the cumulative percentages of days with precipitation against the cumulative percentage of the rainfall amounts that they contribute, can be evaluated through the Concentration Index. The Concentration Index has been applied to the contiguous United States using a gridded climate dataset of daily precipitation data, at a resolution of 0.25°, provided by CPC/NOAA/OAR/Earth System Research Laboratory, for the period between 1956 and 2006. At the same time, other rainfall indices and variables such as the annual coefficient of variation, seasonal rainfall regimes and the probabilities of a day with precipitation have been presented with a view to explaining spatial CI patterns. The spatial distribution of the CI in the contiguous United States is geographically consistent, reflecting the principal physiographic and climatic units of the country. Likewise, linear correlations have been established between the CI and geographical factors such as latitude, longitude and altitude. In the latter case the Pearson correlation coefficient (r) between this factor and the CI is −0.51 (p-value ","tags":["Concentration Index","Contiguous United States","daily precipitation","precipitation indices","spatial–temporal patterns"],"title":"Concentration of Daily Precipitation in the Contiguous United States","type":"publication"},{"authors":["P Fdez-Arroyabe","D Royé"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1483228800,"objectID":"395d06650f0c20c6eb0045e172f386ce","permalink":"/es/publication/chapter_springer_2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/es/publication/chapter_springer_2017/","section":"publication","summary":"Co-creation of scientific knowledge based on new technologies and big data sources is one of the main challenges for the digital society in the XXI century. Data management and the analysis of patterns among datasets based on machine learning and artificial intelligence has become essential for many sectors nowadays. The development of real time health-related climate services represents an example where abundant structured and unstructured information and transdisciplinary research are needed. The study of the interactions between atmospheric processes and human health through a big data approach can reveal the hidden value of data. The Oxyalert technological platform is presented as an example of a digital biometeorological infrastructure able to forecast, at an individual level, oxygen changes impacts on human health.","tags":["co-creation","interdisciplinarity","transdisciplinarity","morbidity","climate services","digital divide","big data","health"],"title":"Co-creation and Participatory Design of Big Data Infrastructures on the Field of Human Health Related Climate Services","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1483225200,"objectID":"668ea53b1680bfb427e06db9308dfdd2","permalink":"/es/more/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/es/more/","section":"","summary":"","tags":null,"title":"M�s","type":"page"},{"authors":["D Royé","J Taboada","A Ezpeleta-Martí","N Lorenzo"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1459468800,"objectID":"103600468f174f2c7309eee0a4356933","permalink":"/es/publication/cwt_galicia_resp_2016/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/es/publication/cwt_galicia_resp_2016/","section":"publication","summary":"The link between various pathologies and atmospheric conditions has been a constant topic of study over recent decades in many places across the world; knowing more about it enables us to pre-empt the worsening of certain diseases, thereby optimizing medical resources. This study looked specifically at the connections in winter between respiratory diseases and types of atmospheric weather conditions (Circulation Weather Types, CWT) in Galicia, a region in the north-western corner of the Iberian Peninsula. To do this, the study used hospital admission data associated with these pathologies as well as an automatic classification of weather types. The main result obtained was that weather types giving rise to an increase in admissions due to these diseases are those associated with cold, dry weather, such as those in the east and south-east, or anticyclonic types. A second peak was associated with humid, hotter weather, generally linked to south-west weather types. In the future, this result may help to forecast the increase in respiratory pathologies in the region some days in advance.","tags":["weather type","respiratory diseases","hospital admissions","human health","Spain"],"title":"Winter circulation weather types and hospital admissions for respiratory diseases in Galicia, Spain","type":"publication"},{"authors":["D Royé","A Ezpeleta-Martí"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1448928000,"objectID":"b3b3af88b0383548a9a15d5635968b2a","permalink":"/es/publication/hotnights_age_2015/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/es/publication/hotnights_age_2015/","section":"publication","summary":"En este trabajo se aplica una metodología nueva al estudio de las noches calurosas, también denominadas «tropicales», en Galicia y en Portugal de cara a identificar aquellas noches en las que la población pueda verse afectada por estrés térmico. La utilización de dos indicadores obtenidos a través de datos semihorarios ha permitido definir con más detalle las características térmicas de las noches entre mayo y octubre, pudiendo así evaluar con más precisión el riesgo para el bienestar y la salud de la población. Se produce un importante aumento de la frecuencia de noches tropicales y noches cálidas en la fachada atlántica, desde el norte de Galicia hasta el sur de Portugal. La menor latitud y la proximidad al litoral están relacionadas con la mayor persistencia del calor y del estrés térmico durante estas noches. En áreas de interior la persistencia es menor. Las noches calurosas son más frecuentes e intensas en el centro de las ciudades, por el efecto de la isla de calor urbana.","tags":["noches tropicales","estrés térmico","isla de calor","Galicia","Portugal"],"title":"Análisis de las noches tropicales en la fachada atlántica de la Península Ibérica. Una propuesta metodológica","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"Los datos usados están disponibles aquí. Otras bases de datos para España en formato ncdf pueden ser descargadas:\nAEMET\n Rejilla 20km y 50km (precipitación y temperatura)\n Rejilla 5km (precipitación)\n  CSIC\n Rejilla 5km (precipitación)  ","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1448928000,"objectID":"df5e959cdcb892f902f61f17b25f9552","permalink":"/es/publication/ncdf_2015/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/es/publication/ncdf_2015/","section":"publication","summary":"La información espacio-temporal es en la actualidad un elemento clave en disciplinas como la Climatología y la Meteorología. Un formato de uso muy extendido es el de las bases de datos netCDF, que permiten obtener una estructura multidimensional e intercambiar los datos de forma independiente al sistema operativo empleado. En este artículo se introduce el uso de estas bases de datos con el entorno de software libre R. Para ello se utiliza una cuadrícula de la temperatura máxima de la Península Ibérica para el período 1971-2007. El objetivo es poder leer y visualizar el formato netCDF realizando ejemplos de cálculos globales y otros más específicos. Finalmente se muestra la aplicabilidad en un caso de estudio: la amplitud diurna en la Península Ibérica para los meses de enero y agosto 2006.","tags":["netCDF","R","climatología","temperatura","matriz","base de datos"],"title":"El uso de bases de datos climáticos netCDF con estructura matricial en el entorno de R","type":"publication"}]