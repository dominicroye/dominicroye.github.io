---
output:
  blogdown::html_page:
    toc: true
    number_sections: true
categories: ["tidyverse","R","R:elementary"]
title: A very short introduction to Tidyverse
date: '2020-06-06'
slug: short-introduction-tidyverse
tags: ["introduction","visualization","manipulation", "data", "COVID-19"]
header:
  caption: ''
  image: 'mobility_residence.png'
lang: en
summary: "The tidyverse universe of packages, a collection of packages specially focused on data science, marked a milestone in R programming. In this post I am going to summarize very briefly the most essential to start in this world. The tidyverse grammar follows a common structure in all functions. The most essential thing is that the first argument is the object and then come the rest of the arguments. In addition, a set of verbs is provided to facilitate the use of the functions. The tidyverse philosophy and grammar of functions structure is also reflected in other packages that make its use compatible with the collection of tidyverse."
---


# Tidyverse

The ``tidyverse`` universe of packages, a collection of packages specially focused on data science, marked a milestone in R programming. In this post I am going to summarize very briefly the most essential to start in this world. The tidyverse grammar follows a common structure in all functions. The most essential thing is that the first argument is the object and then come the rest of the arguments. In addition, a set of verbs is provided to facilitate the use of the functions. The ``tidyverse`` philosophy and grammar of functions are also reflected in other packages that make its use compatible with the collection. For example, the ``sf`` package ([simple feature](https://r-spatial.github.io/sf/articles/sf1.html)) is a standardized way to encode spatial vector data and allows the use of multiple functions that we can find in the ``dplyr`` package.

The core of the `tidyverse` collection is made up of the following packages:

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(knitr)

tb <- data.frame(paquete=c("ggplot2", "purrr", "tibble", "dplyr", "tidyr", "stringr", "readr", "forcats"),
                 descripcion=c("Grammar for creating graphics",
                               "R functional programming",
                                "Modern and effective table system",
                                "Grammar for data manipulation",
                                "Set of functions to create tidy data",
                                "Function set to work with characters",
                                "An easy and fast way to import data",
                                "Tools to easily work with factors"))

kable(tb,booktabs = TRUE,col.names=c("Package","Description"))

```

In addition to the mentioned packages, `lubridate` is also used very frequently to work with dates and times, and also `readxl` which allows us to import files in Excel format. To know all the available packages we can use the function ``tidyverse_packages()``.

```{r echo=FALSE,warning=FALSE,message=FALSE}
# load package
library(tidyverse)

tidyverse_packages()
```

It is very easy to get conflicts between functions, that is, that the same function name exists in several packages. To avoid this, we can write the name of the package in front of the function we want to use, separated by the colon symbol written twice (`package_name::function_name`).

Before I get started with the packages, I hope it will be a really short introduction, some comments on the style when programming in R.

# Style guide

In R there is no universal style guide, that is, in the R syntax it is not necessary to follow specific rules for our scripts. But it is recommended to work in a homogeneous, uniform, legible and clear way when writing scripts. The ``tidyverse`` collection has its own guide (https://style.tidyverse.org/).

The most important recommendations are:

- Avoid using more than 80 characters per line to allow reading the complete code.
- Always use a space after a comma, never before.
- The operators (==, +, -, <-,%>%, etc.) must have a space before and after.
- There is no space between the name of a function and the first parenthesis, nor between the last argument and the final parenthesis of a function.
- Avoid reusing names of functions and common variables (`c <- 5` vs. `c()`)
- Sort the script separating the parts with the comment form `# Import data -----`
- Avoid accent marks or special symbols in names, files, routes, etc.
- Object names must follow a constant structure: `day_one`, `day_1`.

It is advisable to use a correct indentation for multiple arguments of a function or functions chained by the `pipe` operator (`%>%`).

# Pipe %>%

To facilitate working in data management, manipulation and visualization, the `magrittr` package introduces the famous *pipe* operator in the form `%>%` with the aim of combining various functions without the need to assign the result to a new object. The *pipe* operator passes the output of a function applied to the first argument of the next function. This way of combining functions allows you to chain several steps simultaneously, to perform sequential tasks. In the very simple example below, we pass the vector `1:5` to the `mean()` function to calculate the average. You should know that there are a couple of other pipe operators in the same package. 

```{r}
1:5 %>% mean()
```

# Tidyverse packages

## Read and write data

The `readr` package makes it easy to read or write multiple file formats using functions that start with `read_*` or `write_*`.
In comparison to R Base, `readr` functions are faster; they handle problematic column names, and dates are automatically converted. The imported tables are of class `tibble` (*tbl_df*), a modern version of `data.frame` from the `tibble` package. In the same sense, you can use the `read_excel()` function of the ``readxl`` package to import data from Excel sheets (more details also in this [blog post](https://dominicroye.github.io/en/2019/import-excel-sheets-with-r/)). In the following example, we import the mobility data registered by Google ([link](https://www.google.com/covid19/mobility/)) during the last months of the COVID-19 pandemic ([download](/files/Global_Mobility_Report.csv)).

```{r echo=FALSE}
library(knitr)
df <- data.frame(funcion=c("read_csv() o read_csv2()",
                           "read_delim()", "read_table()"),
                 descripcion=c("coma or semicolon (CSV)",
                               "general separator", "whitespace-separated"))


kable(df,booktabs = TRUE,col.names=c("Function","Description"))

```


```{r}
# load package
library(tidyverse)

google_mobility <- read_csv("Global_Mobility_Report.csv")
google_mobility

```

Important is to take a look at the argument names, since they change in the `readr` functions. For example, the well-known `header = TRUE` argument of `read.csv()` is in this case `col_names = TRUE`. More details can be found in the [Cheat-Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf) of `readr`.

## Character manipulations

For working with strings we use the `stringr` package, whose functions always start with `str_*` followed by a verb and the first argument.

Some of these functions are as follows:

```{r echo=FALSE}
library(knitr)
df <- data.frame(funcion=c("str_replace()", "str_c()",
                           "str_detect()", "str_extract()",
                           "str_sub()", "str_length()"),
                 descripcion=c("replace patterns",
                                "combine characters",
                                "detect patterns",
                                "extract patterns",
                                "extract by position",
                                "length of string"))


kable(df,booktabs = TRUE,col.names=c("Function","Description"))

```

Regular expressions are often used for character patterns. For example, the regular expression `[aeiou]` matches any single character that is a vowel. The use of square brackets `[]` corresponds to character classes. For example, `[abc]` corresponds to each letter regardless of its position. `[a-z]`, `[A-Z]` or `[0-9]` each between a and z or 0 and 9. And finally, `[:punct:]` punctuation, etc. With curly braces "{}" we can indicate the number of the previous element, `{2}` would be twice, {1,2} between one and two, etc. Also with `$` or `^` we can indicate if the pattern starts at the beginning or ends at the end. More details and patterns can be found in the [Cheat-Sheet](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf) of `stringr`. 

```{r}
# replace 'er' at the end with empty space

str_replace(month.name, "er$", "")

str_replace(month.name, "^Ma", "")

# combine characters

a <- str_c(month.name, 1:12, sep = "_")
a

# collapse combination

str_c(month.name, collapse = ", ")

# detect patterns

str_detect(a, "_[1-5]{1}")

# extract patterns

str_extract(a, "_[1-9]{1,2}")

# extract the characters between position 1 and 2

str_sub(month.name, 1, 2)

# string length of each month

str_length(month.name)

# the '.' represents the object passed by the pipe operator %>%
str_length(month.name) %>% 
   str_c(month.name, ., sep = ".")

```

A very useful function is `str_glue()` to interpolate characters.

```{r}
name <- c("Juan", "Michael")
age <- c(50, 80) 
date_today <- Sys.Date()

str_glue(
  "My name is {name}, ",
  "I'am {age}, ",
  "and my birth year is {format(date_today-age*365, '%Y')}."
)
```


## Management of dates and times

The `lubridate` package is very powerful in handling dates and times. It allows us to create R recognized objects with functions (like `ymd()` or `ymd_hms()`) and we can even make calculations.

We only must know the following abbreviations:

- `ymd`: represents `y:year`, `m: month`, `d:day`
- `hms`: represents `h:hour`, `m:minutes`, `s:seconds`

```{r}
# load package
library(lubridate)

# date vector
dat <- c("1999/12/31", "2000/01/07", "2005/05/20","2010/03/25")

# date-time vector
dat_time <- c("1988-08-01 05:00", "2000-02-01 22:00")

# convert to date class
dat <- ymd(dat) 
dat

# other date formats
dmy("05-02-2000")
ymd("20000506")

# convert to POSIXct
dat_time <- ymd_hm(dat_time)
dat_time

# different date formats
dat_mix <- c("1999/12/05", "05-09-2008", "2000/08/09", "25-10-2019")

# mixted formats with known convention found in ?strptime
parse_date_time(dat_mix, order = c("%Y/%m/%d", "%d-%m-%Y"))


```

More useful functions:

```{r}
# extract the year
year(dat)

# the month
month(dat)
month(dat, label = TRUE) # as label

# the day of the week
wday(dat)
wday(dat, label = TRUE) # as label

# the hour
hour(dat_time)

# add 10 days
dat + days(10)

# add 1 month
dat + months(1)

```

Finally, the ``make_date()`` function is very useful to create dates from different date parts, such as the year, month, etc.

```{r}
# create date from its elements, here with year and month
make_date(2000, 5)

# create date with time
make_datetime(2005, 5, 23, 5)
```

More details can be found in the [Cheat-Sheet](https://github.com/rstudio/cheatsheets/raw/master/lubridate.pdf) of `lubridate`.

## Table and vector manipulation

The `dplyr` and `tidyr` packages provide us with a data manipulation grammar, a set of useful verbs to solve common problems. The most important functions are:

```{r echo=FALSE}
library(knitr)
df <- data.frame(funcion = c("mutate()","select()",
                           "filter()","summarise()","arrange()",
                           "group_by()", "rename()"),
                 descripcion = c("add new variables or modify existing ones",
                                "select variables",
                                "filter", "summarize/reduce", "sort", "group", "rename columns"))


kable(df,booktabs = TRUE, col.names=c("Function","Description"))
```

In case you haven't done it before, we import the mobility data.

```{r}
google_mobility <- read_csv("Global_Mobility_Report.csv")
```

### Select and rename

We can select or remove columns with the `select()` function, using the name or index of the column. To delete columns we make use of the negative sign. The `rename` function helps in renaming columns with either the same name or their index.

```{r}
residential_mobility <- select(google_mobility, 
                               country_region_code:sub_region_1, 
                               date, 
                               residential_percent_change_from_baseline) %>% 
                        rename(resi = 5)
```

### Filter and sort

To filter data, we use `filter()` with logical operators (`|`, `==`, `>`, etc) or functions that return a logical value (`str_detect()`, `is.na()` , etc.). The `arrange()` function sorts from least to greatest for one or multiple variables (with the negative sign `-` the order is reversed from greatest to least).

```{r}
filter(residential_mobility, 
       country_region_code == "US")

filter(residential_mobility, 
       country_region_code == "US", 
       sub_region_1 == "New York")

filter(residential_mobility, 
       resi > 50) %>% 
          arrange(-resi)
```

### Group and summarize

Where do we find greater variability between regions in each country on April 1, 2020?

To answer this question, we first filter the data and then we group by the country column. When we use the `summarize()` function after grouping, it allows us to summarize by these groups. Moreover, combining `group_by()` with the `mutate()` function modifies columns in each group separately. In `summarize()` we calculate the maximum, minimum value and the difference between both extremes creating new columns.

```{r, warning=FALSE}

resi_variability <- residential_mobility %>% 
                        filter(date == ymd("2020-04-01"),
                               !is.na(sub_region_1)) %>% 
                          group_by(country_region) %>% 
                           summarise(mx = max(resi, na.rm = TRUE), 
                                    min = min(resi, na.rm = TRUE),
                                    range = abs(mx)-abs(min))

arrange(resi_variability, -range)

```

### Join tables

How can we filter the data to get a subset of Europe?

To do this, we import a spatial dataset with the country code and a column of regions. Detailed explanations about the `sf` (*simple feature*) package, I'll leave for another post.

```{r}
library(rnaturalearth) # package of spatial vectorial data

# world limits
wld <- ne_countries(returnclass = "sf")

# filter the countries with iso code and select the two columns of interest
wld <- filter(wld, !is.na(iso_a2)) %>% select(iso_a2, subregion)

# plot
plot(wld)
```

Other `dplyr` functions allow us to join tables: `*_join ()`. Depending on which table (left or right) you want to join, the functions change: `left_join()`, `right_join()` or even `full_join()`. The `by` argument is not necessary as long as both tables have a column in common. However, in this case the variable names are different, so we use the following way: `c("country_region_code"="iso_a2")`. The `forcats` package of `tidyverse` has many useful functions for handling categorical variables (`factors`), variables that have a fixed and known set of possible values. All `forcats` functions have the prefix `fct_*`. For example, in this case we use `fct_reorder()` to reorder the country labels in order of the maximum based on the residential mobility records. Finally, we create a new column ``"resi_real"`` to change the reference value, the average or baseline, from 0 to 100.

```{r}
subset_europe <- filter(residential_mobility, 
                        is.na(sub_region_1),
                        !is.na(resi)) %>%
                 left_join(wld, by = c("country_region_code"="iso_a2")) %>% 
                 filter(subregion %in% c("Northern Europe",
                                         "Southern Europe",
                                          "Western Europe",
                                          "Eastern Europe")) %>%
                 mutate(resi_real = resi + 100,
                        region = fct_reorder(country_region, 
                                             resi, 
                                            .fun = "max", 
                                            .desc = FALSE)) %>% 
                select(-geometry, -sub_region_1)

str(subset_europe)
```

### Long and wide tables

Before we go to create graphics with `ggplot2`, it is very common to modify the table between two main formats, long and wide. A table is tidy when 1) each variable is a column 2) each observation/case is a row and 3) each type of observational unit forms a table.

```{r}
# subset
mobility_selection <- select(subset_europe, country_region_code, date:resi)
mobility_selection


# wide table
mobi_wide <- pivot_wider(mobility_selection, 
                         names_from = country_region_code,
                         values_from = resi)
mobi_wide

# back to long table
pivot_longer(mobi_wide,
             2:36,
             names_to = "country_code",
             values_to = "resi")
```

Another group of functions you should take a look at are: `separate()`, `case_when()`, `complete()`. More details can be found in the [Cheat-Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) of `dplyr`.

## Visualize data

`ggplot2` is a modern system for data visualization with a huge variety of options. Unlike the R Base graphic system, in `ggplot2` a different grammar is used. The grammar of graphics (gg) consists of the sum of several independent layers or objects that are combined using `+` to construct the final graph. `ggplot` differentiates between data, what is displayed and how it is displayed.

* *data*: our dataset (`data.frame` or `tibble`)

* *aesthetics*: with the `aes()` function we indicate the variables that correspond to the x, y, z, ... axes, or when it is intended to apply graphic parameters (color, size, shape) according to a variable. It is possible to include `aes()` in `ggplot()` or in the corresponding function to a geometry `geom_ *`.

* *geometries*: are `geom_ *` objects that indicate the geometry to be used, (eg: `geom_point()`, `geom_line()`, `geom_boxplot()`, etc.).

* *scales*: are objects of type `scales_ *` (eg, `scale_x_continous()`, `scale_colour_manual()`) to manipulate axes, define colors, etc.

* *statistics*: are `stat_ *` objects (eg, `stat_density()`) that allow to apply statistical transformations.

More details can be found in the [Cheat-Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf) of `ggplot2`. `ggplot` is constantly supplemented by extensions for geometries or other graphical options (see https://exts.ggplot2.tidyverse.org/ggiraph.html), for graphical ideas have a look a the R Graph Gallery (https://www.r-graph-gallery.com/). 

### Line and scatter plot

We create a subset of our mobility data for residences and parks, filtering the records for Italian regions. In addition, we divide the mobility values in percentage by 100 to obtain the fraction, since `ggplot2` allows us to indicate the unit of percentage in the label argument (see last plot in this section).

```{r, dpi = 300}
# create subset
it <- filter(google_mobility, 
             country_region == "Italy", 
             is.na(sub_region_1)) %>% 
      mutate(resi = residential_percent_change_from_baseline/100,   
             parks = parks_percent_change_from_baseline/100)


# line plot
ggplot(it, 
       aes(date, resi)) + 
  geom_line()


# scatter plot
ggplot(it, 
       aes(parks, resi)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

To modify the axes, we use the different `scale_*` functions that we must adapt to the scales of measurement (date, discrete, continuous, etc.). The `labs()` function helps us define the axis, legend and plot titles. Finally, we add the style of the graph with `theme_light()` (others are `theme_bw()`, `theme_minimal()`, etc.). We could also make changes to all graphic elements through `theme()`.

```{r, dpi = 300}
# time serie plot
ggplot(it, 
       aes(date, resi)) + 
  geom_line(colour = "#560A86", size = 0.8) +
  scale_x_date(date_breaks = "10 days", 
               date_labels = "%d %b") +
  scale_y_continuous(breaks = seq(-0.1, 1, 0.1), 
                     labels = scales::percent) +
  labs(x = "", 
       y = "Residential mobility",
       title = "Mobility during COVID-19") +
  theme_light()

# scatter plot
ggplot(it, 
       aes(parks, resi)) + 
  geom_point(alpha = .4, size = 2) +
  geom_smooth(method = "lm") +
  scale_x_continuous(breaks = seq(-1, 1.4, 0.2), 
                     labels = scales::percent) +
  scale_y_continuous(breaks = seq(-1, 1, 0.1), 
                     labels = scales::percent) +
  labs(x = "Park mobility", 
       y = "Residential mobility",
       title = "Mobility during COVID-19") +
  theme_light()
```


### Boxplot

We can visualize different aspects of the mobility with other geometries. Here we will create boxplots for each European country representing the variability of mobility between and within countries during the COVID-19 pandemic. 

```{r, dpi = 300}
# subset
subset_europe_reg <- filter(residential_mobility, 
                           !is.na(sub_region_1),
                           !is.na(resi)) %>%
                     left_join(wld, by = c("country_region_code"="iso_a2")) %>% 
                     filter(subregion %in% c("Northern Europe",
                                         "Southern Europe",
                                          "Western Europe",
                                          "Eastern Europe")) %>% 
                     mutate(resi = resi/100, 
                            country_region = fct_reorder(country_region, resi))

# boxplot
ggplot(subset_europe_reg, 
       aes(country_region, resi, fill = subregion)) + 
  geom_boxplot() +
  scale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +
  scale_fill_brewer(palette = "Set1") +
  coord_flip() +
   labs(x = "", 
       y = "Residential mobility",
       title = "Mobility during COVID-19", 
       fill = "") +
  theme_minimal()

```


### Heatmap

To visualize the mobility trend of all European countries it is recommended to use a heatmap instead of a bundle of lines. Before building the graph, we will create a vector of Sundays for the x-axis labels in the observation period.

```{r}
# sequence of dates
df <- data.frame(d = seq(ymd("2020-02-15"), ymd("2020-06-07"), "day"))

# filter on Sundays 
sundays <- df %>% 
            mutate(wd = wday(d, week_start = 1)) %>% 
             filter(wd == 7) %>% 
              pull(d)
```

```{r, echo=FALSE, results = 'hide'}
Sys.setlocale("LC_TIME", "English")
```

To difference between European regions, we will use a color fill for the boxplots. We can set the color type with `scale_fill_*`, in this case, from the viridis scheme. In addition, the `guides()` function can modify the color bar of the legend. Finally, here we see the use of `theme()` with additional changes to `theme_minimal()`.

```{r, dpi = 300, fig.width = 12.25, fig.height = 8}
# headmap
ggplot(subset_europe, 
       aes(date, region, fill = resi_real)) +
  geom_tile() +
  scale_x_date(breaks = sundays,
               date_labels = "%d %b") +
  scale_fill_viridis_c(option = "A", 
                       breaks = c(91, 146),
                       labels = c("Less", "More"), 
                       direction = -1) +
  theme_minimal() +
  theme(legend.position = "top", 
        title = element_text(size = 14),
        panel.grid.major.x = element_line(colour = "white", linetype = "dashed"),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.ontop = TRUE,
        plot.margin = margin(r = 1, unit = "cm")) +
  labs(y = "", 
       x = "", 
       fill = "", 
       title = "Mobility trends for places of residence",
       caption = "Data: google.com/covid19/mobility/") +
  guides(fill = guide_colorbar(barwidth = 10, 
                               barheight = .5,
                               label.position = "top", 
                               ticks = FALSE)) +
  coord_cartesian(expand = FALSE)
```


## Apply functions on vectors or lists

The `purrr` package contains a set of advanced functional programming functions for working with functions and vectors. The known `lapply()` family of R Base corresponds to the `map()` functions in this package. One of the biggest advantages is being able to reduce the use of loops (`for`, etc.).

```{r}
# list of two vectors
vec_list <- list(x = 1:10, y = 50:70)

# calculate the average for each one
map(vec_list, mean)

# change the output type map_* (dbl, chr, lgl, etc.)
map_dbl(vec_list, mean)

```

Finally, a more complex example. We calculate the correlation coefficient between residential and park mobility in all European countries. To get a tidy summary of a model or test we use the `tidy()` function of the `broom` package.


```{r}
library(broom) # tidy outputs

# custom function
cor_test <- function(x, formula) { 
  
df <- cor.test(as.formula(formula), data = x) %>% tidy()

return(df)
  
}

# prepare the data
europe_reg <- filter(google_mobility, 
                           !is.na(sub_region_1),
                           !is.na(residential_percent_change_from_baseline)) %>%
                     left_join(wld, by = c("country_region_code"="iso_a2")) %>% 
                     filter(subregion %in% c("Northern Europe",
                                         "Southern Europe",
                                          "Western Europe",
                                          "Eastern Europe"))

# apply the function to each country creating a list
cor_mobility <- europe_reg %>%
                 split(.$country_region_code) %>% 
                   map(cor_test, formula = "~ residential_percent_change_from_baseline + parks_percent_change_from_baseline")  

cor_mobility[1:5]

```

As we've seen before, there are subfunctions of `map_*` to get an object of another class instead of a list, here for a bind `data.frame`.

```{r}
cor_mobility <- europe_reg %>%
                  split(.$country_region_code) %>% 
                     map_df(cor_test, 
                            formula = "~ residential_percent_change_from_baseline + parks_percent_change_from_baseline", 
                            .id = "country_code")

arrange(cor_mobility, estimate)
```


Other practical examples here in this [post](https://dominicroye.github.io/en/2019/import-excel-sheets-with-r/) or this [other](https://dominicroye.github.io/en/2019/tidy-correlation-tests-in-r/). More details can be found in the [Cheat-Sheet](https://github.com/rstudio/cheatsheets/raw/master/purrr.pdf) of `purrr`.
