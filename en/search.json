[{"authors":null,"categories":["R","R:elementary","visualization"],"content":"\r\rThe climate of a place is usually presented through climographs that combine monthly precipitation and temperature in a single chart. However, it is also interesting to visualize the climate on a daily scale showing the thermal amplitude and the daily average temperature. To do this, the averages for each day of the year of daily minimums, maximums and means are calculated.\nThe annual climate cycle presents a good opportunity to use a radial or polar chart which allows us to clearly visualize seasonal patterns.\nPackages\rWe will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\rjanitor\rSimple functions to examine and clean data\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r# packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(fs)\rlibrary(janitor)\r\rPreparation\rData\rWe download the temperature data for a selection of US cities here. You can access other cities of the entire world through the WMO or GHCN datasets at NCDC/NOAA.\n\rImport\rTo import the temperature time series of each city, which we find in several files, we apply the read_csv() function using map_df(). The dir_ls() function of the fs package returns the list of files with csv extension. The suffix df of map() indicates that we want to join all imported tables into a single one. For those with less experience with tidyverse, I recommend a short introduction on this blog post.\nThen we obtain the names of the weather stations and define a new vector with the new city names.\n# import data\rmeteo \u0026lt;- dir_ls(regexp = \u0026quot;.csv$\u0026quot;) %\u0026gt;% map_df(read_csv)\rmeteo\r## # A tibble: 211,825 x 12\r## STATION NAME LATITUDE LONGITUDE ELEVATION DATE TAVG TMAX TMIN\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-01 6.8 NA NA\r## 2 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-02 8.4 NA NA\r## 3 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-03 11 NA NA\r## 4 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-04 -7.2 NA NA\r## 5 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-05 -10.2 NA NA\r## 6 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-06 -4.6 NA NA\r## 7 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-07 -7.1 NA NA\r## 8 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-08 -5.8 NA NA\r## 9 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-09 2.9 NA NA\r## 10 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-10 3.9 NA NA\r## # ... with 211,815 more rows, and 3 more variables: TAVG_ATTRIBUTES \u0026lt;chr\u0026gt;,\r## # TMAX_ATTRIBUTES \u0026lt;chr\u0026gt;, TMIN_ATTRIBUTES \u0026lt;chr\u0026gt;\r# station names\rstats_names \u0026lt;- unique(meteo$NAME)\rstats_names\r## [1] \u0026quot;CHICAGO OHARE INTERNATIONAL AIRPORT, IL US\u0026quot; ## [2] \u0026quot;LAGUARDIA AIRPORT, NY US\u0026quot; ## [3] \u0026quot;MIAMI INTERNATIONAL AIRPORT, FL US\u0026quot; ## [4] \u0026quot;HOUSTON INTERCONTINENTAL AIRPORT, TX US\u0026quot; ## [5] \u0026quot;ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPORT, GA US\u0026quot;\r## [6] \u0026quot;SAN FRANCISCO INTERNATIONAL AIRPORT, CA US\u0026quot; ## [7] \u0026quot;SEATTLE TACOMA AIRPORT, WA US\u0026quot; ## [8] \u0026quot;DENVER INTERNATIONAL AIRPORT, CO US\u0026quot; ## [9] \u0026quot;MCCARRAN INTERNATIONAL AIRPORT, NV US\u0026quot;\r# new city names\rcities \u0026lt;- c(\u0026quot;CHICAGO\u0026quot;, \u0026quot;NEW YORK\u0026quot;, \u0026quot;MIAMI\u0026quot;, \u0026quot;HOUSTON\u0026quot;, \u0026quot;ATLANTA\u0026quot;, \u0026quot;SAN FRANCISCO\u0026quot;, \u0026quot;SEATTLE\u0026quot;, \u0026quot;DENVER\u0026quot;, \u0026quot;LAS VEGAS\u0026quot;)\r\rModify\rIn the first step, we will modify the original data, 1) selecting only the columns of interest, 2) filtering the period 1991-2020, 3) defining the new city names, 4) calculating the average temperature where it is absent, 5) cleaning the column names, and 6) creating a new variable with the days of the year. The clean_names() function of the janitor package is very useful for getting clean column names.\nmeteo \u0026lt;- select(meteo, NAME, DATE, TAVG:TMIN) %\u0026gt;% filter(DATE \u0026gt;= \u0026quot;1991-01-01\u0026quot;, DATE \u0026lt;= \u0026quot;2020-12-31\u0026quot;) %\u0026gt;% mutate(NAME = factor(NAME, stats_names, cities),\rTAVG = ifelse(is.na(TAVG), (TMAX+TMIN)/2, TAVG),\ryd = yday(DATE)) %\u0026gt;% clean_names()\rIn the next step, we calculate the daily maximum, minimum and mean temperature for each day of the year. It now only remains to convert the days of the year into a dummy date. Here we use the year 2000 since it is a leap year, and we have a total of 366 days.\n# estimate the daily averages\rmeteo_yday \u0026lt;- group_by(meteo, name, yd) %\u0026gt;% summarise(ta = mean(tavg, na.rm = TRUE),\rtmx = mean(tmax, na.rm = TRUE),\rtmin = mean(tmin, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;name\u0026#39;. You can override using the `.groups` argument.\rmeteo_yday\r## # A tibble: 3,294 x 5\r## # Groups: name [9]\r## name yd ta tmx tmin\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 CHICAGO 1 -3.77 0.537 -7.86\r## 2 CHICAGO 2 -2.64 1.03 -6.68\r## 3 CHICAGO 3 -2.88 0.78 -6.93\r## 4 CHICAGO 4 -2.86 0.753 -7.10\r## 5 CHICAGO 5 -4.13 -0.137 -8.33\r## 6 CHICAGO 6 -4.50 -1.15 -8.05\r## 7 CHICAGO 7 -4.70 -0.493 -8.57\r## 8 CHICAGO 8 -3.97 0.147 -8.02\r## 9 CHICAGO 9 -3.47 0.547 -7.49\r## 10 CHICAGO 10 -3.41 1.09 -7.64\r## # ... with 3,284 more rows\r# convert the days of the year into a dummy date\rmeteo_yday \u0026lt;- mutate(meteo_yday, yd = as_date(yd, origin = \u0026quot;1999-12-31\u0026quot;))\r\r\rCreating the climate circles\rPredefinitions\rWe define a divergent vector of various hues.\ncol_temp \u0026lt;- c(\u0026quot;#cbebf6\u0026quot;,\u0026quot;#a7bfd9\u0026quot;,\u0026quot;#8c99bc\u0026quot;,\u0026quot;#974ea8\u0026quot;,\u0026quot;#830f74\u0026quot;,\r\u0026quot;#0b144f\u0026quot;,\u0026quot;#0e2680\u0026quot;,\u0026quot;#223b97\u0026quot;,\u0026quot;#1c499a\u0026quot;,\u0026quot;#2859a5\u0026quot;,\r\u0026quot;#1b6aa3\u0026quot;,\u0026quot;#1d9bc4\u0026quot;,\u0026quot;#1ca4bc\u0026quot;,\u0026quot;#64c6c7\u0026quot;,\u0026quot;#86cabb\u0026quot;,\r\u0026quot;#91e0a7\u0026quot;,\u0026quot;#c7eebf\u0026quot;,\u0026quot;#ebf8da\u0026quot;,\u0026quot;#f6fdd1\u0026quot;,\u0026quot;#fdeca7\u0026quot;,\r\u0026quot;#f8da77\u0026quot;,\u0026quot;#fcb34d\u0026quot;,\u0026quot;#fc8c44\u0026quot;,\u0026quot;#f85127\u0026quot;,\u0026quot;#f52f26\u0026quot;,\r\u0026quot;#d10b26\u0026quot;,\u0026quot;#9c042a\u0026quot;,\u0026quot;#760324\u0026quot;,\u0026quot;#18000c\u0026quot;)\rWe create a table with the x-axis grid lines.\ngrid_x \u0026lt;- tibble(x = seq(ymd(\u0026quot;2000-01-01\u0026quot;), ymd(\u0026quot;2000-12-31\u0026quot;), \u0026quot;month\u0026quot;), y = rep(-10, 12), xend = seq(ymd(\u0026quot;2000-01-01\u0026quot;), ymd(\u0026quot;2000-12-31\u0026quot;), \u0026quot;month\u0026quot;), yend = rep(41, 12))\rWe define all the style elements of the graph in our own theme theme_cc().\ntheme_cc \u0026lt;- function(){ theme_minimal(base_family = \u0026quot;Montserrat\u0026quot;) %+replace%\rtheme(plot.title = element_text(hjust = 0.5, colour = \u0026quot;white\u0026quot;, size = 30, margin = margin(b = 20)),\rplot.caption = element_text(colour = \u0026quot;white\u0026quot;, size = 9, hjust = .5, vjust = -30),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.margin = margin(1, 1, 2, 1, unit = \u0026quot;cm\u0026quot;),\raxis.text.x = element_text(face = \u0026quot;italic\u0026quot;, colour = \u0026quot;white\u0026quot;),\raxis.title.y = element_blank(),\raxis.text.y = element_blank(),\rlegend.title = element_text(colour = \u0026quot;white\u0026quot;),\rlegend.position = \u0026quot;bottom\u0026quot;,\rlegend.justification = 0.5,\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;),\rstrip.text = element_text(colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 14),\rpanel.spacing.y = unit(1, \u0026quot;lines\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rpanel.grid = element_blank()\r) }\r\rGraph\rWe start by building a chart for New York City only. We will use geom_linerange() to define line range with the daily maximum and minimum temperature. Also, we will draw the range line colour based on the mean temperature. Finally, we can adjust alpha and size to get a nicer look.\n# filter New York\rny_city \u0026lt;- filter(meteo_yday, name == \u0026quot;NEW YORK\u0026quot;) # graph\rggplot(ny_city) + geom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion()) +\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;,\rdate_labels = \u0026quot;%b\u0026quot;) +\rlabs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) \rTo get the polar graph it would only be necessary to add the coord_polar() function.\n# polar chart\rggplot(ny_city) + geom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion()) +\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;,\rdate_labels = \u0026quot;%b\u0026quot;) +\rcoord_polar() +\rlabs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) \rIn the final graph, we add the grid defining the lines on the y-axis with geom_hline() and those on the x-axis with geom_segement(). The most important thing here is the facet_wrap() function, which allows multiple facets of charts. The formula format is used to specify how the facets are created: row ~ column. If we do not have a second variable, a point . is indicated in the formula. In addition, we make changes to the appearance of the colour bar with guides() and guide_colourbar(), and we include the theme_cc() style.\nggplot(meteo_yday) + geom_hline(yintercept = c(-10, 0, 10, 20, 30, 40), colour = \u0026quot;white\u0026quot;, size = .4) +\rgeom_segment(data = grid_x , aes(x = x, y = y, xend = xend, yend = yend), linetype = \u0026quot;dashed\u0026quot;, colour = \u0026quot;white\u0026quot;, size = .2) +\rgeom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion())+\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rguides(colour = guide_colourbar(barwidth = 15,\rbarheight = 0.5, title.position = \u0026quot;top\u0026quot;)\r) +\rfacet_wrap(~name, nrow = 3) +\rcoord_polar() + labs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) +\rtheme_cc()\r\r\r","date":1630713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630713600,"objectID":"dc53a6d3a61760ad2cfdd22209088f21","permalink":"https://dominicroye.github.io/en/2021/climate-circles/","publishdate":"2021-09-04T00:00:00Z","relpermalink":"/en/2021/climate-circles/","section":"post","summary":"The climate of a place is usually presented through climographs that combine monthly precipitation and temperature in a single chart. However, it is also interesting to visualize the climate on a daily scale showing the thermal amplitude and the daily average temperature. To do this, the averages for each day of the year of the daily minimums, maximums and averages are calculated. The annual climate cycle presents a good opportunity to use a radial or polar which allows us to clearly visualize seasonal patterns.","tags":["climate","polar","temperatures"],"title":"Climate circles","type":"post"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide"],"categories":null,"content":"","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"5f3de44d03c87d48589c2168f1d7a5a8","permalink":"https://dominicroye.github.io/en/publication/mediterranean_drought_2021/","publishdate":"2021-07-22T00:00:00Z","relpermalink":"/en/publication/mediterranean_drought_2021/","section":"publication","summary":"Drought is one of the most complex climate-related phenomena and is expected to progressively affect our lives by causing very serious environmental and socioeconomic damage by the end of the 21st century. In this study, we have extracted a dataset of exceptional meteorological drought events between 1975 and 2019 at the country and subregional scales. Each drought event was described by its start and end date, intensity, severity, duration, areal extent, peak month and peak area. To define such drought events and their characteristics, separate analyses based on three drought indices were performed at 12-month timescale: the Standardized Precipitation Index (SPI), the Standardized Precipitation Evapotranspiration Index (SPEI), and the Reconnaissance Drought Index (RDI). A multivariate combined drought index (DXI) was developed by merging the previous three indices for more understanding of droughts’ features at the country and subregional levels. Principal component analysis (PCA) was used to identify five different drought subregions based on DXI-12 values for 312 Mediterranean stations and a new special score was defined to classify the multi-subregional exceptional drought events across the Mediterranean Basin (MED). The results indicated that extensive drought events occurred more frequently since the late 1990s, showing several drought hotspots in the last decades in the southeastern Mediterranean and northwest Africa. In addition, the results showed that the most severe events were more detected when more than single drought index was used. The highest percentage area under drought was also observed through combining the variations of three drought indices. Furthermore, the drought area in both dry and humid areas in the MED has also experienced a remarkable increase since the late 1990s. Based on a comparison of the drought events during the two periods—1975–1996 and 1997–2019—we find that the current dry conditions in the MED are more severe, intense, and frequent than the earlier period; moreover, the strongest dry conditions occurred in last two decades. The SPEI-12 and RDI-12 have a higher capacity in providing a more comprehensive description of the dry conditions because of the inclusion of temperature or atmospheric evaporative demand in their scheme. A complex range of atmospheric circulation patterns, particularly the Western Mediterranean Oscillation (WeMO) and East Atlantic/West Russia (EATL/WRUS), appear to play an important role in severe, intense and region-wide droughts, including the two most severe droughts, 1999–2001 and 2007–2012, with lesser influence of the NAO, ULMO and SCAND.","tags":["climate change","drought event","Mediterranean basin","meteorological drought","SPEI"],"title":"Mediterranean-Scale Drought: Regional Datasets for Exceptional Meteorological Drought Events during 1975-2019","type":"publication"},{"authors":["D Royé","A Tobias","A Figueiras","S Gestal","A Santurtun","C Iñiguez"],"categories":null,"content":"","date":1626480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626480000,"objectID":"4ef7f8885f68fa674351f5f2590b28ec","permalink":"https://dominicroye.github.io/en/publication/prescriptions_spain_er_2021/","publishdate":"2021-07-17T00:00:00Z","relpermalink":"/en/publication/prescriptions_spain_er_2021/","section":"publication","summary":"Background: The increased risk of mortality during periods of high and low temperatures has been well established. However, most of the studies used daily counts of deaths or hospitalisations as health outcomes, although they are the ones at the top of the health impact pyramid reflecting only a limited proportion of patients with the most severe cases. Objectives: This study evaluates the relationship between short-term exposure to the daily mean temperature and medication prescribed for the respiratory system in five Spanish cities. Methods: We fitted time series regression models to cause-specific medical prescriptions, including different respiratory subgroups and age groups. We included a distributed lag non-linear model with lags up to 14 days for daily mean temperature. City-specific associations were summarised as overall-cumulative exposure-response curves. Results: We found a positive association between cause-specific medical prescriptions and daily mean temperature with a non-linear inverted J- or V-shaped relationship in most cities. Between 0.3% and 0.6% of all respiratory prescriptions were attributed to cold for Madrid, Zaragoza and Pamplona, while in cities with only cold effects the attributable fractions were estimated as 19.2% for Murcia and 13.5% for Santander. Heat effects in Madrid, Zaragoza and Pamplona showed higher fractions between 8.7% and 17.2%. The estimated costs are in general higher for heat effects, showing annual values ranging between €191,905 and €311,076 for heat per 100,000 persons. Conclusions: This study provides novel evidence of the effects of the thermal environment on the prescription of medication for respiratory disorders in Spain, showing that low and high temperatures lead to an increase in the number of such prescriptions. The consumption of medication can reflect exposure to the environment with a lesser degree of severity in terms of morbidity.","tags":["Spain","drugs","medical prescriptions","respiratory","temperature"],"title":"Temperature-related effects on respiratory medical prescriptions in Spain","type":"publication"},{"authors":["A Martí","D Royé"],"categories":null,"content":"","date":1626307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626307200,"objectID":"1cdd1068b8b6b9efe30ab141cb89fe8d","permalink":"https://dominicroye.github.io/en/publication/hotnights_madrid_2021/","publishdate":"2021-07-15T00:00:00Z","relpermalink":"/en/publication/hotnights_madrid_2021/","section":"publication","summary":"In this work, a new methodology is applied to study hot nights, also called 'tropical nights', in the metropolitan area of Madrid. To evaluate hot nights from a temporal and spatial perspective in which the population may be affected by thermal stress, high resolution gridded hourly temperature data were used. The use of two indicators obtained through hourly data, together with the climatic information provided by the UrbClim model, allowed to evaluate the thermal night characteristics of July between 2008 and 2017 at a detailed scale. Hence we were able to estimate precisely the risk to the well-being and health of the population. The results show great interurban variability in terms of intensity and duration of heat stress and a significant correlation between heat island intensities and excess heat. Likewise, a close relationship between the typologies of land uses and urban structures defined in the Urban Atlas and the indices of night heat excess has been established.","tags":["Madrid","hot nights","urban climate","heat stress","night temperature"],"title":"Intensity and duration of heat stress in summer in the urban area of Madrid","type":"publication"},{"authors":["Q Zhao","et al."],"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625961600,"objectID":"df6bd117fad672b89f6dd9df86bceec8","permalink":"https://dominicroye.github.io/en/publication/mort_grid_lancet_2021/","publishdate":"2021-07-11T00:00:00Z","relpermalink":"/en/publication/mort_grid_lancet_2021/","section":"publication","summary":"Background Exposure to cold or hot temperatures is associated with premature deaths. We aimed to evaluate the global, regional, and national mortality burden associated with non-optimal ambient temperatures. Methods In this modelling study, we collected time-series data on mortality and ambient temperatures from 750 locations in 43 countries and five meta-predictors at a grid size of 0·5° × 0·5° across the globe. A three-stage analysis strategy was used. First, the temperature–mortality association was fitted for each location by use of a time-series regression. Second, a multivariate meta-regression model was built between location-specific estimates and meta-predictors. Finally, the grid-specific temperature–mortality association between 2000 and 2019 was predicted by use of the fitted meta-regression and the grid-specific meta-predictors. Excess deaths due to non-optimal temperatures, the ratio between annual excess deaths and all deaths of a year (the excess death ratio), and the death rate per 100 000 residents were then calculated for each grid across the world. Grids were divided according to regional groupings of the UN Statistics Division. Findings Globally, 5 083 173 deaths (95% empirical CI [eCI] 4 087 967–5 965 520) were associated with non-optimal temperatures per year, accounting for 9·43% (95% eCI 7·58–11·07) of all deaths (8·52% [6·19–10·47] were cold-related and 0·91% [0·56–1·36] were heat-related). There were 74 temperature-related excess deaths per 100 000 residents (95% eCI 60–87). The mortality burden varied geographically. Of all excess deaths, 2 617 322 (51·49%) occurred in Asia. Eastern Europe had the highest heat-related excess death rate and Sub-Saharan Africa had the highest cold-related excess death rate. From 2000–03 to 2016–19, the global cold-related excess death ratio changed by −0·51 percentage points (95% eCI −0·61 to −0·42) and the global heat-related excess death ratio increased by 0·21 percentage points (0·13–0·31), leading to a net reduction in the overall ratio. The largest decline in overall excess death ratio occurred in South-eastern Asia, whereas excess death ratio fluctuated in Southern Asia and Europe. Interpretation Non-optimal temperatures are associated with a substantial mortality burden, which varies spatiotemporally. Our findings will benefit international, national, and local communities in developing preparedness and prevention strategies to reduce weather-related impacts immediately and under climate change scenarios.","tags":["mortality","hot nights","human health","climate change","tropical nights"],"title":"Global, regional, and national burden of mortality associated with non-optimal ambient temperatures from 2000 to 2019: a three-stage modelling study","type":"publication"},{"authors":["D Royé","F Sera","A Tobías","R Lowe","A Gasparrini","M Pascal","F de'Donato","B Nunes","JP Teixeira"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"7805dc4b6a8eed52932e0336624261df","permalink":"https://dominicroye.github.io/en/publication/hot_nights_europe_2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/en/publication/hot_nights_europe_2021/","section":"publication","summary":"Background: There is strong evidence concerning the impact of heat stress on mortality, particularly from high temperatures. However, few studies to our knowledge emphasize the importance of hot nights, which may prevent necessary nocturnal rest. Objectives: In this study, we use hot-night duration and excess to predict daily cause-specific mortality in summer, using multiple cities across Southern Europe. Methods: We fitted time series regression models to summer cause-specific mortality, including natural, respiratory, and cardiovascular causes, in 11 cities across four countries. We included a distributed lag non-linear model with lags up to 7 days for hot night duration and excess adjusted by daily mean temperature. We summarized city-specific associations as overall-cumulative exposure–response curves at the country level using meta-analysis. Results: We found positive but generally non-linear associations between relative risk of cause-specific mortality and duration and excess of hot nights. Relative risk of duration associated with nonaccidental mortality in Portugal was 1.29 (95% CI, 1.07 to 1.54); other associations were imprecise, but we also found positive city-specific estimates for Rome and Madrid. Risk of hot-night excess ranged from 1.12 (95% CI, 1.05 to 1.20) for France to 1.37 (95% CI, 1.26 to 1.48) for Portugal. Risk estimates for excess were consistently higher than for duration. Conclusions: This study provides new evidence that, over a wider range of locations, hot night indices are strongly associated with cause-specific deaths. Modeling the impact of thermal characteristics during summer nights on mortality could improve decisionmaking for preventive public health strategies.","tags":["mortality","hot nights","human health","climate change","tropical nights"],"title":"Effects of hot nights on mortality in Southern Europe","type":"publication"},{"authors":null,"categories":["gis","R","R:intermediate","visualization"],"content":"\r\rCartography firefly\rFirefly maps are promoted and described by\rJohn Nelson who published a post in 2016 about its characteristics. However, these types of maps are linked to ArcGIS, which has led me to try to recreate them in R. The recent ggplot2 extension ggshadow facilitates the creation of this cartographic style. It is characterized by three elements 1) a dark and unsaturated basemap (eg satellite imagery) 2) a masked vignette and highlighted area and 3) a single bright thematic layer. The essential are the colors and the brightness that is achieved with cold colors, usually neon colors. John Nelson explains more details in this post.\nWhat is the firefly style for? In the words of John Nelson: “the map style that captures our attention and dutifully honors the First Law of Geography”. John refers to what was said by Waldo Tobler\r“everything is related to everything else, but near things are more related than distant things” (Tobler 1970).\nIn this post we will visualize all earthquakes recorded in southwestern Europe with a magnitude greater than 3.\n\rPackages\rWe will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rplotwidgets\rContains functions for color conversion (RGB, HSL)\r\rterra\rImport, export and manipulate raster (raster successor package)\r\rraster\rImport, export and manipulate raster\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rggshadow\rggplot2 extension for shaded and glow geometries\r\rggspatial\rggplot2 extension for spatial objects\r\rggnewscale\rggplot2 extension to create multiple scales\r\rjanitor\rSimple functions to examine and clean data\r\rrnaturalearth\rVector maps of the world ‘Natural Earth’\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;plotwidgets\u0026quot;)) install.packages(\u0026quot;plotwidgets\u0026quot;)\rif(!require(\u0026quot;ggshadow\u0026quot;)) install.packages(\u0026quot;ggshadow\u0026quot;)\rif(!require(\u0026quot;ggspatial\u0026quot;)) install.packages(\u0026quot;ggspatial\u0026quot;)\rif(!require(\u0026quot;ggnewscale\u0026quot;)) install.packages(\u0026quot;ggnewscale\u0026quot;)\rif(!require(\u0026quot;janitor\u0026quot;)) install.packages(\u0026quot;janitor\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r# load packages\rlibrary(raster)\rlibrary(terra)\rlibrary(sf)\rlibrary(tidyverse)\rlibrary(plotwidgets)\rlibrary(ggshadow)\rlibrary(ggspatial)\rlibrary(ggnewscale)\rlibrary(janitor)\rlibrary(rnaturalearth)\r\rPreparation\rData\rFirst we download all the necessary data. For the base map we will use the Blue Marble imagery via the access to worldview.earthdata.nasa.gov where I have downloaded a selection of the area of interest in geoTiff format with a resolution of 1 km. It is important to adjust the resolution to the necessary detail of the map.\n\rBlue Marble selection via worldview.earthdata.nasa.gov ( ~ 66 MB) download\rRecords of historical earthquakes in southwestern Europe from IGN download\r\r\rImport\rThe first thing we do is to import the RGB Blue Marble raster and the earthquake data. To import the raster I use the new package terra which is the successor of the raster package. You can find a recent comparison here. Not all packages are yet compatible with the new SpatRaster class, so we also need the raster package.\n# earthquakes\rearthquakes \u0026lt;- read.csv2(\u0026quot;catalogoComunSV_1621713848556.csv\u0026quot;)\rstr(earthquakes)\r## \u0026#39;data.frame\u0026#39;: 149724 obs. of 10 variables:\r## $ Evento : chr \u0026quot; 33\u0026quot; \u0026quot; 34\u0026quot; \u0026quot; 35\u0026quot; \u0026quot; 36\u0026quot; ...\r## $ Fecha : chr \u0026quot; 02/03/1373\u0026quot; \u0026quot; 03/03/1373\u0026quot; \u0026quot; 08/03/1373\u0026quot; \u0026quot; 19/03/1373\u0026quot; ...\r## $ Hora : chr \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; ...\r## $ Latitud : chr \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; ...\r## $ Longitud : chr \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; ...\r## $ Prof...Km. : int NA NA NA NA NA NA NA NA NA NA ...\r## $ Inten. : chr \u0026quot; VIII-IX\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ...\r## $ Mag. : chr \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ...\r## $ Tipo.Mag. : int NA NA NA NA NA NA NA NA NA NA ...\r## $ LocalizaciÃ³n: chr \u0026quot;RibagorÃ§a.L\u0026quot; \u0026quot;RibagorÃ§a.L\u0026quot; \u0026quot;RibagorÃ§a.L\u0026quot; \u0026quot;RibagorÃ§a.L\u0026quot; ...\r# Blue Marble RGB raster\rbm \u0026lt;- rast(\u0026quot;snapshot-2017-11-30T00_00_00Z.tiff\u0026quot;)\rbm # contains three layers (red, green, blue)\r## class : SpatRaster ## dimensions : 7156, 7156, 3 (nrow, ncol, nlyr)\r## resolution : 0.008789272, 0.008789272 (x, y)\r## extent : -33.49823, 29.39781, 15.77547, 78.67151 (xmin, xmax, ymin, ymax)\r## coord. ref. : +proj=longlat +datum=WGS84 +no_defs ## source : snapshot-2017-11-30T00_00_00Z.tiff ## red-grn-blue: 1, 2, 3 ## names : sna_1, sna_2, sna_3\r# plot\rplotRGB(bm)\r# country boundaries\rlimits \u0026lt;- ne_countries(scale = 50, returnclass = \u0026quot;sf\u0026quot;)\r\rEarthquakes\rIn this step we clean the imported earthquakes data. 1) We convert longitude, latitude and magnitude into numeric using the parse_number() function and clean the column names with the clean_names() function, 2) We create a spatial object sf and project it using the EPSG:3035 corresponding to ETRS89-extended/LAEA Europe.\n# we clean the data and create an sf object\rearthquakes \u0026lt;- earthquakes %\u0026gt;% clean_names() %\u0026gt;%\rmutate(across(c(mag, latitud, longitud), parse_number)) %\u0026gt;%\rst_as_sf(coords = c(\u0026quot;longitud\u0026quot;, \u0026quot;latitud\u0026quot;), crs = 4326) %\u0026gt;% st_transform(3035) # project to Laea\r\rBlue Marble background Map\rWe cropped the background map to a smaller extent, but we still haven’t limited to the final area yet.\n# clip to the desired area\rbm \u0026lt;- crop(bm, extent(-20, 10, 30, 50)) # W, E, S, N\rTo obtain an unsaturated version of the Blue Marble RGB raster, we must apply a function created for this purpose. In this, we use the rgb2hsl() function from the plotwidgets package, which helps us converting RGB to HSL and vice versa. The HSL model is defined by Hue, Saturation, Lightness. The last two parameters are expressed in ratio or percentage. The hue is defined on a color wheel from 0 to 360º. 0 is red, 120 is green, 240 is blue. To change the saturation we only have to reduce the value of S.\n# function to change saturation from RGB\rsaturation \u0026lt;- function(rgb, s = .5){\rhsl \u0026lt;- rgb2hsl(as.matrix(rgb))\rhsl[2, ] \u0026lt;- s\rrgb_new \u0026lt;- as.vector(t(hsl2rgb(hsl)))\rreturn(rgb_new)\r}\rWe employ our saturation() function using the app() function that applies it to each pixel with the three RGB layers. We add the argument s, which defines the desired saturation level. This step may take several minutes. Then we project our RGB image.\n# apply the function to unsaturate with 5%\rbm_desat \u0026lt;- app(bm, saturation, s = .05)\r# plot new RGB image\rplotRGB(bm_desat)\r# project bm_desat \u0026lt;- terra::project(bm_desat, \u0026quot;epsg:3035\u0026quot;)\r\r\rFirefly map construction\rBoundaries and graticules\rBefore starting to build the map, we create graticules and set the final map limits.\n# define the final map extent\rbx \u0026lt;- tibble(x = c(-13, 6.7), y = c(31, 47)) %\u0026gt;% st_as_sf(coords = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), crs = 4326) %\u0026gt;%\rst_transform(3035) %\u0026gt;% st_bbox()\r# create map graticules\rgrid \u0026lt;- st_graticule(earthquakes) \r\rMap with image background\rThe layer_spatial() function of ggspatial allows us to add an RGB raster without major problems, however, it still does not support the newSpatRaster class. Therefore, we must convert it to the stack class with the stack() function. It is also possible to use instead of geom_sf(), the layer_spatial() function for vector objects of class sf orsp.\nggplot() +\rlayer_spatial(data = stack(bm_desat)) + # blue marble background map\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) + # country boundaries\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void()\r\rMap with background and earthquakes\rTo create the glow effect on firefly maps, we use the geom_glowpoint() function from the ggshadow package. There is also the same function for lines. Since our data is of spatial class sf and the geometry sf is not directly supported, we must indicate as an argument stats = \"sf_coordinates\" and inside aes() indicate geometry = geometry. We will map the size of the points as a function of magnitude. In addition, we filter those earthquakes with a magnitude greater than 3.\nInside the geom_glowpoint() function, 1) we define the desired color for the point and the glow effect, 2) the degree of transparency with alpha either for the point or for the glow. Finally, in the scale_size() function we set the range (minimum, maximum) of the size that the points will have.\nggplot() +\rlayer_spatial(data = stack(bm_desat)) +\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = grid, colour = \u0026quot;white\u0026quot;, size = .1, alpha = .5) +\rgeom_glowpoint(data = filter(earthquakes, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .8,\rcolor = \u0026quot;#6bb857\u0026quot;,\rshadowcolour = \u0026quot;#6bb857\u0026quot;,\rshadowalpha = .1,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.1, 1.5)) +\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void()\r\rFinal map\rThe glow effect of firefly maps is characterized by having a white tone or a lighter tone in the center of the points. To achieve this, we must duplicate the previous created layer, changing only the color and make the glow points smaller.\nBy default, ggplot2 does not allow to use multiple scales for the same characteristic (size, color, etc) of different layers. But the ggnewscale package gives us the ability to incorporate multiple scales of a feature from different layers. The only important thing to achieve this is the order in which each layer (geom) and scale is added. First we must add the geometry and then its corresponding scale. We indicate with new_scale('size') that the next layer and scale is a new one independent of the previous one. If we used color or fill it would be done with new_scale_*().\nggplot() +\rlayer_spatial(data = stack(bm_desat)) +\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = grid, colour = \u0026quot;white\u0026quot;, size = .1, alpha = .5) +\rgeom_glowpoint(data = filter(earthquakes, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .8,\rcolor = \u0026quot;#6bb857\u0026quot;,\rshadowcolour = \u0026quot;#6bb857\u0026quot;,\rshadowalpha = .1,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.1, 1.5)) +\rnew_scale(\u0026quot;size\u0026quot;) +\rgeom_glowpoint(data = filter(earthquakes, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .6,\rshadowalpha = .05,\rcolor = \u0026quot;#ffffff\u0026quot;,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.01, .7)) +\rlabs(title = \u0026quot;EARTHQUAKES\u0026quot;) +\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void() +\rtheme(plot.title = element_text(size = 50, vjust = -5, colour = \u0026quot;white\u0026quot;, hjust = .95))\rggsave(\u0026quot;firefly_map.png\u0026quot;, width = 15, height = 15, units = \u0026quot;in\u0026quot;, dpi = 300)\r\r\r","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"c2a54a1f6d7c3596e2e624a76a79812d","permalink":"https://dominicroye.github.io/en/2021/firefly-cartography/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/en/2021/firefly-cartography/","section":"post","summary":"*Firefly* maps are promoted and described by [John Nelson](https://twitter.com/John_M_Nelson) who published a [post](https://adventuresinmapping.com/2016/10/17/firefly-cartography/) in 2016 about its characteristics. However, these types of maps are linked to ArcGIS, which has led me to try to recreate them in R.","tags":["firefly","map","earthquake","cartography"],"title":"Firefly cartography","type":"post"},{"authors":["A Vicedo-Cabrera","N Scovronick","F Sera","D Royé","et al."],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"950129f57ee2a7f643e96587be0fb8e0","permalink":"https://dominicroye.github.io/en/publication/attribution_nature_climatechange_2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/en/publication/attribution_nature_climatechange_2021/","section":"publication","summary":"Climate change affects human health; however, there have been no large-scale, systematic efforts to quantify the heat-related human health impacts that have already occurred due to climate change. Here, we use empirical data from 732 locations in 43 countries to estimate the mortality burdens associated with the additional heat exposure that has resulted from recent human-induced warming, during the period 1991–2018. Across all study countries, we find that 37.0% (range 20.5–76.3%) of warm-season heat-related deaths can be attributed to anthropogenic climate change and that increased mortality is evident on every continent. Burdens varied geographically but were of the order of dozens to hundreds of deaths per year in many locations. Our findings support the urgent need for more ambitious mitigation and adaptation strategies to minimize the public health impacts of climate change.","tags":["mortality","human health","climate change","attribution"],"title":"The burden of heat-related mortality attributable to recent human-induced climate change","type":"publication"},{"authors":[],"categories":null,"content":" \nVideos    The average temperature of 24 hours in August 2020 for Europe. Data: ERA5-Land.\n   The average temperature of 24 hours in January 2020. Data: ERA5-Land.\n   Smoothed daily rainfall throughout the year in Australia. Data: SILO.\n   Smoothed daily maximum temperature throughout the year in Australia. Data: SILO.\n   How do the spatial patterns of daily precipitation change throughout the year in Europe? Data: E-OBS.\n   Smoothed daily maximum temperature throughout the year in the contiguous USA. Data: PRISM.\n   Smoothed daily maximum temperature throughout the year in Europe. Data: E-OBS.\n   How do the spatial patterns of daily precipitation change throughout the year in mainland Spain and the Balearic Islands? Data: SPREAD.\n   Smoothed daily sea surface temperature throughout the year for the Northeast Atlantic, the Mediterranean, North and Black Sea. Data: NOAA/NODC.\n   Probability of a summer day (maximum temperature greater than 25ºC/77ºF) through the year in Europe. Data: E-OBS.\n   Probability of a summer day (maximum temperature greater than 25ºC/77ºF) through the year in the Contiguous United States. Data: PRISM.\n   Probability of a summer day (maximum temperature greater than 25ºC) through the year in Australia. Data: SILO.\n   Probability of a frost day (minimum temperature less than 0ºC) through the year in Europe. Data: E-OBS 18e.\n   Probability of a frost day (minimum temperature less than 0ºC/32ºF) through the year in the Contiguous United States. Data: PRISM. Platform: Google Earth Engine.\n\nGraphics \n\rWhere do we find more droughts in mainland Spain and the Balearic Islands? Idea based on John Nelson more. Data: CSIC\r\r\rMonthly sun hour anomalies of 2017 in comparison to normal period from the Iberian Peninsula and Balearic Islands.\r\r\rClimate circles for several European cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: ECA\u0026amp;D.\r\r\rClimate circles for several US cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: NOAA.\r\r\r¿Cómo fue el año 2019? Calendario de viento para el año 2019 en Santiago de Compostela. Applicación: MeteoExtremos Galicia.\r\r\r¿Cómo fue el año 2019? Calendario de la temperatura máxima para el año 2019 en Santiago de Compostela. Applicación: MeteoExtremos Galicia.\r\r\r¿Cómo fue el año 2019? Calendario de precipìtación diaria para el año 2019 en Santiago de Compostela. Applicación: MeteoExtremos Galicia.\r\r\rAnomalías de horas de sol entre 1983-2019 para la Península Ibérica. Datos: EUMETSAT.\r\r\rAnomalías de horas de sol entre 1983-2017 para Francia. Datos: EUMETSAT.\r\r\rAnomalías de horas de sol entre 1983-2017 para Alemania. Datos: EUMETSAT.\r\r\rAnomalías de horas de sol en 2017 para Santiago de Compostela. Datos: EUMETSAT.\r\r\r¿A qué hora se suelen alcanzar las temperaturas máximas o mínimas? Aquí la distribución para Santiago de Compostela. Datos: Meteogalicia.\r\r\r¿A qué hora se suelen alcanzar las temperaturas máximas o mínimas? Aquí la distribución para Vigo. Datos: Meteogalicia.\r\r\rComparison between the period 1950-1979 and 1980-2019 for the average number of days with minimum temperature above 20ºC in Europe. Data: ECA\u0026amp;D.\r\r\rComparison between the period 1950-1979 and 1980-2019 for the average number of days with maximum temperature above 40ºC. Data: ECA\u0026amp;D.\r\r\rComparison between the period 1950-1979 and 1980-2019 for the average number of days with minimum temperature above 20ºC. Data: ECA\u0026amp;D.\r\r\rLa canícula empieza de media a partir del 24 de julio. Es interesante que en la fachada atlántica las temperaturas más altas son habituales a partir del 8 de agosto, un retraso debido al efecto del océano atlántico. Datos: ECA\u0026amp;D.\r\r\rHurricane Irma and Jose. Surface wind conditions on September 6, 2017 at 23:00 PM. Data: ERA-5/Copernicus\r\r\rEx-Hurrican Leslie. Surface wind conditions on October 14, 2018 at 00:00 AM. Data: ERA-5/Copernicus\r\r\rComparison of extreme temperatures 2011-2019 in Spain. Data: AEMET OPEN\r\r\rProbability of a summer day (maximum temperature \u0026gt; 25ºC) throughout the year for three cities in Spain. Data: AEMET OPEN\r\r\rThis April 2020 has been extraordinarily cloudy. 90% of the Iberian Peninsula has experienced a cloudiness greater than 86% (normal would be 65%). Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rEste noviembre 2019 ha sido extraordinariamente nublado. El 90% de la Península Ibérica ha vivido una nubosidad superior al 90% (lo normal serían 70%). Datos: NASA/MODIS. Plataforma: Google Earth Engine.\r\r\rÍndice de Concentración de la precipitación diaria en EEUU. Datos: PRISM. Más detalles aquí.\r\r\rPromedio de Grados Día de Calefacción y Refrigeración (1950-2018) en España. Como valor de referencia he usado 15.5ºC y 22ºC de temperatura media diaria, respectivamente. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Santander. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Sevilla. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Santiago. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en Valencia. Datos: ECA\u0026amp;D.\r\r\rLas anomalías invernales de temperatura y precipitación en A Coruña. Datos: ECA\u0026amp;D.\r\r\rAnomalías de la temperatura media diaria en invierno de Madrid. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Santander. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Barcelona. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Santiago. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Bilbao. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Sevilla. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de A Coruña. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rAnomalías de la temperatura media diaria en invierno de Málaga. Datos: ECA\u0026amp;D. Las series temporales se homogeneizaron con climatol.\r\r\rPromedio del primer día de verano en Europa (temperatura máxima \u0026gt;25ºC). Datos: E-OSB 18e.\r\r\rDistribución del promedio del primer día de verano en Europa (temperatura máxima \u0026gt;25ºC). Datos: E-OSB 18e.\r\r\rWarming stripes for several Spanish cities. These graphs represent and communicate climate change in a very illustrative and effective way. Data: ECA\u0026amp;D. Time series were homogenized with climatol. More: post.\r\r\r62 years of annual anomalies of precipitation (%) in peninsular Spain in a single graphic. Data: SPREAD.\r\r\rSummer anomaly of temperature and precipitation in Barcelona 1914-2018. Data: ECA\u0026amp;D.\r\r\rSummer anomaly of temperature and precipitation in Madrid 1920-2018. Data: ECA\u0026amp;D.\r\r\rSummer anomaly of temperature and precipitation in Santiago de Compostela 1950-2018. Data: ECA\u0026amp;D.\r\r\rMonthly precipitation anomaly registered in Santiago de Compostela in 2017. Data: ECA\u0026amp;D.\r\r\rMonthly precipitation anomaly registered in Barcelona in 2018. Data: ECA\u0026amp;D, opendata.aemet.es.\r\r\rAverage of consecutive days without rainfall 1950-2012. Data: SPREAD.\r\r\rAverage of consecutive days without rainfall by seasons 1950-2012 in the Iberian Peninsula. Data: SPREAD.\r\r\rDistribution of temperature anomalies in autumn according to different decades in Barcelona. You can clearly see how the autumn is getting warmer due to global warming. Data: ECA\u0026amp;D.\r\r\rDistribution of temperature anomalies in autumn according to different decades in Santiago de Compostela. You can clearly see how the autumn is getting warmer due to global warming. Data: ECA\u0026amp;D.\r\r\rClimate circles for several Spanish cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: ECA\u0026amp;D.\r\r\rGraphic definition of climate and weather. The difference between weather and climate is particularly a scale of time. Single atmospheric conditions over a short period of time is weather, and climate is the statistical description of all these single condicions over a relatively long period of time.\r\r\rClimate circles for several Chilean cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: explorador.cr2.cl.\r\r\rSummer months, mild winters, lot of sun and little wind, the climatic preferences for the Galician population. Map is based on survey results. More: article (galician).\r\r\rWhere is the lightning activity concentrated in a few days in Galicia? Values toward 1 indicate that a few days contribute much of all the lightning; instead, values toward 0 are places where more regularity is observed. More: article.\r\r\rHow is the lightning activity distributed annual and by seasons in Galicia? Data: meteogalicia. More: article.\r\r\rHow is the lightning activity distributed by month and hour in Galicia? Data: meteogalicia. More: article.\r\r\rSeasonal rainfall regime, i.e. ranking seasons according to average precipitation (1956-2006) in descending order in the contiguous United States. (P, spring; S, summer; A, autumn; W, winter). More: article, dataset.\r\r\rConcentration of Daily Precipitation (1956-2006) in the contiguous United States. (P, spring; S, summer; A, autumn; W, winter). The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. More: article, dataset.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) through the year in the pensinular Spain. Data: STEAD from Research Group climayagua.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) for different dates in the pensinular Spain. Data: STEAD from Research Group climayagua.\r\r\rAnnual sunhours for Germany in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: ECA\u0026amp;D, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rWarming stripes for Lisboa. These graphs represent and communicate climate change in a very illustrative and effective way. Data: GISTEMP. More: post.\r\r\rWhere do we observe the trajectories of extratropical cyclones in Europe? Here the frequency for the months October to March between 1979-2010. Data: extra-tropical cyclone tracks.\r\r\rWarming stripes for Madrid. These graphs represent and communicate climate change in a very illustrative and effective way. Data: ECA\u0026amp;D. More: post.\r\r\rAnnual sunhours for Galicia (Spain) in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: Meteogalicia, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rAnnual sunhours for Spain in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: ECA\u0026amp;D, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rNumber of snow days on the ground in the Iberian Peninsula (2002-2017). The daily images with a binary code (condition: Snow_Cover_Daily_Tile == 200, and Fractional_Snow_Cover \u0026gt; 90) have been reclassified and than summed up and divided by the number of years. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for summer 2018 and normal 2001-2017 in the Iberian Peninsular. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for march 2018 and march 2001-2018 in the Iberian Peninsular. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAccumulated precipitation of 2017 compared to other years in Valladolid. Data: ECA\u0026amp;D.\r\r\rAccumulated precipitation of 2017 compared to other years in Vigo. Data: ECA\u0026amp;D.\r\r ","date":1620684000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620684000,"objectID":"0540a522c0da792daafe21d2d251604c","permalink":"https://dominicroye.github.io/en/graphs/climate/","publishdate":"2021-05-11T00:00:00+02:00","relpermalink":"/en/graphs/climate/","section":"graphs","summary":" ","tags":["climate","weather","datavis","atmosphere","temperature"],"title":"Climate and Weather","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rDaily contribution of coal to electricity generation in Germany. Data: energy-charts.de\r\r\rLa contribución diaria del carbón a la generación eléctrica en España desde 2011. Hasta el 17 de agosto de 2019 no ha habido ningún día con menos del 1%. Data: REData\r\r\rLight pollution by municipality in 2015. What is the municipality that emits the most artificial light? The map shows the Coefficient of Variation (standard deviation/mean) of each municipalities. Data: VIIRS Nighttime Lights NOAA (2015).\r\r\rWhich coast has more light pollution in Peninsular Spain? The coast margins include a buffer of 5km. Data: VIIRS Nighttime Lights NOAA (2016).\r\r\rLight pollution by municipality in 2015. What is the municipality that emits the most artificial light? The map shows the Coefficient of Variation (standard deviation/mean) of each municipalities. Data: VIIRS Nighttime Lights NOAA (2015).\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in the Iberian Peninsula. Clearly stand out Barcelona and Madrid. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in Germany. Clearly stands out the Rhine-Ruhr metropolitan region. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in the contiguous United States. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAir pollution in the Iberian Peninsula. Annual average of PM2.5 for 2016 seen by MODIS/MISR/SeaWiFS. Clearly visible are Madrid and Barcelona. Data: SEDAC.\r\r\rOur human footprint in the Iberian Peninsula, or rather, the pressure we exert on the terrestrial ecosystem. Data: Global terrestrial Human Footprint .\r\r ","date":1620684000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620684000,"objectID":"0570828f5e1287a69c4e732cc5b69020","permalink":"https://dominicroye.github.io/en/graphs/environment/","publishdate":"2021-05-11T00:00:00+02:00","relpermalink":"/en/graphs/environment/","section":"graphs","summary":" ","tags":["datavis","environment","nature","pollution","ecosystem"],"title":"Environment","type":"graphs"},{"authors":[],"categories":null,"content":" \nVideos \n   3d model of Nazaré Canyon off the coast of #Nazare, Portugal. Well known as hotspot for big wave surfing.\n\nGráficos \n\rEvolution of global disasters since 1900 by risk groups. Data: emdat.be/\r\r\rUrban growth seen by the year of construction in Paris, France. Data: ign.fr\r\r\rUrban growth seen by the year of construction in Madrid, Spain. Data: Catastro More: post.\r\r\rUrban growth seen by the year of construction in Santiago de Compostela, Spain. Data: Catastro More: post.\r\r\rDistribution of travel time deviations from the median to high-density urban centers via surface transport in Europe by NUTS-1 in 2015. Data: resourcewatch.org.\r\r\rRuggedness or the elevation difference between adjacent cells of a DEM for the Iberian Peninsula.\r\r\rDistribution of vine, fruits and olivar in Europe. Data: CORINE Land Cover\r\r\rForest canopy height with more than 10m for the Iberian Peninsula. Data: Simard et al. 2011\r\r\rWorlds\u0026rsquo;s lighthouses included in the OpenStreetMap database. More: post\r\r\rLas venas azules de la Tierra para la Península Ibérica. La ciencia es arte. El ancho de la línea refleja el tamaño del río. Datos: https://hydrosheds.org/\r\r\rRiver flow directions were estimated using vectorial data.\r\r\rRiver flow directions were estimated using vectorial data.\r\r\rOrientations of coast segements were estimated using vectorial data with a resolución of 9 vertices to 1 km.\r\r\rOrientations of coast segements were estimated using vectorial data with a resolución of 9 vertices to 1 km.\r\r\rMobility trends for places of residence during COVID-19 epidemy in Europe based on data from Google. Data: Google COVID19\r\r\rDriving mobility during COVID-19 epidemy by country based data from Apple. Data: Apple COVID19\r\r\rSignificant wave height with a return period of 10 years. Particularly in the Costa da Morte there is a possibility of waves exceeding 10 meters once every 10 years. The Rias are highly protected. Data: Wave Atlas of Meteogalicia\r\r\rOur research found that, in most Mediterranean European countries, the amount of burnt area is increasingly related with a lower number of #wildfires. I estimated ad hoc the same index for NSW Australia with similar results. More: article.\r\r\rOur research found that, in most Mediterranean European countries, the amount of burnt area is increasingly related with a lower number of #wildfires. I estimated ad hoc the same index for California with similar results. More: article.\r\r\rSpatial patterns of cemeteries in Northwest Spain (number per 1,000 inhabitants) based on OpenStreetMaps. Data: OpenStreetMap, IGE More: post.\r\r\rSpatial patterns of cemeteries in Spain (number per 10,000 inhabitants) based on OpenStreetMaps. Data: OpenStreetMap, INE More: post.\r\r\rUrban growth seen by the year of construction for the six largest Spanish cities. Data: Catastro.\r\r\rAnimation of urban growth seen by the year of construction in Valencia, Spain. Data: Catastro.\r\r\rUrban growth seen by the year of construction in Valencia, Spain. Data: Catastro.\r\r\rDistribution of the year of building construction in Spanish provincial capitals from 1850. Data: Catastro.\r\r\rDistribution of travel time to high-density urban centers via surface transport in Europe by NUTS-1 in 2015. Data: resourcewatch.org.\r\r\rDistribution of travel time to high-density urban centers via surface transport in Spain by province in 2015. Data: resourcewatch.org.\r\r\rIndice de Precios al Consumidor: Alquiler de viviendas (variación anual) 2002-2019 en España por provincia. Datos: INE.\r\r\rTiempo de viaje a centros urbanos de alta densidad a través del transporte de superficie en España. Datos: resourcewatch.org.\r\r\rRanking del tiempo de viaje a centros urbanos de alta densidad a través del transporte de superficie según provincias españolas. Datos: resourcewatch.org.\r\r\rInspirado por el gran trabajo de @geo_coe, he creado un mapa de elevación para el río serpenteante Ebro, tramo medio entre Logroño y Zaragoza en España. Datos: Modelo Digital del Terreno - MDT05.\r\r\rInspirado por el gran trabajo de @geo_coe, he creado un mapa de elevación para el río serpenteante Alagón, afluente más largo del río Tajo en España. Datos: Modelo Digital del Terreno - MDT05.\r\r\rUrban growth of Santiag de Compostela from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rFlight routes of the ten busiest airports by passenger traffic in Europe based on 24 hour data for each airport. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rThe Sky View Factor is very useful urban spatial indicator for radiation and thermal environmental assessment. SVF describes how visible is the sky (0: the entire sky is blocked from view; 1: free view on the whole sky). Estimation made using SAGA-GIS. Data: LiDAR-IGN.\r\r\rEuropean flight density based on 24 hour data for each of the ten busiest airports by passenger. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of Frankfurt airport based on 24 hour data. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of the busiest airports in the Iberian Peninsula based on 24 hour data. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of Paris Charles de Gaulle airport based on 24 hour data. Data: https://www.flightradar24.com/. More: link\r\r\rUrban growth of Madrid from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rTotal hours of fishing activity per km2 for the year 2016 in the Iberian Peninsula. Data: https://globalfishingwatch.org/\r\r\rTotal hours of fishing activity per km2 for the year 2016 in the Mediterranean. Data: https://globalfishingwatch.org/\r\r\rDistribution of gas stations (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of drinking water (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (spanish),script\r\r\rDistribution of gas and charging stations (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of drinking water (Point Of Interest) in the World, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (Spanish),script\r\r\rAnother perspective on the world. Distance to the sea (the more black, the further away is the sea). Euclidean distance estimation made with R. More: article (Spanish)\r\r\rDistribution of building heights at 10 meter resolution in European capitals. Data: COPERNICUS\r\r\rDifferences of building heights at 10 meter resolution in European capitals. Data: COPERNICUS\r\r\rUrban growth of Barcelona from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rNumber of published articles in ElPaís by year for the term \u0026lsquo;wildfire\u0026rsquo;. Data: elpais\r\r\rDistribution of fastfood restaurants in the contiguous United States, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of fastfood restaurants per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rSpain leads access to Open Data in the EU. Data: europendataportal\r\r\rDistribution of fastfood restaurants in Europe., extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rEvolution of adult obesity in Europe between 1975 and 2016. Data: ourwoldindata, script.\r\r\rNational Overdose Death in the US by different drugs since 1999, showing a horrifying trend. Data: ourwoldindata.\r\r ","date":1620684000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620684000,"objectID":"1de12ae97ed63c8a065f7603f22e9e1c","permalink":"https://dominicroye.github.io/en/graphs/geography/","publishdate":"2021-05-11T00:00:00+02:00","relpermalink":"/en/graphs/geography/","section":"graphs","summary":" ","tags":["datavis","geography","distribution","human","physical"],"title":"Geography","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rThe map of Educational inequality in Spain. Data: INE.\r\r\rThe map of inequality in Spain based on income and Gini index 2018. «Your life depends on where you live». Data: INE More: post.\r\r\rThe map of inequality in Spain based on income and Gini index 2017. «Your life depends on where you live». Data: INE More: post.\r\r\rSpatial patterns of cemeteries in Europe based on OpenStreetMaps. Data: OpenStreetMap More: post.\r\r\rThe ratio of cars with diesel engine in Germany. Data: Kraftfahrt Bundesamt\r\r\rPopulation movements registered with mobile phones in Spain. Data: INE\r\r\rPopulation movement registered with mobile phones at 12:00 in the morning compared to 20:00 in Spain. Data: INE\r\r\rPopulation movements registered with mobile phones in the COVID19 pandemic in Spain. Data: INE\r\r\rPopulation movements registered with mobile phones in november 2019 in Spain. Data: INE\r\r\rPopulation point clouds of Ourense since 1975. Data: IGE\r\r\rPopulation pyramid of Galicia since 1975. Data: IGE\r\r\rPopulation pyramid of Galician provinces since 1975. Data: IGE\r\r\rPopulation pyramid of Spanish autonomous community since 1998. Data: INE\r\r ","date":1620684000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620684000,"objectID":"f995b7608a372052bdfa5bed0c14c213","permalink":"https://dominicroye.github.io/en/graphs/population/","publishdate":"2021-05-11T00:00:00+02:00","relpermalink":"/en/graphs/population/","section":"graphs","summary":" ","tags":["datavis","demography","human","aging"],"title":"Population","type":"graphs"},{"authors":["E de Schrijver","CL Folly","R Schneider","D Royé","OH Franco","A Gasparrini","AM Vicedo-Cabrera"],"categories":null,"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"55e7d80ec0ded583e86cecadf7dd747b","permalink":"https://dominicroye.github.io/en/publication/mortality_gridded_datasets_2021/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/en/publication/mortality_gridded_datasets_2021/","section":"publication","summary":"New gridded climate datasets (GCDs) on spatially‐resolved modelled weather data have recently been released to explore the impacts of climate change. GCDs have been suggested as potential alternatives to weather station data in epidemiological assessments on health impacts of temperature and climate change. These can be particularly useful for assessment in regions that have remained understudied due to limited or low quality weather station data. However to date, no study has critically evaluated the application of GCDs of variable spatial resolution in temperature‐mortality assessments across regions of different orography, climate and size. Here we explored the performance of population‐weighted daily mean temperature data from the global ERA5 reanalysis dataset in the 10 regions in the United Kingdom and the 26 cantons in Switzerland, combined with two local high‐resolution GCDs (Haduk‐grid UKPOC‐9 and MeteoSwiss‐grid‐product, respectively) and compared these to weather station data and unweighted homologous series. We applied quasi‐Poisson time series regression with distributed lag non‐linear models to obtain the GCD‐ and region‐specific temperature‐mortality associations and calculated the corresponding cold‐ and heat‐related excess mortality. Although the five exposure datasets yielded different average area‐level temperature estimates, these deviations did not result in substantial variations in the temperature‐mortality association or impacts. Moreover, local population‐weighted GCDs showed better overall performance, suggesting that they could be excellent alternatives to help advance knowledge on climate change impacts in remote regions with large climate and population distribution variability, which has remained largely unexplored in present literature due to the lack of reliable exposure data.","tags":["gridded climate dataset","spatiotemporal analysis","reanalysis","heat","cold","mortality","climate change"],"title":"A Comparative Analysis of the Temperature‐Mortality Risks Using Different Weather Datasets Across Heterogeneous Regions","type":"publication"},{"authors":["N Lorenzo","A Díaz-Poso","D Royé"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"cd2bda4cbd18859f7a8c47c86bd06980","permalink":"https://dominicroye.github.io/en/publication/ehf_climate_change_2021/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/en/publication/ehf_climate_change_2021/","section":"publication","summary":"Heatwaves are the most relevant extreme climatic events, particularly in the context of global warming and the related increasing impacts on society and the natural environment. This work presents an analysis of climate change scenarios with simulations from the EURO-CORDEX project using the excess heat factor over the Iberian Peninsula. We focus on climate change projections of the heatwave intensity and spatial distribution, which are evaluated for the near future (2021–2050) relative to a reference past climate (1971–2000). Heatwave projections show a general significant increase in intensity, frequency, duration and spatial extent for the whole region. The average change in heatwave intensity is 104% for the whole Iberian Peninsula for the near future 2021–2050. The largest changes occur in the eastern-central region, rising to 150% for the Mediterranean coast and the Pyrenees. The greater spatial extent of heatwaves strongly suggests increased human exposure, increased energy demand, and implications for fire risk. This spatial trend is predicted to continue in the near future with increases in the maximum spatial heatwave extent ranging from 6% to 8% per decade.","tags":["mortalidad","noches calurosas","salud humana","cambio climático","noches tropicales"],"title":"Heatwave intensity on the Iberian Peninsula: Future climate projections","type":"publication"},{"authors":["BR Wright","B Laffineur","D Royé","G Armstrong","RJ Fensham"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"efb6b9a70c9d816af88d57c29857a9c7","permalink":"https://dominicroye.github.io/en/publication/megafires_australia_2021/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/en/publication/megafires_australia_2021/","section":"publication","summary":"Large, high-severity wildfires, or ‘megafires’, occur periodically in arid Australian spinifex (Triodia spp.) grasslands after high rainfall periods that trigger fuel accumulation. It has been suggested that these fires are unprecedented in the modern era and were formerly constrained by Aboriginal patch-burning that kept landscape fuel levels low. This assumption deserves scrutiny, as evidence from fire-prone systems globally indicates that weather factors often the primary determinant behind megafire incidence, and that fuel management does not mitigate such fires during periods of climatic extreme. We reviewed explorer’s diaries, anthropologist’s reports, and remotely sensed data from the Australian Western Desert for evidence of large rainfall linked fires during the pre-contact period when traditional Aboriginal patch-burning was still being practiced. We used only observations that contained empiric estimates of fire sizes. Concurrently, we employed remote rainfall data and the Oceanic Niño Index to relate fire size to likely seasonal conditions at the time the observations were made. Numerous records were found of small fires during periods of average and below-average rainfall conditions, but no evidence of large-scale fires during these times. By contrast, there was strong evidence of large-scale wildfires during a high-rainfall period in the early 1870s, some of which are estimated to have burnt areas up to 700 000 ha. Our literature review also identified several Western Desert Aboriginal mythologies that refer to large-scale conflagrations. As oral traditions sometimes corroborate historic events, these myths may add further evidence that large fires are an inherent feature of spinifex grassland fire regimes. Overall, the results suggest that, contrary to predictions of the patch-burn mosaic hypothesis, traditional Aboriginal burning did not modulate spinifex fire size during periods of extreme-high arid zone rainfall. The mechanism behind this appears to be plant assemblages in seral spinifex vegetation that comprise highly flammable non-spinifex tussock grasses that rapidly accumulate high fuel loads under favorable precipitation conditions. Our finding that fuel management does not prevent megafires under extreme conditions in arid Australia has parallels with the primacy of climatic factors as drivers of megafires in the forests of temperate Australia.","tags":["arid vegetation","fire ecology","grass-fire feedbacks","patch-burning","indigenous land management"],"title":"Rainfall-linked megafires as innate fire regime elements in arid Australian spinifex (Triodia spp.) grasslands","type":"publication"},{"authors":null,"categories":["gis","R","R:advanced","visualization"],"content":"\r\rInitial considerations\rA disadvantage of choropleth maps is that they tend to distort the relationship between the true underlying geography and the represented variable. It is because the administrative divisions do not usually coincide with the geographical reality where people live. Besides, large areas appear to have a weight that they do not really have because of sparsely populated regions. To better reflect reality, more realistic population distributions are used, such as land use. With Geographic Information Systems techniques, it is possible to redistribute the variable of interest as a function of a variable with a smaller spatial unit.\nWith point data, the redistribution process is simply clipping points with population based on land use, usually classified as urban. We could also crop and mask with land use polygons when we have a vectorial polygon layer, but an interesting alternative is the same data in raster format. We will see how we can make a dasymetric map using raster data with a resolution of 100 m. This post will use data from census sections of the median income and the Gini index for Spain. We will make a dasymetric and bivariate map, representing both variables with two ranges of colours on the same map.\n\rPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rpatchwork\rSimple grammar to combine separate ggplots into the same graphic\r\rraster\rImport, export and manipulate raster\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rbiscale\rTools and Palettes for Bivariate Thematic Mapping\r\rsysfonts\rLoad fonts in R\r\rshowtext\rUse fonts more easily in R graphs\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;patchwork\u0026quot;)) install.packages(\u0026quot;patchwork\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;biscale\u0026quot;)) install.packages(\u0026quot;biscale\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# packages\rlibrary(tidyverse)\rlibrary(sf)\rlibrary(readxl)\rlibrary(biscale)\rlibrary(patchwork)\rlibrary(raster)\rlibrary(sysfonts)\rlibrary(showtext)\rlibrary(raster)\r\rPreparation\rData\rFirst we download all the necessary data. With the exception of the CORINE Land Cover (~ 200 MB), the data stored on this blog can be obtained directly via the indicated links.\n\rCORINE Land Cover 2018 (geotiff): COPERNICUS\rIncome data and Gini index (excel) [INE]: download\rCensus limits of Spain (vectorial) [INE]: download\r\r\rImport\rThe first thing we do is to import the land use raster, the income and Gini index data, and the census boundaries.\n# raster of CORINE LAND COVER 2018\rurb \u0026lt;- raster(\u0026quot;U2018_CLC2018_V2020_20u1.tif\u0026quot;)\r## Warning in showSRID(uprojargs, format = \u0026quot;PROJ\u0026quot;, multiline = \u0026quot;NO\u0026quot;, prefer_proj\r## = prefer_proj): Discarded datum Unknown based on GRS80 ellipsoid in Proj4\r## definition\r# income data and Gini index\rrenta \u0026lt;- read_excel(\u0026quot;30824.xlsx\u0026quot;)\rgini \u0026lt;- read_excel(\u0026quot;37677.xlsx\u0026quot;)\r# census boundaries\rlimits \u0026lt;- read_sf(\u0026quot;SECC_CE_20200101.shp\u0026quot;) \r\rLand uses\rIn this first step we filter the census sections to obtain those of the Autonomous Community of Madrid, and we create the municipal limits. To dissolve the polygons of census tracts we apply the function group_by() in combination with summarise().\n# filter the Autonomous Community of Madrid\rlimits \u0026lt;- filter(limits, NCA == \u0026quot;Comunidad de Madrid\u0026quot;)\r# obtain the municipal limits\rmun_limit \u0026lt;- group_by(limits, CUMUN) %\u0026gt;% summarise()\rIn the next step we cut the land use raster with the limits of Madrid. I recommend always using the crop() function first and then mask(), the first function crop to the required extent and the second mask the values. Subsequently, we remove all the cells that correspond to 1 or 2 (urban continuous, discontinuous). Finally, we project the raster.\n# project the limits\rlimits_prj \u0026lt;- st_transform(limits, projection(urb))\r# crop and mask urb_mad \u0026lt;- crop(urb, limits_prj) %\u0026gt;% mask(limits_prj)\r# remove non-urban pixels\rurb_mad[!urb_mad %in% 1:2] \u0026lt;- NA # plot the raster\rplot(urb_mad)\r# project\rurb_mad \u0026lt;- projectRaster(urb_mad, crs = CRS(\u0026quot;+proj=longlat +datum=WGS84 +no_defs\u0026quot;))\rIn this step, we convert the raster data into a point sf object.\n# transform the raster to xyz and a sf object\rurb_mad \u0026lt;- as.data.frame(urb_mad, xy = TRUE, na.rm = TRUE) %\u0026gt;%\rst_as_sf(coords = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), crs = 4326)\r# add the columns of the coordinates\rurb_mad \u0026lt;- urb_mad %\u0026gt;% rename(urb = 1) %\u0026gt;% cbind(st_coordinates(urb_mad))\r\rIncome data and Gini index\rThe format of the Excels does not coincide with the original of the INE, since I have cleaned the format before in order to make this post easier. What remains is to create a column with the codes of the census sections and exclude data that correspond to another administrative level.\n## income and Gini index data\rrenta_sec \u0026lt;- mutate(renta, NATCODE = str_extract(CUSEC, \u0026quot;[0-9]{5,10}\u0026quot;), nc_len = str_length(NATCODE),\rmun_name = str_remove(CUSEC, NATCODE) %\u0026gt;% str_trim()) %\u0026gt;%\rfilter(nc_len \u0026gt; 5)\rgini_sec \u0026lt;- mutate(gini, NATCODE = str_extract(CUSEC, \u0026quot;[0-9]{5,10}\u0026quot;), nc_len = str_length(NATCODE),\rmun_name = str_remove(CUSEC, NATCODE) %\u0026gt;% str_trim()) %\u0026gt;%\rfilter(nc_len \u0026gt; 5)\rIn the next step we join both tables with the census tracts using left_join() and convert columns of interest in numerical mode.\n# join both the income and Gini tables with the census limits\rmad \u0026lt;- left_join(limits, renta_sec, by = c(\u0026quot;CUSEC\u0026quot;=\u0026quot;NATCODE\u0026quot;)) %\u0026gt;% left_join(gini_sec, by = c(\u0026quot;CUSEC\u0026quot;=\u0026quot;NATCODE\u0026quot;))\r# convert selected columns to numeric\rmad \u0026lt;- mutate_at(mad, c(23:27, 30:31), as.numeric)\r\rBivariate variable\rTo create a bivariate map we must construct a single variable that combines different classes of two variables. Usually we make three classes of each variable which leads to nine combinations; in our case, the average income and the Gini index. The biscale package includes helper functions to carry out this process. With the bi_class() function we create the classification variable using quantiles as algorithm. Since in both variables we find missing values, we correct those combinations between both variables where an NA appears.\n# create bivariate classification\rmapbivar \u0026lt;- bi_class(mad, GINI_2017, RNMP_2017, style = \u0026quot;quantile\u0026quot;, dim = 3) %\u0026gt;% mutate(bi_class = ifelse(str_detect(bi_class, \u0026quot;NA\u0026quot;), NA, bi_class))\r# results\rhead(dplyr::select(mapbivar, GINI_2017, RNMP_2017, bi_class))\r## Simple feature collection with 6 features and 3 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: 415538.9 ymin: 4451487 xmax: 469341.7 ymax: 4552422\r## projected CRS: ETRS89 / UTM zone 30N\r## # A tibble: 6 x 4\r## GINI_2017 RNMP_2017 bi_class geometry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;MULTIPOLYGON [m]\u0026gt;\r## 1 NA NA \u0026lt;NA\u0026gt; (((446007.9 4552348, 446133.7 4552288, 446207.8 ~\r## 2 31 13581 2-2 (((460243.8 4487756, 460322.4 4487739, 460279 44~\r## 3 30 12407 2-2 (((457392.5 4486262, 457391.6 4486269, 457391.1 ~\r## 4 34.3 13779 3-2 (((468720.8 4481374, 468695.5 4481361, 468664.6 ~\r## 5 33.5 9176 3-1 (((417140.2 4451736, 416867.5 4451737, 416436.8 ~\r## 6 26.2 10879 1-1 (((469251.9 4480826, 469268.1 4480797, 469292.6 ~\rWe finish by redistributing the inequality variable over the pixels of urban land use. The st_join() function joins the data with the land use points.\n# redistribute urban pixels to inequality\rmapdasi \u0026lt;- st_join(urb_mad, st_transform(mapbivar, 4326))\r\r\rMap building\rLegend and font\rBefore constructing both maps we must create the legend using the bi_legend() function. In the function we define the titles for each variable, the number of dimensions and the color scale. Finally, we add the Montserrat font for the final titles in the graphic.\n# bivariate legend\rlegend2 \u0026lt;- bi_legend(pal = \u0026quot;DkViolet\u0026quot;,\rdim = 3,\rxlab = \u0026quot;Higher inequality\u0026quot;,\rylab = \u0026quot;Higher income\u0026quot;,\rsize = 9)\r# download font\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\rshowtext_auto()\r\rDasymetric map\rWe build this map using geom_tile() for the pixels and geom_sf() for the municipal boundaries. In addition, it will be the map on the right where we also place the legend. To add the legend we use the annotation_custom() function indicating the position in the geographical coordinates of the map. The biscale package also helps us with the color definition via the bi_scale_fill() function.\np2 \u0026lt;- ggplot(mapdasi) + geom_tile(aes(X, Y, fill = bi_class), show.legend = FALSE) +\rgeom_sf(data = mun_limit, color = \u0026quot;grey80\u0026quot;, fill = NA, size = 0.2) +\rannotation_custom(ggplotGrob(legend2), xmin = -3.25, xmax = -2.65,\rymin = 40.55, ymax = 40.95) +\rbi_scale_fill(pal = \u0026quot;DkViolet\u0026quot;, dim = 3, na.value = \u0026quot;grey90\u0026quot;) +\rlabs(title = \u0026quot;dasymetric\u0026quot;, x = \u0026quot;\u0026quot;, y =\u0026quot;\u0026quot;) +\rbi_theme() +\rtheme(plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 30, face = \u0026quot;bold\u0026quot;)) +\rcoord_sf(crs = 4326)\r\rChoropleth map\rThe choropleth map is built in a similar way to the previous map with the difference that we use geom_sf().\np1 \u0026lt;- ggplot(mapbivar) + geom_sf(aes(fill = bi_class), colour = NA, size = .1, show.legend = FALSE) +\rgeom_sf(data = mun_limit, color = \u0026quot;white\u0026quot;, fill = NA, size = 0.2) +\rbi_scale_fill(pal = \u0026quot;DkViolet\u0026quot;, dim = 3, na.value = \u0026quot;grey90\u0026quot;) +\rlabs(title = \u0026quot;choropleth\u0026quot;, x = \u0026quot;\u0026quot;, y =\u0026quot;\u0026quot;) +\rbi_theme() +\rtheme(plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 30, face = \u0026quot;bold\u0026quot;)) +\rcoord_sf(crs = 4326)\r\rMerge both maps\rWith the help of the patchwork package, we combine both maps in a single row, first the choropleth map and on its right the dasymmetric map. More details of the grammar used for the combination of graphics here.\n# combine p \u0026lt;- p1 | p2\r# final map\rp\r\r\r","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"eca8b97d08404774ad33c59729056128","permalink":"https://dominicroye.github.io/en/2021/bivariate-dasymetric-map/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/en/2021/bivariate-dasymetric-map/","section":"post","summary":"A disadvantage of choropleth maps is that they tend to distort the relationship between the true underlying geography and the represented variable. It is because the administrative divisions do not usually coincide with the geographical reality where people live. Besides, large areas appear to have a weight that they do not really have because of sparsely populated regions. To better reflect reality, more realistic population distributions are used, such as land use. With Geographic Information Systems techniques, it is possible to redistribute the variable of interest as a function of a variable with a smaller spatial unit.","tags":["bivariate","map","inequality","income","Madrid","urban"],"title":"Bivariate dasymetric map","type":"post"},{"authors":["P Fdez-Arroyabe","K Kourtidis","C Haldoupis","et al."],"categories":null,"content":"","date":1609977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609977600,"objectID":"517057957142d7af914979ca2088a928","permalink":"https://dominicroye.github.io/en/publication/glossary_electricity_2020/","publishdate":"2021-01-07T00:00:00Z","relpermalink":"/en/publication/glossary_electricity_2020/","section":"publication","summary":"There is an increasing interest to study the interactions between atmospheric electrical parameters and living organisms at multiple scales. So far, relatively few studies have been published that focus on possible biological effects of atmospheric electric and magnetic fields. To foster future work in this area of multidisciplinary research, here we present a glossary of relevant terms. Its main purpose is to facilitate the process of learning and communication among the different scientific disciplines working on this topic. While some definitions come from existing sources, other concepts have been re-defined to better reflect the existing and emerging scientific needs of this multidisciplinary and transdisciplinary area of research.","tags":["Atmospheric electricity phenomena","Atmospheric electric field","Biological effects","Biometeorological profile","Glossary"],"title":"Glossary on atmospheric electricity and its effects on biology","type":"publication"},{"authors":null,"categories":["visualization","R","R:intermediate"],"content":"\r\rRecently I was looking for a visual representation to show the daily changes of temperature, precipitation and wind in an application xeo81.shinyapps.io/MeteoExtremosGalicia (in Spanish), which led me to use a heatmap in the form of a calendar. The shiny application is updated every four hours with new data showing calendars for each weather station. The heatmap as a calendar allows you to visualize any variable with a daily time reference.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rlubridate\rEasy manipulation of dates and times\r\rragg\rragg provides a set of high quality and high performance raster devices\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ragg\u0026quot;)) install.packages(\u0026quot;ragg\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r# paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ragg)\rFor those with less experience with tidyverse, I recommend the short introduction on this blog post.\n\rData\rIn this example we will use the daily precipitation of Santiago de Compostela for this year 2020 (until December 20) download.\n# import the data\rdat_pr \u0026lt;- read_csv(\u0026quot;precipitation_santiago.csv\u0026quot;)\rdat_pr\r## # A tibble: 355 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-01-01 0 ## 2 2020-01-02 0 ## 3 2020-01-03 5.4\r## 4 2020-01-04 0 ## 5 2020-01-05 0 ## 6 2020-01-06 0 ## 7 2020-01-07 0 ## 8 2020-01-08 1 ## 9 2020-01-09 3.8\r## 10 2020-01-10 0 ## # ... with 345 more rows\r\rPreparation\rIn the first step we must 1) complement the time series from December 21 to December 31 with NA, 2) add the day of the week, the month, the week number and the day. Depending on whether we want each week to start on Sunday or Monday, we indicate it in the wday() function.\ndat_pr \u0026lt;- dat_pr %\u0026gt;% complete(date = seq(ymd(\u0026quot;2020-01-01\u0026quot;), ymd(\u0026quot;2020-12-31\u0026quot;), \u0026quot;day\u0026quot;)) %\u0026gt;%\rmutate(weekday = wday(date, label = T, week_start = 1), month = month(date, label = T, abbr = F),\rweek = isoweek(date),\rday = day(date))\rIn the next step we need to make a change in the week of the year, which is because in certain years there may be, for example, a few days at the end of the year as the first week of the following year. We also create two new columns. On the one hand, we categorize precipitation into 14 classes and on the other, we define a white text color for darker tones in the heatmap.\ndat_pr \u0026lt;- mutate(dat_pr, week = case_when(month == \u0026quot;December\u0026quot; \u0026amp; week == 1 ~ 53,\rmonth == \u0026quot;January\u0026quot; \u0026amp; week %in% 52:53 ~ 0,\rTRUE ~ week),\rpcat = cut(pr, c(-1, 0, .5, 1:5, 7, 9, 15, 20, 25, 30, 300)),\rtext_col = ifelse(pcat %in% c(\u0026quot;(15,20]\u0026quot;, \u0026quot;(20,25]\u0026quot;, \u0026quot;(25,30]\u0026quot;, \u0026quot;(30,300]\u0026quot;), \u0026quot;white\u0026quot;, \u0026quot;black\u0026quot;)) dat_pr \r## # A tibble: 366 x 8\r## date pr weekday month week day pcat text_col\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; ## 1 2020-01-01 0 Wed January 1 1 (-1,0] black ## 2 2020-01-02 0 Thu January 1 2 (-1,0] black ## 3 2020-01-03 5.4 Fri January 1 3 (5,7] black ## 4 2020-01-04 0 Sat January 1 4 (-1,0] black ## 5 2020-01-05 0 Sun January 1 5 (-1,0] black ## 6 2020-01-06 0 Mon January 2 6 (-1,0] black ## 7 2020-01-07 0 Tue January 2 7 (-1,0] black ## 8 2020-01-08 1 Wed January 2 8 (0.5,1] black ## 9 2020-01-09 3.8 Thu January 2 9 (3,4] black ## 10 2020-01-10 0 Fri January 2 10 (-1,0] black ## # ... with 356 more rows\r\rVisualization\rFirst we create a color ramp from Brewer colors.\n# color ramp\rpubu \u0026lt;- RColorBrewer::brewer.pal(9, \u0026quot;PuBu\u0026quot;)\rcol_p \u0026lt;- colorRampPalette(pubu)\rSecond, before building the chart, we define a custom theme as a function. To do this, we specify all the elements and their modifications with the help of the theme() function.\ntheme_calendar \u0026lt;- function(){\rtheme(aspect.ratio = 1/2,\raxis.title = element_blank(),\raxis.ticks = element_blank(),\raxis.text.y = element_blank(),\raxis.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rpanel.grid = element_blank(),\rpanel.background = element_blank(),\rstrip.background = element_blank(),\rstrip.text = element_text(family = \u0026quot;Montserrat\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 15),\rlegend.position = \u0026quot;top\u0026quot;,\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5),\rlegend.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 9, hjust = 1),\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = 1, size = 8),\rpanel.border = element_rect(colour = \u0026quot;grey\u0026quot;, fill=NA, size=1),\rplot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5, size = 26, face = \u0026quot;bold\u0026quot;, margin = margin(0,0,0.5,0, unit = \u0026quot;cm\u0026quot;)),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5, size = 16)\r)\r}\rFinally, we build the final chart using geom_tile() and specify the day of the week as the X axis and the week number as the Y axis. As you can see in the variable of the week number (-week), I change the sign so that the first day of each month is in the first row. With geom_text() we add the number of each day with its color according to what we defined previously. In guides we make the adjustments of the colorbar and in scale_fill/colour_manual() we define the corresponding colors. An important step is found in facet_wrap() where we specify the facets composition of each month. The facets should have free scales and the ideal would be a 4 x 3 facet distribution. It is possible to modify the position of the day number to another using the arguments nudge_* in geom_text() (eg bottom-right corner: nudge_x = .35, nudge_y = -.25).\n ggplot(dat_pr, aes(weekday, -week, fill = pcat)) +\rgeom_tile(colour = \u0026quot;white\u0026quot;, size = .4) + geom_text(aes(label = day, colour = text_col), size = 2.5) +\rguides(fill = guide_colorsteps(barwidth = 25, barheight = .4,\rtitle.position = \u0026quot;top\u0026quot;)) +\rscale_fill_manual(values = c(\u0026quot;white\u0026quot;, col_p(13)),\rna.value = \u0026quot;grey90\u0026quot;, drop = FALSE) +\rscale_colour_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;), guide = FALSE) + facet_wrap(~ month, nrow = 4, ncol = 3, scales = \u0026quot;free\u0026quot;) +\rlabs(title = \u0026quot;How is 2020 being in Santiago?\u0026quot;, subtitle = \u0026quot;Precipitation\u0026quot;,\rcaption = \u0026quot;Data: Meteogalicia\u0026quot;,\rfill = \u0026quot;mm\u0026quot;) +\rtheme_calendar()\rTo export we will use the ragg package, which provides higher performance and quality than the standard raster devices provided by grDevices.\nggsave(\u0026quot;pr_calendar.png\u0026quot;, height = 10, width = 8, device = agg_png())\rIn other heatmap calendars I have added the predominant wind direction of each day as an arrow using geom_arrow() from the metR package (it can be seen in the aforementioned application).\n\r","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"d6f4c9eff4fc25fe3f3acecbc2b988a4","permalink":"https://dominicroye.github.io/en/2020/a-heatmap-as-calendar/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/en/2020/a-heatmap-as-calendar/","section":"post","summary":"Recently I was looking for a visual representation to show the daily changes of temperature, precipitation and wind in an application [xeo81.shinyapps.io/MeteoExtremosGalicia](https://xeo81.shinyapps.io/MeteoExtremosGalicia/) (in Spanish), which led me to use a heatmap in the form of a calendar. The [shiny](https://shiny.rstudio.com/) application is updated every four hours with new data showing calendars for each weather station.","tags":["calendar","temperature","climate","heatmap"],"title":"A heatmap as calendar","type":"post"},{"authors":["C Iñiguez","D Royé","A Tobías"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"ec9dfe83edeb7d770acf58b2051ee49d","permalink":"https://dominicroye.github.io/en/publication/morti-morbilidad_spain_2021/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/en/publication/morti-morbilidad_spain_2021/","section":"publication","summary":"Background: Climate change is a severe public health challenge. Understanding to what extent fatal and non-fatal consequences of specific diseases are associated with temperature may help to improve the effectiveness of preventive public health efforts. This study examines the effects of temperature on deaths and hospital admissions by cardiovascular and respiratory diseases, empathizing the difference between mortality and morbidity. Methods: Daily counts for mortality and hospital admissions by cardiovascular and respiratory diseases were collected for the 52 provincial capital cities in Spain, between 1990 and 2014. The association with temperature in each city was investigated by means of distributed lag non-linear models using quasiPoisson regression. City-specific exposure-response curves were pooled by multivariate random-effects meta-analysis to obtain countrywide risk estimates of mortality and hospitalizations due to heat and cold, and attributable fractions were computed. Results: Heat and cold exposure were identified to be associated with increased risk of cardiovascular and respiratory mortality. Heat was not found to have an impact on hospital admissions. The estimated fraction of mortality attributable to cold was of greater magnitude in hospitalizations (17.5% for cardiovascular and 12.5% for respiratory diseases) compared to deaths (9% and 2.7%, respectively). Conclusions: There were noteworthy differences between temperature-related mortality and hospital admissions regarding cardiovascular and respiratory diseases, hence reinforcing the convenience of cause-specific measures to prevent temperature-related deaths.","tags":["Temperature","Mortality","Hospital admissions","Cardiovascular","Respiratory","Spain","Distributed lag non-linear models"],"title":"Contrasting patterns of temperature related mortality and hospitalization by cardiovascular and respiratory diseases in 52 Spanish cities","type":"publication"},{"authors":["S Gestal","D Royé","L Sánchez Santos","A Figueiras"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"a46c9065db97a4426aad0e54715908d9","permalink":"https://dominicroye.github.io/en/publication/emergency_calls_spain_2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/en/publication/emergency_calls_spain_2020/","section":"publication","summary":"Introduction and objectives. The increase in mortality and hospital admissions associated with high and low temperatures is well established. However, less is known about the influence of extreme ambient temperature conditions on cardiovascular ambulance dispatches. This study seeks to evaluate the effects of minimum and maximum daily temperatures on cardiovascular morbidity in the cities of Vigo and A Coruña in North-West Spain, using emergency medical calls during the period 2005–2017. Methods. For the purposes of analysis, we employed a quasi-Poisson time series regression model, within a distributed non-linear lag model by exposure variable and city. The relative risks of cold- and heat-related calls were estimated for each city and temperature model. Results. A total of 70,537 calls were evaluated, most of which were associated with low maximum and minimum temperatures on cold days in both cities. At maximum temperatures, significant cold-related effects were observed at lags of 3–6 days in Vigo and 5–11 days in A Coruña. At minimum temperatures, cold-related effects registered a similar pattern in both cities, with significant relative risks at lags of 4 to 12 days in A Coruña. Heat-related effects did not display a clearly significant pattern. Conclusions. An increase in cardiovascular morbidity is observed with moderately low temperatures without extremes being required to establish an effect. Public health prevention plans and warning systems should consider including moderate temperature range in the prevention of cardiovascular morbidity.","tags":["emergency calls","extreme temperatures","cardiovascular","Galicia","Spain","distributed non-linear lag models"],"title":"Impact of Extreme Temperatures on Ambulance Dispatches Due to Cardiovascular Causes in North-West Spain","type":"publication"},{"authors":null,"categories":["visualization","R","R:advanced"],"content":"\r\rIn the field of data visualization, the animation of spatial data in its temporal dimension can show fascinating changes and patterns. As a result of one of the last publications in the social networks that I have made, I was asked to make a post about how I created it. Well, here we go to start with an example of data from mainland Spain. You can find more animations in the graphics section of my blog.\nI couldn\u0026#39;t resist to make another animation. Smoothed daily maximum temperature throughout the year in Europe. #rstats #ggplot2 #dataviz #climate pic.twitter.com/ZC9L0vh3vR\n\u0026mdash; Dominic Royé (@dr_xeo) May 9, 2020  Packages\rIn this post we will use the following packages:\n\r\r\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rrnaturalearth\rVector maps of the world ‘Natural Earth’\r\rlubridate\rEasy manipulation of dates and times\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rraster\rImport, export and manipulate raster\r\rggthemes\rThemes for ggplot2\r\rgifski\rCreate gifs\r\rshowtext\rUse fonts more easily in R graphs\r\rsysfonts\rLoad fonts in R\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;ggthemes\u0026quot;)\rif(!require(\u0026quot;gifski\u0026quot;)) install.packages(\u0026quot;gifski\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# packages\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggthemes)\rlibrary(sf)\rlibrary(rnaturalearth)\rlibrary(extrafont)\rlibrary(showtext)\rlibrary(RColorBrewer)\rlibrary(gifski)\rFor those with less experience with tidyverse, I recommend the short introduction on this blog (post).\n\rPreparation\rData\rFirst, we need to download the STEAD dataset of the maximum temperature (tmax_pen.nc) in netCDF format from the CSIC repository here (the size of the data is 2 GB). It is a set of data with a spatial resolution of 5 km and includes daily maximum temperatures from 1901 to 2014. In climatology and meteorology, a widely used format is that of netCDF databases, which allow to obtain a multidimensional structure and to exchange data independently of the usued operating system. It is a space-time format with a regular or irregular grid. The multidimensional structure in the form of arrays or cubes can handle not only spatio-temporal data but also multivariate ones. In our dataset we will have an array of three dimensions: longitude, latitude and time of the maximum temperature.\nRoyé 2015. Sémata: Ciencias Sociais e Humanidades 27:11-37\n\r\rImport the dataset\rThe netCDF format with .nc extension can be imported via two main packages: 1) ncdf4 and 2) raster. Actually, the raster package use the first package to import the netCDF datasets. In this post we will use the raster package since it is somewhat easier, with some very useful and more universal functions for all types of raster format. The main import functions are: raster(), stack() and brick(). The first function only allows you to import a single layer, instead, the last two functions are used for multidimensional data. In our dataset we only have one variable, therefore it would not be necessary to use the varname argument.\n# import netCDF data\rtmx \u0026lt;- brick(\u0026quot;tmax_pen.nc\u0026quot;, varname = \u0026quot;tx\u0026quot;)\r## Loading required namespace: ncdf4\rtmx # metadata\r## class : RasterBrick ## dimensions : 190, 230, 43700, 41638 (nrow, ncol, ncell, nlayers)\r## resolution : 0.0585, 0.045 (x, y)\r## extent : -9.701833, 3.753167, 35.64247, 44.19247 (xmin, xmax, ymin, ymax)\r## crs : +proj=longlat +datum=WGS84 +no_defs ## source : tmax_pen.nc ## names : X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ... ## Time (days since 1901-01-01): 1, 41638 (min, max)\r## varname : tx\rThe RasterBrick object details show you all the necessary metadata: the resolution, the dimensions or the type of projection, or the name of the variable. It also tells us that it only points to the data source and has not imported them into the memory, which makes it easier to work with large datasets.\nTo access any layer we use [[ ]] with the corresponding index. So we can easily plot any day of the 41,638 days we have.\n# map any day\rplot(tmx[[200]], col = rev(heat.colors(7)))\r\rCalculate the average temperature\rIn this step the objective is to calculate the average maximum temperature for each day of the year. Therefore, the first thing we do is to create a vector, indicating the day of the year for the entire time series. In the raster package we have the stackApply() function that allows us to apply another function on groups of layers, or rather, indexes. Since our dataset is large, we include this function in parallelization functions.\nFor the parallelization we start and end always with the beginClusterr() and endCluster(). In the first function we must indicate the number of cores we want to use. In this case, I use 4 of 7 possible cores, however, the number must be changed according to the characteristics of each CPU, the general rule is n-1. So the clusterR function execute a function in parallel with multiple cores. The first argument corresponds to the raster object, the second to the used function, and as list argument we pass the arguments of the stackApply() function: the indexes that create the groups and the function used for each of the groups. Adding the argument progress = 'text' shows a progress bar of the calculation process.\nFor the US dataset I did the preprocessing, the calculation of the average, in a cloud computing platform through Google Earth Engine, which makes the whole process faster. In the case of Australia the preprocessing was more complex as the dataset is separated in multiple netCDF files for each year.\r # convert the dates between 1901 and 2014 to days of the year\rtime_days \u0026lt;- yday(seq(as_date(\u0026quot;1901-01-01\u0026quot;), as_date(\u0026quot;2014-12-31\u0026quot;), \u0026quot;day\u0026quot;))\r# calculate the average\rbeginCluster(4)\rtmx_mean \u0026lt;- clusterR(tmx, stackApply, args = list(indices = time_days, fun = mean))\rendCluster()\r\rSmooth the temperature variability\rBefore we start to smooth the time series of our RasterBrick, an example of why we do it. We extract a pixel from our dataset at coordinates -1º of longitude and 40º of latitude using the extract() function. Since the function with the same name appears in several packages, we must change to the form package_name::function_name. The result is a matrix with a single row corresponding to the pixel and 366 columns of the days of the year. The next step is to create a data.frame with a dummy date and the extracted maximum temperature.\n# extract a pixel\rpoint_ts \u0026lt;- raster::extract(tmx_mean, matrix(c(-1, 40), nrow = 1))\rdim(point_ts) # dimensions\r## [1] 1 366\r# create a data.frame\rdf \u0026lt;- data.frame(date = seq(as_date(\u0026quot;2000-01-01\u0026quot;), as_date(\u0026quot;2000-12-31\u0026quot;), \u0026quot;day\u0026quot;),\rtmx = point_ts[1,])\r# visualize the maximum temperature\rggplot(df, aes(date, tmx)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rlabs(y = \u0026quot;maximum temperature\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rThe graph clearly shows the still existing variability, which would cause an animation to fluctuate quite a bit. Therefore, we create a smoothing function based on a local polynomial regression fit (LOESS), more details can be found in the help of the loess() function. The most important argument is span, which determines the degree of smoothing, the smaller the value the less smooth the curve will be. I found the best result showed a value of 0.5.\ndaily_smooth \u0026lt;- function(x, span = 0.5){\rif(all(is.na(x))){\rreturn(x) } else {\rdf \u0026lt;- data.frame(yd = 1:366, ta = x)\rm \u0026lt;- loess(ta ~ yd, span = span, data = df)\rest \u0026lt;- predict(m, 1:366)\rreturn(est)\r}\r}\rWe apply our new smoothing function to the extracted time series and make some changes to be able to visualize the difference between the original and smoothed data.\n# smooth the temperature\rdf \u0026lt;- mutate(df, tmx_smoothed = daily_smooth(tmx)) %\u0026gt;% pivot_longer(2:3, names_to = \u0026quot;var\u0026quot;, values_to = \u0026quot;temp\u0026quot;)\r# visualize the difference\rggplot(df, aes(date, temp, colour = var)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rscale_colour_manual(values = c(\u0026quot;#f4a582\u0026quot;, \u0026quot;#b2182b\u0026quot;)) +\rlabs(y = \u0026quot;maximum temperature\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rAs we see in the graph, the smoothed curve follows the original curve very well. In the next step we apply our function to the RasterBrick with the calc() function. The function returns as many layers as those returned by the function used for each of the time series.\n# smooth the RasterBrick\rtmx_smooth \u0026lt;- calc(tmx_mean, fun = daily_smooth)\r\r\rVisualization\rPreparation\rTo visualize the maximum temperatures throughout the year, first, we convert the RasterBrick to a data.frame, including longitude and latitude, but removing all time series without values (NA).\n# convert to data.frame\rtmx_mat \u0026lt;- as.data.frame(tmx_smooth, xy = TRUE, na.rm = TRUE)\r# rename the columns tmx_mat \u0026lt;- set_names(tmx_mat, c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, str_c(\u0026quot;D\u0026quot;, 1:366)))\rstr(tmx_mat[, 1:10])\r## \u0026#39;data.frame\u0026#39;: 20676 obs. of 10 variables:\r## $ lon: num -8.03 -7.98 -7.92 -7.86 -7.8 ...\r## $ lat: num 43.8 43.8 43.8 43.8 43.8 ...\r## $ D1 : num 10.5 10.3 10 10.9 11.5 ...\r## $ D2 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D3 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D4 : num 10.6 10.4 10.1 10.9 11.5 ...\r## $ D5 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D6 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D7 : num 10.6 10.4 10.2 11 11.6 ...\r## $ D8 : num 10.6 10.4 10.2 11 11.6 ...\rSecond, we import the administrative boundaries with the ne_countries() function from the rnaturalearth package, limiting the extension to the region of the Iberian Peninsula, southern France and northern Africa.\n# import global boundaries\rmap \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;) %\u0026gt;% st_cast(\u0026quot;MULTILINESTRING\u0026quot;)\r# limit the extension\rmap \u0026lt;- st_crop(map, xmin = -10, xmax = 5, ymin = 35, ymax = 44) \r## although coordinates are longitude/latitude, st_intersection assumes that they are planar\r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r# map of boundaries\rplot(map)\r## Warning: plotting the first 9 out of 94 attributes; use max.plot = 94 to plot\r## all\rThird, we create a vector with the day of the year as labels in order to include them later in the animation. In addition, we define the break points for the maximum temperature, adapted to the distribution of our data, to obtain a categorization with a total of 20 classes.\nFourth, we apply the cut() function with the breaks to all the columns with temperature data of each day of the year.\n# labels of day of the year\rlab \u0026lt;- as_date(0:365, \u0026quot;2000-01-01\u0026quot;) %\u0026gt;% format(\u0026quot;%d %B\u0026quot;)\r# breaks for the temperature data\rct \u0026lt;- c(-5, 0, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 40, 45)\r# categorized data with fixed breaks\rtmx_mat_cat \u0026lt;- mutate_at(tmx_mat, 3:368, cut, breaks = ct)\rFifth, we download the Montserrat font and define the colors corresponding to the created classes.\n# download font\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# use of showtext with 300 DPI\rshowtext_opts(dpi = 300)\rshowtext_auto()\r# define the color ramp\rcol_spec \u0026lt;- colorRampPalette(rev(brewer.pal(11, \u0026quot;Spectral\u0026quot;)))\r\rStatic map\rIn this first plot we make a map of May 29 (day 150). I am not going to explain all the details of the construction with ggplot2, however, it is important to note that I use the aes_string() function instead of aes() to use the column names in string format. With the geom_raster() function we add the gridded temperature data as the first layer of the graph and with geom_sf() the boundaries in sf class. Finally, the guide_colorsteps() function allows you to create a nice legend based on the classes created by the cut() function.\nggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = \u0026quot;D150\u0026quot;)) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Average maximum temperature during the year in Spain\u0026quot;, subtitle = lab[150], caption = \u0026quot;Reference period 1901-2014. Data: STEAD\u0026quot;,\rfill = \u0026quot;ºC\u0026quot;)\r\rAnimation of the whole year\rThe final animation consists of creating a gif from all the images of 366 days, in principle, the gganimate package could be used, but in my experience it is slower, since it requires a data.frame in long format. In this example a long table would have more than seven million rows. So what we do here is to use a loop over the columns and join all the created images with the gifski package that also uses gganimate for rendering.\nBefore looping we create a vector with the time steps or names of the columns, and another vector with the name of the images, including the name of the folder. In order to obtain a list of images ordered by their number, we must maintain three figures, filling the positions on the left with zeros.\ntime_step \u0026lt;- str_c(\u0026quot;D\u0026quot;, 1:366)\rfiles \u0026lt;- str_c(\u0026quot;./ta_anima/D\u0026quot;, str_pad(1:366, 3, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;), \u0026quot;.png\u0026quot;)\rLastly, we include the above plot construction in a for loop.\nfor(i in 1:366){\rggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = time_step[i])) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Average maximum temperature during the year in Spain\u0026quot;, subtitle = lab[i], caption = \u0026quot;Reference period 1901-2014. Data: STEAD\u0026quot;,\rfill = \u0026quot;ºC\u0026quot;)\rggsave(files[i], width = 8.28, height = 7.33, type = \u0026quot;cairo\u0026quot;)\r}\rAfter having created images for each day of the year, we only have to create the gif.\ngifski(files, \u0026quot;tmx_spain.gif\u0026quot;, width = 800, height = 700, loop = FALSE, delay = 0.05)\r\r\r","date":1602374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602374400,"objectID":"cc2a362ef9387a5a4c016a7791a5e8b8","permalink":"https://dominicroye.github.io/en/2020/climate-animation-of-maximum-temperatures/","publishdate":"2020-10-11T00:00:00Z","relpermalink":"/en/2020/climate-animation-of-maximum-temperatures/","section":"post","summary":"In the field of data visualization, the animation of spatial data in its temporal dimension can show fascinating changes and patterns. As a result of one of the last publications in the social networks that I have made, I was asked to make a post about how I created it. Well, here we go to start with an example of data from mainland Spain.","tags":["animation","temperature","climte","GIS"],"title":"Climate animation of maximum temperatures","type":"post"},{"authors":null,"categories":["gis","R","R:advanced"],"content":"\r\rI recently created a visualization of the distribution of river flow directions and also of coastal orientations. Following its publication in social networks (here), I was asked to make a post about how I did it. Well, here we go to start with an example of rivers, coastal orientation is somewhat more complex. I did the same for a selection of European rivers here in this tweet. However, originally I started with the orientation of the European coasts.\nHave you ever wondered where the European #coasts are oriented? #rstats #ggplot2 #geography #dataviz pic.twitter.com/tpWVxSoHlw\n\u0026mdash; Dominic Royé (@dr_xeo) May 26, 2020  Packages\rIn this post we will use the following packages:\n\r\r\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rremotes\rInstallation from remote repositories\r\rRQGIS3\rInterface between R and QGIS3\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rggtext\rImproved text rendering support for ggplot2\r\rsysfonts\rLoad fonts in R\r\rshowtext\rUse fonts more easily in R graphs\r\rcircular\rFunctions for working with circular data\r\rgeosphere\rSpherical trigonometry for geographic applications\r\r\r\rIn the case of the RQGIS3 package, it is necessary to install QGIS in OSGeo4W here. I will explain the reason for using QGIS later.\n# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;remotes\u0026quot;)) install.packages(\u0026quot;remotes\u0026quot;)\rif(!require(\u0026quot;RQGIS3\u0026quot;)) remotes::install_github(\u0026quot;jannes-m/RQGIS3\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggtext\u0026quot;)) install.packages(\u0026quot;ggtext\u0026quot;)\rif(!require(\u0026quot;circular\u0026quot;)) install.packages(\u0026quot;circular\u0026quot;)\rif(!require(\u0026quot;geosphere\u0026quot;)) install.packages(\u0026quot;geosphere\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# packages\rlibrary(sf)\rlibrary(tidyverse)\rlibrary(ggtext)\rlibrary(circular)\rlibrary(geosphere)\rlibrary(RQGIS3)\rlibrary(showtext)\rlibrary(sysfonts)\r\rInitial considerations\rAngles in vectorial lines are based on the angle between two vertices, and the number of vertices depends on the complexity, and therefore the resolution, of the vector data. Consequently, there can be differences in using different resolutions of a spatial line, either from the coast or from the river as in this example. A straight line is simply constructed with two points of longitude and latitude.\nRelated to this is fractality, an apparently irregular structure but that is repeated at different scales, known from coastlines or also from river. The most paradoxical feature is that the length of a coastline depends on the measurement scale, the smaller the measurement increment, the longer is the measured coastline.\nThere are two possibilities of obtaining the vertice angles. In the first one we calculate the angle between all consecutive vertices.\nFor example, imagine two points, Madrid (-3.71, 40.43) and Barcelona (2.14, 41.4).\nWhat is the angle of a straight line between both cities?\nbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4))\r## [1] 77.62391\rWe see that it is 77º, that is, northeast direction. But what if we go from Barcelona to Madrid?\nbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43))\r## [1] 257.6239\rThe angle is different because we move from the northeast to the southwest. We can easily invert the direction to get the opposite angle.\n# opposite angle of Barcelona -\u0026gt; Madrid\rbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43)) - 180\r## [1] 77.62391\r# opposite angle of Madrid -\u0026gt; Barcelona\rbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4)) + 180\r## [1] 257.6239\rThe direction in which we calculate the angles is important. In the case of rivers, it is expected to be the direction of flow from origin to the mouth, however, a problem may be that the vertices, which build the lines, are not geographically ordered in the attribute table. Another problem may be that the vertices start at the mouth which would give the reverse angle as we have seen before.\nHowever, there is an easier way. We can take advantage of the attributes of projected coordinate systems (Robinson projection, etc.) that include the angle between the vertices. We will use this last approach in this post. Still, we must pay close attention to the results as stated above.\n\rPreparation\rData\rWe download the central lines of the largest rivers in the world (here), also accessible in Zeenatul Basher et al. 2018.\n\rImport and project\rThe first thing we do is to import, project the spatial lines and delete the third dimension Z, chaining the following functions: st_read() helps us import any vector format, st_zm() delete the dimension Z or M of a geometry and st_transform() projects the vector data to the new projection in proj4 format. We combine the functions with the famous pipe (%\u0026gt;%) that facilitates the application of a sequence of functions on a data set, more details in this post. All functions in the sf package start with st_* with reference to the spatial character, similar to PostGIS. In the same style as PostGIS, verbs are used as function names.\nproj_rob \u0026lt;- \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m no_defs\u0026quot;\rriver_line \u0026lt;- st_read(\u0026quot;RiverHRCenterlinesCombo.shp\u0026quot;) %\u0026gt;% st_zm() %\u0026gt;% st_transform(proj_rob)\r## Reading layer `RiverHRCenterlinesCombo\u0026#39; from data source ## `D:\\OneDriveUSC\\OneDrive - Universidade de Santiago de Compostela\\Documentos\\GitHub\\blogR_update\\content\\post\\en\\2020-07-24-river-flow-directions\\RiverHRCenterlinesCombo.shp\u0026#39; ## using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 78 features and 6 fields\r## Geometry type: MULTILINESTRING\r## Dimension: XYZ\r## Bounding box: xmin: -164.7059 ymin: -36.97094 xmax: 151.5931 ymax: 72.64474\r## z_range: zmin: 0 zmax: 0\r## Geodetic CRS: WGS 84\r\rExtract the angles\rIn the next step we have to extract the vertice angles. Unfortunately, as far as I know, it is not possible to extract the attributes with some function from the sf package. Although the function st_coordinates() returns the coordinates, it does not include other attributes. Therefore, we must use another way, and that is the open software Quantum GIS in which we can find a tool to extract all the vertice attributes. We could import the vector data into QGIS Desktop and export the vertices from there, but it is also possible to access the QGIS tools from R directly.\nFor this, we need to have QGIS installed in OSGeo4W. The RQGIS3 package allows us to use very easily all the tools of the software in R. First we use the set_env() function to define all the necessary QGIS paths and start the API with open_app().\n# paths to QGIS\rset_env()\r## Trying to find QGIS in C:/\r## $root\r## [1] \u0026quot;C:/Program Files/QGIS 3.18\u0026quot;\r## ## $qgis_prefix_path\r## [1] \u0026quot;C:/Program Files/QGIS 3.18/apps/qgis\u0026quot;\r## ## $python_plugins\r## [1] \u0026quot;C:/Program Files/QGIS 3.18/apps/qgis/python/plugins\u0026quot;\r## ## $platform\r## [1] \u0026quot;Windows\u0026quot;\r# start of QGIS Python\ropen_app()\r## Warning in check_for_server(): Hey there! According to our internal checks, you are trying to run RQGIS3 on a Windows server.\r## Please note that this is only possible if you imitate a x-display.\r## QGIS needs this in the background to be able to execute its processing modules.\r## Note that you need to start the x-display with admin rights\rThe find_algorithms() function helps us to search for different QGIS tools. In addition the get_usage() function specifies the way of usage with all the required parameters.\n# search tools\rfind_algorithms(search_term = \u0026quot;vertices\u0026quot;, name_only = TRUE)\r## [1] \u0026quot;native:exportmeshvertices\u0026quot; ## [2] \u0026quot;native:extractspecificvertices\u0026quot; ## [3] \u0026quot;native:extractvertices\u0026quot; ## [4] \u0026quot;native:filterverticesbym\u0026quot; ## [5] \u0026quot;native:filterverticesbyz\u0026quot; ## [6] \u0026quot;native:removeduplicatevertices\u0026quot; ## [7] \u0026quot;saga:convertpolygonlineverticestopoints\u0026quot;\r# usage of tool\rget_usage(alg = \u0026quot;native:extractvertices\u0026quot;)\r## Extract vertices (native:extractvertices)\r## ## This algorithm takes a line or polygon layer and generates a point layer with points representing the vertices in the input lines or polygons. The attributes associated to each point are the same ones associated to the line or polygon that the point belongs to.\r## ## Additional fields are added to the point indicating the vertex index (beginning at 0)\r## the vertex’s part and its index within the part (as well as its ring for polygons)\r## distance along original geometry and bisector angle of vertex for original geometry.\r## ## ## ----------------\r## Input parameters\r## ----------------\r## ## INPUT: Input layer\r## ## Parameter type: QgsProcessingParameterFeatureSource\r## ## Accepted data types:\r## - str: layer ID\r## - str: layer name\r## - str: layer source\r## - QgsProcessingFeatureSourceDefinition\r## - QgsProperty\r## - QgsVectorLayer\r## ## OUTPUT: Vertices\r## ## Parameter type: QgsProcessingParameterFeatureSink\r## ## Accepted data types:\r## - str: destination vector file\r## e.g. d:/test.shp\r## - str: memory: to store result in temporary memory layer\r## - str: using vector provider ID prefix and destination URI\r## e.g. postgres:… to store result in PostGIS table\r## - QgsProcessingOutputLayerDefinition\r## - QgsProperty\r## ## ----------------\r## Outputs\r## ----------------\r## ## OUTPUT: \u0026lt;QgsProcessingOutputVectorLayer\u0026gt;\r## Vertices\rIn our case the tool to extract the vertices is simple and only has one input and one output. The function run_qgis() executes a QGIS tool indicating the algorithm and its arguments. The advantage of using the algorithm directly from R is that we can pass objects of class sf (or sp) and raster that we have imported or created in R. As output we create a geojson, it could also be of another vector format, and we save it in a temporary folder. At the same time we indicate to import the result directly into R (load_output = TRUE).\nriver_vertices \u0026lt;- run_qgis(alg = \u0026quot;native:extractvertices\u0026quot;,\rINPUT = river_line,\rOUTPUT = file.path(tempdir(), \u0026quot;rivers_world_vertices.geojson\u0026quot;),\rload_output = TRUE)\r## $OUTPUT\r## [1] \u0026quot;C:/Users/xeo19/AppData/Local/Temp/RtmpageMET/rivers_world_vertices.geojson\u0026quot;\rCurrently on Windows there seem to be problems with the proj library. In principle, if the function ends up creating the river_vertices object, you should not worry. Otherwise, I recommend looking at the discussion in the issue opened at gitbub.\r \rSelection\rBefore continuing with the distribution estimation of the angles, we filter some rivers of interest. The functions of the tidyverse collection are compatible with the sf package. In the last post I made an introduction to tidyverse here.\nriver_vertices \u0026lt;- filter(river_vertices, NAME %in% c(\u0026quot;Mississippi\u0026quot;, \u0026quot;Colorado\u0026quot;, \u0026quot;Amazon\u0026quot;, \u0026quot;Nile\u0026quot;, \u0026quot;Orange\u0026quot;, \u0026quot;Ganges\u0026quot;, \u0026quot;Yangtze\u0026quot;, \u0026quot;Danube\u0026quot;,\r\u0026quot;Mackenzie\u0026quot;, \u0026quot;Lena\u0026quot;, \u0026quot;Murray\u0026quot;, \u0026quot;Niger\u0026quot;)\r) river_vertices \r## Simple feature collection with 94702 features and 11 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -10377520 ymin: -3953778 xmax: 13124340 ymax: 7507359\r## Geodetic CRS: WGS 84\r## # A tibble: 94,702 x 12\r## NAME SYSTEM name_alt scalerank rivernum Length_km vertex_index vertex_part\r## * \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 0 0\r## 2 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 1 0\r## 3 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 2 0\r## 4 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 3 0\r## 5 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 4 0\r## 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 5 0\r## 7 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 6 0\r## 8 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 7 0\r## 9 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 8 0\r## 10 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3344. 9 0\r## # ... with 94,692 more rows, and 4 more variables: vertex_part_index \u0026lt;int\u0026gt;,\r## # distance \u0026lt;dbl\u0026gt;, angle \u0026lt;dbl\u0026gt;, geometry \u0026lt;POINT [°]\u0026gt;\r\r\rEstimate the distribution\rTo visualize the distribution we can use either a histogram or a density graph. But in the case of estimating the probability density function, we find a mathematical problem when applying it to circular data. For circular data we should not use the density() standard function of R since in our data a direction of 360º is the same at 0º, which would cause errors in this range of values. It is a general problem for different statistical metrics. More statistical details are explained in the circular package. This package allows you to define the characteristics of circular data (unit, data type, rotation, etc.) as an object class in R.\nSo what we do is to build a function that estimates the density and returns a table with the angles (x) and the density estimates (y). Since rivers have different lengths, and we want to see differences regardless of that, we normalize the estimates using the maximum value. Unlike the density() function, in which the smoothing bandwidth bw is optimized, here it is required to indicate it manually. It is similar to defining the bar width in a histogram. There is an optimization function for the bandwidth, bw.nrd.circular() that could be used here.\ndens_circ \u0026lt;- function(x){\rdens \u0026lt;- density.circular(circular(x$angle, units = \u0026quot;degrees\u0026quot;),\rbw = 70, kernel = \u0026quot;vonmises\u0026quot;,\rcontrol.circular = list(units = \u0026quot;degrees\u0026quot;))\rdf \u0026lt;- data.frame(x = dens$x, y = dens$y/max(dens$y))\rreturn(df)\r}\rFinally, we estimate the density of each river in our selection. We use the split() function of R Base to get a table of each river in a list object. Then we apply our density estimation function to the list with the function map_df() from the purrr package. The suffix _df allows us to get a joined table, instead of a list with the results of each river. However, it is necessary to indicate the name of the column with the argument .id, which will contain the name of each river. Otherwise we would not know how to differentiate the results. Also here I recommend reading more details in the last post about tidyverse here.\ndens_river \u0026lt;- split(river_vertices, river_vertices$NAME) %\u0026gt;% map_df(dens_circ, .id = \u0026quot;river\u0026quot;)\r# results\rhead(dens_river)\r## river x y\r## 1 Amazon 0.000000 0.2399907\r## 2 Amazon 0.704501 0.2492548\r## 3 Amazon 1.409002 0.2585758\r## 4 Amazon 2.113503 0.2679779\r## 5 Amazon 2.818004 0.2774859\r## 6 Amazon 3.522505 0.2871232\r\rVisualization\rNow we only have to make the graph through the famous ggplot package. First we add a new font Montserrat for it use in this plot.\n# font download\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# use of showtext\rshowtext_opts(dpi = 200)\rshowtext_auto() \rIn the next step we create two objects with the title and the plot caption. In the title we are using an html code to color part of the text instead of a legend. You can use html very easily with the ggtext package.\n# title with html\rtitle \u0026lt;- \u0026quot;Relative distribution of river \u0026lt;span style=\u0026#39;color:#011FFD;\u0026#39;\u0026gt;\u0026lt;strong\u0026gt;flow direction\u0026lt;/strong\u0026gt;\u0026lt;/span\u0026gt; in the world\u0026quot;\rcaption \u0026lt;- \u0026quot;Based on data from Zeenatul Basher, 20180215\u0026quot;\rThe background grid that creates ggplot by default for polar coordinates did not convince me, so we create a table with x axis background lines.\ngrid_x \u0026lt;- tibble(x = seq(0, 360 - 22.5, by = 22.5), y = rep(0, 16), xend = seq(0, 360 - 22.5, by = 22.5), yend = rep(Inf, 16))\rNext we define all the styles of the graph. The most important thing in this step is the element_textbox() function of the ggtext package to be able to interpret our html code incorporated into the title.\ntheme_polar \u0026lt;- function(){\rtheme_minimal() %+replace%\rtheme(axis.title.y = element_blank(),\raxis.text.y = element_blank(),\rlegend.title = element_blank(),\rplot.title = element_textbox(family = \u0026quot;Montserrat\u0026quot;, hjust = 0.5, colour = \u0026quot;white\u0026quot;, size = 15),\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\raxis.text.x = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\rstrip.text = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rpanel.grid = element_blank()\r)\r}\rFinally we build the graph: 1) We use the geom_hline() function with different y intersection points to create the background grid. The geom_segment() function creates the x grid. 2) We create the density area using the geom_area() function. 3) In scale_x_continous() we define a negative lower limit so that it does not collapse at a small point. The labels of the eight main directions are indicated in the scale_y_continous() function, and 4) Finally, we change to a polar coordinate system and set the variable to create facets.\nggplot() +\rgeom_hline(yintercept = c(0, .2, .4, .6, .8, 1), colour = \u0026quot;white\u0026quot;) +\rgeom_segment(data = grid_x , aes(x = x, y = y, xend = xend, yend = yend), linetype = \u0026quot;dashed\u0026quot;, col = \u0026quot;white\u0026quot;) +\rgeom_area(data = dens_river, aes(x = x, y = y, ymin = 0, ymax = y), alpha = .7, colour = NA, show.legend = FALSE,\rfill = \u0026quot;#011FFD\u0026quot;) + scale_y_continuous(limits = c(-.2, 1), expand = c(0, 0)) +\rscale_x_continuous(limits = c(0, 360), breaks = seq(0, 360 - 22.5, by = 22.5),\rminor_breaks = NULL,\rlabels = c(\u0026quot;N\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NE\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;E\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SE\u0026quot;, \u0026quot;\u0026quot;,\r\u0026quot;S\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SW\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NW\u0026quot;, \u0026quot;\u0026quot;)) +\rcoord_polar() + facet_wrap(river ~ ., ncol = 4) +\rlabs(title = title, caption = caption, x = \u0026quot;\u0026quot;) +\rtheme_polar()\r\r","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595548800,"objectID":"6af1aa4569e0cf556c195b2f7e8c706e","permalink":"https://dominicroye.github.io/en/2020/river-flow-directions/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/en/2020/river-flow-directions/","section":"post","summary":"I recently created a visualization of the distribution of river flow directions and also of coastal orientations. Following its publication in social networks, I was asked to make a post about how I did it. Well, here we go to start with an example of rivers, coastal orientation is somewhat more complex.","tags":["directions","river","fluvial","orientation","distribution"],"title":"River flow directions","type":"post"},{"authors":null,"categories":["tidyverse","R","R:elementary"],"content":"\r\r\r1 Tidyverse\r2 Style guide\r3 Pipe %\u0026gt;%\r4 Tidyverse packages\r\r4.1 Read and write data\r4.2 Character manipulations\r4.3 Management of dates and times\r4.4 Table and vector manipulation\r\r4.4.1 Select and rename\r4.4.2 Filter and sort\r4.4.3 Group and summarize\r4.4.4 Join tables\r4.4.5 Long and wide tables\r\r4.5 Visualize data\r\r4.5.1 Line and scatter plot\r4.5.2 Boxplot\r4.5.3 Heatmap\r\r4.6 Apply functions on vectors or lists\r\r\r\r1 Tidyverse\rThe tidyverse universe of packages, a collection of packages specially focused on data science, marked a milestone in R programming. In this post I am going to summarize very briefly the most essential to start in this world. The tidyverse grammar follows a common structure in all functions. The most essential thing is that the first argument is the object and then come the rest of the arguments. In addition, a set of verbs is provided to facilitate the use of the functions. The tidyverse philosophy and grammar of functions are also reflected in other packages that make its use compatible with the collection. For example, the sf package (simple feature) is a standardized way to encode spatial vector data and allows the use of multiple functions that we can find in the dplyr package.\nThe core of the tidyverse collection is made up of the following packages:\n\r\rPackage\rDescription\r\r\r\rggplot2\rGrammar for creating graphics\r\rpurrr\rR functional programming\r\rtibble\rModern and effective table system\r\rdplyr\rGrammar for data manipulation\r\rtidyr\rSet of functions to create tidy data\r\rstringr\rFunction set to work with characters\r\rreadr\rAn easy and fast way to import data\r\rforcats\rTools to easily work with factors\r\r\r\rIn addition to the mentioned packages, lubridate is also used very frequently to work with dates and times, and also readxl which allows us to import files in Excel format. To know all the available packages we can use the function tidyverse_packages().\n## [1] \u0026quot;broom\u0026quot; \u0026quot;cli\u0026quot; \u0026quot;crayon\u0026quot; \u0026quot;dbplyr\u0026quot; ## [5] \u0026quot;dplyr\u0026quot; \u0026quot;dtplyr\u0026quot; \u0026quot;forcats\u0026quot; \u0026quot;googledrive\u0026quot; ## [9] \u0026quot;googlesheets4\u0026quot; \u0026quot;ggplot2\u0026quot; \u0026quot;haven\u0026quot; \u0026quot;hms\u0026quot; ## [13] \u0026quot;httr\u0026quot; \u0026quot;jsonlite\u0026quot; \u0026quot;lubridate\u0026quot; \u0026quot;magrittr\u0026quot; ## [17] \u0026quot;modelr\u0026quot; \u0026quot;pillar\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;readr\u0026quot; ## [21] \u0026quot;readxl\u0026quot; \u0026quot;reprex\u0026quot; \u0026quot;rlang\u0026quot; \u0026quot;rstudioapi\u0026quot; ## [25] \u0026quot;rvest\u0026quot; \u0026quot;stringr\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;tidyr\u0026quot; ## [29] \u0026quot;xml2\u0026quot; \u0026quot;tidyverse\u0026quot;\rIt is very easy to get conflicts between functions, that is, that the same function name exists in several packages. To avoid this, we can write the name of the package in front of the function we want to use, separated by the colon symbol written twice (package_name::function_name).\nBefore I get started with the packages, I hope it will be a really short introduction, some comments on the style when programming in R.\n\r2 Style guide\rIn R there is no universal style guide, that is, in the R syntax it is not necessary to follow specific rules for our scripts. But it is recommended to work in a homogeneous, uniform, legible and clear way when writing scripts. The tidyverse collection has its own guide (https://style.tidyverse.org/).\nThe most important recommendations are:\n\rAvoid using more than 80 characters per line to allow reading the complete code.\rAlways use a space after a comma, never before.\rThe operators (==, +, -, \u0026lt;-,%\u0026gt;%, etc.) must have a space before and after.\rThere is no space between the name of a function and the first parenthesis, nor between the last argument and the final parenthesis of a function.\rAvoid reusing names of functions and common variables (c \u0026lt;- 5 vs. c())\rSort the script separating the parts with the comment form # Import data -----\rAvoid accent marks or special symbols in names, files, routes, etc.\rObject names must follow a constant structure: day_one, day_1.\r\rIt is advisable to use a correct indentation for multiple arguments of a function or functions chained by the pipe operator (%\u0026gt;%).\n\r3 Pipe %\u0026gt;%\rTo facilitate working in data management, manipulation and visualization, the magrittr package introduces the famous pipe operator in the form %\u0026gt;% with the aim of combining various functions without the need to assign the result to a new object. The pipe operator passes the output of a function applied to the first argument of the next function. This way of combining functions allows you to chain several steps simultaneously, to perform sequential tasks. In the very simple example below, we pass the vector 1:5 to the mean() function to calculate the average. You should know that there are a couple of other pipe operators in the same package.\n1:5 %\u0026gt;% mean()\r## [1] 3\r\r4 Tidyverse packages\r4.1 Read and write data\rThe readr package makes it easy to read or write multiple file formats using functions that start with read_* or write_*.\rIn comparison to R Base, readr functions are faster; they handle problematic column names, and dates are automatically converted. The imported tables are of class tibble (tbl_df), a modern version of data.frame from the tibble package. In the same sense, you can use the read_excel() function of the readxl package to import data from Excel sheets (more details also in this blog post). In the following example, we import the mobility data registered by Google (link) during the last months of the COVID-19 pandemic (download).\n\r\rFunction\rDescription\r\r\r\rread_csv() o read_csv2()\rcoma or semicolon (CSV)\r\rread_delim()\rgeneral separator\r\rread_table()\rwhitespace-separated\r\r\r\r# load package\rlibrary(tidyverse)\rgoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Rows: 516697 Columns: 13\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (6): country_region_code, country_region, sub_region_1, sub_region_2, i...\r## dbl (6): retail_and_recreation_percent_change_from_baseline, grocery_and_ph...\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rgoogle_mobility\r## # A tibble: 516,697 x 13\r## country_region_code country_region sub_region_1 sub_region_2 iso_3166_2_code\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 2 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 3 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 4 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 5 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 9 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 10 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # ... with 516,687 more rows, and 8 more variables: census_fips_code \u0026lt;chr\u0026gt;,\r## # date \u0026lt;date\u0026gt;, retail_and_recreation_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # grocery_and_pharmacy_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # parks_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # transit_stations_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # workplaces_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # residential_percent_change_from_baseline \u0026lt;dbl\u0026gt;\rImportant is to take a look at the argument names, since they change in the readr functions. For example, the well-known header = TRUE argument of read.csv() is in this case col_names = TRUE. More details can be found in the Cheat-Sheet of readr.\n\r4.2 Character manipulations\rFor working with strings we use the stringr package, whose functions always start with str_* followed by a verb and the first argument.\nSome of these functions are as follows:\n\r\rFunction\rDescription\r\r\r\rstr_replace()\rreplace patterns\r\rstr_c()\rcombine characters\r\rstr_detect()\rdetect patterns\r\rstr_extract()\rextract patterns\r\rstr_sub()\rextract by position\r\rstr_length()\rlength of string\r\r\r\rRegular expressions are often used for character patterns. For example, the regular expression [aeiou] matches any single character that is a vowel. The use of square brackets [] corresponds to character classes. For example, [abc] corresponds to each letter regardless of its position. [a-z], [A-Z] or [0-9] each between a and z or 0 and 9. And finally, [:punct:] punctuation, etc. With curly braces “{}” we can indicate the number of the previous element, {2} would be twice, {1,2} between one and two, etc. Also with $ or ^ we can indicate if the pattern starts at the beginning or ends at the end. More details and patterns can be found in the Cheat-Sheet of stringr.\n# replace \u0026#39;er\u0026#39; at the end with empty space\rstr_replace(month.name, \u0026quot;er$\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;March\u0026quot; \u0026quot;April\u0026quot; \u0026quot;May\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;Septemb\u0026quot; \u0026quot;Octob\u0026quot; \u0026quot;Novemb\u0026quot; \u0026quot;Decemb\u0026quot;\rstr_replace(month.name, \u0026quot;^Ma\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;rch\u0026quot; \u0026quot;April\u0026quot; \u0026quot;y\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;September\u0026quot; \u0026quot;October\u0026quot; \u0026quot;November\u0026quot; \u0026quot;December\u0026quot;\r# combine characters\ra \u0026lt;- str_c(month.name, 1:12, sep = \u0026quot;_\u0026quot;)\ra\r## [1] \u0026quot;January_1\u0026quot; \u0026quot;February_2\u0026quot; \u0026quot;March_3\u0026quot; \u0026quot;April_4\u0026quot; \u0026quot;May_5\u0026quot; ## [6] \u0026quot;June_6\u0026quot; \u0026quot;July_7\u0026quot; \u0026quot;August_8\u0026quot; \u0026quot;September_9\u0026quot; \u0026quot;October_10\u0026quot; ## [11] \u0026quot;November_11\u0026quot; \u0026quot;December_12\u0026quot;\r# collapse combination\rstr_c(month.name, collapse = \u0026quot;, \u0026quot;)\r## [1] \u0026quot;January, February, March, April, May, June, July, August, September, October, November, December\u0026quot;\r# detect patterns\rstr_detect(a, \u0026quot;_[1-5]{1}\u0026quot;)\r## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE\r# extract patterns\rstr_extract(a, \u0026quot;_[1-9]{1,2}\u0026quot;)\r## [1] \u0026quot;_1\u0026quot; \u0026quot;_2\u0026quot; \u0026quot;_3\u0026quot; \u0026quot;_4\u0026quot; \u0026quot;_5\u0026quot; \u0026quot;_6\u0026quot; \u0026quot;_7\u0026quot; \u0026quot;_8\u0026quot; \u0026quot;_9\u0026quot; \u0026quot;_1\u0026quot; \u0026quot;_11\u0026quot; \u0026quot;_12\u0026quot;\r# extract the characters between position 1 and 2\rstr_sub(month.name, 1, 2)\r## [1] \u0026quot;Ja\u0026quot; \u0026quot;Fe\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ap\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Au\u0026quot; \u0026quot;Se\u0026quot; \u0026quot;Oc\u0026quot; \u0026quot;No\u0026quot; \u0026quot;De\u0026quot;\r# string length of each month\rstr_length(month.name)\r## [1] 7 8 5 5 3 4 4 6 9 7 8 8\r# the \u0026#39;.\u0026#39; represents the object passed by the pipe operator %\u0026gt;%\rstr_length(month.name) %\u0026gt;% str_c(month.name, ., sep = \u0026quot;.\u0026quot;)\r## [1] \u0026quot;January.7\u0026quot; \u0026quot;February.8\u0026quot; \u0026quot;March.5\u0026quot; \u0026quot;April.5\u0026quot; \u0026quot;May.3\u0026quot; ## [6] \u0026quot;June.4\u0026quot; \u0026quot;July.4\u0026quot; \u0026quot;August.6\u0026quot; \u0026quot;September.9\u0026quot; \u0026quot;October.7\u0026quot; ## [11] \u0026quot;November.8\u0026quot; \u0026quot;December.8\u0026quot;\rA very useful function is str_glue() to interpolate characters.\nname \u0026lt;- c(\u0026quot;Juan\u0026quot;, \u0026quot;Michael\u0026quot;)\rage \u0026lt;- c(50, 80) date_today \u0026lt;- Sys.Date()\rstr_glue(\r\u0026quot;My name is {name}, \u0026quot;,\r\u0026quot;I\u0026#39;am {age}, \u0026quot;,\r\u0026quot;and my birth year is {format(date_today-age*365, \u0026#39;%Y\u0026#39;)}.\u0026quot;\r)\r## My name is Juan, I\u0026#39;am 50, and my birth year is 1971.\r## My name is Michael, I\u0026#39;am 80, and my birth year is 1941.\r\r4.3 Management of dates and times\rThe lubridate package is very powerful in handling dates and times. It allows us to create R recognized objects with functions (like ymd() or ymd_hms()) and we can even make calculations.\nWe only must know the following abbreviations:\n\rymd: represents y:year, m: month, d:day\rhms: represents h:hour, m:minutes, s:seconds\r\r# load package\rlibrary(lubridate)\r## ## Attaching package: \u0026#39;lubridate\u0026#39;\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## date, intersect, setdiff, union\r# date vector\rdat \u0026lt;- c(\u0026quot;1999/12/31\u0026quot;, \u0026quot;2000/01/07\u0026quot;, \u0026quot;2005/05/20\u0026quot;,\u0026quot;2010/03/25\u0026quot;)\r# date-time vector\rdat_time \u0026lt;- c(\u0026quot;1988-08-01 05:00\u0026quot;, \u0026quot;2000-02-01 22:00\u0026quot;)\r# convert to date class\rdat \u0026lt;- ymd(dat) dat\r## [1] \u0026quot;1999-12-31\u0026quot; \u0026quot;2000-01-07\u0026quot; \u0026quot;2005-05-20\u0026quot; \u0026quot;2010-03-25\u0026quot;\r# other date formats\rdmy(\u0026quot;05-02-2000\u0026quot;)\r## [1] \u0026quot;2000-02-05\u0026quot;\rymd(\u0026quot;20000506\u0026quot;)\r## [1] \u0026quot;2000-05-06\u0026quot;\r# convert to POSIXct\rdat_time \u0026lt;- ymd_hm(dat_time)\rdat_time\r## [1] \u0026quot;1988-08-01 05:00:00 UTC\u0026quot; \u0026quot;2000-02-01 22:00:00 UTC\u0026quot;\r# different date formats\rdat_mix \u0026lt;- c(\u0026quot;1999/12/05\u0026quot;, \u0026quot;05-09-2008\u0026quot;, \u0026quot;2000/08/09\u0026quot;, \u0026quot;25-10-2019\u0026quot;)\r# mixted formats with known convention found in ?strptime\rparse_date_time(dat_mix, order = c(\u0026quot;%Y/%m/%d\u0026quot;, \u0026quot;%d-%m-%Y\u0026quot;))\r## [1] \u0026quot;1999-12-05 UTC\u0026quot; \u0026quot;2008-09-05 UTC\u0026quot; \u0026quot;2000-08-09 UTC\u0026quot; \u0026quot;2019-10-25 UTC\u0026quot;\rMore useful functions:\n# extract the year\ryear(dat)\r## [1] 1999 2000 2005 2010\r# the month\rmonth(dat)\r## [1] 12 1 5 3\rmonth(dat, label = TRUE) # as label\r## [1] dic ene may mar\r## 12 Levels: ene \u0026lt; feb \u0026lt; mar \u0026lt; abr \u0026lt; may \u0026lt; jun \u0026lt; jul \u0026lt; ago \u0026lt; sep \u0026lt; ... \u0026lt; dic\r# the day of the week\rwday(dat)\r## [1] 6 6 6 5\rwday(dat, label = TRUE) # as label\r## [1] vi\\\\. vi\\\\. vi\\\\. ju\\\\.\r## Levels: do\\\\. \u0026lt; lu\\\\. \u0026lt; ma\\\\. \u0026lt; mi\\\\. \u0026lt; ju\\\\. \u0026lt; vi\\\\. \u0026lt; sá\\\\.\r# the hour\rhour(dat_time)\r## [1] 5 22\r# add 10 days\rdat + days(10)\r## [1] \u0026quot;2000-01-10\u0026quot; \u0026quot;2000-01-17\u0026quot; \u0026quot;2005-05-30\u0026quot; \u0026quot;2010-04-04\u0026quot;\r# add 1 month\rdat + months(1)\r## [1] \u0026quot;2000-01-31\u0026quot; \u0026quot;2000-02-07\u0026quot; \u0026quot;2005-06-20\u0026quot; \u0026quot;2010-04-25\u0026quot;\rFinally, the make_date() function is very useful to create dates from different date parts, such as the year, month, etc.\n# create date from its elements, here with year and month\rmake_date(2000, 5)\r## [1] \u0026quot;2000-05-01\u0026quot;\r# create date with time\rmake_datetime(2005, 5, 23, 5)\r## [1] \u0026quot;2005-05-23 05:00:00 UTC\u0026quot;\rMore details can be found in the Cheat-Sheet of lubridate.\n\r4.4 Table and vector manipulation\rThe dplyr and tidyr packages provide us with a data manipulation grammar, a set of useful verbs to solve common problems. The most important functions are:\n\r\rFunction\rDescription\r\r\r\rmutate()\radd new variables or modify existing ones\r\rselect()\rselect variables\r\rfilter()\rfilter\r\rsummarise()\rsummarize/reduce\r\rarrange()\rsort\r\rgroup_by()\rgroup\r\rrename()\rrename columns\r\r\r\rIn case you haven’t done it before, we import the mobility data.\ngoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Rows: 516697 Columns: 13\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (6): country_region_code, country_region, sub_region_1, sub_region_2, i...\r## dbl (6): retail_and_recreation_percent_change_from_baseline, grocery_and_ph...\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r4.4.1 Select and rename\rWe can select or remove columns with the select() function, using the name or index of the column. To delete columns we make use of the negative sign. The rename function helps in renaming columns with either the same name or their index.\nresidential_mobility \u0026lt;- select(google_mobility, country_region_code:sub_region_1, date, residential_percent_change_from_baseline) %\u0026gt;% rename(resi = 5)\r\r4.4.2 Filter and sort\rTo filter data, we use filter() with logical operators (|, ==, \u0026gt;, etc) or functions that return a logical value (str_detect(), is.na() , etc.). The arrange() function sorts from least to greatest for one or multiple variables (with the negative sign - the order is reversed from greatest to least).\nfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;)\r## # A tibble: 304,648 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States \u0026lt;NA\u0026gt; 2020-02-15 -1\r## 2 US United States \u0026lt;NA\u0026gt; 2020-02-16 -1\r## 3 US United States \u0026lt;NA\u0026gt; 2020-02-17 5\r## 4 US United States \u0026lt;NA\u0026gt; 2020-02-18 1\r## 5 US United States \u0026lt;NA\u0026gt; 2020-02-19 0\r## 6 US United States \u0026lt;NA\u0026gt; 2020-02-20 1\r## 7 US United States \u0026lt;NA\u0026gt; 2020-02-21 0\r## 8 US United States \u0026lt;NA\u0026gt; 2020-02-22 -1\r## 9 US United States \u0026lt;NA\u0026gt; 2020-02-23 -1\r## 10 US United States \u0026lt;NA\u0026gt; 2020-02-24 0\r## # ... with 304,638 more rows\rfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;, sub_region_1 == \u0026quot;New York\u0026quot;)\r## # A tibble: 7,068 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States New York 2020-02-15 0\r## 2 US United States New York 2020-02-16 -1\r## 3 US United States New York 2020-02-17 9\r## 4 US United States New York 2020-02-18 3\r## 5 US United States New York 2020-02-19 2\r## 6 US United States New York 2020-02-20 2\r## 7 US United States New York 2020-02-21 3\r## 8 US United States New York 2020-02-22 -1\r## 9 US United States New York 2020-02-23 -1\r## 10 US United States New York 2020-02-24 0\r## # ... with 7,058 more rows\rfilter(residential_mobility, resi \u0026gt; 50) %\u0026gt;% arrange(-resi)\r## # A tibble: 32 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 KW Kuwait Al Farwaniyah Governorate 2020-05-14 56\r## 2 KW Kuwait Al Farwaniyah Governorate 2020-05-21 55\r## 3 SG Singapore \u0026lt;NA\u0026gt; 2020-05-01 55\r## 4 KW Kuwait Al Farwaniyah Governorate 2020-05-28 54\r## 5 PE Peru Metropolitan Municipalit~ 2020-04-10 54\r## 6 EC Ecuador Pichincha 2020-03-27 53\r## 7 KW Kuwait Al Farwaniyah Governorate 2020-05-11 53\r## 8 KW Kuwait Al Farwaniyah Governorate 2020-05-13 53\r## 9 KW Kuwait Al Farwaniyah Governorate 2020-05-20 53\r## 10 SG Singapore \u0026lt;NA\u0026gt; 2020-04-10 53\r## # ... with 22 more rows\r\r4.4.3 Group and summarize\rWhere do we find greater variability between regions in each country on April 1, 2020?\nTo answer this question, we first filter the data and then we group by the country column. When we use the summarize() function after grouping, it allows us to summarize by these groups. Moreover, combining group_by() with the mutate() function modifies columns in each group separately. In summarize() we calculate the maximum, minimum value and the difference between both extremes creating new columns.\nresi_variability \u0026lt;- residential_mobility %\u0026gt;% filter(date == ymd(\u0026quot;2020-04-01\u0026quot;),\r!is.na(sub_region_1)) %\u0026gt;% group_by(country_region) %\u0026gt;% summarise(mx = max(resi, na.rm = TRUE), min = min(resi, na.rm = TRUE),\rrange = abs(mx)-abs(min))\rarrange(resi_variability, -range)\r## # A tibble: 94 x 4\r## country_region mx min range\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Nigeria 43 6 37\r## 2 United States 35 6 29\r## 3 India 36 15 21\r## 4 Malaysia 45 26 19\r## 5 Philippines 40 21 19\r## 6 Vietnam 28 9 19\r## 7 Colombia 41 24 17\r## 8 Ecuador 44 27 17\r## 9 Argentina 35 19 16\r## 10 Chile 30 14 16\r## # ... with 84 more rows\r\r4.4.4 Join tables\rHow can we filter the data to get a subset of Europe?\nTo do this, we import a spatial dataset with the country code and a column of regions. Detailed explanations about the sf (simple feature) package, I’ll leave for another post.\nlibrary(rnaturalearth) # package of spatial vectorial data\r# world limits\rwld \u0026lt;- ne_countries(returnclass = \u0026quot;sf\u0026quot;)\r# filter the countries with iso code and select the two columns of interest\rwld \u0026lt;- filter(wld, !is.na(iso_a2)) %\u0026gt;% select(iso_a2, subregion)\r# plot\rplot(wld)\rOther dplyr functions allow us to join tables: *_join (). Depending on which table (left or right) you want to join, the functions change: left_join(), right_join() or even full_join(). The by argument is not necessary as long as both tables have a column in common. However, in this case the variable names are different, so we use the following way: c(\"country_region_code\"=\"iso_a2\"). The forcats package of tidyverse has many useful functions for handling categorical variables (factors), variables that have a fixed and known set of possible values. All forcats functions have the prefix fct_*. For example, in this case we use fct_reorder() to reorder the country labels in order of the maximum based on the residential mobility records. Finally, we create a new column \"resi_real\" to change the reference value, the average or baseline, from 0 to 100.\nsubset_europe \u0026lt;- filter(residential_mobility, is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;%\rmutate(resi_real = resi + 100,\rregion = fct_reorder(country_region, resi, .fun = \u0026quot;max\u0026quot;, .desc = FALSE)) %\u0026gt;% select(-geometry, -sub_region_1)\rstr(subset_europe)\r## tibble [3,988 x 7] (S3: tbl_df/tbl/data.frame)\r## $ country_region_code: chr [1:3988] \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; ...\r## $ country_region : chr [1:3988] \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; ...\r## $ date : Date[1:3988], format: \u0026quot;2020-02-15\u0026quot; \u0026quot;2020-02-16\u0026quot; ...\r## $ resi : num [1:3988] -2 -2 0 0 1 0 1 -2 0 -1 ...\r## $ subregion : chr [1:3988] \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; ...\r## $ resi_real : num [1:3988] 98 98 100 100 101 100 101 98 100 99 ...\r## $ region : Factor w/ 35 levels \u0026quot;Belarus\u0026quot;,\u0026quot;Ukraine\u0026quot;,..: 18 18 18 18 18 18 18 18 18 18 ...\r\r4.4.5 Long and wide tables\rBefore we go to create graphics with ggplot2, it is very common to modify the table between two main formats, long and wide. A table is tidy when 1) each variable is a column 2) each observation/case is a row and 3) each type of observational unit forms a table.\n# subset\rmobility_selection \u0026lt;- select(subset_europe, country_region_code, date:resi)\rmobility_selection\r## # A tibble: 3,988 x 3\r## country_region_code date resi\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 AT 2020-02-15 -2\r## 2 AT 2020-02-16 -2\r## 3 AT 2020-02-17 0\r## 4 AT 2020-02-18 0\r## 5 AT 2020-02-19 1\r## 6 AT 2020-02-20 0\r## 7 AT 2020-02-21 1\r## 8 AT 2020-02-22 -2\r## 9 AT 2020-02-23 0\r## 10 AT 2020-02-24 -1\r## # ... with 3,978 more rows\r# wide table\rmobi_wide \u0026lt;- pivot_wider(mobility_selection, names_from = country_region_code,\rvalues_from = resi)\rmobi_wide\r## # A tibble: 114 x 36\r## date AT BA BE BG BY CH CZ DE DK EE ES\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 -2 -1 -1 0 -1 -1 -2 -1 0 0 -2\r## 2 2020-02-16 -2 -1 1 -3 0 -1 -1 0 1 0 -2\r## 3 2020-02-17 0 -1 0 -2 0 1 0 0 1 1 -1\r## 4 2020-02-18 0 -1 0 -2 0 1 0 1 1 1 0\r## 5 2020-02-19 1 -1 0 -1 -1 1 0 1 1 0 -1\r## 6 2020-02-20 0 -1 0 0 -1 0 0 1 1 0 -1\r## 7 2020-02-21 1 -2 0 -1 -1 1 0 2 1 1 -2\r## 8 2020-02-22 -2 -1 0 0 -2 -2 -3 0 1 0 -2\r## 9 2020-02-23 0 -1 0 -3 -1 -1 0 0 0 -2 -3\r## 10 2020-02-24 -1 -1 4 -1 0 0 0 4 0 16 0\r## # ... with 104 more rows, and 24 more variables: FI \u0026lt;dbl\u0026gt;, FR \u0026lt;dbl\u0026gt;, GB \u0026lt;dbl\u0026gt;,\r## # GR \u0026lt;dbl\u0026gt;, HR \u0026lt;dbl\u0026gt;, HU \u0026lt;dbl\u0026gt;, IE \u0026lt;dbl\u0026gt;, IT \u0026lt;dbl\u0026gt;, LT \u0026lt;dbl\u0026gt;, LU \u0026lt;dbl\u0026gt;,\r## # LV \u0026lt;dbl\u0026gt;, MD \u0026lt;dbl\u0026gt;, MK \u0026lt;dbl\u0026gt;, NL \u0026lt;dbl\u0026gt;, NO \u0026lt;dbl\u0026gt;, PL \u0026lt;dbl\u0026gt;, PT \u0026lt;dbl\u0026gt;,\r## # RO \u0026lt;dbl\u0026gt;, RS \u0026lt;dbl\u0026gt;, RU \u0026lt;dbl\u0026gt;, SE \u0026lt;dbl\u0026gt;, SI \u0026lt;dbl\u0026gt;, SK \u0026lt;dbl\u0026gt;, UA \u0026lt;dbl\u0026gt;\r# back to long table\rpivot_longer(mobi_wide,\r2:36,\rnames_to = \u0026quot;country_code\u0026quot;,\rvalues_to = \u0026quot;resi\u0026quot;)\r## # A tibble: 3,990 x 3\r## date country_code resi\r## \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 AT -2\r## 2 2020-02-15 BA -1\r## 3 2020-02-15 BE -1\r## 4 2020-02-15 BG 0\r## 5 2020-02-15 BY -1\r## 6 2020-02-15 CH -1\r## 7 2020-02-15 CZ -2\r## 8 2020-02-15 DE -1\r## 9 2020-02-15 DK 0\r## 10 2020-02-15 EE 0\r## # ... with 3,980 more rows\rAnother group of functions you should take a look at are: separate(), case_when(), complete(). More details can be found in the Cheat-Sheet of dplyr.\n\r\r4.5 Visualize data\rggplot2 is a modern system for data visualization with a huge variety of options. Unlike the R Base graphic system, in ggplot2 a different grammar is used. The grammar of graphics (gg) consists of the sum of several independent layers or objects that are combined using + to construct the final graph. ggplot differentiates between data, what is displayed and how it is displayed.\n\rdata: our dataset (data.frame or tibble)\n\raesthetics: with the aes() function we indicate the variables that correspond to the x, y, z, … axes, or when it is intended to apply graphic parameters (color, size, shape) according to a variable. It is possible to include aes() in ggplot() or in the corresponding function to a geometry geom_ *.\n\rgeometries: are geom_ * objects that indicate the geometry to be used, (eg: geom_point(), geom_line(), geom_boxplot(), etc.).\n\rscales: are objects of type scales_ * (eg, scale_x_continous(), scale_colour_manual()) to manipulate axes, define colors, etc.\n\rstatistics: are stat_ * objects (eg, stat_density()) that allow to apply statistical transformations.\n\r\rMore details can be found in the Cheat-Sheet of ggplot2. ggplot is constantly supplemented by extensions for geometries or other graphical options (see https://exts.ggplot2.tidyverse.org/ggiraph.html), for graphical ideas have a look a the R Graph Gallery (https://www.r-graph-gallery.com/).\n4.5.1 Line and scatter plot\rWe create a subset of our mobility data for residences and parks, filtering the records for Italian regions. In addition, we divide the mobility values in percentage by 100 to obtain the fraction, since ggplot2 allows us to indicate the unit of percentage in the label argument (see last plot in this section).\n# create subset\rit \u0026lt;- filter(google_mobility, country_region == \u0026quot;Italy\u0026quot;, is.na(sub_region_1)) %\u0026gt;% mutate(resi = residential_percent_change_from_baseline/100, parks = parks_percent_change_from_baseline/100)\r# line plot\rggplot(it, aes(date, resi)) + geom_line()\r# scatter plot\rggplot(it, aes(parks, resi)) + geom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rTo modify the axes, we use the different scale_* functions that we must adapt to the scales of measurement (date, discrete, continuous, etc.). The labs() function helps us define the axis, legend and plot titles. Finally, we add the style of the graph with theme_light() (others are theme_bw(), theme_minimal(), etc.). We could also make changes to all graphic elements through theme().\n# time serie plot\rggplot(it, aes(date, resi)) + geom_line(colour = \u0026quot;#560A86\u0026quot;, size = 0.8) +\rscale_x_date(date_breaks = \u0026quot;10 days\u0026quot;, date_labels = \u0026quot;%d %b\u0026quot;) +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r# scatter plot\rggplot(it, aes(parks, resi)) + geom_point(alpha = .4, size = 2) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rscale_x_continuous(breaks = seq(-1, 1.4, 0.2), labels = scales::percent) +\rscale_y_continuous(breaks = seq(-1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;Park mobility\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\r4.5.2 Boxplot\rWe can visualize different aspects of the mobility with other geometries. Here we will create boxplots for each European country representing the variability of mobility between and within countries during the COVID-19 pandemic.\n# subset\rsubset_europe_reg \u0026lt;- filter(residential_mobility, !is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;% mutate(resi = resi/100, country_region = fct_reorder(country_region, resi))\r# boxplot\rggplot(subset_europe_reg, aes(country_region, resi, fill = subregion)) + geom_boxplot() +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;) +\rcoord_flip() +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;, fill = \u0026quot;\u0026quot;) +\rtheme_minimal()\r\r4.5.3 Heatmap\rTo visualize the mobility trend of all European countries it is recommended to use a heatmap instead of a bundle of lines. Before building the graph, we will create a vector of Sundays for the x-axis labels in the observation period.\n# sequence of dates\rdf \u0026lt;- data.frame(d = seq(ymd(\u0026quot;2020-02-15\u0026quot;), ymd(\u0026quot;2020-06-07\u0026quot;), \u0026quot;day\u0026quot;))\r# filter on Sundays sundays \u0026lt;- df %\u0026gt;% mutate(wd = wday(d, week_start = 1)) %\u0026gt;% filter(wd == 7) %\u0026gt;% pull(d)\rTo difference between European regions, we will use a color fill for the boxplots. We can set the color type with scale_fill_*, in this case, from the viridis scheme. In addition, the guides() function can modify the color bar of the legend. Finally, here we see the use of theme() with additional changes to theme_minimal().\n# headmap\rggplot(subset_europe, aes(date, region, fill = resi_real)) +\rgeom_tile() +\rscale_x_date(breaks = sundays,\rdate_labels = \u0026quot;%d %b\u0026quot;) +\rscale_fill_viridis_c(option = \u0026quot;A\u0026quot;, breaks = c(91, 146),\rlabels = c(\u0026quot;Less\u0026quot;, \u0026quot;More\u0026quot;), direction = -1) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;top\u0026quot;, title = element_text(size = 14),\rpanel.grid.major.x = element_line(colour = \u0026quot;white\u0026quot;, linetype = \u0026quot;dashed\u0026quot;),\rpanel.grid.minor.x = element_blank(),\rpanel.grid.major.y = element_blank(),\rpanel.ontop = TRUE,\rplot.margin = margin(r = 1, unit = \u0026quot;cm\u0026quot;)) +\rlabs(y = \u0026quot;\u0026quot;, x = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, title = \u0026quot;Mobility trends for places of residence\u0026quot;,\rcaption = \u0026quot;Data: google.com/covid19/mobility/\u0026quot;) +\rguides(fill = guide_colorbar(barwidth = 10, barheight = .5,\rlabel.position = \u0026quot;top\u0026quot;, ticks = FALSE)) +\rcoord_cartesian(expand = FALSE)\r\r\r4.6 Apply functions on vectors or lists\rThe purrr package contains a set of advanced functional programming functions for working with functions and vectors. The known lapply() family of R Base corresponds to the map() functions in this package. One of the biggest advantages is being able to reduce the use of loops (for, etc.).\n# list of two vectors\rvec_list \u0026lt;- list(x = 1:10, y = 50:70)\r# calculate the average for each one\rmap(vec_list, mean)\r## $x\r## [1] 5.5\r## ## $y\r## [1] 60\r# change the output type map_* (dbl, chr, lgl, etc.)\rmap_dbl(vec_list, mean)\r## x y ## 5.5 60.0\rFinally, a more complex example. We calculate the correlation coefficient between residential and park mobility in all European countries. To get a tidy summary of a model or test we use the tidy() function of the broom package.\nlibrary(broom) # tidy outputs\r# custom function\rcor_test \u0026lt;- function(x, formula) { df \u0026lt;- cor.test(as.formula(formula), data = x) %\u0026gt;% tidy()\rreturn(df)\r}\r# prepare the data\reurope_reg \u0026lt;- filter(google_mobility, !is.na(sub_region_1),\r!is.na(residential_percent_change_from_baseline)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;))\r# apply the function to each country creating a list\rcor_mobility \u0026lt;- europe_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;) cor_mobility[1:5]\r## $AT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.360 -12.3 2.68e-32 1009 -0.413 -0.305 Pearson\u0026#39;~ two.sided ## ## $BE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.312 -6.06 0.00000000367 340 -0.405 -0.213 Pears~ two.sided ## ## $BG\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pearson~ two.sided ## ## $CH\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0786 -2.91 0.00370 1360 -0.131 -0.0256 Pearson\u0026#39;s~ two.sided ## ## $CZ\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0837 -3.35 0.000824 1593 -0.132 -0.0347 Pearson\u0026#39;~ two.sided\rAs we’ve seen before, there are subfunctions of map_* to get an object of another class instead of a list, here for a bind data.frame.\ncor_mobility \u0026lt;- europe_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map_df(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;, .id = \u0026quot;country_code\u0026quot;)\rarrange(cor_mobility, estimate)\r## # A tibble: 27 x 9\r## country_code estimate statistic p.value parameter conf.low conf.high method\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 IT -0.831 -71.0 0 2250 -0.844 -0.818 Pears~\r## 2 ES -0.825 -65.4 0 2005 -0.839 -0.811 Pears~\r## 3 PT -0.729 -46.9 2.12e-321 1938 -0.749 -0.707 Pears~\r## 4 FR -0.698 -37.4 3.29e-216 1474 -0.723 -0.671 Pears~\r## 5 GR -0.692 -27.0 1.03e-114 796 -0.726 -0.654 Pears~\r## 6 BG -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pears~\r## 7 RO -0.640 -56.0 0 4517 -0.657 -0.623 Pears~\r## 8 SI -0.627 -11.4 1.98e- 23 200 -0.704 -0.535 Pears~\r## 9 HR -0.579 -21.9 9.32e- 87 954 -0.620 -0.536 Pears~\r## 10 LV -0.544 -6.87 3.84e- 10 112 -0.662 -0.401 Pears~\r## # ... with 17 more rows, and 1 more variable: alternative \u0026lt;chr\u0026gt;\rOther practical examples here in this post or this other. More details can be found in the Cheat-Sheet of purrr.\n\r\r","date":1591401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591401600,"objectID":"952fbca7b9bd8738d1131d90f275f11b","permalink":"https://dominicroye.github.io/en/2020/a-very-short-introduction-to-tidyverse/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/en/2020/a-very-short-introduction-to-tidyverse/","section":"post","summary":"The tidyverse universe of packages, a collection of packages specially focused on data science, marked a milestone in R programming. In this post I am going to summarize very briefly the most essential to start in this world. The tidyverse grammar follows a common structure in all functions. The most essential thing is that the first argument is the object and then come the rest of the arguments. In addition, a set of verbs is provided to facilitate the use of the functions. The tidyverse philosophy and grammar of functions structure is also reflected in other packages that make its use compatible with the collection of tidyverse.","tags":["introduction","visualization","manipulation","data","COVID-19"],"title":"A very short introduction to Tidyverse","type":"post"},{"authors":["A Santurtún","R Almendra","P Fdez-Arroyabe","A Sanchez-Lorenzo","D Royé","MT Zarrabeitia","P Santana"],"categories":null,"content":"","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588032000,"objectID":"bc2223b382b109cd2bd0bc492dbcceb8","permalink":"https://dominicroye.github.io/en/publication/ingresos_cardio_2020/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/en/publication/ingresos_cardio_2020/","section":"publication","summary":"The natural environment has been considered an important determinant of cardiovascular morbidity. This work seeks to assess the impact of the winter thermal environment on hospital admissions from diseases of the circulatory system by using three biometeorological indices in five regions of the Iberian Peninsula. A theoretical index based on a thermophysiological model (Universal Thermal Climate Index [UTCI]) and two experimental biometeorological ones (Net Effective Temperature [NET] and Apparent Temperature [AT]) were estimated in two metropolitan areas of Portugal (Porto and Lisbon) and in three provinces of Spain (Madrid, Barcelona and Valencia). Subsequently, their relationship with hospital admissions, adjusted by NO2 concentration, time, and day of the week, was analyzed using a Generalized Additive Model. As the estimation method, a semi-parametric quasi-Poisson regression was used. Around 53% of the hospitalizations occurred during the cold periods. The admissions rate followed an upward trend over the 9-year period in both capitals (Madrid and Lisbon) as well as in Barcelona. An inverse and statistically significant relationship was found between thermal comfort and hospital admissions in the five regions (p","tags":["Circulatory system diseases","Air temperature","Net effective temperature","Apparent temperature","Universal thermal climate index"],"title":"Predictive value of three thermal confort indices in low temperatures on cardiovascular morbidity in the Iberian Peninsula","type":"publication"},{"authors":null,"categories":["visualization","R","R:intermediate"],"content":"\r\rWhen we visualize precipitation and temperature anomalies, we simply use time series as bar graph indicating negative and positive values in red and blue. However, in order to have a better overview we need both anomalies in a single graph. In this way we could more easly answer the question of whether a particular season or month was dry-warm or wet-cold, and even compare these anomalies in the context of previous years.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rlubridate\rEasy manipulation of dates and times\r\rggrepel\rRepel overlapping text labels in ggplot2\r\r\r\r#we install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggrepel\u0026quot;)) install.packages(\u0026quot;ggrepel\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggrepel)\r\rPreparing the data\rFirst we import the daily precipitation and temperature data from the selected weather station (download). We will use the data from Tenerife South (Spain) [1981-2020] accessible through Open Data AEMET. In R there is a package called meteoland that facilitates the download with specific functions to access data from AEMET (Spanish State Meteorological Agency), Meteogalicia (Galician Meteorological Service) and Meteocat (Catalan Meteorological Service).\nStep 1: import the data\rWe import the data in csv format, the first column is the date, the second column the precipitation (pr) and the last column the average daily temperature (ta).\ndata \u0026lt;- read_csv(\u0026quot;meteo_tenerife.csv\u0026quot;) \r## ## -- Column specification --------------------------------------------------------\r## cols(\r## date = col_date(format = \u0026quot;\u0026quot;),\r## pr = col_double(),\r## ta = col_double()\r## )\rdata\r## # A tibble: 14,303 x 3\r## date pr ta\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1981-01-02 0 17.6\r## 2 1981-01-03 0 16.8\r## 3 1981-01-04 0 17.4\r## 4 1981-01-05 0 17.6\r## 5 1981-01-06 0 17 ## 6 1981-01-07 0 17.6\r## 7 1981-01-08 0 18.6\r## 8 1981-01-09 0 19.8\r## 9 1981-01-10 0 21.5\r## 10 1981-01-11 3.8 17.6\r## # ... with 14,293 more rows\r\rStep 2: preparing the data\rIn the second step we prepare the data to calculate the anomalies. To do this, we create three new columns: the month, the year, and the season of the year. Since our objective is to analyse winter anomalies, we cannot use the calendar year, because winter includes the month of December of one year and the months of January and February of the following. The custom function meteo_yr() extracts the year from a date indicating the starting month. The concept is similar to the hydrological year in which it starts on October 1.\nmeteo_yr \u0026lt;- function(dates, start_month = NULL) {\r# convert to POSIXlt\rdates.posix \u0026lt;- as.POSIXlt(dates)\r# year offset\roffset \u0026lt;- ifelse(dates.posix$mon \u0026gt;= start_month - 1, 1, 0)\r# new year\radj.year = dates.posix$year + 1900 + offset\rreturn(adj.year)\r}\rWe will use many functions of the package collection tidyverse (https://www.tidyverse.org/). The mutate() function helps to add new columns or change existing ones. To define the seasons, we use the case_when() function from the dplyr package, which has many advantages compared to a chain of ifelse(). In case_when() we use two-side formulas, on the one hand the condition and on the other the action when that condition is met. A two-sided formula in R consists of the operator ~. The binary operator %in% allows us to filter several values in a greater set.\ndata \u0026lt;- mutate(data, winter_yr = meteo_yr(date, 12),\rmonth = month(date), season = case_when(month %in% c(12,1:2) ~ \u0026quot;Winter\u0026quot;,\rmonth %in% 3:5 ~ \u0026quot;Spring\u0026quot;,\rmonth %in% 6:8 ~ \u0026quot;Summer\u0026quot;,\rmonth %in% 9:11 ~ \u0026quot;Autum\u0026quot;))\rdata\r## # A tibble: 14,303 x 6\r## date pr ta winter_yr month season\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1981-01-02 0 17.6 1981 1 Winter\r## 2 1981-01-03 0 16.8 1981 1 Winter\r## 3 1981-01-04 0 17.4 1981 1 Winter\r## 4 1981-01-05 0 17.6 1981 1 Winter\r## 5 1981-01-06 0 17 1981 1 Winter\r## 6 1981-01-07 0 17.6 1981 1 Winter\r## 7 1981-01-08 0 18.6 1981 1 Winter\r## 8 1981-01-09 0 19.8 1981 1 Winter\r## 9 1981-01-10 0 21.5 1981 1 Winter\r## 10 1981-01-11 3.8 17.6 1981 1 Winter\r## # ... with 14,293 more rows\r\rStep 3: estimate winter anomalies\rIn the next step we create a subset of the winter months. Then we group by the defined meteorological year and calculate the sum and average for precipitation and temperature, respectively. To facilitate the work, the magrittr package introduces the operator called pipe in the form %\u0026gt;% with the aim of combining several functions without the need to assign the result to a new object. The pipe operator passes the output of a function applied to the first argument of the next function. This way of combining functions allows you to chain several steps simultaneously. The %\u0026gt;% must be understood and pronounced as then.\ndata_inv \u0026lt;- filter(data, season == \u0026quot;Winter\u0026quot;) %\u0026gt;% group_by(winter_yr) %\u0026gt;%\rsummarise(pr = sum(pr, na.rm = TRUE),\rta = mean(ta, na.rm = TRUE))\rNow we only have to calculate the anomalies of precipitation and temperature. The columns pr_mean and ta_mean will contain the climate average, the reference for the anomalies with respect to the normal period 1981-2010. Therefore, we need to filter the values to the period before 2010, which we will do in the usual way of filtering vectors in R. Once we have the references we estimate the anomalies pr_anom and ta_anom. To facilitate the interpretation, in the case of precipitation we express the anomalies as percentage, with the average set at 0% instead of 100%.\nIn addition, we add three required columns with information for the creation of the graph: 1) labyr contains the year of each anomaly as long as it has been greater/less than -+10% or -+0.5ºC, respectively (this is for reducing the number of labels), 2) symb_point is a dummy variable in order to be able to create different symbols between the cases of (1), and 3) lab_font for highlighting in bold the year 2020.\ndata_inv \u0026lt;- mutate(data_inv, pr_mean = mean(pr[winter_yr \u0026lt;= 2010]), ta_mean = mean(ta[winter_yr \u0026lt;= 2010]),\rpr_anom = (pr*100/pr_mean)-100, ta_anom = ta-ta_mean,\rlabyr = case_when(pr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr),\rsymb_point = ifelse(!is.na(labyr), \u0026quot;yes\u0026quot;, \u0026quot;no\u0026quot;),\rlab_font = ifelse(labyr == 2020, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)\r)\r\r\rCreating the graph\rWe will build the chart adding layer by layer the distinctive elements: 1) the background with the different grids (Dry-Warm, Dry-Cold, etc.), 2) the points and labels, and 3) the style adjustments.\nPart 1\rThe idea is that the points with dry-warm anomalies are located in quadrant I (top-right) and those with wet-cold in quadrant III (bottom-left). Therefore, we must invert the sign in the precipitation anomalies. Then we create a data.frame with the label positions of the four quadrants. For the positions in x and y Inf and -Inf are used, which is equivalent to the maximum panel sides with respect to the data. However, it is necessary to adjust the position towards the extreme points within the panel with the known arguments of ggplot2: hjust and vjust.\ndata_inv_p \u0026lt;- mutate(data_inv, pr_anom = pr_anom * -1)\rbglab \u0026lt;- data.frame(x = c(-Inf, Inf, -Inf, Inf), y = c(Inf, Inf, -Inf, -Inf),\rhjust = c(1, 1, 0, 0), vjust = c(1, 0, 1, 0),\rlab = c(\u0026quot;Wet-Warm\u0026quot;, \u0026quot;Dry-Warm\u0026quot;,\r\u0026quot;Wet-Cold\u0026quot;, \u0026quot;Dry-Cold\u0026quot;))\rbglab\r## x y hjust vjust lab\r## 1 -Inf Inf 1 1 Wet-Warm\r## 2 Inf Inf 1 0 Dry-Warm\r## 3 -Inf -Inf 0 1 Wet-Cold\r## 4 Inf -Inf 0 0 Dry-Cold\r\rPart 2\rIn the second part we can start building the chart by adding all graphical elements. First we create the background with different colors of each quadrant. The function annotate() allows adding geometry layers without the use of variables within data.frames. With the geom_hline() and geom_vline() function we mark the quadrants horizontally and vertically using a dashed line. Finally, we draw the labels of each quadrant, using the function geom_text(). When we use other data sources than the main one used in ggplot(), we must indicate it with the argument data in the corresponding geometry function.\ng1 \u0026lt;- ggplot(data_inv_p, aes(pr_anom, ta_anom)) +\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = 0, ymax = Inf, fill = \u0026quot;#fc9272\u0026quot;, alpha = .6) + #wet-warm\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = 0, ymax = Inf, fill = \u0026quot;#cb181d\u0026quot;, alpha = .6) + #dry-warm\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = -Inf, ymax = 0, fill = \u0026quot;#2171b5\u0026quot;, alpha = .6) + #wet-cold\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = -Inf, ymax = 0, fill = \u0026quot;#c6dbef\u0026quot;, alpha = .6) + #dry-cold\rgeom_hline(yintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_vline(xintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = bglab, aes(x, y, label = lab, hjust = hjust, vjust = vjust),\rfontface = \u0026quot;italic\u0026quot;, size = 5, angle = 90, colour = \u0026quot;white\u0026quot;)\rg1\r\rPart 3\rIn the third part we simply add the points of the anomalies and the labels of the years. The geom_text_repel() function is similar to the one known by default in ggplot2, geom_text(), but it repels overlapping text labels away from each other.\ng2 \u0026lt;- g1 + geom_point(aes(fill = symb_point, colour = symb_point),\rsize = 2.8, shape = 21, show.legend = FALSE) +\rgeom_text_repel(aes(label = labyr, fontface = lab_font),\rmax.iter = 5000, size = 3.5) g2\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\rPart 4\rIn the last part we adjust, in addition to the general style, the axes, the color type and the (sub)title. Remember that we changed the sign on precipitation anomalies. Hence, we must use the arguments breaks and labels in the function scale_x_continouous() to reverse the sign in the labels corresponding to the breaks.\ng3 \u0026lt;- g2 + scale_x_continuous(\u0026quot;Precipitation anomaly in %\u0026quot;,\rbreaks = seq(-100, 250, 10) * -1,\rlabels = seq(-100, 250, 10),\rlimits = c(min(data_inv_p$pr_anom), 100)) +\rscale_y_continuous(\u0026quot;Mean temperature anomaly in ºC\u0026quot;,\rbreaks = seq(-2, 2, 0.5)) +\rscale_fill_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;)) +\rscale_colour_manual(values = rev(c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;))) +\rlabs(title = \u0026quot;Winter anomalies in Tenerife South\u0026quot;, caption = \u0026quot;Data: AEMET\\nNormal period 1981-2010\u0026quot;) +\rtheme_bw()\rg3\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\r\r","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"383cf2261394202e33675dd1ab34b53a","permalink":"https://dominicroye.github.io/en/2020/visualize-climate-anomalies/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/en/2020/visualize-climate-anomalies/","section":"post","summary":"When we visualize precipitation and temperature anomalies, we simply use time series as bar graph indicating negative and positive values in red and blue. However, in order to have a better overview we need both anomalies in a single graph. In this way we could more easly answer the question of whether a particular season or month was dry-warm or wet-cold, and even compare these anomalies in the context of previous years.","tags":["anomaly","precipitation","temperature","climate","points"],"title":"Visualize climate anomalies","type":"post"},{"authors":["R Monjo","D Royé","J Martin-Vide"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"0063787ba565ca8c8497c7951af2a9f3","permalink":"https://dominicroye.github.io/en/publication/drought_class_2020/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/en/publication/drought_class_2020/","section":"publication","summary":"Drought duration strongly depends on the definition thereof. In meteorology, dryness is habitually measured by means of fixed thresholds (e.g. 0.1 or 1 mm usually define dry spells) or climatic mean values (as is the case of the Standard-ised Precipitation Index), but this also depends on the aggregation time interval considered. However, robust measurements of drought duration are required for analysing the statistical significance of possible changes. Herein we have climatically classified the drought duration around the world according to their similarity to the voids of the Cantor set. Dryness time structure 5 can be concisely measured by the n-index (from the regular/irregular alternation of dry/wet spells), which is closely related to the Gini index and to a Cantor-based exponent. This enables the world's climates to be classified into six large types based upon a new measure of drought duration. We performed the dry-spell analysis using the full global gridded daily Multi-Source Weighted-Ensemble Precipitation (MSWEP) dataset. The MSWEP combines gauge-, satellite-, and reanalysis-based data to provide reliable precipitation estimates. The study period comprises the years 1979-2016 (total of 45165 days), and a spatial 10 resolution of 0.5°, with a total of 259,197 grid points. Data set is publicly available at https://doi.org/10.5281/zenodo.3247041 (Monjo et al., 2019).","tags":["drought","classification","world","lacunarity","spatio-temporal patterns","dry spells"],"title":"Meteorological drought lacunarity around the world and its classification","type":"publication"},{"authors":["D Royé","F Tedim","J Martin-Vide","M Salis","J Vendrell","R Lovreglio","C Bouillon","V Leone"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"38384d9eb7330b815ac81013a4f15327","permalink":"https://dominicroye.github.io/en/publication/incendios_ci_2019/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/en/publication/incendios_ci_2019/","section":"publication","summary":"The most widely used metrics to characterize wildfire regime and estimate the impact of wildfires are total burnt area (BA) and the number of fire events (FE). However, these are insufficient to analyse the threat to society of a new fire regime characterized by a higher occurrence of very large events. To overcome this weakness, we propose the use of a Concentration Index (CIB) which makes it possible to identify spatio-temporal patterns. The frequency distribution of BA follows a negative exponential distribution almost everywhere, in which a small minority of FE are responsible of the majority of BA. In this article, the spatio-temporal behaviour of BA is analysed in Western Mediterranean Europe, with particular focus on Portugal, Spain, France and Italy, using data from the European Forest Fire Information System and national wildfire databases. This is the first time that the CI has been applied to wildfire events. This research shows that, in most Mediterranean European countries, the amount of BA is increasingly related with a lower number of fires. The spatio-temporal distribution of CIB shows high variability in all of the countries analysed in Europe. Portugal and Spain show increasing significant trends of CIB +7.6% (p-value = 0.001) and +1.3% per decade (p-value = 0.003). Statistically significant correlations for Portugal, Spain and Italy are also found between the annual CIB and several teleconnection indices. The application of the CIB demonstrates its discriminatory ability, which is a key point in detecting vulnerable areas and temporal trends under climate change.","tags":["wildfire","concentration index","Europe","teleconnection","spatio-temporal patterns"],"title":"Wildfire burnt area patterns and trends in Western Mediterranean Europe via the application of a concentration index","type":"publication"},{"authors":["D Royé","A Tobías","C Iñiguez"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"774e7433c596887493ddca020d4167f6","permalink":"https://dominicroye.github.io/en/publication/era5_esp_2020/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/en/publication/era5_esp_2020/","section":"publication","summary":"Background: Most studies use temperature observation data from weather stations near the analyzed region or city as the reference point for the exposure-response association. Climatic reanalysis data sets have already been used for climate studies, but are not yet used routinely in environmental epidemiology. Methods: We compared the mortality-temperature association using weather station temperature and ERA-5 reanalysis data for the 52 provincial capital cities in Spain, using time-series regression with distributed lag non-linear models. Results: The shape of temperature distribution is very close between the weather station and ERA-5 reanalysis data (correlation from 0.90 to 0.99). The overall cumulative exposure-response curves are very similar in their shape and risks estimates for cold and heat effects, although risk estimates for ERA-5 were slightly lower than for weather station temperature. Conclusions: Reanalysis data allow the estimation of the health effects of temperature, even in areas located far from weather stations or without any available.","tags":["Spain","temperature","reanalysis","ERA-5","mortality","weather stations"],"title":"Comparison of temperature-mortality associations using observed weather station and reanalysis data in 52 Spanish cities","type":"publication"},{"authors":null,"categories":["spatial analysis","R","R:elementary","gis"],"content":"\r\rThe first post of this year 2020, I will dedicate to a question that I was recently asked. The question was how to calculate the shortest distance between different points and how to know which is the closest point. When we work with spatial data in R, currently the easiest thing is to use the sf package in combination with the tidyverse collection of packages. We also use the units package which is very useful for working with units of measurement.\nPackages\r\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\runits\rSupport for measurement units in R vectors, matrices and arrays: propagation, conversion, derivation\r\rmaps\rDraw geographical maps\r\rrnaturalearth\rHold and facilitate interaction with Natural Earth map data\r\r\r\r# install the necessary packages\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;units\u0026quot;)) install.packages(\u0026quot;units\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;maps\u0026quot;)) install.packages(\u0026quot;maps\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r# load packages\rlibrary(maps)\rlibrary(sf) library(tidyverse)\rlibrary(units)\rlibrary(rnaturalearth)\r\rMeasurement units\rThe use of vectors and matrices with the units class allows us to calculate and transform units of measurement.\n# length\rl \u0026lt;- set_units(1:10, m)\rl\r## Units: [m]\r## [1] 1 2 3 4 5 6 7 8 9 10\r# convert units\rset_units(l, cm)\r## Units: [cm]\r## [1] 100 200 300 400 500 600 700 800 900 1000\r# sum different units\rset_units(l, cm) + l\r## Units: [cm]\r## [1] 200 400 600 800 1000 1200 1400 1600 1800 2000\r# area\ra \u0026lt;- set_units(355, ha)\rset_units(a, km2)\r## 3.55 [km2]\r# velocity\rvel \u0026lt;- set_units(seq(20, 50, 10), km/h)\rset_units(vel, m/s)\r## Units: [m/s]\r## [1] 5.555556 8.333333 11.111111 13.888889\r\rCapital cities of the world\rWe will use the capital cities of the whole world with the objective of calculating the distance to the nearest capital city and indicating the name/country.\n# set of world cities with coordinates\rhead(world.cities) # from the maps package\r## name country.etc pop lat long capital\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 31.31 34.34 0\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 31.32 34.35 0\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 30.55 72.11 0\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 29.36 47.98 0\r## 5 \u0026#39;Abud Palestine 2456 32.03 35.07 0\r## 6 \u0026#39;Abwein Palestine 3434 32.03 35.20 0\rTo convert points with longitude and latitude into a spatial object of class sf, we use the function st_as_sf(), indicating the coordinate columns and the coordinate reference system (WSG84, epsg: 4326).\n# convert the points into an sf object with CRS WSG84\rcities \u0026lt;- st_as_sf(world.cities, coords = c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;), crs = 4326)\rcities\r## Simple feature collection with 43645 features and 4 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -178.8 ymin: -54.79 xmax: 179.81 ymax: 78.93\r## Geodetic CRS: WGS 84\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 0 POINT (34.34 31.31)\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 0 POINT (34.35 31.32)\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 0 POINT (72.11 30.55)\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 0 POINT (47.98 29.36)\r## 5 \u0026#39;Abud Palestine 2456 0 POINT (35.07 32.03)\r## 6 \u0026#39;Abwein Palestine 3434 0 POINT (35.2 32.03)\r## 7 \u0026#39;Adadlay Somalia 9198 0 POINT (44.65 9.77)\r## 8 \u0026#39;Adale Somalia 5492 0 POINT (46.3 2.75)\r## 9 \u0026#39;Afak Iraq 22706 0 POINT (45.26 32.07)\r## 10 \u0026#39;Afif Saudi Arabia 41731 0 POINT (42.93 23.92)\rIn the next step, we filter by the capital cities encoded in the column capital with 1. The advantage of the sf package is the possibility of applying functions of the tidyverse collection to manipulate the attributes. In addition, we add a column with new labels using the str_c() function of the stringr package, which is similar to that of R Base paste().\n# filter the capital cities\rcapitals \u0026lt;- filter(cities, capital == 1)\r# create a new label combining name and country\rcapitals \u0026lt;- mutate(capitals, city_country = str_c(name, \u0026quot; (\u0026quot;, country.etc, \u0026quot;)\u0026quot;))\rcapitals \r## Simple feature collection with 230 features and 5 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -176.13 ymin: -51.7 xmax: 179.2 ymax: 78.21\r## Geodetic CRS: WGS 84\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Amman Jordan 1303197 1 POINT (35.93 31.95)\r## 2 Abu Dhabi United Arab Emirates 619316 1 POINT (54.37 24.48)\r## 3 Abuja Nigeria 178462 1 POINT (7.17 9.18)\r## 4 Accra Ghana 2029143 1 POINT (-0.2 5.56)\r## 5 Adamstown Pitcairn 51 1 POINT (-130.1 -25.05)\r## 6 Addis Abeba Ethiopia 2823167 1 POINT (38.74 9.03)\r## 7 Agana Guam 1041 1 POINT (144.75 13.47)\r## 8 Algiers Algeria 2029936 1 POINT (3.04 36.77)\r## 9 Alofi Niue 627 1 POINT (-169.92 -19.05)\r## 10 Amsterdam Netherlands 744159 1 POINT (4.89 52.37)\r## city_country\r## 1 \u0026#39;Amman (Jordan)\r## 2 Abu Dhabi (United Arab Emirates)\r## 3 Abuja (Nigeria)\r## 4 Accra (Ghana)\r## 5 Adamstown (Pitcairn)\r## 6 Addis Abeba (Ethiopia)\r## 7 Agana (Guam)\r## 8 Algiers (Algeria)\r## 9 Alofi (Niue)\r## 10 Amsterdam (Netherlands)\r\rCalculate distances\rGeographical distance (Euclidean or greater circle) is calculated with the st_distance() function, either between two points, between one point and others or between all points. In the latter case we obtain a symmetric matrix of distances (NxN), taken pairwise between the elements of the capital city set. In the diagonal we find the combinations between the same points giving all null values.\n\r\r\rA\rB\rC\r\rA\r0\r340\r259\r\rB\r340\r0\r337\r\rC\r259\r337\r0\r\r\r\rFor instance, when we want to know the distance from Amsterdam to Abu Dhabi, Washington and Tokyo we pass two spatial objects.\n# calculate distance\rdist_amsterdam \u0026lt;- st_distance(slice(capitals, 10), slice(capitals, c(2, 220, 205)))\rdist_amsterdam # distance in meters\r## Units: [m]\r## [,1] [,2] [,3]\r## [1,] 5167859 6203802 9316790\rThe result is a matrix with a single row or column (depending on the order of the spatial objects) with a class of units. Thus it is possible to convert easily to another unit of measure. If we want to obtain a vector without class units, we only have to apply the function as.vector().\n# change from m to km\rset_units(dist_amsterdam, \u0026quot;km\u0026quot;)\r## Units: [km]\r## [,1] [,2] [,3]\r## [1,] 5167.859 6203.802 9316.79\r# units class to vector\ras.vector(dist_amsterdam)\r## [1] 5167859 6203802 9316790\rIn the second step, we estimate the distance matrix between all the capital cities. It is important to convert the null values to NA to subsequently obtain the correct matrix index.\n# calculate distance\rm_distance \u0026lt;- st_distance(capitals)\r# matrix\rdim(m_distance)\r## [1] 230 230\r# change m to km\rm_distance_km \u0026lt;- set_units(m_distance, km)\r# replace the distance of 0 m with NA\rm_distance_km[m_distance_km == set_units(0, km)] \u0026lt;- NA\rWhen the result is of the units class, it is necessary to use the same class to be able to make logical queries. For example, set_units(1, m) == set_units(1, m) vs. set_units(1, m) == 1.\r To obtain the shortest distance, in addition to its position, we use the apply () function which in turn allows us to apply the function which.min() and min() on each row. It would also be possible to use the function on columns giving the same result. Finally, we add the results as new columns with the mutate() function. The indices in pos allow us to obtain the names of the nearest cities.\n# get the index (position) of the city and the distance\rpos \u0026lt;- apply(m_distance_km, 1, which.min)\rdist \u0026lt;- apply(m_distance_km, 1, min, na.rm = TRUE)\r# add the distance and get the name of the city\rcapitals \u0026lt;- mutate(capitals, nearest_city = city_country[pos], geometry_nearest = geometry[pos],\rdistance_city = dist)\r\rMap of distances to the next capital city\rFinally, we build a map representing the distance in proportional circles. To do this, we use the usual grammar of ggplot() by adding the geometry geom_sf(), first for the world map as background and then for the cities. In aes() we indicate, with the argument size = distance_city, the variable which we want to map proportionally. The theme_void() function removes all style elements. In addition, we define with the function coord_sf() a new projection indicating the proj4 format.\n# world map\rworld \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;)\r# map\rggplot(world) +\rgeom_sf(fill = \u0026quot;black\u0026quot;, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = capitals, aes(size = distance_city),\ralpha = 0.7,\rfill = \u0026quot;#bd0026\u0026quot;,\rshape = 21,\rshow.legend = \u0026#39;point\u0026#39;) +\rcoord_sf(crs = \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\u0026quot;) +\rlabs(size = \u0026quot;Distance (km)\u0026quot;, title = \u0026quot;Distance to the next capital city\u0026quot;) +\rtheme_void()\r\r","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579392000,"objectID":"4c04ce5a3af4b6e95d8f8cd4c9ed9b65","permalink":"https://dominicroye.github.io/en/2020/geographic-distance/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/en/2020/geographic-distance/","section":"post","summary":"The first post of this year 2020, I will dedicate to a question that I was recently asked. The question was how to calculate the shortest distance between different points and how to know which is the closest point. When we work with spatial data in R, currently the easiest thing is to use the ``sf`` package in combination with the ``tidyverse`` collection of packages. We also use the ``units`` package which is very useful for working with units of measurement.","tags":["distance","points","cities"],"title":"Geographic distance","type":"post"},{"authors":["F Tedim","V Leone","M Coughlan","C Bouillon","G Xanthopoulos","D Royé","F.J.M. Correia","C Ferreira"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"bd42afa22ca6d6c53067fc30b08ca937","permalink":"https://dominicroye.github.io/en/publication/chapter_elsevier_2019/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/en/publication/chapter_elsevier_2019/","section":"publication","summary":"Extreme wildfires events (EWEs) represent a minority among all wildfires but are a true challenge for societies as they exceed the current control capacity even in the best prepared regions of the world and they create destruction and a disproportionately number of fatalities. Recent events in Portugal, Chile, Greece, Australia, Canada, and the USA provide evidence that EWEs are an escalating worldwide problem, exceeding all previous records. Despite the challenges put by climate change, the occurrence of EWEs and disasters is not an ecological inevitability. In this chapter the rationale of the definition of EWEs and the integration of potential consequences on people and assets in a novel wildfire classification scheme are proposed and discussed. They are excellent instruments to enhance wildfire risk and crisis communication programs and to define appropriate prevention, mitigation, and response measures which are crucial to build up citizens' safety.","tags":["Control capacity","Disaster Extreme wildfire event (EWE)","Fire intensity","Mitigation","Preparedness","Prevention","Rate of spread","Socioeconomic system (SES)","Wildfire classification"],"title":"Extreme wildfire events: the definition","type":"publication"},{"authors":["D Royé","R Codesido","A Tobías","M Taracido"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a73436a2d1de6ea8746f06dcc35dcba8","permalink":"https://dominicroye.github.io/en/publication/ehf_esp_2020/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/en/publication/ehf_esp_2020/","section":"publication","summary":"In the current context of climate change, heat waves have become a significant problem for human health. This study assesses the effects of heat wave intensity on mortality (natural, respiratory and cardiovascular causes) in four of the largest cities of Spain (Barcelona, Bilbao, Madrid and Seville) during the period between 1990 and 2014. To model the heat wave severity the Excess Heat Factor (EHF) was used. The EHF is a two-component index. The first is the comparison of the three-day average daily mean temperature with the 95th percentile. The second component is a measure of the temperatures reached during the three-day period compared with the recent past (the previous 30 days). The city-specific exposure-response curves showed a non-linear J-shaped relationship between mortality and the EHF. Overall city-specific mortality risk estimates for 1th vs. 99th percentile increases range from the highest mortality risk with 2.73 (95% CI: 2.34-3.18) in Seville to a risk of 1.78 (95% CI: 1.62-1.97) and 1.78 (95% CI: 1.45-2.19) in Barcelona and Bilbao, respectively. When we compare our results with risk estimates for the analyzed Spanish cities in other studies, the heat wave related mortality risks seem to be clearly higher. Furthermore, it has been demonstrated that different heat wave days of the same event do not present the same degree of severity/intensity. Thus, the intensity of a heat wave is an important mortality risk indicator during heat wave days. Due to the low number of studies on the EHF as a heat wave intensity indicator and heat-related mortality and morbidity, further research is required to validate its application in other geographic areas and focus populations.","tags":["Spain","heat wave","Heat Excess Factor","mortality"],"title":"Heat wave intensity and daily mortality in four of the largest cities of Spain","type":"publication"},{"authors":null,"categories":["visualization","R","R:elementary","gis"],"content":"\r\rThe General Directorate for the Cadastre of Spain has spatial information of the all buildings except for the Basque Country and Navarra. This data set is part of the implementation of INSPIRE, the Space Information Infrastructure in Europe. More information can be found here. We will use the links (urls) in ATOM format, which is an RSS type for web feeds, allowing us to obtain the download link for each municipality.\nThis blog post is a reduced version of the case study that you can find in our recent publication - Introduction to GIS with R - published by Dominic Royé and Roberto Serrano-Notivoli (in Spanish).\r Packages\r\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\rfeedeR\rImport feeds RSS or ATOM\r\rtmap\rEasy creation of thematic maps\r\rclassInt\rCreate univariate class intervals\r\rsysfonts\rLoading system fonts and Google Fonts\r\rshowtext\rUsing fonts more easily in R graphs\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;feedeR\u0026quot;)) install.packages(\u0026quot;feedeR\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;tmap\u0026quot;)) install.packages(\u0026quot;tmap\u0026quot;)\rif(!require(\u0026quot;classInt\u0026quot;)) install.packages(\u0026quot;classInt\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;rvest\u0026quot;)) install.packages(\u0026quot;rvest\u0026quot;)\r# load packages\rlibrary(feedeR)\rlibrary(sf) library(fs)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(classInt)\rlibrary(tmap)\rlibrary(rvest)\r\rDownload links\rThe first url will give us access to a list of provinces, territorial headquarters (they do not always coincide with the oficial province), with new RSS links, which include the final download link for each municipality. In this case, we will download the buildings of Valencia. Cadastre data is updated every six months.\nurl \u0026lt;- \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.bu.atom.xml\u0026quot;\r# import RSS feed with provincial links\rprov_enlaces \u0026lt;- feed.extract(url)\rstr(prov_enlaces) # object is a list\r## List of 4\r## $ title : chr \u0026quot;Download service of Buildings. Territorial Office\u0026quot;\r## $ link : chr \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.BU.atom.xml\u0026quot;\r## $ updated: POSIXct[1:1], format: \u0026quot;2021-03-04\u0026quot;\r## $ items : tibble[,5] [52 x 5] (S3: tbl_df/tbl/data.frame)\r## ..$ title : chr [1:52] \u0026quot;Territorial office 02 Albacete\u0026quot; \u0026quot;Territorial office 03 Alicante\u0026quot; \u0026quot;Territorial office 04 Almería\u0026quot; \u0026quot;Territorial office 05 Avila\u0026quot; ...\r## ..$ date : POSIXct[1:52], format: \u0026quot;2021-03-04\u0026quot; \u0026quot;2021-03-04\u0026quot; ...\r## ..$ link : chr [1:52] \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/02/ES.SDGC.bu.atom_02.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/03/ES.SDGC.bu.atom_03.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/04/ES.SDGC.bu.atom_04.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/05/ES.SDGC.bu.atom_05.xml\u0026quot; ...\r## ..$ description: chr [1:52] \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; ...\r## ..$ hash : chr [1:52] \u0026quot;d21ebb7975e59937\u0026quot; \u0026quot;bdba5e149f09e9d8\u0026quot; \u0026quot;03bcbcc7c5be2e17\u0026quot; \u0026quot;8a154202dd778143\u0026quot; ...\r# extract the table with the links\rprov_enlaces_tab \u0026lt;- as_tibble(prov_enlaces$items) %\u0026gt;% mutate(title = repair_encoding(title))\r## Warning: `html_encoding_repair()` was deprecated in rvest 1.0.0.\r## Instead, re-load using the `encoding` argument of `read_html()`\r## Best guess: UTF-8 (100% confident)\rprov_enlaces_tab\r## # A tibble: 52 x 5\r## title date link description hash ## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ d21ebb~\r## 2 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ bdba5e~\r## 3 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ 03bcbc~\r## 4 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ 8a1542~\r## 5 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ 7d3fd3~\r## 6 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ 9c0874~\r## 7 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ ff722b~\r## 8 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ b431aa~\r## 9 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ f79c65~\r## 10 \u0026quot;Territorial~ 2021-03-04 00:00:00 http://www.catastro.mi~ \u0026quot;\\n\\n\\t\\t ~ d702a6~\r## # ... with 42 more rows\rNow, we access and download the data from Valencia. To filter the final download link we use the filter() function of the dplyr package, searching for the name of the territorial headquarter and then the name of the municipality in capital letters with the str_detect() function of stringr. The pull() function allows us to extract a column from a data.frame.\nCurrently the feed.extract() function does not import correctly in the encoding UTF-8 under Windows. For this reason, in some cities a bad codification of special characters may appear “CÃ¡diz”. To solve this problem we apply the repair_encoding() function of the rvest package. Nevertheless, problems can arise that have to be corrected manually.\r # filter the province and get the RSS link\rval_atom \u0026lt;- filter(prov_enlaces_tab, str_detect(title, \u0026quot;Valencia\u0026quot;)) %\u0026gt;% pull(link)\r# import the RSS\rval_enlaces \u0026lt;- feed.extract(val_atom)\r# get the table with the download links\rval_enlaces_tab \u0026lt;- val_enlaces$items\rval_enlaces_tab \u0026lt;- mutate(val_enlaces_tab, title = repair_encoding(title),\rlink = repair_encoding(link)) \r## Best guess: UTF-8 (100% confident)\r## Best guess: UTF-8 (100% confident)\r# filter the table with the name of the city\rval_link \u0026lt;- filter(val_enlaces_tab, str_detect(title, \u0026quot;VALENCIA\u0026quot;)) %\u0026gt;% pull(link)\rval_link\r## [1] \u0026quot;http://www.catastro.minhap.es/INSPIRE/Buildings/46/46900-VALENCIA/A.ES.SDGC.BU.46900.zip\u0026quot;\r\rData download\rThe download is done with the download.file() function that only has two main arguments, the download link and the path with the file name. In this case, we use the tempfile() function, which is useful for creating temporary files, that is, files that only exist in the memory for a certain time.\rThe file we download has the extension *.zip, so we must unzip it with another function (unzip()), which requires the name of the file and the name of the folder, where we want to unzip it. Finally, the URLencode() function encodes an URL address that contains special characters.\n# create a temporary file\rtemp \u0026lt;- tempfile()\r# download the data\rdownload.file(URLencode(val_link), temp)\r# unzip to a folder called buildings\runzip(temp, exdir = \u0026quot;buildings_valencia\u0026quot;) # change the name according to the city\r\rImport the data\rTo import the data we use the dir_ls() function of the fs package, which can obtain the files and folders of a specific path while filtering through a text pattern (regexp : regular expression). We apply the st_read() function of the sf package to the Geography Markup Language (GML) file.\n# get the path with the file\rfile_val \u0026lt;- dir_ls(\u0026quot;buildings_valencia\u0026quot;, regexp = \u0026quot;building.gml\u0026quot;) # change the folder if needed\r# import the data\rbuildings_val \u0026lt;- st_read(file_val)\r## Reading layer `Building\u0026#39; from data source `D:\\OneDriveUSC\\OneDrive - Universidade de Santiago de Compostela\\Documentos\\GitHub\\blogR_update\\content\\post\\en\\2019-11-01-visualize-urban-growth\\buildings_valencia\\A.ES.SDGC.BU.46900.building.gml\u0026#39; using driver `GML\u0026#39;\r## Simple feature collection with 36279 features and 24 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: 720608 ymin: 4351286 xmax: 734981.9 ymax: 4382906\r## Projected CRS: ETRS89 / UTM zone 30N\r\rData preparation\rWe only have to convert the column of the construction year (beginning) into a Date class. The date column contains some dates in --01-01 format, which does not correspond to any recognizable date. Therefore, we replace the first - with 0000.\nbuildings_val \u0026lt;- mutate(buildings_val, beginning = str_replace(beginning, \u0026quot;^-\u0026quot;, \u0026quot;0000\u0026quot;) %\u0026gt;% ymd_hms() %\u0026gt;% as_date()\r)\r## Warning: 5 failed to parse.\r\rDistribution chart\rBefore creating the maps of the construction years, which will reflect urban growth, we will make a graph of distribution of the beginning variable. We can clearly identify periods of urban expansion. We will use the ggplot2 package with the geometry of geom_density() for this purpose. The font_add_google() function of the sysfonts package allows us to download and include font families from Google.\n#font download\rsysfonts::font_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r#use showtext for fonts\rshowtext::showtext_auto() \r# limit the period after 1750\rfilter(buildings_val, beginning \u0026gt;= \u0026quot;1750-01-01\u0026quot;) %\u0026gt;%\rggplot(aes(beginning)) + geom_density(fill = \u0026quot;#2166ac\u0026quot;, alpha = 0.7) +\rscale_x_date(date_breaks = \u0026quot;20 year\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) +\rtheme_minimal(base_family = \u0026quot;Montserrat\u0026quot;) +\rlabs(y = \u0026quot;\u0026quot;,x = \u0026quot;\u0026quot;, title = \u0026quot;Evolution of urban development\u0026quot;)\r\rBuffer of 2,5 km for Valencia\rTo visualize better the distribution of urban growth, we limit the map to a radius of 2.5 km from the city center. Therefore, we use the geocode_OSM() function of the tmaptools package to obtain the coordinates of Valencia in class sf. Then we project the points to the system we use for the buildings (EPSG: 25830). The st_crs() function returns the coordinate system of a spatial object sf. Finally, we create with the function st_buffer() a buffer with 2500 m and the intersection with our building data. It is also possible to create a buffer in the form of a rectangle indicating the style with the argument endCapStyle =\" SQUARE \".\n# get the coordinates of Valencia\rciudad_point \u0026lt;- tmaptools::geocode_OSM(\u0026quot;Valencia\u0026quot;, as.sf = TRUE)\r# project the points\rciudad_point \u0026lt;- st_transform(ciudad_point, st_crs(buildings_val))\r# create the buffer\rpoint_bf \u0026lt;- st_buffer(ciudad_point, 2500) # radius of 2500 m\r# get the intersection between the buffer and the building\rbuildings_val25 \u0026lt;- st_intersection(buildings_val, point_bf)\r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r\rPrepare data for mapping\rWe categorize the year into 15 groups using quartiles. It is also possible to modify the number of classes or the applied method (eg jenks, fisher, etc), you can find more details in the help ?classIntervals.\n# find 15 classes\rbr \u0026lt;- classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;)\r## Warning in classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;): var\r## has missing values, omitted in finding classes\r# create labels\rlab \u0026lt;- names(print(br, under = \u0026quot;\u0026lt;\u0026quot;, over = \u0026quot;\u0026gt;\u0026quot;, cutlabels = FALSE))\r## style: quantile\r## \u0026lt; 1890 1890 - 1912 1912 - 1925 1925 - 1930 1930 - 1940 1940 - 1950 ## 939 1361 957 595 1708 1055 ## 1950 - 1958 1958 - 1962 1962 - 1966 1966 - 1970 1970 - 1973 1973 - 1978 ## 1454 1029 1224 1160 1154 1191 ## 1978 - 1988 1988 - 1999 \u0026gt; 1999 ## 1148 1111 1207\r# categorize the year\rbuildings_val25 \u0026lt;- mutate(buildings_val25, yr_cl = cut(year(beginning), br$brks, labels = lab, include.lowest = TRUE))\r\rMap of Valencia\rFor the mapping, we will use the tmap package. It is an interesting alternative to ggplot2. It is a package of functions specialized in creating thematic maps. The philosophy of the package follows the same as in ggplot2, creating multiple layers with different functions, which always start with tm_ *and combine with +. Building a map with tmap always starts with tm_shape(), where the data, we want to draw, is defined. Then we add the corresponding geometry to the data type (tm_polygon(), tm_border(), tm_dots() or even tm_raster()). The tm_layout() function help us to configure the map style.\nWhen we need more colors than the maximum allowed by RColorBrewer, we can pass the colors to the colorRampPalette() function. This function interpolates a set of given colors.\n# colours\rcol_spec \u0026lt;- RColorBrewer::brewer.pal(11, \u0026quot;Spectral\u0026quot;)\r# colour ramp function\rcol_spec_fun \u0026lt;- colorRampPalette(col_spec)\r# create the final map\rtm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15), # adapt to the number of classes\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;) +\rtm_layout(bg.color = \u0026quot;black\u0026quot;,\router.bg.color = \u0026quot;black\u0026quot;,\rlegend.outside = TRUE,\rlegend.text.color = \u0026quot;white\u0026quot;,\rlegend.text.fontfamily = \u0026quot;Montserrat\u0026quot;, panel.label.fontfamily = \u0026quot;Montserrat\u0026quot;,\rpanel.label.color = \u0026quot;white\u0026quot;,\rpanel.label.bg.color = \u0026quot;black\u0026quot;,\rpanel.label.size = 5,\rpanel.label.fontface = \u0026quot;bold\u0026quot;)\rWe can export our map using the function tmap_save(\"name.png\", dpi = 300). I recommend using the dpi = 300 argument for a good image quality.\nAn alternative way to the tmap package is ggplot2.\n# create the final map\rggplot(buildings_val25) +\rgeom_sf(aes(fill = yr_cl), colour = \u0026quot;transparent\u0026quot;) +\rscale_fill_manual(values = col_spec_fun(15)) + # adapt to the number of classes\rlabs(title = \u0026quot;VALÈNCIA\u0026quot;, fill = \u0026quot;\u0026quot;) +\rguides(fill = guide_legend(keywidth = .7, keyheight = 2.7)) +\rtheme_void(base_family = \u0026quot;Montserrat\u0026quot;) +\rtheme(panel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rlegend.justification = .5,\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;, size = 12),\rplot.title = element_text(colour = \u0026quot;white\u0026quot;, hjust = .5, size = 60,\rmargin = margin(t = 30)),\rplot.caption = element_text(colour = \u0026quot;white\u0026quot;,\rmargin = margin(b = 20), hjust = .5, size = 16),\rplot.margin = margin(r = 40, l = 40))\rTo export the result of ggplot we can use the function ggsave(\"name.png\").\n\rDynamic map with leaflet\rA very interesting advantage is the tmap_leaflet() function of the tmap package to easily pass a map created in the same frame to leaflet.\n# tmap object\rm \u0026lt;- tm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15), # adapt to the number of classes\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;)\r# dynamic map\rtmap_leaflet(m)\r\r\r","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"116837f1a59e17f1770b86a1a75ffaef","permalink":"https://dominicroye.github.io/en/2019/visualize-urban-growth/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/en/2019/visualize-urban-growth/","section":"post","summary":"The General Directorate for the Cadastre of Spain has spatial information of the all buildings except for the Basque Country and Navarra. This data set is part of the implementation of [INSPIRE](https://inspire.ec.europa.eu/), the Space Information Infrastructure in Europe. More information can be found [here](http://www.catastro.meh.es/webinspire/index.html). We will use the links (*urls*) in *ATOM* format, which is an RSS type for web feeds, allowing us to obtain the download links for each municipality.","tags":["urban growth","city","urban geography"],"title":"Visualize urban growth","type":"post"},{"authors":["D Royé","R Serrano-Notivoli"],"categories":null,"content":"","date":1570406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570406400,"objectID":"f7cd7f11c5130c310097e88c0adb1b17","permalink":"https://dominicroye.github.io/en/publication/manual_rgis_2019/","publishdate":"2019-10-07T00:00:00Z","relpermalink":"/en/publication/manual_rgis_2019/","section":"publication","summary":"R tiene, como lenguaje de programación enfocado al análisis estadístico, todos los ingredientes para ser usado como herramienta de análisis espacial y representación cartográﬁca: es gratuito, permite personalizar, replicar y compartir los análisis de cualquier nivel de diﬁcultad y no tiene ninguna limitación en cuanto a cantidad de información a procesar o tipos de formato diferentes para gestionar. Esto le sitúa en una situación de ventaja que mejora día a día, gracias a su amplia comunidad de usuarios, respecto a un SIG (Sistema de Información Geográﬁca) convencional. Este manual explica, sin necesidad de conocimientos previos, cómo desarrollar con R todos los análisis disponibles en un SIG, con ejemplos sencillos y multitud de casos prácticos. Además, se muestran las enormes posibilidades de representación cartográﬁca, que van mucho más allá de la simple creación de mapas. R permite, desde exportar a cualquier formato de archivo, hasta crear mapas dinámicos para supublicación en Internet.","tags":["R","manual","visualisation","GIS"],"title":"Introducción a los SIG con R","type":"publication"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","A Benhamrouche"],"categories":null,"content":"","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"45f7b9f717406a99013ba41aedd39837","permalink":"https://dominicroye.github.io/en/publication/ijc_teleconection_medit_2019/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/en/publication/ijc_teleconection_medit_2019/","section":"publication","summary":"This study has addressed the spatiotemporal distribution of the daily rainfall concentration and its relation to the teleconnection patterns across the Mediterranean (MR). Daily Concentration Index (CI) and the ordered n index () are used at annual time scale to reveal the statistical structure of precipitation across the MR based on 233 daily rainfall series for the period 1975–2015. Eight teleconnection patterns ,North Atlantic Oscillation (NAO), Mediterranean Oscillation (MO), Western Mediterranean Oscillation (WeMO), Upper Level Mediterranean Oscillation index (ULMO), East Atlantic (EA) pattern, East Atlantic/West Russia (EATL/WRUS) pattern, Scandinavia (SCAND) pattern and Southern Oscillation (SO) at annual time scale are selected. The spatiotemporal patterns in precipitation concentration indices, annual precipitation and their teleconnections with previous large-scale circulations are investigated. Results show a strong connection between the CI and the (r = 0.70, p","tags":["Mediterranean","n-index","concentration index","teleconnection patterns","daily precipitation"],"title":"Spatiotemporal variability of daily precipitation concentration and its relationship to teleconnection patterns over the Mediterranean during 1975-2015","type":"publication"},{"authors":["D Royé","MT Zarrabeitia","P Fdez-Arroyabe","A Álvarez-Gutiérrez","A Santurtún"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"9386a67b5c062730063f0a60839e8ab3","permalink":"https://dominicroye.github.io/en/publication/iam_cantabria_2018/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/en/publication/iam_cantabria_2018/","section":"publication","summary":"Introduction and objectives. The role of the environment on cardiovascular health is becoming more prominent in the context of global change. The aim of this study was to analyze the relationship between apparent temperature (AT) and air pollutants and acute myocardial infarction (AMI) and to study the temporal pattern of this disease and its associated mortality. Methods. We performed a time-series study of admissions for AMI in Cantabria between 2001 and 2015. The association between environmental variables (including a biometeorological index, apparent AT) and AMI was analyzed using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of these variables on AMI, a lag non-linear model was fitted in a generalized additive model. Results. The incidence rate and the mortality followed a downward trend during the study period (CC=–0.714; P=.0002). An annual pattern was found in hospital admissions (P=.005), with the highest values being registered in winter; a weekly trend was also identified, reaching a minimum during the weekends (P=.000005). There was an inverse association between AT and the number of hospital admissions due to AMI and a direct association with particulate matter with a diameter smaller than 10 μm. Conclusions. Hospital admissions for AMI followed a downward trend between 2007 and 2015. Mortality associated with admissions due to this diagnosis has decreased. Predictive factors for this disease were AT and particulate matter with a diameter smaller than 10 μm.","tags":["Acute myocardial infarction","Apparent temperature","Air pollutants","Particulate matter"],"title":"Role of Apparent Temperature and Air Pollutants in Hospital Admissions for Acute Myocardial Infarction in the North of Spain","type":"publication"},{"authors":null,"categories":["visualization","R","R:intermediate"],"content":"\r\rNormally when we visualize monthly precipitation anomalies, we simply use a bar graph indicating negative and positive values with red and blue. However, it does not explain the general context of these anomalies. For example, what was the highest or lowest anomaly in each month? In principle, we could use a boxplot to visualize the distribution of the anomalies, but in this particular case they would not fit aesthetically, so we should look for an alternative. Here I present a very useful graphic form.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rreadr\rImport data\r\rggthemes\rThemes for ggplot2\r\rlubridate\rEasy manipulation of dates and times\r\rcowplot\rEasy creation of multiple graphics with ggplot2\r\r\r\r#we install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;cowplot\u0026quot;)) install.packages(\u0026quot;cowplot\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#packages\rlibrary(tidyverse) #include readr\rlibrary(ggthemes)\rlibrary(cowplot)\rlibrary(lubridate)\r\rPreparing the data\rFirst we import the daily precipitation of the selected weather station (download). We will use data from Santiago de Compostela (Spain) accessible through ECA\u0026amp;D.\nStep 1: import the data\rWe not only import the data in csv format, but we also make the first changes. We skip the first 21 rows that contain information about the weather station. In addition, we convert the date to the date class and replace missing values (-9999) with NA. The precipitation is given in 0.1 mm, therefore, we must divide the values by 10. Then we select the columns DATE and RR, and rename them.\ndata \u0026lt;- read_csv(\u0026quot;RR_STAID001394.txt\u0026quot;, skip = 21) %\u0026gt;%\rmutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10)) %\u0026gt;%\rselect(DATE:RR) %\u0026gt;% rename(date = DATE, pr = RR)\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\rdata\r## # A tibble: 27,606 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1943-11-01 0.6\r## 2 1943-11-02 0 ## 3 1943-11-03 0 ## 4 1943-11-04 0 ## 5 1943-11-05 0 ## 6 1943-11-06 0 ## 7 1943-11-07 0 ## 8 1943-11-08 0 ## 9 1943-11-09 0 ## 10 1943-11-10 0 ## # ... with 27,596 more rows\r\rStep 2: creating monthly values\rIn the second step we calculate the monthly amounts of precipitation. To do this, a) we limit the period to the years after 1950, b) we add the month with its labels and the year as variables.\ndata \u0026lt;- mutate(data, mo = month(date, label = TRUE), yr = year(date)) %\u0026gt;%\rfilter(date \u0026gt;= \u0026quot;1950-01-01\u0026quot;) %\u0026gt;%\rgroup_by(yr, mo) %\u0026gt;% summarise(prs = sum(pr, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;yr\u0026#39;. You can override using the `.groups` argument.\rdata\r## # A tibble: 833 x 3\r## # Groups: yr [70]\r## yr mo prs\r## \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Jan 55.6\r## 2 1950 Feb 349. ## 3 1950 Mar 85.8\r## 4 1950 Apr 33.4\r## 5 1950 May 272. ## 6 1950 Jun 111. ## 7 1950 Jul 35.4\r## 8 1950 Aug 76.4\r## 9 1950 Sep 85 ## 10 1950 Oct 53 ## # ... with 823 more rows\r\rStep 3: estimating anomalies\rNow we must estimate the normals of each month and join this table to our main data in order to calculate the monthly anomaly. We express the anomalies in percentage and subtract 100 to set the average to 0. In addition, we create a variable which indicates if the anomaly is negative or positive, and another with the date.\npr_ref \u0026lt;- filter(data, yr \u0026gt; 1981, yr \u0026lt;= 2010) %\u0026gt;%\rgroup_by(mo) %\u0026gt;%\rsummarise(pr_ref = mean(prs))\rdata \u0026lt;- left_join(data, pr_ref, by = \u0026quot;mo\u0026quot;)\rdata \u0026lt;- mutate(data, anom = (prs*100/pr_ref)-100, date = str_c(yr, as.numeric(mo), 1, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd(),\rsign= ifelse(anom \u0026gt; 0, \u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;) %\u0026gt;% factor(c(\u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;)))\rWe can do a first test graph of anomalies (the classic one), for that we filter the year 2018. In this case we use a bar graph, remember that by default the function geom_bar() applies the counting of the variable. However, in this case we know y, hence we indicate with the argument stat = \"identity\" that it should use the given value in aes().\nfilter(data, yr == 2018) %\u0026gt;%\rggplot(aes(date, anom, fill = sign)) + geom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(-100, 100, 20)) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rlabs(y = \u0026quot;Precipitation anomaly (%)\u0026quot;, x = \u0026quot;\u0026quot;) +\rtheme_hc()\r\rStep 4: calculating the statistical metrics\rIn this last step we estimate the maximum, minimum value, the 25%/75% quantiles and the interquartile range per month of the entire time series.\ndata_norm \u0026lt;- group_by(data, mo) %\u0026gt;%\rsummarise(mx = max(anom),\rmin = min(anom),\rq25 = quantile(anom, .25),\rq75 = quantile(anom, .75),\riqr = q75-q25)\rdata_norm\r## # A tibble: 12 x 6\r## mo mx min q25 q75 iqr\r## \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Jan 193. -89.6 -43.6 56.3 99.9\r## 2 Feb 320. -96.5 -51.2 77.7 129. ## 3 Mar 381. -100 -40.6 88.2 129. ## 4 Apr 198. -93.6 -51.2 17.1 68.3\r## 5 May 141. -90.1 -45.2 17.0 62.2\r## 6 Jun 419. -99.3 -58.2 50.0 108. ## 7 Jul 311. -98.2 -77.3 27.1 104. ## 8 Aug 264. -100 -68.2 39.8 108. ## 9 Sep 241. -99.2 -64.9 48.6 113. ## 10 Oct 220. -99.0 -54.5 4.69 59.2\r## 11 Nov 137. -98.8 -44.0 39.7 83.7\r## 12 Dec 245. -91.8 -49.8 36.0 85.8\r\r\rCreating the graph\rTo create the anomaly graph with legend it is necessary to separate the main graph from the legends.\nPart 1\rIn this first part we are adding layer by layer the different elements: 1) the range of anomalies maximum-minimum 2) the interquartile range and 3) the anomalies of the year 2018.\n#range of anomalies maximum-minimum\rg1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#adding interquartile range\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#adding anomalies of the year 2018 g1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr == 2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE)\rg1.3\rFinally we change some last style settings.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Precipitation anomaly (%)\u0026quot;,\rbreaks = seq(-100, 500, 25),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Precipitation anomaly in Santiago de Compostela 2018\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Data: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\r\rPart 2\rWe still need a legend. First we create it for the normals.\n#legend data\rlegend \u0026lt;- filter(data_norm, mo == \u0026quot;Jan\u0026quot;)\rlegend_lab \u0026lt;- gather(legend, stat, y, mx:q75) %\u0026gt;%\rmutate(stat = factor(stat, stat, c(\u0026quot;maximum\u0026quot;,\r\u0026quot;minimum\u0026quot;,\r\u0026quot;Quantile 25%\u0026quot;,\r\u0026quot;Quantile 75%\u0026quot;)) %\u0026gt;%\ras.character())\r## Warning: attributes are not identical across measure variables;\r## they will be dropped\r#legend graph\rg2 \u0026lt;- legend %\u0026gt;% ggplot()+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;, width = 0.2) +\rgeom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;, width = 0.2) +\rgeom_text(data = legend_lab, aes(x = mo, y = y+c(12,-8,-10,12), label = stat), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.18, y = 40, label = \u0026quot;Period 1950-2018\u0026quot;, angle = 90, size = 3) +\rtheme_void() + theme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg2\rSecond, we create another legend for the current anomalies.\n#legend data\rlegend2 \u0026lt;- filter(data, yr == 1950, mo %in% c(\u0026quot;Jan\u0026quot;,\u0026quot;Feb\u0026quot;)) %\u0026gt;% ungroup() %\u0026gt;% select(mo, anom, sign)\rlegend2[2,1] \u0026lt;- \u0026quot;Jan\u0026quot;\rlegend_lab2 \u0026lt;- data.frame(mo = rep(\u0026quot;Jan\u0026quot;, 3), anom= c(110, 3, -70), label = c(\u0026quot;Positive anomaly\u0026quot;, \u0026quot;Average\u0026quot;, \u0026quot;Negative anomaly\u0026quot;))\r#legend graph\rg3 \u0026lt;- ggplot() + geom_bar(data = legend2,\raes(x = mo, y = anom, fill = sign),\ralpha = .6, colour = \u0026quot;NA\u0026quot;, stat = \u0026quot;identity\u0026quot;, show.legend = FALSE, width = 0.2) +\rgeom_segment(aes(x = .85, y = 0, xend = 1.15, yend = 0), linetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = legend_lab2, aes(x = mo, y = anom+c(10,5,-13), label = label), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.25, y = 20, label =\u0026quot;Reference 1971-2010\u0026quot;, angle = 90, size = 3) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rtheme_void() +\rtheme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg3\r\rPart 3\rFinally, we only have to join the graph and the legends with the help of the cowplot package. The main function of cowplot is plot_grid() which is used for combining different graphs. However, in this case it is necessary to use more flexible functions to create less common formats. The ggdraw() function configures the basic layer of the graph, and the functions that are intended to operate on this layer start with draw_*.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .3, width = 1, height = 0.6) +\rdraw_plot(g2, x = 0, y = .15, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .15, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomaly2016_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r\rMultiple facets\rIn this section we will make the same graph as in the previous one, but for several years.\nPart 1\rFirst we need to filter again by set of years, in this case from 2016 to 2018, using the operator %in%, we also add the function facet_grid() to ggplot, which allows us to plot the graph according to a variable. The formula used for the facet function is similar to the use in models: variable_by_row ~ variable_by_column. When we do not have a variable in the column, we should use the ..\n#range of anomalies maximum-minimum\rg1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#adding the interquartile range\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#adding the anomalies of the year 2016-2018\rg1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr %in% 2016:2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE) +\rfacet_grid(yr ~ .)\rg1.3\rFinally we change some last style settings.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Anomalía de precipitación (%)\u0026quot;,\rbreaks = seq(-100, 500, 50),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Anomalía de precipitación en Santiago de Compostela\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Datos: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\rWe use the same legend created for the previous graph.\n\r\rPart 2\rFinally, we join the graph and the legends with the help of the cowplot package. The only thing we must adjust here are the arguments in the draw_plot() function to correctly place the different parts.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .18, width = 1, height = 0.8) +\rdraw_plot(g2, x = 0, y = .08, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .08, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomaly20162018_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562457600,"objectID":"a85307e41f42183629b3cecc43762b6e","permalink":"https://dominicroye.github.io/en/2019/visualize-monthly-precipitation-anomalies/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/en/2019/visualize-monthly-precipitation-anomalies/","section":"post","summary":"Normally when we visualize monthly precipitation anomalies, we simply use a bar graph indicating negative and positive values with red and blue. However, it does not explain the general context of these anomalies. For example, what was the highest or lowest anomaly in each month? In principle, we could use a *boxplot* to visualize the distribution of the anomalies, but in this particular case they would not fit aesthetically, so we should look for an alternative. Here I present a very useful graphic form.","tags":["anomalies","precipitation","climate","boxplot"],"title":"Visualize monthly precipitation anomalies","type":"post"},{"authors":["A Martí","J Taboada","D Royé","X Fonseca"],"categories":null,"content":"","date":1560384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560384000,"objectID":"4cd534699f62bcb4b4d46633d326cf5d","permalink":"https://dominicroye.github.io/en/publication/os_tempos_2019/","publishdate":"2019-06-13T00:00:00Z","relpermalink":"/en/publication/os_tempos_2019/","section":"publication","summary":"Que récords climáticos se alcanzaron en Galicia? Cales son os lugares máis calorosos? E os máis fríos? Onde chove máis? Onde se rexistran máis días de precipitación? Que zonas gozan dun maior número de horas de sol? Cales teñen maior nebulosidade? Que lugares son os máis ventosos? Como está a afectar o cambio climático a Galicia? Neste libro atoparás as respostas a estas e a outras preguntas relacionadas co clima de Galicia e os diversos tipos de tempo que o caracterizan. Nas súas páxinas explícase como se producen os fenómenos meteorolóxicos máis habituais no noso territorio: as inversións térmicas, as néboas costeiras e orográficas, as illas de calor urbanas, os tipos de precipitación, o efecto foehn, as brisas mariñas, o arco da vella etc. A través de exemplos concretos, analízanse tamén os riscos climáticos que afectan regularmente a Galicia, como vagas de calor, temporais de neve, cicloxéneses explosivas e temporais de choiva e vento, tormentas, secas, tornados... Tamén poderás coñecer como está a cambiar o clima da nosa comunidade debido ao quecemento global e cales son os escenarios de futuro.","tags":["tempo","Galicia","clima","divulgación","gallego"],"title":"Os tempos e o clima de Galicia","type":"publication"},{"authors":["A Vélez","J Martin-Vide","D Royé","O Santaella"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"7bc2a7ffe0ae57e6714ccd7d00de9cc6","permalink":"https://dominicroye.github.io/en/publication/ci_pr_2018/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/en/publication/ci_pr_2018/","section":"publication","summary":"The present study analyzes spatial patterns of precipitation Concentration Index (CI) in Puerto Rico considering the daily precipitation data of precipitation-gauging stations during 1971-2010. The South and East interior parts of Puerto Rico are characterized by higher CI and the West and North-West parts show lower CI. The annual CI and the rainy season CI show a gradient from South-East to North-West and the dry season CI shows a gradient from South to North. Another difference between the rainy season CI and dry season CI is that the former shows the lowest values of CI while the latter shows the highest values of CI. The different types of seasonal precipitation seem to play a major role on the spatial CI distribution. However, the local relief plays a major role in the spatial patterns due to the effect of the air circulation by the mountains. These findings can contribute to basin-scale water resource management (ooding, soil erosion, etc.) and conservation of the ecological environment.","tags":["Concentration Index","Puerto Rico","precipitation","spatial–temporal patterns"],"title":"Spatial Analysis of Daily Precipitation Concentration in Puerto Rico","type":"publication"},{"authors":null,"categories":["statistics","R","R:advanced"],"content":"\r\rWhen we try to estimate the correlation coefficient between multiple variables, the task is more complicated in order to obtain a simple and tidy result. A simple solution is to use the tidy() function from the {broom} package. In this post we are going to estimate the correlation coefficients between the annual precipitation of several Spanish cities and climate teleconnections indices: download. The data of the teleconnections are preprocessed, but can be downloaded directly from crudata.uea.ac.uk. The daily precipitation data comes from ECA\u0026amp;D.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rbroom\rConvert results of statistical functions (lm, t.test, cor.test, etc.) into tidy tables\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\r\r\r#install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;broom\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(broom)\rlibrary(fs)\rlibrary(lubridate)\r\rImport data\rFirst we have to import the daily precipitation of the selected weather stations.\nCreate a vector with all precipitation files using the function dir_ls() of the {fs} package.\rImport the data using the map_df() function of the {purrr} package that applies another function to a vector or list, and joins them together in a single data.frame.\rSelect the columns that interest us, b) Convert the date string into a date object using the ymd() function of the {lubridate} package, c) Create a new column yr with the years, d) Divide the precipitation values by 10 and reclassify absent values -9999 by NA, e) Finally, reclassify the ID of each weather station creating a factor with new labels.\r\r\rMore details about the use of the dir_ls() and map_df() functions can be found in this previous post.\n#precipitation files\rfiles \u0026lt;- dir_ls(regexp = \u0026quot;txt\u0026quot;)\rfiles\r## RR_STAID001393.txt RR_STAID001394.txt RR_STAID002969.txt RR_STAID003946.txt ## RR_STAID003969.txt\r#import all files and join them together\rpr \u0026lt;- files %\u0026gt;% map_df(read_csv, skip = 20)\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## ## ## -- Column specification --------------------------------------------------------\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## ## ## -- Column specification --------------------------------------------------------\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## ## ## -- Column specification --------------------------------------------------------\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## ## ## -- Column specification --------------------------------------------------------\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\rpr\r## # A tibble: 133,343 x 5\r## STAID SOUID DATE RR Q_RR\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1393 20611 19470301 0 0\r## 2 1393 20611 19470302 5 0\r## 3 1393 20611 19470303 0 0\r## 4 1393 20611 19470304 33 0\r## 5 1393 20611 19470305 15 0\r## 6 1393 20611 19470306 0 0\r## 7 1393 20611 19470307 85 0\r## 8 1393 20611 19470308 3 0\r## 9 1393 20611 19470309 0 0\r## 10 1393 20611 19470310 0 0\r## # ... with 133,333 more rows\r#create levels for the factor id \u0026lt;- unique(pr$STAID)\r#the corresponding labels\rlab \u0026lt;- c(\u0026quot;Bilbao\u0026quot;, \u0026quot;Santiago\u0026quot;, \u0026quot;Barcelona\u0026quot;, \u0026quot;Madrid\u0026quot;, \u0026quot;Valencia\u0026quot;)\r#first changes\rpr \u0026lt;- select(pr, STAID, DATE, RR) %\u0026gt;% mutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10), STAID = factor(STAID, id, lab), yr = year(DATE)) pr\r## # A tibble: 133,343 x 4\r## STAID DATE RR yr\r## \u0026lt;fct\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1947-03-01 0 1947\r## 2 Bilbao 1947-03-02 0.5 1947\r## 3 Bilbao 1947-03-03 0 1947\r## 4 Bilbao 1947-03-04 3.3 1947\r## 5 Bilbao 1947-03-05 1.5 1947\r## 6 Bilbao 1947-03-06 0 1947\r## 7 Bilbao 1947-03-07 8.5 1947\r## 8 Bilbao 1947-03-08 0.3 1947\r## 9 Bilbao 1947-03-09 0 1947\r## 10 Bilbao 1947-03-10 0 1947\r## # ... with 133,333 more rows\rWe still need to filter and calculate the annual amount of precipitation. Actually, it is not correct to sum up precipitation without taking into account that there are missing values, but it should be enough for this practice. Then, we change the table format with the spread() function, passing from a long to a wide table, that is, we want to obtain one column per weather station.\npr_yr \u0026lt;- filter(pr, DATE \u0026gt;= \u0026quot;1950-01-01\u0026quot;, DATE \u0026lt; \u0026quot;2018-01-01\u0026quot;) %\u0026gt;%\rgroup_by(STAID, yr)%\u0026gt;%\rsummarise(pr = sum(RR, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;STAID\u0026#39;. You can override using the `.groups` argument.\rpr_yr\r## # A tibble: 324 x 3\r## # Groups: STAID [5]\r## STAID yr pr\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1950 1342 ## 2 Bilbao 1951 1306.\r## 3 Bilbao 1952 1355.\r## 4 Bilbao 1953 1372.\r## 5 Bilbao 1954 1428.\r## 6 Bilbao 1955 1062.\r## 7 Bilbao 1956 1254.\r## 8 Bilbao 1957 968.\r## 9 Bilbao 1958 1272.\r## 10 Bilbao 1959 1450.\r## # ... with 314 more rows\rpr_yr \u0026lt;- spread(pr_yr, STAID, pr)\rpr_yr\r## # A tibble: 68 x 6\r## yr Bilbao Santiago Barcelona Madrid Valencia\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA\r## 2 1951 1306. 2344. 1072. 798. NA\r## 3 1952 1355. 1973. 415. 524. NA\r## 4 1953 1372. 973. 683. 365. NA\r## 5 1954 1428. 1348. 581. 246. NA\r## 6 1955 1062. 1769. 530. 473. NA\r## 7 1956 1254. 1533. 695. 480. NA\r## 8 1957 968. 1599. 635. 424. NA\r## 9 1958 1272. 2658. 479. 482. NA\r## 10 1959 1450. 2847. 1006 665. NA\r## # ... with 58 more rows\rThe next step is to import the climate teleconnection indices.\n#teleconnections\rtelecon \u0026lt;- read_csv(\u0026quot;teleconnections_indices.csv\u0026quot;)\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## yr = col_double(),\r## NAO = col_double(),\r## WeMO = col_double(),\r## EA = col_double(),\r## `POL-EUAS` = col_double(),\r## `EATL/WRUS` = col_double(),\r## MO = col_double(),\r## SCAND = col_double(),\r## AO = col_double()\r## )\rtelecon\r## # A tibble: 68 x 9\r## yr NAO WeMO EA `POL-EUAS` `EATL/WRUS` MO SCAND AO\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 0.49 0.555 -0.332 0.0217 -0.0567 0.335 0.301 -0.199 ## 2 1951 -0.07 0.379 -0.372 0.402 -0.419 0.149 -0.00667 -0.365 ## 3 1952 -0.37 0.693 -0.688 -0.0117 -0.711 0.282 0.0642 -0.675 ## 4 1953 0.4 -0.213 -0.727 -0.0567 -0.0508 0.216 0.0233 -0.0164 ## 5 1954 0.51 1.20 -0.912 0.142 -0.318 0.386 0.458 -0.000583\r## 6 1955 -0.64 0.138 -0.824 -0.0267 0.154 0.134 0.0392 -0.362 ## 7 1956 0.17 0.617 -1.29 -0.197 0.0617 0.256 0.302 -0.163 ## 8 1957 -0.02 0.321 -0.952 -0.638 -0.167 0.322 -0.134 -0.342 ## 9 1958 0.12 0.941 -0.243 0.138 0.661 0.296 0.279 -0.868 ## 10 1959 0.49 -0.055 -0.23 -0.0142 0.631 0.316 0.725 -0.0762 ## # ... with 58 more rows\rFinally we need to join both tables by year.\ndata_all \u0026lt;- left_join(pr_yr, telecon, by = \u0026quot;yr\u0026quot;)\rdata_all\r## # A tibble: 68 x 14\r## yr Bilbao Santiago Barcelona Madrid Valencia NAO WeMO EA\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA 0.49 0.555 -0.332\r## 2 1951 1306. 2344. 1072. 798. NA -0.07 0.379 -0.372\r## 3 1952 1355. 1973. 415. 524. NA -0.37 0.693 -0.688\r## 4 1953 1372. 973. 683. 365. NA 0.4 -0.213 -0.727\r## 5 1954 1428. 1348. 581. 246. NA 0.51 1.20 -0.912\r## 6 1955 1062. 1769. 530. 473. NA -0.64 0.138 -0.824\r## 7 1956 1254. 1533. 695. 480. NA 0.17 0.617 -1.29 ## 8 1957 968. 1599. 635. 424. NA -0.02 0.321 -0.952\r## 9 1958 1272. 2658. 479. 482. NA 0.12 0.941 -0.243\r## 10 1959 1450. 2847. 1006 665. NA 0.49 -0.055 -0.23 ## # ... with 58 more rows, and 5 more variables: POL-EUAS \u0026lt;dbl\u0026gt;, EATL/WRUS \u0026lt;dbl\u0026gt;,\r## # MO \u0026lt;dbl\u0026gt;, SCAND \u0026lt;dbl\u0026gt;, AO \u0026lt;dbl\u0026gt;\r\rCorrelation test\rA correlation test between paired samples can be done with the cor.test() function of R Base. In this case between the annual precipitation of Bilbao and the NAO index.\ncor_nao_bil \u0026lt;- cor.test(data_all$Bilbao, data_all$NAO,\rmethod = \u0026quot;spearman\u0026quot;)\r## Warning in cor.test.default(data_all$Bilbao, data_all$NAO, method = \u0026quot;spearman\u0026quot;):\r## Cannot compute exact p-value with ties\rcor_nao_bil\r## ## Spearman\u0026#39;s rank correlation rho\r## ## data: data_all$Bilbao and data_all$NAO\r## S = 44372, p-value = 0.2126\r## alternative hypothesis: true rho is not equal to 0\r## sample estimates:\r## rho ## 0.1531149\rstr(cor_nao_bil)\r## List of 8\r## $ statistic : Named num 44372\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## $ parameter : NULL\r## $ p.value : num 0.213\r## $ estimate : Named num 0.153\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ null.value : Named num 0\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ alternative: chr \u0026quot;two.sided\u0026quot;\r## $ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## $ data.name : chr \u0026quot;data_all$Bilbao and data_all$NAO\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;htest\u0026quot;\rWe see that the result is in an unmanageable and untidy format. It is a console summary of the correlation with all the statistical parameters necessary to get a conclusion about the relationship. The orginal structure is a list of vectors. However, the tidy() function of the {broom} package allows us to convert the result into a table format.\ntidy(cor_nao_bil)\r## # A tibble: 1 x 5\r## estimate statistic p.value method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.153 44372. 0.213 Spearman\u0026#39;s rank correlation rho two.sided\r\rApply the correlation test to multiple variables\rThe objective is to apply the correlation test to all weather stations and climate teleconnection indices.\nFirst, we must pass the table to the long format, that is, create a column/variable for the city and for the value of the corresponding precipitation. Then we repeat the same for the teleconnections indices.\ndata \u0026lt;- gather(data_all, city, pr, Bilbao:Valencia)%\u0026gt;%\rgather(telecon, index, NAO:AO)\rdata\r## # A tibble: 2,720 x 5\r## yr city pr telecon index\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Bilbao 1342 NAO 0.49\r## 2 1951 Bilbao 1306. NAO -0.07\r## 3 1952 Bilbao 1355. NAO -0.37\r## 4 1953 Bilbao 1372. NAO 0.4 ## 5 1954 Bilbao 1428. NAO 0.51\r## 6 1955 Bilbao 1062. NAO -0.64\r## 7 1956 Bilbao 1254. NAO 0.17\r## 8 1957 Bilbao 968. NAO -0.02\r## 9 1958 Bilbao 1272. NAO 0.12\r## 10 1959 Bilbao 1450. NAO 0.49\r## # ... with 2,710 more rows\rTo apply the test to all cities, we need the corresponding groupings. Therefore, we use the group_by() function for indicating the two groups: city and telecon. In addition, we apply the nest() function of the {tidyr} package ({tidyverse} collection) with the aim of creating lists of tables nested per row. In other words, in each row of each city and teleconnection index we will have a new table that contains the year, the precipitation value and the value of each teleconection, correspondingly.\ndata_nest \u0026lt;- group_by(data, city, telecon) %\u0026gt;% nest()\rdata_nest\r## # A tibble: 40 x 3\r## # Groups: city, telecon [40]\r## city telecon data ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 7 Santiago WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 8 Barcelona WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 9 Madrid WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## 10 Valencia WeMO \u0026lt;tibble [68 x 3]\u0026gt;\r## # ... with 30 more rows\rstr(slice(data_nest, 1))\r## grouped_df [40 x 3] (S3: grouped_df/tbl_df/tbl/data.frame)\r## $ city : chr [1:40] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr [1:40] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data :List of 40\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## - attr(*, \u0026quot;groups\u0026quot;)= tibble [40 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ city : chr [1:40] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr [1:40] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows : list\u0026lt;int\u0026gt; [1:40] ## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..$ : int 7\r## .. ..$ : int 8\r## .. ..$ : int 9\r## .. ..$ : int 10\r## .. ..$ : int 11\r## .. ..$ : int 12\r## .. ..$ : int 13\r## .. ..$ : int 14\r## .. ..$ : int 15\r## .. ..$ : int 16\r## .. ..$ : int 17\r## .. ..$ : int 18\r## .. ..$ : int 19\r## .. ..$ : int 20\r## .. ..$ : int 21\r## .. ..$ : int 22\r## .. ..$ : int 23\r## .. ..$ : int 24\r## .. ..$ : int 25\r## .. ..$ : int 26\r## .. ..$ : int 27\r## .. ..$ : int 28\r## .. ..$ : int 29\r## .. ..$ : int 30\r## .. ..$ : int 31\r## .. ..$ : int 32\r## .. ..$ : int 33\r## .. ..$ : int 34\r## .. ..$ : int 35\r## .. ..$ : int 36\r## .. ..$ : int 37\r## .. ..$ : int 38\r## .. ..$ : int 39\r## .. ..$ : int 40\r## .. ..@ ptype: int(0) ## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi TRUE\rThe next step is to create a function, in which we define the correlation test and pass it to the clean format using the tidy() function, which we apply to each groupings.\ncor_fun \u0026lt;- function(df) cor.test(df$pr, df$index, method = \u0026quot;spearman\u0026quot;) %\u0026gt;% tidy()\rNow we only have to apply our function to the column that contains the tables for each combination between city and teleconnection. To do this, we use the map() function that applies another function to a vector or list. What we do is create a new column that contains the result, a statistical summary table, for each combination.\ndata_nest \u0026lt;- mutate(data_nest, model = map(data, cor_fun))\rdata_nest\r## # A tibble: 40 x 4\r## # Groups: city, telecon [40]\r## city telecon data model ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 7 Santiago WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 8 Barcelona WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 9 Madrid WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 10 Valencia WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## # ... with 30 more rows\rstr(slice(data_nest, 1))\r## grouped_df [40 x 4] (S3: grouped_df/tbl_df/tbl/data.frame)\r## $ city : chr [1:40] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr [1:40] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data :List of 40\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA 798 524 365 246 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 1800 2344 1973 973 1348 ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num [1:68] 0.555 0.379 0.693 -0.213 1.196 ...\r## $ model :List of 40\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.00989\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 52912\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.936\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.295\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 67832\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.161\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43966\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.19\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.255\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 65754\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0361\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.0203\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 53460\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.869\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.178\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43082\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.161\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43970\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.19\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.0292\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 50862\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.813\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.185\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 62070\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.131\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.256\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 65825\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0348\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.0155\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 51584\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.9\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.0457\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 54788\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.711\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.153\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 44372\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.213\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.147\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 44670\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.23\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.357\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 33688\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.00296\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.404\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 31242\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.000706\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.313\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 65806\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0102\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.304\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 65369\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0123\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.0643\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 46893\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.605\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.497\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 75028\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 2.42e-05\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.291\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 64692\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0169\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.0835\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 45930\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.501\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.306\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 34766\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.012\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.109\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 44660\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.38\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.443\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 75608\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.00018\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.01\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 52919\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.935\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.176\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43170\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.151\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.19\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 62364\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.12\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.181\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 61902\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.139\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.0504\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 49752\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.682\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.44\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 29356\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.000203\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.332\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 35014\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.00594\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.211\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 19574\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.129\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.0672\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 26472\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.632\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.0542\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 23460\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.7\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.0478\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 25990\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.733\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.113\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 27600\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.422\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.0971\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 22396\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.488\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.0795\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 26776\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.57\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.252\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 31056\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0688\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## - attr(*, \u0026quot;groups\u0026quot;)= tibble [40 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ city : chr [1:40] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr [1:40] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows : list\u0026lt;int\u0026gt; [1:40] ## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..$ : int 7\r## .. ..$ : int 8\r## .. ..$ : int 9\r## .. ..$ : int 10\r## .. ..$ : int 11\r## .. ..$ : int 12\r## .. ..$ : int 13\r## .. ..$ : int 14\r## .. ..$ : int 15\r## .. ..$ : int 16\r## .. ..$ : int 17\r## .. ..$ : int 18\r## .. ..$ : int 19\r## .. ..$ : int 20\r## .. ..$ : int 21\r## .. ..$ : int 22\r## .. ..$ : int 23\r## .. ..$ : int 24\r## .. ..$ : int 25\r## .. ..$ : int 26\r## .. ..$ : int 27\r## .. ..$ : int 28\r## .. ..$ : int 29\r## .. ..$ : int 30\r## .. ..$ : int 31\r## .. ..$ : int 32\r## .. ..$ : int 33\r## .. ..$ : int 34\r## .. ..$ : int 35\r## .. ..$ : int 36\r## .. ..$ : int 37\r## .. ..$ : int 38\r## .. ..$ : int 39\r## .. ..$ : int 40\r## .. ..@ ptype: int(0) ## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi TRUE\rHow can we undo the list of tables in each row of our table?\nFirst we eliminate the column with the data and then simply we can apply the unnest() function.\ncorr_pr \u0026lt;- select(data_nest, -data) %\u0026gt;% unnest()\r## Warning: `cols` is now required when using unnest().\r## Please use `cols = c(model)`\rcorr_pr\r## # A tibble: 40 x 7\r## # Groups: city, telecon [40]\r## city telecon estimate statistic p.value method alternative\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Bilbao NAO 0.153 44372. 2.13e-1 Spearman\u0026#39;s rank corr~ two.sided ## 2 Santiago NAO -0.181 61902. 1.39e-1 Spearman\u0026#39;s rank corr~ two.sided ## 3 Barcelo~ NAO -0.0203 53460. 8.69e-1 Spearman\u0026#39;s rank corr~ two.sided ## 4 Madrid NAO -0.291 64692. 1.69e-2 Spearman\u0026#39;s rank corr~ two.sided ## 5 Valencia NAO -0.113 27600. 4.22e-1 Spearman\u0026#39;s rank corr~ two.sided ## 6 Bilbao WeMO 0.404 31242 7.06e-4 Spearman\u0026#39;s rank corr~ two.sided ## 7 Santiago WeMO 0.332 35014 5.94e-3 Spearman\u0026#39;s rank corr~ two.sided ## 8 Barcelo~ WeMO 0.0292 50862 8.13e-1 Spearman\u0026#39;s rank corr~ two.sided ## 9 Madrid WeMO 0.109 44660 3.80e-1 Spearman\u0026#39;s rank corr~ two.sided ## 10 Valencia WeMO -0.252 31056 6.88e-2 Spearman\u0026#39;s rank corr~ two.sided ## # ... with 30 more rows\rThe result is a table in which we can see the correlations and their statistical significance for each city and teleconnection index.\n\rHeatmap of the results\rFinally, we make a heatmap of the obtained result. But, previously we create a column that indicates whether the correlation is significant with p-value less than 0.05.\ncorr_pr \u0026lt;- mutate(corr_pr, sig = ifelse(p.value \u0026lt;0.05, \u0026quot;Sig.\u0026quot;, \u0026quot;Non Sig.\u0026quot;))\rggplot()+\rgeom_tile(data = corr_pr,\raes(city, telecon, fill = estimate),\rsize = 1,\rcolour = \u0026quot;white\u0026quot;)+\rgeom_tile(data = filter(corr_pr, sig == \u0026quot;Sig.\u0026quot;),\raes(city, telecon),\rsize = 1,\rcolour = \u0026quot;black\u0026quot;,\rfill = \u0026quot;transparent\u0026quot;)+\rgeom_text(data = corr_pr,\raes(city, telecon, label = round(estimate, 2),\rfontface = ifelse(sig == \u0026quot;Sig.\u0026quot;, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)))+\rscale_fill_gradient2(breaks = seq(-1, 1, 0.2))+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, p.value = \u0026quot;\u0026quot;)+\rtheme_minimal()+\rtheme(panel.grid.major = element_blank(),\rpanel.border = element_blank(),\rpanel.background = element_blank(),\raxis.ticks = element_blank())\r\r","date":1555459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"80bb1a83759c286dad15b616f8eb9216","permalink":"https://dominicroye.github.io/en/2019/tidy-correlation-tests-in-r/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/en/2019/tidy-correlation-tests-in-r/","section":"post","summary":"When we try to estimate the correlation coefficient between multiple variables, the task is more complicated in order to obtain a simple and tidy result. A simple solution is to use the ``tidy()`` function from the *{broom}* package. As an example, in this post we are going to estimate the correlation coefficients between the annual precipitation of several Spanish cities and climate teleconnections indices.","tags":["correlation","variables","tidy","tests"],"title":"Tidy correlation tests in R","type":"post"},{"authors":["M Lemus-Canovas","JA Lopez-Bustins","J Martin-Vide","D Royé"],"categories":null,"content":"","date":1555286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555286400,"objectID":"e180a1f5657a478e802087d80217785c","permalink":"https://dominicroye.github.io/en/publication/synoptreg_2019/","publishdate":"2019-04-15T00:00:00Z","relpermalink":"/en/publication/synoptreg_2019/","section":"publication","summary":"Spatial knowledge of the climatic or environmental variables associated with the most frequent circulation types is essential with regard to developing strategies to address the risk of avalanches, floods, soil erosion, air pollution or other natural hazards. In order to derive an environmental regionalization, we present an Open Source R package known as synoptReg, which combines the spatialization of environmental variables based on the atmospheric circulation types. The synoptReg package contains a set of functions, which we will employ (1) to perform a PCA-based synoptic classification using an atmospheric variable; (2) to map the spatial distribution of the selected environmental variable based upon the circulation types; (3) to develop a spatial environmental regionalization based on the previous results. We illustrate the usefulness of the package for a case study in the Alps area.","tags":["Alps","Environmental regionalization","R package","synoptReg","Synoptic classification"],"title":"synoptReg: An R package for computing a synoptic climate classification and a spatial regionalization of environmental data","type":"publication"},{"authors":["D Royé","María T Zarrabeitia","Javier Riancho","Ana Santurtún"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"9f3d52d2c337856a95d57a1727a613c7","permalink":"https://dominicroye.github.io/en/publication/ictus_madrid_2019/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/en/publication/ictus_madrid_2019/","section":"publication","summary":"The understanding of the role of environment on the pathogenesis of stroke is gaining importance in the context of climate change. This study analyzes the temporal pattern of ischemic stroke (IS) in Madrid, Spain, during a 13-year period (2001-2013), and the relationship between ischemic stroke (admissions and deaths) incidence and environmental factors on a daily scale by using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of air pollutants and Apparent Temperature (AT), a biometeorological index which represents human thermal comfort on IS, a lag non-linear model was fitted in a generalized additive model. The mortality rate followed a downward trend over the studied period, however admission rates progressively increased. Our results show that both increases and decreases in AT had a marked relationship with IS deaths, while hospital admissions were only associated with low AT. When analyzing the cumulative effects (for lag 0 to 14 days), with an AT of 1.7°C (percentile 5%) a RR of 1.20 (95% CI, 1.05-1.37) for IS mortality and a RR of 1.09 (95% CI, 0.91-1.29) for morbidity is estimated. Concerning gender differences, men show higher risks of mortality in low temperatures and women in high temperatures. No significant relationship was found between air pollutant concentrations and IS morbi mortality, but this result must be interpreted with caution, since there are strong spatial fluctuations of the former between nearby geographical areas that make it difficult to perform correlation analyses.","tags":["short‐term effects","Spain","Madrid","thermal environment","ischemic stroke","air pollutants","apparent temperature","mortality","hospital admissions"],"title":"A time series analysis of the relationship between Apparent Temperature, Air Pollutants and Ischemic Stroke in Madrid, Spain","type":"publication"},{"authors":null,"categories":["management","R","R:intermediate"],"content":"\rWe usually work with different data sources, and sometimes we can find tables distributed over several Excel sheets. In this post we are going to import the average daily temperature of Madrid and Berlin which is found in two Excel files with sheets for each year between 2000 and 2005: download.\nPackages\rIn this post we will use the following packages:\n\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rreadxl\rImport Excel files\r\r\r\r#install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;readxl\u0026quot;)) install.packages(\u0026quot;readxl\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(fs)\rlibrary(readxl)\rBy default, the read_excel() function imports the first sheet. To import a different sheet it is necessary to indicate the number or name with the argument sheet (second argument).\n#import first sheet\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;)\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#import third sheet\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;, 3)\r## # A tibble: 365 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2002-01-01 00:00:00 8.7 2002\r## 2 2002-01-02 00:00:00 7.4 2002\r## 3 2002-01-03 00:00:00 8.5 2002\r## 4 2002-01-04 00:00:00 9.2 2002\r## 5 2002-01-05 00:00:00 9.3 2002\r## 6 2002-01-06 00:00:00 7.3 2002\r## 7 2002-01-07 00:00:00 5.4 2002\r## 8 2002-01-08 00:00:00 5.6 2002\r## 9 2002-01-09 00:00:00 6.8 2002\r## 10 2002-01-10 00:00:00 6.1 2002\r## # ... with 355 more rows\rThe excel_sheets() function can extract the names of the sheets.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rpath %\u0026gt;%\rexcel_sheets()\r## [1] \u0026quot;2000\u0026quot; \u0026quot;2001\u0026quot; \u0026quot;2002\u0026quot; \u0026quot;2003\u0026quot; \u0026quot;2004\u0026quot; \u0026quot;2005\u0026quot;\rThe results are the sheet names and we find the years from 2000 to 2005. The most important function to read multiple sheets is map() of the {purrr} package, which is part of the {tidyverse] collection. map() allows you to apply a function to each element of a vector or list.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap(read_excel,\rpath = path)\rstr(mad)\r## List of 6\r## $ 2000:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 366 obs. of 3 variables:\r## ..$ date: POSIXct[1:366], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:366] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:366] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ 2001:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2001-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 8.2 8.8 7.5 9.2 10 9 5.5 4.6 3 7.9 ...\r## ..$ yr : num [1:365] 2001 2001 2001 2001 2001 ...\r## $ 2002:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2002-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 8.7 7.4 8.5 9.2 9.3 7.3 5.4 5.6 6.8 6.1 ...\r## ..$ yr : num [1:365] 2002 2002 2002 2002 2002 ...\r## $ 2003:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2003-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 9.4 10.8 9.7 9.2 6.3 6.6 3.8 6.4 4.3 3.4 ...\r## ..$ yr : num [1:365] 2003 2003 2003 2003 2003 ...\r## $ 2004:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 366 obs. of 3 variables:\r## ..$ date: POSIXct[1:366], format: \u0026quot;2004-01-01\u0026quot; ...\r## ..$ ta : num [1:366] 6.6 5.9 7.8 8.1 6.4 5.7 5.2 6.9 11.8 12.2 ...\r## ..$ yr : num [1:366] 2004 2004 2004 2004 2004 ...\r## $ 2005:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2005-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 7.1 7.8 6.4 5.6 4.4 6.8 7.4 6 5.2 4.2 ...\r## ..$ yr : num [1:365] 2005 2005 2005 2005 2005 ...\rThe result is a named list with the name of each sheet that contains the data.frame. Since it is the same table in all sheets, we could use the function bind_rows(), however, there is a variant of map() that directly joins all the tables by row: map_df(). If it were necessary to join by column, map_dfc() could be used.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path)\rmad\r## # A tibble: 2,192 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 2,182 more rows\rIn our case we have a column in each sheet (year, but also the date) that differentiates each table. If it were not the case, we should use the name of the sheets as a new column when joining all of them. In bind_rows() it can be done with the .id argument by assigning a name for the column. The same works for map_df().\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path,\r.id = \u0026quot;yr2\u0026quot;)\rstr(mad)\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 4 variables:\r## $ yr2 : chr \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; ...\r## $ date: POSIXct, format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## $ yr : num 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\rBut how do we import multiple Excel files?\nTo do this, first we must know the dir_ls() function from the {fs} package. Indeed, there is the dir() function of R Base, but the advantages of the recent package are several, especially the compatibility with the {tidyverse} collection.\ndir_ls()\r## berlin_temp.xlsx featured.png index.en.html index.en.Rmd ## madrid_temp.xlsx\r#we can filter the files that we want\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) \r## berlin_temp.xlsx madrid_temp.xlsx\rWe import the two Excel files.\n#without joining\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;%\rmap(read_excel)\r## $berlin_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 1.2 2000\r## 2 2000-01-02 00:00:00 3.6 2000\r## 3 2000-01-03 00:00:00 5.7 2000\r## 4 2000-01-04 00:00:00 5.1 2000\r## 5 2000-01-05 00:00:00 2.2 2000\r## 6 2000-01-06 00:00:00 1.8 2000\r## 7 2000-01-07 00:00:00 4.2 2000\r## 8 2000-01-08 00:00:00 4.2 2000\r## 9 2000-01-09 00:00:00 4.2 2000\r## 10 2000-01-10 00:00:00 1.7 2000\r## # ... with 356 more rows\r## ## $madrid_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#joining with a new id column\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;%\rmap_df(read_excel, .id = \u0026quot;city\u0026quot;)\r## # A tibble: 732 x 4\r## city date ta yr\r## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 berlin_temp.xlsx 2000-01-01 00:00:00 1.2 2000\r## 2 berlin_temp.xlsx 2000-01-02 00:00:00 3.6 2000\r## 3 berlin_temp.xlsx 2000-01-03 00:00:00 5.7 2000\r## 4 berlin_temp.xlsx 2000-01-04 00:00:00 5.1 2000\r## 5 berlin_temp.xlsx 2000-01-05 00:00:00 2.2 2000\r## 6 berlin_temp.xlsx 2000-01-06 00:00:00 1.8 2000\r## 7 berlin_temp.xlsx 2000-01-07 00:00:00 4.2 2000\r## 8 berlin_temp.xlsx 2000-01-08 00:00:00 4.2 2000\r## 9 berlin_temp.xlsx 2000-01-09 00:00:00 4.2 2000\r## 10 berlin_temp.xlsx 2000-01-10 00:00:00 1.7 2000\r## # ... with 722 more rows\rHowever, in this case we only import the first sheet of each Excel file. To solve this problem, we must create our own function. In this function we do what we previously did individually.\nread_multiple_excel \u0026lt;- function(path) {\rpath %\u0026gt;%\rexcel_sheets() %\u0026gt;% set_names() %\u0026gt;% map_df(read_excel, path = path)\r}\rWe apply our created function to import multiple sheets of several Excel files.\n#separately\rdata \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map(read_multiple_excel)\rstr(data)\r## List of 2\r## $ berlin_temp.xlsx:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 3 variables:\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:2192] 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ madrid_temp.xlsx:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 3 variables:\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:2192] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r#joining all data.frames\rdata_df \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map_df(read_multiple_excel,\r.id = \u0026quot;city\u0026quot;)\rstr(data_df)\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 4384 obs. of 4 variables:\r## $ city: chr \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; ...\r## $ date: POSIXct, format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## $ yr : num 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r\r","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"ca84d77cc554e05833562162d0d9260f","permalink":"https://dominicroye.github.io/en/2019/import-excel-sheets-with-r/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/en/2019/import-excel-sheets-with-r/","section":"post","summary":"We usually work with different data sources, and sometimes we can find tables distributed over several Excel sheets. In this post we are going to import the average daily temperature of Madrid and Berlin which is found in two Excel files with sheets for each year between 2000 and 2005.","tags":["excel","sheets","import"],"title":"Import Excel sheets with R","type":"post"},{"authors":["D Royé","N Lorenzo","D Rasilla","A Martí"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"15c539a23e016cefebced36892b897d8","permalink":"https://dominicroye.github.io/en/publication/cloudiness_ip_2018/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/en/publication/cloudiness_ip_2018/","section":"publication","summary":"This paper presents the first systematic study of the relationships between atmospheric circulation types (CT) and cloud fraction (CF) over the whole Iberian Peninsula, using satellite data from the MODIS (MOD09GA and MYD09GA) cloud mask for the period 2001--2017. The high level of detail, in combination with a classification for circulation patterns, provides us with relevant information about the spatio-temporal variability of cloudiness and the main mechanisms affecting the genesis of clouds. The results show that westerly CTs are the most influential, followed by cyclonic types, in cloudiness in the west of the Iberian Peninsula. Westerly flows, however, do not affect the Mediterranean coastline, which is dominated by easterly CTs, suggesting that local factors such as convective processes, orography and proximity to a body of warm water could play a major role in cloudiness processes. The Cantabrian Coast also has a particularly characteristic cloudiness dominated by northerly CTs. In general, the results found in this study are in line with the few studies that exist on cloudiness in the Iberian Peninsula. Furthermore, the results are geographically consistent, showing links to synoptic forcing in terms of atmospheric circulation patterns and the impact of the Iberian Peninsula's complex orography upon this element of the climate system.","tags":["cloudiness","circulation types","Iberian Peninsula","MODIS","weather","spatio-temporal patterns"],"title":"Spatio-temporal variations of cloud fraction based on circulation types in the Iberian Peninsula","type":"publication"},{"authors":null,"categories":["gis","R","R:elementary"],"content":"\r\rThe distance to the sea is a fundamental variable in geography, especially relevant when it comes to modeling. For example, in interpolations of air temperature, the distance to the sea is usually used as a predictor variable, since there is a casual relationship between the two that explains the spatial variation. How can we estimate the (shortest) distance to the coast in R?\nPackages\rIn this post we will use the following libraries:\n\r\r\r\rLibrary\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rraster\rImport, export and manipulate raster\r\rrnaturalearth\rSet of vector maps ‘natural earth’\r\rRColorBrewer\rColor palettes\r\r\r\r#install the libraries if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r#packages\rlibrary(rnaturalearth)\rlibrary(sf)\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(RColorBrewer)\r\rThe coast of Iceland as an example\rOur example in this post will be Iceland, and, as it is an island territory it will facilitate the tutorial showing the process in a simple manner. The rnaturalearth package allows you to import the boundaries of countries (with different administrative levels) from around the world. The data comes from the platform naturalearthdata.com. I recommend exploring the package, more info here. The ne_countries( ) function imports the country boundaries. In this case we indicate with the argument scale the resolution (10, 50 or 110m), with country we indicate the specific country of interest and with returnclass we determine which class we want (sf or sp), in our case sf (simple feature).\nworld \u0026lt;- ne_countries(scale = 50) #world map with 50m resolution\rplot(world) #sp class by default\r#import the limits of Iceland\riceland \u0026lt;- ne_countries(scale = 10, country = \u0026quot;Iceland\u0026quot;, returnclass = \u0026quot;sf\u0026quot;)\r#info of our spatial vector object\riceland\r## Simple feature collection with 1 feature and 94 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: -24.53991 ymin: 63.39671 xmax: -13.50292 ymax: 66.56415\r## CRS: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\r## featurecla scalerank labelrank sovereignt sov_a3 adm0_dif level\r## 188 Admin-0 country 0 3 Iceland ISL 0 2\r## type admin adm0_a3 geou_dif geounit gu_a3 su_dif subunit\r## 188 Sovereign country Iceland ISL 0 Iceland ISL 0 Iceland\r## su_a3 brk_diff name name_long brk_a3 brk_name brk_group abbrev postal\r## 188 ISL 0 Iceland Iceland ISL Iceland \u0026lt;NA\u0026gt; Iceland IS\r## formal_en formal_fr name_ciawf note_adm0 note_brk name_sort\r## 188 Republic of Iceland \u0026lt;NA\u0026gt; Iceland \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Iceland\r## name_alt mapcolor7 mapcolor8 mapcolor9 mapcolor13 pop_est pop_rank\r## 188 \u0026lt;NA\u0026gt; 1 4 4 9 339747 10\r## gdp_md_est pop_year lastcensus gdp_year economy\r## 188 16150 2017 NA 2016 2. Developed region: nonG7\r## income_grp wikipedia fips_10_ iso_a2 iso_a3 iso_a3_eh iso_n3\r## 188 1. High income: OECD NA IC IS ISL ISL 352\r## un_a3 wb_a2 wb_a3 woe_id woe_id_eh woe_note adm0_a3_is\r## 188 352 IS ISL 23424845 23424845 Exact WOE match as country ISL\r## adm0_a3_us adm0_a3_un adm0_a3_wb continent region_un subregion\r## 188 ISL NA NA Europe Europe Northern Europe\r## region_wb name_len long_len abbrev_len tiny homepart min_zoom\r## 188 Europe \u0026amp; Central Asia 7 7 7 NA 1 0\r## min_label max_label ne_id wikidataid name_ar name_bn name_de name_en\r## 188 2 7 1159320917 Q189 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Island Iceland\r## name_es name_fr name_el name_hi name_hu name_id name_it name_ja name_ko\r## 188 Islandia Islande \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Izland Islandia Islanda \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt;\r## name_nl name_pl name_pt name_ru name_sv name_tr name_vi name_zh\r## 188 IJsland Islandia Islândia \u0026lt;NA\u0026gt; Island Izlanda Iceland \u0026lt;NA\u0026gt;\r## geometry\r## 188 MULTIPOLYGON (((-14.56363 6...\r#here Iceland\rplot(iceland)\rBy default, the plot( ) function with the class sf creates as many facets of the map as there are variables in it. To limit this behavior we can use either a variable name plot(iceland[\"admin\"]) or the limit argument plot(iceland, max.plot = 1). With the argument max.plot = 1 the function uses the first available variable of the map.\nIn addition, we see in the information of the object sf that the projection is WGS84 with decimal degrees (EPSG code: 4326). For the calculation of distances it is more convenient to use meters instead of degrees. Because of this, the first thing we do is to transform the map of Iceland to UTM Zone 27 (EPSG code: 3055). More information about EPSG and projections here. For that purpose, we use the st_transform( ) function. We simply indicate the map and the EPSG code.\n#transform to UTM\riceland \u0026lt;- st_transform(iceland, 3055)\r\rCreate a fishnet of points\rWe still need the points where we want to know the distance. In our case it will be a regular fishnet of points in Iceland with a resolution of 5km. We do this with the function st_make_grid( ), indicating the resolution in the unit of the coordinate system (meters in our case) with the argument cellsize, and what geometry we would like to create what (polygons, centers or corners).\n#create the fishnet\rgrid \u0026lt;- st_make_grid(iceland, cellsize = 5000, what = \u0026quot;centers\u0026quot;)\r#our fishnet with the extension of Iceland\rplot(grid)\r#only extract the points in the limits of Iceland\rgrid \u0026lt;- st_intersection(grid, iceland) #our fishnet now\rplot(grid)\r\rCalculating the distance\rTo estimate the distance we use the st_distance( ) function that returns a vector of distances for all our points in the fishnet. But first it is necessary to transform the map of Iceland from a polygon shape (MULTIPOLYGON) to a line (MULTILINESTRING). More details with ?st_cast.\n#transform Iceland from polygon shape to line\riceland \u0026lt;- st_cast(iceland, \u0026quot;MULTILINESTRING\u0026quot;)\r#calculation of the distance between the coast and our points\rdist \u0026lt;- st_distance(iceland, grid)\r#distance with unit in meters\rhead(dist[1,])\r## Units: [m]\r## [1] 790.7906 1151.4360 1270.7603 3128.9057 2428.5677 4197.7472\r\rPlotting the calculated distance\rOnce obtained the distance for our points, we can combine them with the coordinates and plot them in ggplot2. For this, we create a data.frame. The object dist is a matrix of one column, so we have to convert it to a vector with the function as.vector( ). In addition, we divide by 1000 to convert the distance in meters to km. The st_coordinates( ) function extracts the coordinates of our points. For the final visualization we use a vector of colors with the RdGy palette (more here).\n#create a data.frame with the distance and the coordinates of the points\rdf \u0026lt;- data.frame(dist = as.vector(dist)/1000,\rst_coordinates(grid))\r#structure\rstr(df)\r## \u0026#39;data.frame\u0026#39;: 4104 obs. of 3 variables:\r## $ dist: num 0.791 1.151 1.271 3.129 2.429 ...\r## $ X : num 608796 613796 583796 588796 593796 ...\r## $ Y : num 7033371 7033371 7038371 7038371 7038371 ...\r#colors col_dist \u0026lt;- brewer.pal(11, \u0026quot;RdGy\u0026quot;)\rggplot(df, aes(X, Y, fill = dist))+ #variables\rgeom_tile()+ #geometry\rscale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance\rlabs(fill = \u0026quot;Distance (km)\u0026quot;)+ #legend name\rtheme_void()+ #map theme\rtheme(legend.position = \u0026quot;bottom\u0026quot;) #legend position\r\rExport the distance as a raster\rTo be able to export the estimated distance to the sea of Iceland, we need to use the rasterize( ) function of the library raster.\nFirst, it is necessary to create an empty raster. In this raster we have to indicate the resolution, in our case it is of 5000m, the projection and the extension of the raster.\nWe can extract the projection from the information of the map of Iceland.\n\rThe extension can be extracted from our grid points with the function extent( ). However, this last function needs the class sp, so we pass the object grid in sf format, only for this time, to the class sp using the function as( ) and the argument “Spatial”.\n\r\rIn addition to the above, the data.frame df, that we created earlier, has to be converted into the sf class. Therefore, we apply the function st_as_sf( ) with the argument coords indicating the names of the coordinates. Additionally, we also define the coordinate system that we know.\n\r\r#get the extension\rext \u0026lt;- extent(as(grid, \u0026quot;Spatial\u0026quot;))\r#extent object\rext\r## class : Extent ## xmin : 338795.6 ## xmax : 848795.6 ## ymin : 7033371 ## ymax : 7383371\r#raster destination\rr \u0026lt;- raster(resolution = 5000, ext = ext, crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\r#convert the points to a spatial object class sf\rdist_sf \u0026lt;- st_as_sf(df, coords = c(\u0026quot;X\u0026quot;, \u0026quot;Y\u0026quot;)) %\u0026gt;%\rst_set_crs(3055)\r#create the distance raster\rdist_raster \u0026lt;- rasterize(dist_sf, r, \u0026quot;dist\u0026quot;, fun = mean)\r#raster\rdist_raster\r## class : RasterLayer ## dimensions : 70, 102, 7140 (nrow, ncol, ncell)\r## resolution : 5000, 5000 (x, y)\r## extent : 338795.6, 848795.6, 7033371, 7383371 (xmin, xmax, ymin, ymax)\r## crs : +proj=utm +zone=27 +ellps=intl +units=m +no_defs ## source : memory\r## names : layer ## values : 0.006124901, 115.1712 (min, max)\r#plot the raster\rplot(dist_raster)\r#export the raster\rwriteRaster(dist_raster, file = \u0026quot;dist_islandia.tif\u0026quot;, format = \u0026quot;GTiff\u0026quot;, overwrite = TRUE)\rThe rasterize( ) function is designed to create rasters from an irregular grid. In case we have a regular grid, like this one, we can use an easier alternative way. The rasterFromXYZ( ) function converts a data.frame with longitude, latitude and the variable Z into a raster object. It is important that the order should be longitude, latitude, variables.\nr \u0026lt;- rasterFromXYZ(df[, c(2:3, 1)], crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\rplot(r)\rWith the calculation of distance we can create art, as seen in the header of this post, which includes a world map only with the distance to the sea of all continents. A different perspective to our world (here more (spanish)) .\n\r","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"c7b36d9cdb7e5cf3a4e9369cbd9ba007","permalink":"https://dominicroye.github.io/en/2019/calculating-the-distance-to-the-sea-in-r/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/en/2019/calculating-the-distance-to-the-sea-in-r/","section":"post","summary":"The distance to the sea is a fundamental variable in geography, especially relevant when it comes to modeling. For example, in interpolations of air temperature, the distance to the sea is usually used as a predictor variable, since there is a casual relationship between the two that explains the spatial variation. How can we estimate the (shortest) distance to the coast in R?","tags":["distance","raster","estimation","variable"],"title":"Calculating the distance to the sea in R","type":"post"},{"authors":["F Mori-Gamarra","L Moure-Rodríguez","X Sureda","C Carbiae","D Royé","A Montes-Martínez","F Cadaveira","F Caamaño-Isorna"],"categories":null,"content":"","date":1545350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545350400,"objectID":"0a110e00a459320b291d11a3782d372b","permalink":"https://dominicroye.github.io/en/publication/alcohol_galicia2018/","publishdate":"2018-12-21T00:00:00Z","relpermalink":"/en/publication/alcohol_galicia2018/","section":"publication","summary":"Objective: To assess the influence that alcohol outlet density, off- and on-alcohol premises, and alcohol consumption wield on the consumption patterns of young pre-university students in Galicia (Spain). Method: A cross-sectional analysis of a cohort of students of the University of Santiago de Compostela (Compostela Cohort 2016) was carried out. Consumption prevalence were calculated for each of the municipalities from the first-cycle students’ home residence during the year prior to admission. The association with risky alcohol consumption (RC) and binge-drinking (BD) was assessed with a logistic model considering as independent variables the municipality population, alcohol outlet density of off- premises, density of off- and on- premises and total density of both types of premises in the municipality. Results: The prevalence of RC was 60.5% (95% confidence interval [95%CI]: 58.4-62.5) and the BD was 28.5% (95%CI: 26.7-30.2). A great variability was observed according to the municipality of provenance. The multivariate logistic model showed municipalities with a density of 8.42-9.34 of both types of premises per thousand inhabitants presented a higher risk of RC (odds ratio [OR]: 1,39; 95%CI: 1.09-1.78) and BD (OR: 1.29; 95%CI: 1.01-1.66). Conclusion: These data suggest the importance of including environmental information when studying alcohol consumption. Knowing our environment better could help plan policies that encourage healthier behaviour in the population.","tags":["Alcohol outlet density","Alcohol","Underage drinking","Adolescents"],"title":"Alcohol outlet density and alcohol consumption in Galician youth","type":"publication"},{"authors":null,"categories":["datavis","R","R:elementary"],"content":"\r\rThis year, the so-called warming stripes, which were created by the scientist Ed Hawkins of the University of Reading, became very famous all over the world. These graphs represent and communicate climate change in a very illustrative and effective way.\nVisualising global temperature change since records began in 1850. Versions for USA, central England \u0026amp; Toronto available too: https://t.co/H5Hv9YgZ7v pic.twitter.com/YMzdySrr3A\n\u0026mdash; Ed Hawkins (@ed_hawkins) May 23, 2018  From his idea, I created strips for examples of Spain, like the next one in Madrid.\n#Temperatura anual en #MadridRetiro desde 1920 a 2017. #CambioClimatico #dataviz #ggplot2 (idea de @ed_hawkins 🙏) @Divulgameteo @edupenabad @climayagua @ClimaGroupUB @4gotas_com pic.twitter.com/wmLb5uczpT\n\u0026mdash; Dominic Royé (@dr_xeo) June 2, 2018  In this post I will show how you can create these strips in R with the library ggplot2. Although I must say that there are many ways in R that can lead us to the same result or to a similar one, even within ggplot2.\nData\rIn this case we will use the annual temperatures of Lisbon\rGISS Surface Temperature Analysis, homogenized time series, comprising the period from 1880 to 2018. Monthly temperatures or other time series could also be used. The file can be downloaded here. First, we should, as long as we have not done it, install the collection of tidyverse libraries that also include ggplot2. In addition, we will need the library lubridate for the treatment of dates. Then, we import the data of Lisbon in csv format.\n#install the lubridate and tidyverse libraries\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\r#packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(RColorBrewer)\r#import the annual temperatures\rtemp_lisboa \u0026lt;- read_csv(\u0026quot;temp_lisboa.csv\u0026quot;)\rstr(temp_lisboa)\r## spec_tbl_df [139 x 18] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r## $ YEAR : num [1:139] 1880 1881 1882 1883 1884 ...\r## $ JAN : num [1:139] 9.17 11.37 10.07 10.86 11.16 ...\r## $ FEB : num [1:139] 12 11.8 11.9 11.5 10.6 ...\r## $ MAR : num [1:139] 13.6 14.1 13.5 10.5 12.4 ...\r## $ APR : num [1:139] 13.1 14.4 14 13.8 12.2 ...\r## $ MAY : num [1:139] 15.7 17.3 15.6 14.6 16.4 ...\r## $ JUN : num [1:139] 17 19.2 17.9 17.2 19.1 ...\r## $ JUL : num [1:139] 19.1 21.8 20.3 19.5 21.4 ...\r## $ AUG : num [1:139] 20.6 23.5 21 21.6 22.4 ...\r## $ SEP : num [1:139] 20.7 20 18 18.8 19.5 ...\r## $ OCT : num [1:139] 17.9 16.3 16.4 15.8 16.4 ...\r## $ NOV : num [1:139] 12.5 14.7 13.7 13.5 12.5 ...\r## $ DEC : num [1:139] 11.07 9.97 10.66 9.46 10.25 ...\r## $ D-J-F : num [1:139] 10.7 11.4 10.6 11 10.4 ...\r## $ M-A-M : num [1:139] 14.1 15.2 14.3 12.9 13.6 ...\r## $ J-J-A : num [1:139] 18.9 21.5 19.7 19.4 20.9 ...\r## $ S-O-N : num [1:139] 17 17 16 16 16.1 ...\r## $ metANN: num [1:139] 15.2 16.3 15.2 14.8 15.3 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. YEAR = col_double(),\r## .. JAN = col_double(),\r## .. FEB = col_double(),\r## .. MAR = col_double(),\r## .. APR = col_double(),\r## .. MAY = col_double(),\r## .. JUN = col_double(),\r## .. JUL = col_double(),\r## .. AUG = col_double(),\r## .. SEP = col_double(),\r## .. OCT = col_double(),\r## .. NOV = col_double(),\r## .. DEC = col_double(),\r## .. `D-J-F` = col_double(),\r## .. `M-A-M` = col_double(),\r## .. `J-J-A` = col_double(),\r## .. `S-O-N` = col_double(),\r## .. metANN = col_double()\r## .. )\rWe see in the columns that we have monthly and seasonal values, and the annual temperature value. But before proceeding to visualize the annual temperature, we must replace the missing values 999.9 with NA, using the ifelse( ) function that evaluates a condition and perform the given argument corresponding to true and false.\n#select only the annual temperature and year column\rtemp_lisboa_yr \u0026lt;- select(temp_lisboa, YEAR, metANN)\r#rename the temperature column\rtemp_lisboa_yr \u0026lt;- rename(temp_lisboa_yr, ta = metANN)\r#missing values 999.9\rsummary(temp_lisboa_yr) \r## YEAR ta ## Min. :1880 Min. : 14.53 ## 1st Qu.:1914 1st Qu.: 15.65 ## Median :1949 Median : 16.11 ## Mean :1949 Mean : 37.38 ## 3rd Qu.:1984 3rd Qu.: 16.70 ## Max. :2018 Max. :999.90\rtemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, ta = ifelse(ta == 999.9, NA, ta))\rWhen we use the year as a variable, we do not usually convert it into a date object, however it is advisable. This allows us to use the date functions of the library lubridate and the support functions inside of ggplot2. The str_c( ) function of the library stringr, part of the collection of tidyverse, is similar to paste( ) of R Base that allows us to combine characters by specifying a separator (sep = “-”). The ymd( ) (year month day) function of the lubridate library converts a date character into a Date object. It is possible to combine several functions\rusing the pipe operator %\u0026gt;% that helps to chain without assigning the result to a new object. Its use is very extended especially with the library tidyverse. If you want to know more about its use, here you have a tutorial.\ntemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, date = str_c(YEAR, \u0026quot;01-01\u0026quot;, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd())\r\rCreating the strips\rFirst, we create the style of the graph, specifying all the arguments of the theme we want to adjust. We start with the default style of theme_minimal( ). In addition, we assign\rthe colors from RColorBrewer to an object col_srip. More information about the colors used here.\ntheme_strip \u0026lt;- theme_minimal()+\rtheme(axis.text.y = element_blank(),\raxis.line.y = element_blank(),\raxis.title = element_blank(),\rpanel.grid.major = element_blank(),\rlegend.title = element_blank(),\raxis.text.x = element_text(vjust = 3),\rpanel.grid.minor = element_blank(),\rplot.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;)\r)\rcol_strip \u0026lt;- brewer.pal(11, \u0026quot;RdBu\u0026quot;)\rbrewer.pal.info\r## maxcolors category colorblind\r## BrBG 11 div TRUE\r## PiYG 11 div TRUE\r## PRGn 11 div TRUE\r## PuOr 11 div TRUE\r## RdBu 11 div TRUE\r## RdGy 11 div FALSE\r## RdYlBu 11 div TRUE\r## RdYlGn 11 div FALSE\r## Spectral 11 div FALSE\r## Accent 8 qual FALSE\r## Dark2 8 qual TRUE\r## Paired 12 qual TRUE\r## Pastel1 9 qual FALSE\r## Pastel2 8 qual FALSE\r## Set1 9 qual FALSE\r## Set2 8 qual TRUE\r## Set3 12 qual FALSE\r## Blues 9 seq TRUE\r## BuGn 9 seq TRUE\r## BuPu 9 seq TRUE\r## GnBu 9 seq TRUE\r## Greens 9 seq TRUE\r## Greys 9 seq TRUE\r## Oranges 9 seq TRUE\r## OrRd 9 seq TRUE\r## PuBu 9 seq TRUE\r## PuBuGn 9 seq TRUE\r## PuRd 9 seq TRUE\r## Purples 9 seq TRUE\r## RdPu 9 seq TRUE\r## Reds 9 seq TRUE\r## YlGn 9 seq TRUE\r## YlGnBu 9 seq TRUE\r## YlOrBr 9 seq TRUE\r## YlOrRd 9 seq TRUE\rFor the final graphic we use the geometry geom_tile( ). Since the data does not have a specific value for the Y axis, we need a dummy value, here I used 1. Also, I adjust the width of the color bar in the legend.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile()+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_continuous(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rguides(fill = guide_colorbar(barwidth = 1))+\rlabs(title = \u0026quot;LISBOA 1880-2018\u0026quot;,\rcaption = \u0026quot;Datos: GISS Surface Temperature Analysis\u0026quot;)+\rtheme_strip\rIn case we want to get only the strips, we can use theme_void( ) and the argument show.legend = FALSE in geom_tile( ) to remove all style elements. We can also change the color for the NA values, including the argument na.value = “gray70” in the scale_fill_gradientn( ) function.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile(show.legend = FALSE)+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_discrete(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rtheme_void()\r\r","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"42602d7478f5a984c3ad25eb3362ff7c","permalink":"https://dominicroye.github.io/en/2018/how-to-create-warming-stripes-in-r/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/en/2018/how-to-create-warming-stripes-in-r/","section":"post","summary":"This year, the so-called warming stripes, which were created by the scientist Ed Hawkins of the University of Reading, became very famous all over the world. These graphs represent and communicate climate change in a very illustrative and effective way.","tags":["ggplot2","warming stripes","global warming","visualization"],"title":"How to create 'Warming Stripes' in R","type":"post"},{"authors":null,"categories":["visualization","R:elementary","R","mapping"],"content":"\r\rThe database of Open Street Maps\rRecently I created a map of the distribution of gas stations and electric charging stations in Europe.\nPopulation density through the number of gas stations in Europe. #dataviz @AGE_Oficial @mipazos @simongerman600 @openstreetmap pic.twitter.com/eIUx2yn7ej\n\u0026mdash; Dominic Royé (@dr_xeo) February 25, 2018  How can you obtain this data?\nWell, in this case I used points of interest (POIs) from the database of Open Street Maps (OSM). Obviously OSM not only contains streets and highways, but also information that can be useful when we use a map such as locations of hospitals or gas stations. To avoid downloading the entire OSM and extracting the required information, you can use an overpass API, which allows us to query the OSM database with our own criteria.\nAn easy way to access an overpass API is through overpass-turbo.eu, which even includes a wizard to build a query and display the results on a interactive map. A detailed explanation of the previous web can be found here.\rHowever, we have at our disposal a package osmdata that allows us to create and make queries directly from the R environment. Nevertheless, the use of the overpass-turbo.eu can be useful when we are not sure what we are looking for or when we have some difficulty in building the query.\n\rAccessing the overpass API from R\rThe first step is to install several packages, in case they are not installed. In almost all my scripts I use tidyverse which is a fundamental collection of different packages, including dplyr (data manipulation), ggplot2 (visualization), etc. The sf package is the new standard for working with spatial data and is compatible with ggplot2 and dplyr. Finally, ggmap makes it easier for us to create maps.\n#install the osmdata, sf, tidyverse and ggmap package\rif(!require(\u0026quot;osmdata\u0026quot;)) install.packages(\u0026quot;osmdata\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggmap\u0026quot;)) install.packages(\u0026quot;ggmap\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(osmdata)\rlibrary(sf)\rlibrary(ggmap)\r\rBuild a query\rBefore creating a query, we need to know what we can filter. The available_features( ) function returns a list of available OSM features that have different tags. More details are available in the OSM wiki here.\rFor example, the feature shop contains several tags among others supermarket, fishing, books, etc.\n#the first five features\rhead(available_features())\r## [1] \u0026quot;4wd_only\u0026quot; \u0026quot;abandoned\u0026quot; \u0026quot;abutters\u0026quot; \u0026quot;access\u0026quot; \u0026quot;addr\u0026quot; \u0026quot;addr:city\u0026quot;\r#amenities\rhead(available_tags(\u0026quot;amenity\u0026quot;))\r## [1] \u0026quot;animal_boarding\u0026quot; \u0026quot;animal_breeding\u0026quot; \u0026quot;animal_shelter\u0026quot; \u0026quot;arts_centre\u0026quot; ## [5] \u0026quot;atm\u0026quot; \u0026quot;baby_hatch\u0026quot;\r#shops\rhead(available_tags(\u0026quot;shop\u0026quot;))\r## [1] \u0026quot;agrarian\u0026quot; \u0026quot;alcohol\u0026quot; \u0026quot;anime\u0026quot; \u0026quot;antiques\u0026quot; \u0026quot;appliance\u0026quot; \u0026quot;art\u0026quot;\rThe first query: Where are cinemas in Madrid?\rTo build the query, we use the pipe operator %\u0026gt;%, which helps to chain several functions without assigning the result to a new object. Its use is very extended especially within the tidyverse package collection. If you want to know more about its use, you can find here a tutorial.\nIn the first part of the query we need to indicate the place where we want to extract the information. The getbb( ) function creates a boundering box for a given place, looking for the name. The main function is opq( ) which build the final query. We add our filter criteria with the add_osm_feature( ) function. In this first query we will look for cinemas in Madrid. That’s why we use as key amenity and cinema as tag. There are several formats to obtain the resulting spatial data of the query. The osmdata_*( ) function sends the query to the server and, depending on the suffix * sf/sp/xml, returns a simple feature, spatial or XML format.\n#building the query\rq \u0026lt;- getbb(\u0026quot;Madrid\u0026quot;) %\u0026gt;%\ropq() %\u0026gt;%\radd_osm_feature(\u0026quot;amenity\u0026quot;, \u0026quot;cinema\u0026quot;)\rstr(q) #query structure\r## List of 4\r## $ bbox : chr \u0026quot;40.3119774,-3.8889539,40.6437293,-3.5179163\u0026quot;\r## $ prefix : chr \u0026quot;[out:xml][timeout:25];\\n(\\n\u0026quot;\r## $ suffix : chr \u0026quot;);\\n(._;\u0026gt;;);\\nout body;\u0026quot;\r## $ features: chr \u0026quot; [\\\u0026quot;amenity\\\u0026quot;=\\\u0026quot;cinema\\\u0026quot;]\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr [1:2] \u0026quot;list\u0026quot; \u0026quot;overpass_query\u0026quot;\r## - attr(*, \u0026quot;nodes_only\u0026quot;)= logi FALSE\rcinema \u0026lt;- osmdata_sf(q)\rcinema\r## Object of class \u0026#39;osmdata\u0026#39; with:\r## $bbox : 40.3119774,-3.8889539,40.6437293,-3.5179163\r## $overpass_call : The call submitted to the overpass API\r## $meta : metadata including timestamp and version numbers\r## $osm_points : \u0026#39;sf\u0026#39; Simple Features Collection with 197 points\r## $osm_lines : NULL\r## $osm_polygons : \u0026#39;sf\u0026#39; Simple Features Collection with 11 polygons\r## $osm_multilines : NULL\r## $osm_multipolygons : NULL\rWe see that the result is a list of different spatial objects. In our case, we are only interested in osm_points.\nHow can we visulise these points?\nThe advantage of sf objects is that for ggplot2 already exists a geometry function geom_sf( ). Furthermore, we can include a background map using ggmap. The get_map( ) function downloads the map for a given place. Alternatively, it can be an address, latitude/longitude or a bounding box. The maptype argument allows us to indicate the style or type of map. You can find more details in the help of the ?get_map function.\nWhen we build a graph with ggplot we usually start with ggplot( ). In this case, we start with ggmap( ) that includes the object with our background map. Then we add with geom_sf( ) the points of the cinemas in Madrid. It is important to indicate with the argument inherit.aes = FALSE that it has to use the aesthetic mappings of the spatial object osm_points. In addition, we change the color, fill, transparency (alpha), type and size of the circles.\n#our background map\rmad_map \u0026lt;- get_map(getbb(\u0026quot;Madrid\u0026quot;), maptype = \u0026quot;toner-background\u0026quot;)\r#final map\rggmap(mad_map)+\rgeom_sf(data = cinema$osm_points,\rinherit.aes = FALSE,\rcolour = \u0026quot;#238443\u0026quot;,\rfill = \u0026quot;#004529\u0026quot;,\ralpha = .5,\rsize = 4,\rshape = 21)+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;)\r\rWhere can we find Mercadona supermarkets?\rInstead of obtaining a bounding box with the function getbb( ) we can build our own box. To do this, we create a vector of four elements, the order has to be West/South/East/North. In the query we use two features: name and shop to filter supermarkets that are of this particular brand. Depending on the area or volume of the query, it is necessary to extend the waiting time. By default, the limit is set at 25 seconds (timeout).\nThe map, we create in this case, consists only of the supermarket points. Therefore, we use the usual grammar by adding the geometry geom_sf( ). The theme_void( ) function removes everything except for the points.\n#bounding box for the Iberian Peninsula\rm \u0026lt;- c(-10, 30, 5, 46)\r#building the query\rq \u0026lt;- m %\u0026gt;% opq (timeout = 25*100) %\u0026gt;%\radd_osm_feature(\u0026quot;name\u0026quot;, \u0026quot;Mercadona\u0026quot;) %\u0026gt;%\radd_osm_feature(\u0026quot;shop\u0026quot;, \u0026quot;supermarket\u0026quot;)\r#query\rmercadona \u0026lt;- osmdata_sf(q)\r#final map\rggplot(mercadona$osm_points)+\rgeom_sf(colour = \u0026quot;#08519c\u0026quot;,\rfill = \u0026quot;#08306b\u0026quot;,\ralpha = .5,\rsize = 1,\rshape = 21)+\rtheme_void()\r\r\r","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"701016639d7073a67bf2e8d9a21ef1f3","permalink":"https://dominicroye.github.io/en/2018/accessing-openstreetmap-data-with-r/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/en/2018/accessing-openstreetmap-data-with-r/","section":"post","summary":"Recently I created a map of the distribution of gas stations and electric charging stations in Europe. How can you obtain this data? Well, in this case I used points of interest (POIs) from the database of *Open Street Maps* (OSM). Obviously OSM not only contains the streets and highways, but also information that can be useful when we use a map such as locations of hospitals or gas stations.","tags":["database","overpass API","OSM","Point of interest"],"title":"Accessing OpenStreetMap data with R","type":"post"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","J Bech","FS Rodrigo"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"4de1ab5836dd086cdc9d0fa45b7fbc5c","permalink":"https://dominicroye.github.io/en/publication/appliedgeophysics_2017/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/en/publication/appliedgeophysics_2017/","section":"publication","summary":"The Eastern Mediterranean is one of the most prominent hot spots of climate change in the world and extreme climatic phenomena in this region such as drought or extreme rainfall events are expected to become more frequent and intense. In this study climate extreme indices recommended by the joint World Meteorological Organization Expert Team on Climate Change Detection and Indices are calculated for daily precipitation data in 70 weather stations during 1961–2012. Observed trends and changes in daily precipitation extremes over the EM basin were analysed using the RClimDex package, which was developed by the Climate Research Branch of the Meteorological Service of Canada. Extreme and heavy precipitation events showed globally a statistically significant decrease in the Eastern Mediterranean and, in the southern parts, a significant decrease in total precipitation. The overall analysis of extreme precipitation indices reveals that decreasing trends are generally more frequent than increasing trends. We found statistically significant decreasing trends (reaching 74% of stations for extremely wet days) and increasing trends (reaching 36% of stations for number of very heavy precipitation days). Finally, most of the extreme precipitation indices have a statistically significant positive correlation with annual precipitation, particularly the number of heavy and very heavy precipitation days.","tags":["Eastern Mediterranean","extreme precipitation","trend","spatial temporal distribution"],"title":"Observed Changes in Daily Precipitation Extremes at Annual Timescale Over the Eastern Mediterranean During 1961–2012","type":"publication"},{"authors":null,"categories":["R","R:intermediate"],"content":"\r\r1 Introduction\r2 NCEP\r2.1 Packages\r2.2 Data download\r2.3 Monthly average\r2.4 Visualization\r\r3 ERA-Interim\r3.1 Installation\r3.2 Connection and download with the ECMWF API\r3.3 Processing ncdf\r\r4 Update for accessing ERA-5\r\r\rA friend advised me to introduce R levels as categories. An idea that I now add to each blog post. There are three levels: elementary, intermediate, and advanced. I hope it will help the reader and the R user.\n1 Introduction\rIn this post, I will show how we can download and work directly with data from climatic reanalysis in R. These kind of datasets are a combination of forcast models and data assimilation systems, which allows us to create corrected global grids of recent history of the atmosphere, land surface, and oceans. The two most used reanalyses are NCEP-DO (Reanalysis II) from the NOAA/OAR/ESRL, an improved version of NCEP-NCAR (Reanalysis I), and ERA-Interim from the ECMWF. Since NCEP-DO is the first generation, it is recommended to use third-generation climate reanalysis, especially ERA-Interim. An overview of the current atmospheric reanalysis can be found here. First, let’s see how to access the NCEP data through an R library on CRAN that facilitates the download and handling of the data. Then we will do the same with the ERA-Interim, however, to access this last reanalysis dataset it is necessary to use python and the corresponding API of the ECMWF.\n\r2 NCEP\rTo access the NCEP reanalysis it is required to install the corresponding package RNCEP. The main function is NCEP.gather( ). The resolution of the NCEP reanalysis is 2.5º X 2.5º.\n2.1 Packages\r#install the RNCEP, lubridate and tidyverse packages\rif(!require(\u0026quot;RNCEP\u0026quot;)) install.packages(\u0026quot;RNCEP\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\r#load the packages\rlibrary(RNCEP)\rlibrary(lubridate) #date and time manipulation\rlibrary(tidyverse) #data manipulation and visualization\rlibrary(RColorBrewer) #color schemes\rlibrary(sf) #to import a spatial object and to work with geom_sf in ggplot2\r\r2.2 Data download\rWe will download the air temperature of the 850haPa pressure level for the year 2016. The variables and pressure levels can be found in the details of the function ?NCEP.gather. The reanalysis2 argument allows us to download both version I and version II, being by default FALSE, that is, we access reanalysis I. In all the requests we will obtain data of every 6 hours (00:00, 06:00, 12:00 and 18:00). This supposes a total of 1464 values for the year 2016.\n#define the necessary arguments\rmonth_range \u0026lt;- c(1,12) #period of months\ryear_range \u0026lt;- c(2016,2016) #period of years\rlat_range \u0026lt;- c(30,60) #latitude range\rlon_range \u0026lt;- c(-30,50) #longitude range\rdata \u0026lt;- NCEP.gather(\u0026quot;air\u0026quot;, #name of the variable\r850, #pressure level 850hPa\rmonth_range,year_range,\rlat_range,lon_range,\rreturn.units = TRUE,\rreanalysis2=TRUE)\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r#dimensions dim(data) \r## [1] 13 33 1464\r#we find lon, lat and time with dimnames()\r#date and time\rdate_time \u0026lt;- dimnames(data)[[3]]\rdate_time \u0026lt;- ymd_h(date_time)\rhead(date_time)\r## [1] \u0026quot;2016-01-01 00:00:00 UTC\u0026quot; \u0026quot;2016-01-01 06:00:00 UTC\u0026quot;\r## [3] \u0026quot;2016-01-01 12:00:00 UTC\u0026quot; \u0026quot;2016-01-01 18:00:00 UTC\u0026quot;\r## [5] \u0026quot;2016-01-02 00:00:00 UTC\u0026quot; \u0026quot;2016-01-02 06:00:00 UTC\u0026quot;\r#longitude and latitude\rlat \u0026lt;- dimnames(data)[[1]]\rlon \u0026lt;- dimnames(data)[[2]]\rhead(lon);head(lat)\r## [1] \u0026quot;-30\u0026quot; \u0026quot;-27.5\u0026quot; \u0026quot;-25\u0026quot; \u0026quot;-22.5\u0026quot; \u0026quot;-20\u0026quot; \u0026quot;-17.5\u0026quot;\r## [1] \u0026quot;60\u0026quot; \u0026quot;57.5\u0026quot; \u0026quot;55\u0026quot; \u0026quot;52.5\u0026quot; \u0026quot;50\u0026quot; \u0026quot;47.5\u0026quot;\r\r2.3 Monthly average\rWe see that the downloaded data is an array of three dimensions with [lat, lon, time]. As above mentioned, we extracted latitude, longitude and time. The temperature is given in Kelvin. The objective in the next section will be to show two maps comparing January and July.\n#create our grouping variable\rgroup \u0026lt;- month(date_time) #estimate the average temperature by month data_month \u0026lt;- aperm(\rapply(\rdata, #our data\rc(1,2), #apply to each time series 1:row, 2:column a the mean( ) function\rby, #group by\rgroup, #months\rfunction(x)ifelse(all(is.na(x)),NA,mean(x))),\rc(2,3,1)) #reorder to get an array like the original\rdim(data_month) #850haPa temperature per month January to December\r## [1] 13 33 12\r\r2.4 Visualization\rOnce we got here, we can visualize the 850hPa temperature of January and July with ggplot2. In this example, I use geom_sf( ) from the library sf, which makes the work easier to visualize spatial objects in ggplot (in the near future I will make a post about sf and ggplot). In the dimension of latitude and longitude we saw that it only indicates a value for each row and column. But we need the coordinates of all the cells in the matrix. To create all combinations between two variables we can use the expand.grid( ) function.\n#first we create all the combinations of lon-lat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#as lonlat was a row/column name, it is character, that\u0026#39;s why we convert it into numeric\rlonlat \u0026lt;- apply(lonlat,2,as.numeric)\r#lon and lat are not in the order as we expect\r#row=lon; column=lat\rdata_month \u0026lt;- aperm(data_month,c(2,1,3))\r#subtract 273.15K to convert K to ºC.\rdf \u0026lt;- data.frame(lonlat,\rTa01=as.vector(data_month[,,1])-273.15,\rTa07=as.vector(data_month[,,7])-273.15)\rBefore we can make the map with ggplot2, we have to adapt the table. The shapefile with the countries limits can be downloaded here.\n#convert the wide table into a long one\rdf \u0026lt;- gather(df,month,Ta,Ta01:Ta07)%\u0026gt;%\rmutate(month=factor(month,unique(month),c(\u0026quot;Jan\u0026quot;,\u0026quot;Jul\u0026quot;)))\r#import the countries limits\rlimit \u0026lt;- st_read(\u0026quot;CNTR_RG_03M_2014.shp\u0026quot;)\r## Reading layer `CNTR_RG_03M_2014\u0026#39; from data source `C:\\Users\\xeo19\\Documents\\GitHub\\blogR_update\\content\\post\\en\\2018-09-15-access-to-climate-reanalysis-data-from-r\\CNTR_RG_03M_2014.shp\u0026#39; using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 256 features and 3 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.66068\r## epsg (SRID): NA\r## proj4string: +proj=longlat +ellps=GRS80 +no_defs\r#color scheme\rcolbr \u0026lt;- brewer.pal(11,\u0026quot;RdBu\u0026quot;)\rggplot(df)+\rgeom_tile(aes(lon,lat,fill=Ta))+ #temperature data\rgeom_sf(data=limit,fill=NA,size=.5)+ #limits scale_fill_gradientn(colours=rev(colbr))+\rcoord_sf(ylim=c(30,60),xlim=c(-30,50))+\rscale_x_continuous(breaks=seq(-30,50,10),expand=c(0,0))+\rscale_y_continuous(breaks=seq(30,60,5),expand=c(0,0))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Ta 850hPa (ºC)\u0026quot;)+\rfacet_grid(month~.)+ #plot panels by month\rtheme_bw()\r\r\r3 ERA-Interim\rThe ECMWF offers access to its public databases from a pyhton-API. It is required to be registered on the ECMWF website. You can register here. When dealing with another programming language, in R we have to use an interface between both which allows the library reticulate. We must also have installed a pyhton distribution (version 2.x or 3.x). In the case of Windows we can use anaconda.\nRecently a new package called ecmwfr has been published that facilitates accessing the Copernicus and ECMWF APIs. The major advantage is that it is not necessary to install python. More details here.\r 3.1 Installation\rif(!require(\u0026quot;reticulate\u0026quot;)) install.packages(\u0026quot;reticulate\u0026quot;)\rif(!require(\u0026quot;ncdf4\u0026quot;)) install.packages(\u0026quot;ncdf4\u0026quot;) #to manage netCDF format\r#load packages\rlibrary(reticulate)\rlibrary(ncdf4)\rOnce we have installed anaconda and the package reticulate, we can install the library python ecmwfapi. We can carry out the installation, or through the Windows CMD using the command conda install -c conda-forge ecmwf-api-client, or with the R function py_install( ) from the reticulate package. The same function allows us to install any python library from R.\n#install the python ECMWF API\rpy_install(\u0026quot;ecmwf-api-client\u0026quot;)\r\r3.2 Connection and download with the ECMWF API\rIn order to access the API, it is required to create a file with the user’s information.\nThe “.ecmwfapirc” file must contain the following information:\n{\r\u0026quot;url\u0026quot; : \u0026quot;https://api.ecmwf.int/v1\u0026quot;,\r\u0026quot;key\u0026quot; : \u0026quot;XXXXXXXXXXXXXXXXXXXXXX\u0026quot;,\r\u0026quot;email\u0026quot; : \u0026quot;john.smith@example.com\u0026quot;\r}\rThe key can be obtained with the user account here.\nThe file can be created with the Windows notebook.\nWe create a document “ecmwfapirc.txt”.\rRename this file to “.ecmwfapirc.”\r\rThe last point disappears automatically. Then we save this file in “C:/USERNAME/.ecmwfapirc” or “C:/USERNAME/Documents/.ecmwfapirc”.\n#import the python library ecmwfapi\recmwf \u0026lt;- import(\u0026#39;ecmwfapi\u0026#39;)\r#for this step there must exist the file .ecmwfapirc\rserver = ecmwf$ECMWFDataServer() #start the connection\rOne we get here, how do we create a query? The easiest thing is to go to the website of ECMWF, where we choose the database, in this case ERA-Interim surface, to create a script with all the necessary data. More details about the syntax can be found here. When we proceed on the website, we only have to click on “View MARS Request”. This step takes us to the script in python.\nWith the syntax of the script from the MARS Request, we can create the query in R.\n#we create the query\rquery \u0026lt;-r_to_py(list(\rclass=\u0026#39;ei\u0026#39;,\rdataset= \u0026quot;interim\u0026quot;, #dataset\rdate= \u0026quot;2017-01-01/to/2017-12-31\u0026quot;, #time period\rexpver= \u0026quot;1\u0026quot;,\rgrid= \u0026quot;0.125/0.125\u0026quot;, #resolution\rlevtype=\u0026quot;sfc\u0026quot;,\rparam= \u0026quot;167.128\u0026quot;, # air temperature (2m)\rarea=\u0026quot;45/-10/30/5\u0026quot;, #N/W/S/E\rstep= \u0026quot;0\u0026quot;,\rstream=\u0026quot;oper\u0026quot;,\rtime=\u0026quot;00:00:00/06:00:00/12:00:00/18:00:00\u0026quot;, #hours\rtype=\u0026quot;an\u0026quot;,\rformat= \u0026quot;netcdf\u0026quot;, #format\rtarget=\u0026#39;ta2017.nc\u0026#39; #file name\r))\r#query to get the ncdf\rserver$retrieve(query)\rThe result is a netCDF file that we can process with the library ncdf4.\n\r3.3 Processing ncdf\rIn the next section, the objective will be the extraction of a time serie from the closest coordinate to a given one. We will use the coordinates of Madrid (40.418889, -3.691944).\n#load packages\rlibrary(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\r#open the connection with the ncdf file\rnc \u0026lt;- nc_open(\u0026quot;ta2017.nc\u0026quot;)\r#extract lon and lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r## [1] 121\r## [1] 121\r#extract the time\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#time unit: hours since 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r## $units\r## [1] \u0026quot;hours since 1900-01-01 00:00:00.0\u0026quot;\r## ## $long_name\r## [1] \u0026quot;time\u0026quot;\r## ## $calendar\r## [1] \u0026quot;gregorian\u0026quot;\r#convert the hours into date + hour\r#as_datetime() function of the lubridate package needs seconds\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#import the data\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#close the conection with the ncdf file\rnc_close(nc)\rIn this next section we use the sf package, which is replacing the well known sp and rgdal packages.\n#create all the combinations of lon-lat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#we must convert the coordinates in a spatial object sf\r#we also indicate the coordinate system in EPSG code\rcoord \u0026lt;- st_as_sf(lonlat,coords=c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;))%\u0026gt;%\rst_set_crs(4326)\r#we do the same with our coordinate of Madrid\rpsj \u0026lt;- st_point(c(-3.691944,40.418889))%\u0026gt;%\rst_sfc()%\u0026gt;%\rst_set_crs(4326)\r#plot all points\rplot(st_geometry(coord))\rplot(psj,add=TRUE,pch = 3, col = \u0026#39;red\u0026#39;)\rIn the next steps we calculate the distance of our reference point to all the grid points. Then we look for the one with less distance.\n#add the distance to the points\rcoord \u0026lt;- mutate(coord,dist=st_distance(coord,psj))\r#create a distance matrix with the same dimensions as our data\rdist_mat \u0026lt;- matrix(coord$dist,dim(data)[-3])\r#the arrayInd function is useful to obtain the row and column indexes\rmat_index \u0026lt;- as.vector(arrayInd(which.min(dist_mat), dim(dist_mat)))\r#we extract the time serie and change the unit from K to ºC\r#we convert the time in date + hour\rdf \u0026lt;- data.frame(ta=data[mat_index[1],mat_index[2],],time=timestamp)%\u0026gt;%\rmutate(ta=ta-273.15,time=ymd_hms(time))\rFinally, we visualize our time series.\nggplot(df,\raes(time,ta))+\rgeom_line()+\rlabs(y=\u0026quot;Temperature (ºC)\u0026quot;,\rx=\u0026quot;\u0026quot;)+\rtheme_bw()\r\r\r4 Update for accessing ERA-5\rRecently the new reanalysis ERA-5 with single level or pressure level was made available to users. It is the fifth generation of the European Center for Medium-Range Weather Forecasts (ECMWF) and accessible through a new Copernicus API. The ERA-5 reanalysis has a temporary coverage from 1950 to the present at a horizontal resolution of 30km worldwide, with 137 levels from the surface to a height of 80km. An important difference with respect to the previous ERA-Interim is the temporal resolution with hourly data.\nThe access changes to the Climate Data Store (CDS) infrastructure with its own API. It is possible to download directly from the web or using the Python API in a similar way to the one already presented in this post. However, there are slight differences which I will explain below.\nIt is necessary to have a Copernicus CDS account link\rAgain, you need a account key link\rThere are changes in the Python library and in some arguments of the query.\r\r#load libraries library(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\rlibrary(reticulate)\r#install the CDS API\rconda_install(\u0026quot;r-reticulate\u0026quot;,\u0026quot;cdsapi\u0026quot;, pip=TRUE)\rTo be able to access the API, a requirement is to create a file with the user’s information.\nThe “.cdsapirc” file must contain the following information:\n\rurl: https://cds.climate.copernicus.eu/api/v2\rkey: {uid}:{api-key}\r\rThe key can be obtained with the user account in the User profile.\nThe file can be created in the same way as it has been explained for ERA-Interim.\n#import python CDS-API\rcdsapi \u0026lt;- import(\u0026#39;cdsapi\u0026#39;)\r#for this step there must exist the file .cdsapirc\rserver = cdsapi$Client() #start the connection\rWith the syntax of the script from the Show API request single level, we can create the query in R.\n#we create the query\rquery \u0026lt;- r_to_py(list(\rvariable= \u0026quot;2m_temperature\u0026quot;,\rproduct_type= \u0026quot;reanalysis\u0026quot;,\ryear= \u0026quot;2018\u0026quot;,\rmonth= \u0026quot;07\u0026quot;, #formato: \u0026quot;01\u0026quot;,\u0026quot;01\u0026quot;, etc.\rday= str_pad(1:31,2,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;), time= str_c(0:23,\u0026quot;00\u0026quot;,sep=\u0026quot;:\u0026quot;)%\u0026gt;%str_pad(5,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;),\rformat= \u0026quot;netcdf\u0026quot;,\rarea = \u0026quot;45/-20/35/5\u0026quot; # North, West, South, East\r))\r#query to get the ncdf\rserver$retrieve(\u0026quot;reanalysis-era5-single-levels\u0026quot;,\rquery,\r\u0026quot;era5_ta_2018.nc\u0026quot;)\rIt is possible that the first time an error message is received, given that the required terms and conditions have not yet been accepted. Simply, the indicated link should be followed.\nError in py_call_impl(callable, dots$args, dots$keywords) : Exception: Client has not agreed to the required terms and conditions.. To access this resource, you first need to accept the termsof \u0026#39;Licence to Use Copernicus Products\u0026#39; at https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products\rFrom here we can follow the same steps as with ERA-Interim.\n#open the connection with the file\rnc \u0026lt;- nc_open(\u0026quot;era5_ta_2018.nc\u0026quot;)\r#extract lon, lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r## [1] 41\r## [1] 101\r#extract time\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#time unit: hours from 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r## $units\r## [1] \u0026quot;hours since 1900-01-01 00:00:00.0\u0026quot;\r## ## $long_name\r## [1] \u0026quot;time\u0026quot;\r## ## $calendar\r## [1] \u0026quot;gregorian\u0026quot;\r#we convert the hours into date+time #as_datetime from lubridate needs seconds\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#temperatures in K from july 2018\rhead(timestamp)\r## [1] \u0026quot;2018-07-01 00:00:00 UTC\u0026quot; \u0026quot;2018-07-01 01:00:00 UTC\u0026quot;\r## [3] \u0026quot;2018-07-01 02:00:00 UTC\u0026quot; \u0026quot;2018-07-01 03:00:00 UTC\u0026quot;\r## [5] \u0026quot;2018-07-01 04:00:00 UTC\u0026quot; \u0026quot;2018-07-01 05:00:00 UTC\u0026quot;\r#import temperature data\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#plot 2018-07-01\rfilled.contour(data[,,1])\r#time serie plot for a pixel\rplot(data.frame(date=timestamp,\rta=data[1,5,]),\rtype=\u0026quot;l\u0026quot;)\r#close the conection with the ncdf file\rnc_close(nc)\r\r","date":1537005584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537005584,"objectID":"216967a9625628bc1857d3728930e987","permalink":"https://dominicroye.github.io/en/2018/access-to-climate-reanalysis-data-from-r/","publishdate":"2018-09-15T10:59:44+01:00","relpermalink":"/en/2018/access-to-climate-reanalysis-data-from-r/","section":"post","summary":"In this post, I will show how we can download and work directly with data from climatic reanalysis in R. These kind of datasets are combination of forcast models and data assimilation systems, which allows us to create corrected global grids of recent history of the atmosphere, land surface, and oceans.","tags":["reanalisis","interim","NCEP/NCAR","era","download","ncdf","access","api","python","ECMWF"],"title":"Access to climate reanalysis data from R","type":"post"},{"authors":null,"categories":["datavis","R","R:elementary"],"content":"\r\rWelcome to my blog! I am Dominic Royé, researcher and lecturer of physical geography at the University of Santiago de Compostela. One of my passions is R programming to visualize and analyze any type of data. Hence, my idea of this blog has its origin in my datavis publications I have been cooking in the last year on Twitter on different topics describing the world. In addition, I would like to take advantage of the blog and publish short introductions and explanation on data visualization, management and manipulation in R. I hope you like it. Any suggestion or ideas are welcomed.\nBackground\rI have always wanted to write about the use of the pie chart. The pie chart is widely used in research, teaching, journalism or technical reports. I do not know if it is due to Excel, but even worse than the pie chart itself, is its 3D version (the same for the bar chart). About the 3D versions, I only want to say that they are not recommended, since in these cases the third dimension does not contain any information and therefore it does not help to correctly read the information of the graphic. Regarding the pie chart, among many experts its use is not advised. But why?\nAlready in a study conducted by Simkin (1987) they found that the interpretation and processing of angles is more difficult than that of linear forms. Mostly it is easier to read a bar chart than a pie chart. A problem that becomes very visible when we have; 1) too many categories 2) few differences between categories 3) a misuse of colors as legend or 4) comparisons between various pie charts.\nIn general, to decide what possible graphic representations exist for our data, I recommend using the website www.data-to-viz.com or the Financial Times Visual Vocabulary.\n\nWell, now what alternative ways can we use in R?\n\rAlternatives to the pie chart\rThe dataset we will use about the vaccination status of measles correspond to June 2018 in Europe and come from the ECDC.\n#packages\rlibrary(tidyverse)\rlibrary(scales)\rlibrary(RColorBrewer)\r#data\rmeasles \u0026lt;- data.frame(\rvacc_status=c(\u0026quot;Unvaccinated\u0026quot;,\u0026quot;1 Dose\u0026quot;,\r\u0026quot;\u0026gt;= 2 Dose\u0026quot;,\u0026quot;Unkown Dose\u0026quot;,\u0026quot;Unkown\u0026quot;),\rprop=c(0.75,0.091,0.05,0.012,0.096)\r)\r#we order from the highest to the lowest and fix it with a factor\rmeasles \u0026lt;- arrange(measles,\rdesc(prop))%\u0026gt;%\rmutate(vacc_status=factor(vacc_status,vacc_status))\r\r\rvacc_status\rprop\r\r\r\rUnvaccinated\r0.750\r\rUnkown\r0.096\r\r1 Dose\r0.091\r\r\u0026gt;= 2 Dose\r0.050\r\rUnkown Dose\r0.012\r\r\r\rBar plot or similar\rggplot(measles,aes(vacc_status,prop))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\rggplot(measles,aes(x=vacc_status,prop,ymin=0,ymax=prop))+\rgeom_pointrange()+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\r#custom themes definitions\rtheme_singlebar \u0026lt;- theme_bw()+\rtheme(\rlegend.position = \u0026quot;bottom\u0026quot;,\raxis.title = element_blank(),\raxis.ticks.y = element_blank(),\raxis.text.y = element_blank(),\rpanel.border = element_blank(),\rpanel.grid=element_blank(),\rplot.title=element_text(size=14, face=\u0026quot;bold\u0026quot;)\r)\r#plot\rmutate(measles,\rvacc_status=factor(vacc_status, #we change the order of the categories\rrev(levels(vacc_status))))%\u0026gt;%\rggplot(aes(1,prop,fill=vacc_status))+ #we put 1 in x to create a single bar\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent,\rlimits=c(0,1),\rexpand=c(.01,.01))+\rscale_x_continuous(expand=c(0,0))+\rscale_fill_brewer(\u0026quot;\u0026quot;,palette=\u0026quot;Set1\u0026quot;)+\rcoord_flip()+\rtheme_singlebar\r#we expand our data with numbers from Italy\rmeasles2 \u0026lt;- mutate(measles,\ritaly=c(0.826,0.081,0.053,0.013,0.027),\rvacc_status=factor(vacc_status,rev(levels(vacc_status))))%\u0026gt;%\rrename(europe=\u0026quot;prop\u0026quot;)%\u0026gt;%\rgather(region,prop,europe:italy)\r#plot\rggplot(measles2,aes(region,prop,fill=vacc_status))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,position=\u0026quot;stack\u0026quot;)+ #stack bar\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1),\rexpand=c(0,0))+\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;)+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Vaccination Status\u0026quot;)+\rtheme_minimal()\r\rWaffle plot\r#package\rlibrary(waffle)\r#the waffle function uses a vector with names\rval_measles \u0026lt;- round(measles$prop*100)\rnames(val_measles) \u0026lt;- measles$vacc_status\r#plot\rwaffle(val_measles, #data\rcolors=brewer.pal(5,\u0026quot;Set1\u0026quot;), #colors\rrows=5) #row number \rThe Waffle chart seems very interesting to me when we want to show a proportion of an individual category.\n#data\rmedida \u0026lt;- c(41,59) #data from the OECD 2015\rnames(medida) \u0026lt;- c(\u0026quot;Estudios Superiores\u0026quot;,\u0026quot;Otros estudios\u0026quot;)\r#plot\rwaffle(medida,\rcolors=c(\u0026quot;#377eb8\u0026quot;,\u0026quot;#bdbdbd\u0026quot;),\rrows=5)\r\rTreemap\r#package\rlibrary(treemap)\r#plot\rtreemap(measles,\rindex=\u0026quot;vacc_status\u0026quot;, #variable with categories\rvSize=\u0026quot;prop\u0026quot;, #values\rtype=\u0026quot;index\u0026quot;, #style more in ?treemap\rtitle=\u0026quot;\u0026quot;, palette = brewer.pal(5,\u0026quot;Set1\u0026quot;) #colors\r)\rPersonally, I think that all types of graphic representations have their advantages and disadvantages. However, we currently have a huge variety of alternatives to avoid using the pie chart. If you still want to make a pie chart, which I would not rule out either, I recommend following certain rules, which you can find very well summarized in a recent post by Lisa Charlotte Rost. For example, you should order from the highest to the lowest unless there is a natural order or use a maximum of five categories. Finally, I leave you a link to a cheat sheet from policyviz with basic rules of data visualization. A good reference on graphics using different programs from Excel to R can be found in the book Creating More Effective Graphs (Robbins 2013).\n\rReferences\r\r\r","date":1534896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534896000,"objectID":"3a5ca713880df90e42e9c9eabcbda7c3","permalink":"https://dominicroye.github.io/en/2018/the-pie-chart/","publishdate":"2018-08-22T00:00:00Z","relpermalink":"/en/2018/the-pie-chart/","section":"post","summary":"Welcome to my blog! I am Dominic Royé, researcher and lecturer of physical geography at the University of Santiago de Compostela. One of my passions is R programming to visualize and analyze any type of data. Hence, my idea of this blog has its origin in my datavis publications I have been cooking in the last year on Twitter on different topics describing the world. In addition, I would like to take advantage of the blog and publish short introductions and explanation on data visualization, management and manipulation in R.","tags":["pie chart","data","circular","proportions","first post","treemap","waffle","bar"],"title":"the pie chart","type":"post"},{"authors":["D Royé","A Figueiras","M Taracido-Trunk"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"8c48be3d65e4bed0ce4fe5203eb06bb0","permalink":"https://dominicroye.github.io/en/publication/pharma_coru%C3%B1a_2018/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/en/publication/pharma_coru%C3%B1a_2018/","section":"publication","summary":"The consumption of medication, especially over-the-counter (OTC) drugs, can reflect environmental exposure with a lesser degree of severity in terms of morbidity. The non-linear effects of maximum and minimum apparent temperature on respiratory drug sales in A Coruña from 2006 to 2010 were examined using a distributed lag non-linear model. In particular, low apparent temperatures proved to be associated with increased sales of respiratory drugs. The strongest consistent risk estimates were found for minimum apparent temperatures in respiratory drug sales with an increase of 33.4% (95% CI: 12.5-58.0%) when the temperature changed from 2.8 ºC to −1.4 ºC. These findings may serve to guide the planning of public health interventions in order to predict and manage the health effects of exposure to the thermal environment for lower degrees of morbidity. More precisely, significant increases in the use of measured OTC medication could be used to identify and anticipate influenza outbreaks due to a more sensitive degree of the data source.","tags":["drug sales","pharmacoepidemiology","respiratory cause","short‐term effects","Spain","thermal environment"],"title":"Short-term effects of heat and cold on respiratory drug use. A time-series epidemiological study in A Coruña, Spain","type":"publication"},{"authors":["D Royé","N Lorenzo","J Martin-Vide"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"fd71c0d62978df70b5d41813df70acdf","permalink":"https://dominicroye.github.io/en/publication/lightning_galicia_2018/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/en/publication/lightning_galicia_2018/","section":"publication","summary":"The spatial-temporal patterns of cloud-to-ground (CG) lightning covering the period 2010-2015 over the northwest Iberian Peninsula were investigated. The analysis conducted employed three main methods: the circulation weather types developed by Jenkinson \u0026 Collison, the fit of a generalized additive model for geographic variables and the use of a concentration index for the ratio of lightning strikes and thunderstorm days. The main activity in the summer months can be attributed to situations with eastern or anticyclonic flow due to convection by insolation. In winter, lightning proves to have a frontal origin and is mainly associated with western or cyclonic flow situations which occur with advections of air masses of maritime origin. The largest number of CG discharges occurs under eastern flow and their hybrids with anticyclonic situations. Thunderstorms with greater CG lightning activity, highlighted by a higher Concentration Index, are located in areas with a higher density of lightning strikes, above all in mountainous areas away from the sea. The modeling of lightning density with geographic variables shows the positive influence of altitude and, particularly, distance to the sea, with nonlinear relationships due to the complex orography of the region. Likewise, areas with convex topography receive more lightning strikes than concave ones, a relation which has been demonstrated for the first time from a Generalized Additive Model (GAM).","tags":["thunderstorm","Iberian Peninsula","Concentration Index","weather types","Convexity Index","Generalized Additive Model","cloud-to-ground lightning"],"title":"Spatial–temporal patterns of cloud-to-ground lightning over the northwest Iberian Peninsula during the period 2010–2015","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"735acb4c3b633c5b3cfad91a943dfb61","permalink":"https://dominicroye.github.io/en/publication/hotnights_bcn_2017/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/en/publication/hotnights_bcn_2017/","section":"publication","summary":"Heat-related effects on mortality have been widely analyzed using maximum and minimum temperatures as exposure variables. Nevertheless, the main focus is usually on the former with the minimum temperature being limited in use as far as human health effects are concerned. Therefore, new thermal indices were used in this research to describe the duration of night hours with air temperatures higher than the 95% percentile of the minimum temperature (Hot Night hours) and intensity as the summation of these air temperatures in degrees (Hot Night degrees). An exposure-response relationship between mortality due to natural, respiratory and cardiovascular causes and summer night temperatures was assessed using data from the Barcelona region between 2003 and 2013. The non-linear relationship between the exposure and response variables was modeled using a distributed lag non-linear model. The estimated associations for both exposure variables and mortality shows a relationship with high and medium values that persist significantly up to a lag of 1–2 days. In mortality due to natural causes an increase of 1.1% per 10% (CI95% 0.6–1.5) for Hot Night hours and 5.8% per each 10º (CI95% 3.5–8.2%) for Hot Night degrees is observed. The effects of Hot Night hours reach their maximum with 100% and leads to an increase by 9.2% (CI95% 5.3–13.1%). The hourly description of night heat effects reduced to a single indicator in duration and intensity is a new approach and shows a different perspective and significant heat-related effects on human health.","tags":["heat","mortality","tropical night","hot night","effects","human health","climate change"],"title":"The effects of hot nights on mortality in Barcelona, Spain","type":"publication"},{"authors":["D Royé","J Martin-Vide"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"83752c0287cd3c7c14fd70c98985fa6b","permalink":"https://dominicroye.github.io/en/publication/usa_ci_2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/en/publication/usa_ci_2017/","section":"publication","summary":"The contiguous US exhibits a wide variety of precipitation regimes, first, because of the wide range of latitudes and altitudes. The physiographic units with a basic meridional configuration contribute to the differentiation between east and west in the country while generating some large interior continental spaces. The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. Positive exponential curves, which plot the cumulative percentages of days with precipitation against the cumulative percentage of the rainfall amounts that they contribute, can be evaluated through the Concentration Index. The Concentration Index has been applied to the contiguous United States using a gridded climate dataset of daily precipitation data, at a resolution of 0.25°, provided by CPC/NOAA/OAR/Earth System Research Laboratory, for the period between 1956 and 2006. At the same time, other rainfall indices and variables such as the annual coefficient of variation, seasonal rainfall regimes and the probabilities of a day with precipitation have been presented with a view to explaining spatial CI patterns. The spatial distribution of the CI in the contiguous United States is geographically consistent, reflecting the principal physiographic and climatic units of the country. Likewise, linear correlations have been established between the CI and geographical factors such as latitude, longitude and altitude. In the latter case the Pearson correlation coefficient (r) between this factor and the CI is −0.51 (p-value ","tags":["Concentration Index","Contiguous United States","daily precipitation","precipitation indices","spatial–temporal patterns"],"title":"Concentration of Daily Precipitation in the Contiguous United States","type":"publication"},{"authors":["P Fdez-Arroyabe","D Royé"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"389371f80a93c479ccb639ee58c2c7bf","permalink":"https://dominicroye.github.io/en/publication/chapter_springer_2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/publication/chapter_springer_2017/","section":"publication","summary":"Co-creation of scientific knowledge based on new technologies and big data sources is one of the main challenges for the digital society in the XXI century. Data management and the analysis of patterns among datasets based on machine learning and artificial intelligence has become essential for many sectors nowadays. The development of real time health-related climate services represents an example where abundant structured and unstructured information and transdisciplinary research are needed. The study of the interactions between atmospheric processes and human health through a big data approach can reveal the hidden value of data. The Oxyalert technological platform is presented as an example of a digital biometeorological infrastructure able to forecast, at an individual level, oxygen changes impacts on human health.","tags":["co-creation","interdisciplinarity","transdisciplinarity","morbidity","climate services","digital divide","big data","health"],"title":"Co-creation and Participatory Design of Big Data Infrastructures on the Field of Human Health Related Climate Services","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"f87f687a4a74b8e7fc56e41604e72c84","permalink":"https://dominicroye.github.io/en/more/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/en/more/","section":"","summary":"","tags":null,"title":"More","type":"page"},{"authors":["D Royé","J Taboada","A Ezpeleta-Martí","N Lorenzo"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"c3bdb9563e443f7fc44c7b7eeece88b4","permalink":"https://dominicroye.github.io/en/publication/cwt_galicia_resp_2016/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/en/publication/cwt_galicia_resp_2016/","section":"publication","summary":"The link between various pathologies and atmospheric conditions has been a constant topic of study over recent decades in many places across the world; knowing more about it enables us to pre-empt the worsening of certain diseases, thereby optimizing medical resources. This study looked specifically at the connections in winter between respiratory diseases and types of atmospheric weather conditions (Circulation Weather Types, CWT) in Galicia, a region in the north-western corner of the Iberian Peninsula. To do this, the study used hospital admission data associated with these pathologies as well as an automatic classification of weather types. The main result obtained was that weather types giving rise to an increase in admissions due to these diseases are those associated with cold, dry weather, such as those in the east and south-east, or anticyclonic types. A second peak was associated with humid, hotter weather, generally linked to south-west weather types. In the future, this result may help to forecast the increase in respiratory pathologies in the region some days in advance.","tags":["weather type","respiratory diseases","hospital admissions","human health","Spain"],"title":"Winter circulation weather types and hospital admissions for respiratory diseases in Galicia, Spain","type":"publication"},{"authors":["D Royé","A Ezpeleta-Martí"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"48e40344ca1e4f7a2ab2a6c4510aa260","permalink":"https://dominicroye.github.io/en/publication/hotnights_age_2015/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/en/publication/hotnights_age_2015/","section":"publication","summary":"Analysis of tropical nights on the Atlantic coast of the Iberian peninsula. A proposed methodology. This paper presents a new methodology for the study of warm nights, also called «tropical», in Galicia and Portugal in order to identify those nights where people can be affected by heat stress. The use of two indicators obtained through half-hourly data has allowed us to define in more detail the thermal characteristics of the nights between May and October, thereby being able to more accurately assess the risk to the health and well-being of the population. There is a significant increase in the frequency of tropical nights and warm nights on the Atlantic coast, from the north of Galicia to the south of Portugal. The lower latitude and proximity to the coastline are associated with greater persistence of heat and thermal stress during these nights. In inland areas the persistence is less. The warmest nights are more frequent and intense in centres of the cities, due to the effect of the urban heat island.","tags":["tropical night","thermic stress","heat island","Galicia","Portugal"],"title":"Analysis of tropical nights on the atlantic coast of the Iberian Peninsula. A proposed methodology","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"Used datasets are available for download here. Alternative datasets for Spain in ncdf format can be downloaded:\nAEMET\n Gridded 20km and 50km (precipitation and temperature)\n Gridded 5km (precipitation)\n  CSIC\n Gridded 5km (precipitation)  ","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"28a50037c73f6b009121b5250e4fe2d1","permalink":"https://dominicroye.github.io/en/publication/ncdf_2015/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/en/publication/ncdf_2015/","section":"publication","summary":"A practical introduction in the use of netCDF in the environment of R Spatio-temporal data is currently key to many disciplines, especially to climatology and meteorology. A widespread format is netCDF allowing a multidimensional structure and an exchange of data machine independently. In this article, we introduce the use of these databases with the free software environment R. To do this, we will work with a grid of the maximum temperature of the Iberian Peninsula for the period 1971-2007. The goal is to read and visualize the netCDF format, and make some fist overall and specifi calculations. Finally the applicability is shown in a case study: the diurnal temperature variation in the Iberian Peninsula for January and August 2006. (Spanish)","tags":["netCDF","R","climatology","temperature","matrix","database"],"title":"The use of climate databases netCDF with array structure in the environment of R","type":"publication"}]