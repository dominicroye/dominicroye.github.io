[{"authors":["F Tedim","V Leone","M Coughlan","C Bouillon","G Xanthopoulos","D Royé","F.J.M. Correia","C Ferreira"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"bd42afa22ca6d6c53067fc30b08ca937","permalink":"/en/publication/chapter_elsevier_2019/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/en/publication/chapter_elsevier_2019/","section":"publication","summary":"Extreme wildfires events (EWEs) represent a minority among all wildfires but are a true challenge for societies as they exceed the current control capacity even in the best prepared regions of the world and they create destruction and a disproportionately number of fatalities. Recent events in Portugal, Chile, Greece, Australia, Canada, and the USA provide evidence that EWEs are an escalating worldwide problem, exceeding all previous records. Despite the challenges put by climate change, the occurrence of EWEs and disasters is not an ecological inevitability. In this chapter the rationale of the definition of EWEs and the integration of potential consequences on people and assets in a novel wildfire classification scheme are proposed and discussed. They are excellent instruments to enhance wildfire risk and crisis communication programs and to define appropriate prevention, mitigation, and response measures which are crucial to build up citizens' safety.","tags":["Control capacity","Disaster Extreme wildfire event (EWE)","Fire intensity","Mitigation","Preparedness","Prevention","Rate of spread","Socioeconomic system (SES)","Wildfire classification"],"title":"Extreme wildfire events: the definition","type":"publication"},{"authors":null,"categories":["visualization","R","R:elementary","gis"],"content":"\rThe General Directorate for the Cadastre of Spain has spatial information of the all buildings except for the Basque Country and Navarra. This data set is part of the implementation of INSPIRE, the Space Information Infrastructure in Europe. More information can be found here. We will use the links (urls) in ATOM format, which is an RSS type for web feeds, allowing us to obtain the download link for each municipality.\nThis blog post is a reduced version of the case study that you can find in our recent publication - Introduction to GIS with R - published by Dominic Royé and Roberto Serrano-Notivoli (in Spanish).\n Packages\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\rfeedeR\rImport feeds RSS or ATOM\r\rtmap\rEasy creation of thematic maps\r\rclassInt\rCreate univariate class intervals\r\rsysfonts\rLoading system fonts and Google Fonts\r\rshowtext\rUsing fonts more easily in R graphs\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;feedeR\u0026quot;)) install.packages(\u0026quot;feedeR\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;tmap\u0026quot;)) install.packages(\u0026quot;tmap\u0026quot;)\rif(!require(\u0026quot;classInt\u0026quot;)) install.packages(\u0026quot;classInt\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\r# load packages\rlibrary(feedeR)\rlibrary(sf) library(fs)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(classInt)\rlibrary(tmap)\r\rDownload links\rThe first url will give us access to a list of provinces, territorial headquarters (they do not always coincide with the oficial province), with new RSS links, which include the final download link for each municipality. In this case, we will download the buildings of Valencia. Cadastre data is updated every six months.\nurl \u0026lt;- \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.bu.atom.xml\u0026quot;\r# import RSS feed with provincial links\rprov_enlaces \u0026lt;- feed.extract(url)\rstr(prov_enlaces) # object is a list\r## List of 4\r## $ title : chr \u0026quot;Download service of Buildings. Territorial Office\u0026quot;\r## $ link : chr \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.BU.atom.xml\u0026quot;\r## $ updated: POSIXct[1:1], format: \u0026quot;2019-10-26\u0026quot;\r## $ items :\u0026#39;data.frame\u0026#39;: 52 obs. of 4 variables:\r## ..$ title: chr [1:52] \u0026quot;Territorial office 02 Albacete\u0026quot; \u0026quot;Territorial office 03 Alicante\u0026quot; \u0026quot;Territorial office 04 Almería\u0026quot; \u0026quot;Territorial office 05 Avila\u0026quot; ...\r## ..$ date : POSIXct[1:52], format: \u0026quot;2019-10-26\u0026quot; ...\r## ..$ link : chr [1:52] \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/02/ES.SDGC.bu.atom_02.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/03/ES.SDGC.bu.atom_03.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/04/ES.SDGC.bu.atom_04.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/05/ES.SDGC.bu.atom_05.xml\u0026quot; ...\r## ..$ hash : chr [1:52] \u0026quot;d21ebb7975e59937\u0026quot; \u0026quot;bdba5e149f09e9d8\u0026quot; \u0026quot;03bcbcc7c5be2e17\u0026quot; \u0026quot;8a154202dd778143\u0026quot; ...\r# extract the table with the links\rprov_enlaces_tab \u0026lt;- as_tibble(prov_enlaces$items)\rprov_enlaces_tab\r## # A tibble: 52 x 4\r## title date link hash ## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ d21ebb7~\r## 2 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ bdba5e1~\r## 3 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ 03bcbcc~\r## 4 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ 8a15420~\r## 5 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ 7d3fd37~\r## 6 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ 9c08741~\r## 7 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ ff722b1~\r## 8 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ b431aa6~\r## 9 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ f79c656~\r## 10 Territorial o~ 2019-10-26 00:00:00 http://www.catastro.minhap.~ d702a6a~\r## # ... with 42 more rows\rNow, we access and download the data from Valencia. To filter the final download link we use the filter() function of the dplyr package, searching for the name of the territorial headquarter and then the name of the municipality in capital letters with the str_detect() function of stringr. The pull() function allows us to extract a column from a data.frame.\n# filter the province and get the RSS link\rval_atom \u0026lt;- filter(prov_enlaces_tab, str_detect(title, \u0026quot;Valencia\u0026quot;)) %\u0026gt;% pull(link)\r# import the RSS\rval_enlaces \u0026lt;- feed.extract(val_atom)\r# get the table with the download links\rval_enlaces_tab \u0026lt;- val_enlaces$items\r# filter the table with the name of the city\rval_link \u0026lt;- filter(val_enlaces_tab, str_detect(title, \u0026quot;VALENCIA\u0026quot;)) %\u0026gt;% pull(link)\rval_link\r## [1] \u0026quot;http://www.catastro.minhap.es/INSPIRE/Buildings/46/46900-VALENCIA/A.ES.SDGC.BU.46900.zip\u0026quot;\r\rData download\rThe download is done with the download.file() function that only has two main arguments, the download link and the path with the file name. In this case, we use the tempfile() function, which is useful for creating temporary files, that is, files that only exist in the memory for a certain time.\rThe file we download has the extension *.zip, so we must unzip it with another function (unzip()), which requires the name of the file and the name of the folder, where we want to unzip it. Finally, the URLencode() function encodes an URL address that contains special characters.\n# create a temporary file\rtemp \u0026lt;- tempfile()\r# download the data\rdownload.file(URLencode(val_link), temp)\r# unzip to a folder called buildings\runzip(temp, exdir = \u0026quot;buildings\u0026quot;)\r\rImport the data\rTo import the data we use the dir_ls() function of the fs package, which can obtain the files and folders of a specific path while filtering through a text pattern (regexp : regular expression). We apply the st_read() function of the sf package to the Geography Markup Language (GML) file.\n# get the path with the file\rfile_val \u0026lt;- dir_ls(\u0026quot;buildings\u0026quot;, regexp = \u0026quot;building.gml\u0026quot;)\r# import the data\rbuildings_val \u0026lt;- st_read(file_val)\r## Reading layer `Building\u0026#39; from data source `C:\\Users\\xeo19\\Documents\\GitHub\\blogR_update\\content\\post\\en\\2019-11-01-visualize-urban-growth\\buildings\\A.ES.SDGC.BU.46900.building.gml\u0026#39; using driver `GML\u0026#39;\r## Simple feature collection with 36296 features and 24 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: 720608 ymin: 4351287 xmax: 734982.5 ymax: 4382906\r## epsg (SRID): 25830\r## proj4string: +proj=utm +zone=30 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\r\rData preparation\rWe only have to convert the column of the construction year (beginning) into a Date class. The date column contains some dates in --01-01 format, which does not correspond to any recognizable date. Therefore, we replace the first - with 0000.\nbuildings_val \u0026lt;- mutate(buildings_val, beginning = str_replace(beginning, \u0026quot;^-\u0026quot;, \u0026quot;0000\u0026quot;) %\u0026gt;% ymd_hms() %\u0026gt;% as_date()\r)\r## Warning: 4 failed to parse.\r\rDistribution chart\rBefore creating the maps of the construction years, which will reflect urban growth, we will make a graph of distribution of the beginning variable. We can clearly identify periods of urban expansion. We will use the ggplot2 package with the geometry of geom_density() for this purpose. The font_add_google() function of the sysfonts package allows us to download and include font families from Google.\n#font download\rsysfonts::font_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r#use showtext for fonts\rshowtext::showtext_auto() \r# limit the period after 1750\rfilter(buildings_val, beginning \u0026gt;= \u0026quot;1750-01-01\u0026quot;) %\u0026gt;%\rggplot(aes(beginning)) + geom_density(fill = \u0026quot;#2166ac\u0026quot;, alpha = 0.7) +\rscale_x_date(date_breaks = \u0026quot;20 year\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) +\rtheme_minimal() +\rtheme(title = element_text(family = \u0026quot;Montserrat\u0026quot;),\raxis.text = element_text(family = \u0026quot;Montserrat\u0026quot;)) +\rlabs(y = \u0026quot;\u0026quot;,x = \u0026quot;\u0026quot;, title = \u0026quot;Evolution of urban development\u0026quot;)\r\rBuffer of 2,5 km for Valencia\rTo visualize better the distribution of urban growth, we limit the map to a radius of 2.5 km from the city center. Therefore, we use the geocode_OSM() function of the tmaptools package to obtain the coordinates of Valencia in class sf. Then we project the points to the system we use for the buildings (EPSG: 25830). Finally, we create with the function st_buffer() a buffer with 2500 m and the intersection with our building data. It is also possible to create a buffer in the form of a rectangle indicating the style with the argument endCapStyle =\" SQUARE \".\n# get the coordinates of Valencia\rciudad_point \u0026lt;- tmaptools::geocode_OSM(\u0026quot;Valencia\u0026quot;, as.sf = TRUE)\r# project the points\rciudad_point \u0026lt;- st_transform(ciudad_point, 25830)\r# create the buffer\rpoint_bf \u0026lt;- st_buffer(ciudad_point, 2500)\r# get the intersection between the buffer and the building\rbuildings_val25 \u0026lt;- st_intersection(buildings_val, point_bf)\r## Warning: attribute variables are assumed to be spatially constant\r## throughout all geometries\r\rPrepare data for mapping\rWe categorize the year into 15 groups using quartiles.\n# find 15 classes\rbr \u0026lt;- classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;)\r## Warning in classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;):\r## var has missing values, omitted in finding classes\r# create labels\rlab \u0026lt;- names(print(br, under = \u0026quot;\u0026lt;\u0026quot;, over = \u0026quot;\u0026gt;\u0026quot;, cutlabels = FALSE))\r## style: quantile\r## \u0026lt; 1890 1890 - 1912 1912 - 1925 1925 - 1930 1930 - 1940 1940 - 1950 ## 940 1369 971 596 1719 1080 ## 1950 - 1957 1957 - 1962 1962 - 1966 1966 - 1970 1970 - 1973 1973 - 1977 ## 1227 1266 1233 1165 1161 932 ## 1977 - 1987 1987 - 1999 \u0026gt; 1999 ## 1337 1197 1190\r# categorize the year\rbuildings_val25 \u0026lt;- mutate(buildings_val25, yr_cl = cut(year(beginning), br$brks, labels = lab, include.lowest = TRUE))\r\rMap of Valencia\rFor the mapping, we will use the tmap package. It is an interesting alternative to ggplot2. It is a package of functions specialized in creating thematic maps. The philosophy of the package follows the same as in ggplot2, creating multiple layers with different functions, which always start with tm_ *and combine with +. Building a map with tmap always starts with tm_shape(), where the data, we want to draw, is defined. Then we add the corresponding geometry to the data type (tm_polygon(), tm_border(), tm_dots() or even tm_raster()). The tm_layout() function help us to configure the map style.\nWhen we need more colors than the maximum allowed by RColorBrewer, we can pass the colors to the colorRampPalette() function. This function interpolates a set of given colors.\n# colours\rcol_spec \u0026lt;- RColorBrewer::brewer.pal(11, \u0026quot;Spectral\u0026quot;)\r# colour ramp function\rcol_spec_fun \u0026lt;- colorRampPalette(col_spec)\r# create the final map\rtm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15),\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;) +\rtm_layout(bg.color = \u0026quot;black\u0026quot;,\router.bg.color = \u0026quot;black\u0026quot;,\rlegend.outside = TRUE,\rlegend.text.color = \u0026quot;white\u0026quot;,\rlegend.text.fontfamily = \u0026quot;Montserrat\u0026quot;, panel.label.fontfamily = \u0026quot;Montserrat\u0026quot;,\rpanel.label.color = \u0026quot;white\u0026quot;,\rpanel.label.bg.color = \u0026quot;black\u0026quot;,\rpanel.label.size = 5,\rpanel.label.fontface = \u0026quot;bold\u0026quot;)\r\rDynamic map with leaflet\rA very interesting advantage is the tmap_leaflet() function of the tmap package to easily pass a map created in the same frame to leaflet.\n# tmap object\rm \u0026lt;- tm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15),\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;)\r# dynamic map\rtmap_leaflet(m)\r\r\r","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"116837f1a59e17f1770b86a1a75ffaef","permalink":"/en/2019/visualize-urban-growth/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/en/2019/visualize-urban-growth/","section":"post","summary":"The General Directorate for the Cadastre of Spain has spatial information of the all buildings except for the Basque Country and Navarra. This data set is part of the implementation of [INSPIRE](https://inspire.ec.europa.eu/), the Space Information Infrastructure in Europe. More information can be found [here](http://www.catastro.meh.es/webinspire/index.html). We will use the links (*urls*) in *ATOM* format, which is an RSS type for web feeds, allowing us to obtain the download links for each municipality.","tags":["urban growth","city","urban geography"],"title":"Visualize urban growth","type":"post"},{"authors":["D Royé","Roberto Serrano-Notivoli"],"categories":null,"content":"","date":1570406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570406400,"objectID":"f7cd7f11c5130c310097e88c0adb1b17","permalink":"/en/publication/manual_rgis_2019/","publishdate":"2019-10-07T00:00:00Z","relpermalink":"/en/publication/manual_rgis_2019/","section":"publication","summary":"R tiene, como lenguaje de programación enfocado al análisis estadístico, todos los ingredientes para ser usado como herramienta de análisis espacial y representación cartográﬁca: es gratuito, permite personalizar, replicar y compartir los análisis de cualquier nivel de diﬁcultad y no tiene ninguna limitación en cuanto a cantidad de información a procesar o tipos de formato diferentes para gestionar. Esto le sitúa en una situación de ventaja que mejora día a día, gracias a su amplia comunidad de usuarios, respecto a un SIG (Sistema de Información Geográﬁca) convencional. Este manual explica, sin necesidad de conocimientos previos, cómo desarrollar con R todos los análisis disponibles en un SIG, con ejemplos sencillos y multitud de casos prácticos. Además, se muestran las enormes posibilidades de representación cartográﬁca, que van mucho más allá de la simple creación de mapas. R permite, desde exportar a cualquier formato de archivo, hasta crear mapas dinámicos para supublicación en Internet.","tags":["R","manual","visualisation","GIS"],"title":"Introducción a los SIG con R","type":"publication"},{"authors":["D Royé","F Tedim","J Martin-Vide","M Salis","J Vendrell","R Lovreglio","C Bouillon","V Leone"],"categories":null,"content":"","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569196800,"objectID":"38384d9eb7330b815ac81013a4f15327","permalink":"/en/publication/incendios_ci_2019/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/en/publication/incendios_ci_2019/","section":"publication","summary":"The most widely used metrics to characterize wildfire regime and estimate the impact of wildfires are total burnt area (BA) and the number of fire events (FE). However, these are insufficient to analyse the threat to society of a new fire regime characterized by a higher occurrence of very large events. To overcome this weakness, we propose the use of a Concentration Index (CIB) which makes it possible to identify spatio-temporal patterns. The frequency distribution of BA follows a negative exponential distribution almost everywhere, in which a small minority of FE are responsible of the majority of BA. In this article, the spatio-temporal behaviour of BA is analysed in Western Mediterranean Europe, with particular focus on Portugal, Spain, France and Italy, using data from the European Forest Fire Information System and national wildfire databases. This is the first time that the CI has been applied to wildfire events. This research shows that, in most Mediterranean European countries, the amount of BA is increasingly related with a lower number of fires. The spatio-temporal distribution of CIB shows high variability in all of the countries analysed in Europe. Portugal and Spain show increasing significant trends of CIB +7.6% (p-value = 0.001) and +1.3% per decade (p-value = 0.003). Statistically significant correlations for Portugal, Spain and Italy are also found between the annual CIB and several teleconnection indices. The application of the CIB demonstrates its discriminatory ability, which is a key point in detecting vulnerable areas and temporal trends under climate change.","tags":["wildfire","concentration index","Europe","teleconnection","spatio-temporal patterns"],"title":"Wildfire burnt area patterns and trends in Western Mediterranean Europe via the application of a concentration index","type":"publication"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","A Benhamrouche"],"categories":null,"content":"","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"45f7b9f717406a99013ba41aedd39837","permalink":"/en/publication/ijc_teleconection_medit_2019/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/en/publication/ijc_teleconection_medit_2019/","section":"publication","summary":"This study has addressed the spatiotemporal distribution of the daily rainfall concentration and its relation to the teleconnection patterns across the Mediterranean (MR). Daily Concentration Index (CI) and the ordered n index () are used at annual time scale to reveal the statistical structure of precipitation across the MR based on 233 daily rainfall series for the period 1975–2015. Eight teleconnection patterns ,North Atlantic Oscillation (NAO), Mediterranean Oscillation (MO), Western Mediterranean Oscillation (WeMO), Upper Level Mediterranean Oscillation index (ULMO), East Atlantic (EA) pattern, East Atlantic/West Russia (EATL/WRUS) pattern, Scandinavia (SCAND) pattern and Southern Oscillation (SO) at annual time scale are selected. The spatiotemporal patterns in precipitation concentration indices, annual precipitation and their teleconnections with previous large-scale circulations are investigated. Results show a strong connection between the CI and the (r = 0.70, p","tags":["Mediterranean","n-index","concentration index","teleconnection patterns","daily precipitation"],"title":"Spatiotemporal variability of daily precipitation concentration and its relationship to teleconnection patterns over the Mediterranean during 1975-2015","type":"publication"},{"authors":["D Royé","MT Zarrabeitia","P Fdez-Arroyabe","A Álvarez-Gutiérrez","A Santurtún"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"9386a67b5c062730063f0a60839e8ab3","permalink":"/en/publication/iam_cantabria_2018/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/en/publication/iam_cantabria_2018/","section":"publication","summary":"Introduction and objectives. The role of the environment on cardiovascular health is becoming more prominent in the context of global change. The aim of this study was to analyze the relationship between apparent temperature (AT) and air pollutants and acute myocardial infarction (AMI) and to study the temporal pattern of this disease and its associated mortality. Methods. We performed a time-series study of admissions for AMI in Cantabria between 2001 and 2015. The association between environmental variables (including a biometeorological index, apparent AT) and AMI was analyzed using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of these variables on AMI, a lag non-linear model was fitted in a generalized additive model. Results. The incidence rate and the mortality followed a downward trend during the study period (CC=–0.714; P=.0002). An annual pattern was found in hospital admissions (P=.005), with the highest values being registered in winter; a weekly trend was also identified, reaching a minimum during the weekends (P=.000005). There was an inverse association between AT and the number of hospital admissions due to AMI and a direct association with particulate matter with a diameter smaller than 10 μm. Conclusions. Hospital admissions for AMI followed a downward trend between 2007 and 2015. Mortality associated with admissions due to this diagnosis has decreased. Predictive factors for this disease were AT and particulate matter with a diameter smaller than 10 μm.","tags":["Acute myocardial infarction","Apparent temperature","Air pollutants","Particulate matter"],"title":"Role of Apparent Temperature and Air Pollutants in Hospital Admissions for Acute Myocardial Infarction in the North of Spain","type":"publication"},{"authors":null,"categories":["visualization","R","R:intermediate"],"content":"\rNormally when we visualize monthly precipitation anomalies, we simply use a bar graph indicating negative and positive values with red and blue. However, it does not explain the general context of these anomalies. For example, what was the highest or lowest anomaly in each month? In principle, we could use a boxplot to visualize the distribution of the anomalies, but in this particular case they would not fit aesthetically, so we should look for an alternative. Here I present a very useful graphic form.\nPackages\rIn this post we will use the following packages:\n\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rreadr\rImport data\r\rggthemes\rThemes for ggplot2\r\rlubridate\rEasy manipulation of dates and times\r\rcowplot\rEasy creation of multiple graphics with ggplot2\r\r\r\r#we install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;cowplot\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#packages\rlibrary(tidyverse) #include readr\rlibrary(ggthemes)\rlibrary(cowplot)\rlibrary(lubridate)\r\rPreparing the data\rFirst we import the daily precipitation of the selected weather station (download). We will use data from Santiago de Compostela (Spain) accessible through ECA\u0026amp;D.\nStep 1: import the data\rWe not only import the data in csv format, but we also make the first changes. We skip the first 21 rows that contain information about the weather station. In addition, we convert the date to the date class and replace missing values (-9999) with NA. The precipitation is given in 0.1 mm, therefore, we must divide the values by 10. Then we select the columns DATE and RR, and rename them.\ndata \u0026lt;- read_csv(\u0026quot;RR_STAID001394.txt\u0026quot;, skip = 21) %\u0026gt;%\rmutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10)) %\u0026gt;%\rselect(DATE:RR) %\u0026gt;% rename(date = DATE, pr = RR)\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\rdata\r## # A tibble: 27,606 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1943-11-01 0.6\r## 2 1943-11-02 0 ## 3 1943-11-03 0 ## 4 1943-11-04 0 ## 5 1943-11-05 0 ## 6 1943-11-06 0 ## 7 1943-11-07 0 ## 8 1943-11-08 0 ## 9 1943-11-09 0 ## 10 1943-11-10 0 ## # ... with 27,596 more rows\r\rStep 2: creating monthly values\rIn the second step we calculate the monthly amounts of precipitation. To do this, a) we limit the period to the years after 1950, b) we add the month with its labels and the year as variables.\ndata \u0026lt;- mutate(data, mo = month(date, label = TRUE), yr = year(date)) %\u0026gt;%\rfilter(date \u0026gt;= \u0026quot;1950-01-01\u0026quot;) %\u0026gt;%\rgroup_by(yr, mo) %\u0026gt;% summarise(prs = sum(pr, na.rm = TRUE))\rdata\r## # A tibble: 833 x 3\r## # Groups: yr [70]\r## yr mo prs\r## \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Jan 55.6\r## 2 1950 Feb 349. ## 3 1950 Mar 85.8\r## 4 1950 Apr 33.4\r## 5 1950 May 272. ## 6 1950 Jun 111. ## 7 1950 Jul 35.4\r## 8 1950 Aug 76.4\r## 9 1950 Sep 85 ## 10 1950 Oct 53 ## # ... with 823 more rows\r\rStep 3: estimating anomalies\rNow we must estimate the normals of each month and join this table to our main data in order to calculate the monthly anomaly. We express the anomalies in percentage and subtract 100 to set the average to 0. In addition, we create a variable which indicates if the anomaly is negative or positive, and another with the date.\npr_ref \u0026lt;- filter(data, yr \u0026gt; 1981, yr \u0026lt;= 2010) %\u0026gt;%\rgroup_by(mo) %\u0026gt;%\rsummarise(pr_ref = mean(prs))\rdata \u0026lt;- left_join(data, pr_ref, by = \u0026quot;mo\u0026quot;)\rdata \u0026lt;- mutate(data, anom = (prs*100/pr_ref)-100, date = str_c(yr, as.numeric(mo), 1, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd(),\rsign= ifelse(anom \u0026gt; 0, \u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;))\rWe can do a first test graph of anomalies (the classic one), for that we filter the year 2018. In this case we use a bar graph, remember that by default the function geom_bar() applies the counting of the variable. However, in this case we know y, hence we indicate with the argument stat = \"identity\" that it should use the given value in aes().\nfilter(data, yr == 2018) %\u0026gt;%\rggplot(aes(date, anom, fill = sign)) + geom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(-100, 100, 20)) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rlabs(y = \u0026quot;Precipitation anomaly (%)\u0026quot;, x = \u0026quot;\u0026quot;) +\rtheme_hc()\r\rStep 4: calculating the statistical metrics\rIn this last step we estimate the maximum, minimum value, the 25%/75% quantiles and the interquartile range per month of the entire time series.\ndata_norm \u0026lt;- group_by(data, mo) %\u0026gt;%\rsummarise(mx = max(anom),\rmin = min(anom),\rq25 = quantile(anom, .25),\rq75 = quantile(anom, .75),\riqr = q75-q25)\rdata_norm\r## # A tibble: 12 x 6\r## mo mx min q25 q75 iqr\r## \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Jan 193. -89.6 -43.6 56.3 99.9\r## 2 Feb 320. -96.5 -51.2 77.7 129. ## 3 Mar 381. -100 -40.6 88.2 129. ## 4 Apr 198. -93.6 -51.2 17.1 68.3\r## 5 May 141. -90.1 -45.2 17.0 62.2\r## 6 Jun 419. -99.3 -58.2 50.0 108. ## 7 Jul 311. -98.2 -77.3 27.1 104. ## 8 Aug 264. -100 -68.2 39.8 108. ## 9 Sep 241. -99.2 -64.9 48.6 113. ## 10 Oct 220. -99.0 -54.5 4.69 59.2\r## 11 Nov 137. -98.8 -44.0 39.7 83.7\r## 12 Dec 245. -91.8 -49.8 36.0 85.8\r\r\rCreating the graph\rTo create the anomaly graph with legend it is necessary to separate the main graph from the legends.\nPart 1\rIn this first part we are adding layer by layer the different elements: 1) the range of anomalies maximum-minimum 2) the interquartile range and 3) the anomalies of the year 2018.\n#range of anomalies maximum-minimum\rg1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#adding interquartile range\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#adding anomalies of the year 2018 g1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr == 2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE)\rg1.3\rFinally we change some last style settings.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Precipitation anomaly (%)\u0026quot;,\rbreaks = seq(-100, 500, 25),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Precipitation anomaly in Santiago de Compostela 2018\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Data: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\r\rPart 2\rWe still need a legend. First we create it for the normals.\n#legend data\rlegend \u0026lt;- filter(data_norm, mo == \u0026quot;Jan\u0026quot;)\rlegend_lab \u0026lt;- gather(legend, stat, y, mx:q75) %\u0026gt;%\rmutate(stat = factor(stat, stat, c(\u0026quot;maximum\u0026quot;,\r\u0026quot;minimum\u0026quot;,\r\u0026quot;Quantile 25%\u0026quot;,\r\u0026quot;Quantile 75%\u0026quot;)) %\u0026gt;%\ras.character())\r#legend graph\rg2 \u0026lt;- legend %\u0026gt;% ggplot()+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;, width = 0.2) +\rgeom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;, width = 0.2) +\rgeom_text(data = legend_lab, aes(x = mo, y = y+c(12,-8,-10,12), label = stat), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.18, y = 40, label = \u0026quot;Period 1950-2018\u0026quot;, angle = 90, size = 3) +\rtheme_void() + theme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg2\rSecond, we create another legend for the current anomalies.\n#legend data\rlegend2 \u0026lt;- filter(data, yr == 1950, mo %in% c(\u0026quot;Jan\u0026quot;,\u0026quot;Feb\u0026quot;)) %\u0026gt;% ungroup() %\u0026gt;% select(mo, anom, sign)\rlegend2[2,1] \u0026lt;- \u0026quot;Jan\u0026quot;\rlegend_lab2 \u0026lt;- data.frame(mo = rep(\u0026quot;Jan\u0026quot;, 3), anom= c(110, 3, -70), label = c(\u0026quot;Positive anomaly\u0026quot;, \u0026quot;Average\u0026quot;, \u0026quot;Negative anomaly\u0026quot;))\r#legend graph\rg3 \u0026lt;- ggplot() + geom_bar(data = legend2,\raes(x = mo, y = anom, fill = sign),\ralpha = .6, colour = \u0026quot;NA\u0026quot;, stat = \u0026quot;identity\u0026quot;, show.legend = FALSE, width = 0.2) +\rgeom_segment(aes(x = .85, y = 0, xend = 1.15, yend = 0), linetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = legend_lab2, aes(x = mo, y = anom+c(10,5,-13), label = label), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.25, y = 20, label =\u0026quot;Reference 1971-2010\u0026quot;, angle = 90, size = 3) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rtheme_void() +\rtheme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg3\r\rPart 3\rFinally, we only have to join the graph and the legends with the help of the cowplot package. The main function of cowplot is plot_grid() which is used for combining different graphs. However, in this case it is necessary to use more flexible functions to create less common formats. The ggdraw() function configures the basic layer of the graph, and the functions that are intended to operate on this layer start with draw_*.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .3, width = 1, height = 0.6) +\rdraw_plot(g2, x = 0, y = .15, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .15, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomaly2016_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r\rMultiple facets\rIn this section we will make the same graph as in the previous one, but for several years.\nPart 1\rFirst we need to filter again by set of years, in this case from 2016 to 2018, using the operator %in%, we also add the function facet_grid() to ggplot, which allows us to plot the graph according to a variable. The formula used for the facet function is similar to the use in models: variable_by_row ~ variable_by_column. When we do not have a variable in the column, we should use the ..\n#range of anomalies maximum-minimum\rg1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#adding the interquartile range\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#adding the anomalies of the year 2016-2018\rg1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr %in% 2016:2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE) +\rfacet_grid(yr ~ .)\rg1.3\rFinally we change some last style settings.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Anomalía de precipitación (%)\u0026quot;,\rbreaks = seq(-100, 500, 50),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Anomalía de precipitación en Santiago de Compostela\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Datos: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\rWe use the same legend created for the previous graph.\n\r\rPart 2\rFinally, we join the graph and the legends with the help of the cowplot package. The only thing we must adjust here are the arguments in the draw_plot() function to correctly place the different parts.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .18, width = 1, height = 0.8) +\rdraw_plot(g2, x = 0, y = .08, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .08, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomaly20162018_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562457600,"objectID":"a85307e41f42183629b3cecc43762b6e","permalink":"/en/2019/visualize-monthly-precipitation-anomalies/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/en/2019/visualize-monthly-precipitation-anomalies/","section":"post","summary":"Normally when we visualize monthly precipitation anomalies, we simply use a bar graph indicating negative and positive values with red and blue. However, it does not explain the general context of these anomalies. For example, what was the highest or lowest anomaly in each month? In principle, we could use a *boxplot* to visualize the distribution of the anomalies, but in this particular case they would not fit aesthetically, so we should look for an alternative. Here I present a very useful graphic form.","tags":["anomalies","precipitation","climate","boxplot"],"title":"Visualize monthly precipitation anomalies","type":"post"},{"authors":["A Martí","J Taboada","D Royé","X Fonseca"],"categories":null,"content":"","date":1560384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560384000,"objectID":"4cd534699f62bcb4b4d46633d326cf5d","permalink":"/en/publication/os_tempos_2019/","publishdate":"2019-06-13T00:00:00Z","relpermalink":"/en/publication/os_tempos_2019/","section":"publication","summary":"Que récords climáticos se alcanzaron en Galicia? Cales son os lugares máis calorosos? E os máis fríos? Onde chove máis? Onde se rexistran máis días de precipitación? Que zonas gozan dun maior número de horas de sol? Cales teñen maior nebulosidade? Que lugares son os máis ventosos? Como está a afectar o cambio climático a Galicia? Neste libro atoparás as respostas a estas e a outras preguntas relacionadas co clima de Galicia e os diversos tipos de tempo que o caracterizan. Nas súas páxinas explícase como se producen os fenómenos meteorolóxicos máis habituais no noso territorio: as inversións térmicas, as néboas costeiras e orográficas, as illas de calor urbanas, os tipos de precipitación, o efecto foehn, as brisas mariñas, o arco da vella etc. A través de exemplos concretos, analízanse tamén os riscos climáticos que afectan regularmente a Galicia, como vagas de calor, temporais de neve, cicloxéneses explosivas e temporais de choiva e vento, tormentas, secas, tornados... Tamén poderás coñecer como está a cambiar o clima da nosa comunidade debido ao quecemento global e cales son os escenarios de futuro.","tags":["tempo","Galicia","clima","divulgación","gallego"],"title":"Os tempos e o clima de Galicia","type":"publication"},{"authors":["A Vélez","J Martin-Vide","D Royé","O Santaella"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"7bc2a7ffe0ae57e6714ccd7d00de9cc6","permalink":"/en/publication/ci_pr_2018/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/en/publication/ci_pr_2018/","section":"publication","summary":"The present study analyzes spatial patterns of precipitation Concentration Index (CI) in Puerto Rico considering the daily precipitation data of precipitation-gauging stations during 1971-2010. The South and East interior parts of Puerto Rico are characterized by higher CI and the West and North-West parts show lower CI. The annual CI and the rainy season CI show a gradient from South-East to North-West and the dry season CI shows a gradient from South to North. Another difference between the rainy season CI and dry season CI is that the former shows the lowest values of CI while the latter shows the highest values of CI. The different types of seasonal precipitation seem to play a major role on the spatial CI distribution. However, the local relief plays a major role in the spatial patterns due to the effect of the air circulation by the mountains. These findings can contribute to basin-scale water resource management (ooding, soil erosion, etc.) and conservation of the ecological environment.","tags":["Concentration Index","Puerto Rico","precipitation","spatial–temporal patterns"],"title":"Spatial Analysis of Daily Precipitation Concentration in Puerto Rico","type":"publication"},{"authors":null,"categories":["statistics","R","R:advanced"],"content":"\rWhen we try to estimate the correlation coefficient between multiple variables, the task is more complicated in order to obtain a simple and tidy result. A simple solution is to use the tidy() function from the {broom} package. In this post we are going to estimate the correlation coefficients between the annual precipitation of several Spanish cities and climate teleconnections indices: download. The data of the teleconnections are preprocessed, but can be downloaded directly from crudata.uea.ac.uk. The daily precipitation data comes from ECA\u0026amp;D.\nPackages\rIn this post we will use the following packages:\n\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rbroom\rConvert results of statistical functions (lm, t.test, cor.test, etc.) into tidy tables\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\r\r\r#install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;broom\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(broom)\rlibrary(fs)\rlibrary(lubridate)\r\rImport data\rFirst we have to import the daily precipitation of the selected weather stations.\nCreate a vector with all precipitation files using the function dir_ls() of the {fs} package.\rImport the data using the map_df() function of the {purrr} package that applies another function to a vector or list, and joins them together in a single data.frame.\rSelect the columns that interest us, b) Convert the date string into a date object using the ymd() function of the {lubridate} package, c) Create a new column yr with the years, d) Divide the precipitation values by 10 and reclassify absent values -9999 by NA, e) Finally, reclassify the ID of each weather station creating a factor with new labels.\r\r\rMore details about the use of the dir_ls() and map_df() functions can be found in this previous post.\n#precipitation files\rfiles \u0026lt;- dir_ls(regexp = \u0026quot;txt\u0026quot;)\rfiles\r## RR_STAID001393.txt RR_STAID001394.txt RR_STAID002969.txt ## RR_STAID003946.txt RR_STAID003969.txt\r#import all files and join them together\rpr \u0026lt;- files %\u0026gt;% map_df(read_csv, skip = 20)\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\r## Parsed with column specification:\r## cols(\r## STAID = col_double(),\r## SOUID = col_double(),\r## DATE = col_double(),\r## RR = col_double(),\r## Q_RR = col_double()\r## )\rpr\r## # A tibble: 133,343 x 5\r## STAID SOUID DATE RR Q_RR\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1393 20611 19470301 0 0\r## 2 1393 20611 19470302 5 0\r## 3 1393 20611 19470303 0 0\r## 4 1393 20611 19470304 33 0\r## 5 1393 20611 19470305 15 0\r## 6 1393 20611 19470306 0 0\r## 7 1393 20611 19470307 85 0\r## 8 1393 20611 19470308 3 0\r## 9 1393 20611 19470309 0 0\r## 10 1393 20611 19470310 0 0\r## # ... with 133,333 more rows\r#create levels for the factor id \u0026lt;- unique(pr$STAID)\r#the corresponding labels\rlab \u0026lt;- c(\u0026quot;Bilbao\u0026quot;, \u0026quot;Santiago\u0026quot;, \u0026quot;Barcelona\u0026quot;, \u0026quot;Madrid\u0026quot;, \u0026quot;Valencia\u0026quot;)\r#first changes\rpr \u0026lt;- select(pr, STAID, DATE, RR) %\u0026gt;% mutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10), STAID = factor(STAID, id, lab), yr = year(DATE)) pr\r## # A tibble: 133,343 x 4\r## STAID DATE RR yr\r## \u0026lt;fct\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1947-03-01 0 1947\r## 2 Bilbao 1947-03-02 0.5 1947\r## 3 Bilbao 1947-03-03 0 1947\r## 4 Bilbao 1947-03-04 3.3 1947\r## 5 Bilbao 1947-03-05 1.5 1947\r## 6 Bilbao 1947-03-06 0 1947\r## 7 Bilbao 1947-03-07 8.5 1947\r## 8 Bilbao 1947-03-08 0.3 1947\r## 9 Bilbao 1947-03-09 0 1947\r## 10 Bilbao 1947-03-10 0 1947\r## # ... with 133,333 more rows\rWe still need to filter and calculate the annual amount of precipitation. Actually, it is not correct to sum up precipitation without taking into account that there are missing values, but it should be enough for this practice. Then, we change the table format with the spread() function, passing from a long to a wide table, that is, we want to obtain one column per weather station.\npr_yr \u0026lt;- filter(pr, DATE \u0026gt;= \u0026quot;1950-01-01\u0026quot;, DATE \u0026lt; \u0026quot;2018-01-01\u0026quot;) %\u0026gt;%\rgroup_by(STAID, yr)%\u0026gt;%\rsummarise(pr = sum(RR, na.rm = TRUE))\rpr_yr\r## # A tibble: 324 x 3\r## # Groups: STAID [5]\r## STAID yr pr\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1950 1342 ## 2 Bilbao 1951 1306.\r## 3 Bilbao 1952 1355.\r## 4 Bilbao 1953 1372.\r## 5 Bilbao 1954 1428.\r## 6 Bilbao 1955 1062.\r## 7 Bilbao 1956 1254.\r## 8 Bilbao 1957 968.\r## 9 Bilbao 1958 1272.\r## 10 Bilbao 1959 1450.\r## # ... with 314 more rows\rpr_yr \u0026lt;- spread(pr_yr, STAID, pr)\rpr_yr\r## # A tibble: 68 x 6\r## yr Bilbao Santiago Barcelona Madrid Valencia\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA\r## 2 1951 1306. 2344. 1072. 798. NA\r## 3 1952 1355. 1973. 415. 524. NA\r## 4 1953 1372. 973. 683. 365. NA\r## 5 1954 1428. 1348. 581. 246. NA\r## 6 1955 1062. 1769. 530. 473. NA\r## 7 1956 1254. 1533. 695. 480. NA\r## 8 1957 968. 1599. 635. 424. NA\r## 9 1958 1272. 2658. 479. 482. NA\r## 10 1959 1450. 2847. 1006 665. NA\r## # ... with 58 more rows\rThe next step is to import the climate teleconnection indices.\n#teleconnections\rtelecon \u0026lt;- read_csv(\u0026quot;teleconnections_indices.csv\u0026quot;)\r## Parsed with column specification:\r## cols(\r## yr = col_double(),\r## NAO = col_double(),\r## WeMO = col_double(),\r## EA = col_double(),\r## `POL-EUAS` = col_double(),\r## `EATL/WRUS` = col_double(),\r## MO = col_double(),\r## SCAND = col_double(),\r## AO = col_double()\r## )\rtelecon\r## # A tibble: 68 x 9\r## yr NAO WeMO EA `POL-EUAS` `EATL/WRUS` MO SCAND AO\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 0.49 0.555 -0.332 0.0217 -0.0567 0.335 0.301 -1.99e-1\r## 2 1951 -0.07 0.379 -0.372 0.402 -0.419 0.149 -0.00667 -3.65e-1\r## 3 1952 -0.37 0.693 -0.688 -0.0117 -0.711 0.282 0.0642 -6.75e-1\r## 4 1953 0.4 -0.213 -0.727 -0.0567 -0.0508 0.216 0.0233 -1.64e-2\r## 5 1954 0.51 1.20 -0.912 0.142 -0.318 0.386 0.458 -5.83e-4\r## 6 1955 -0.64 0.138 -0.824 -0.0267 0.154 0.134 0.0392 -3.62e-1\r## 7 1956 0.17 0.617 -1.29 -0.197 0.0617 0.256 0.302 -1.63e-1\r## 8 1957 -0.02 0.321 -0.952 -0.638 -0.167 0.322 -0.134 -3.42e-1\r## 9 1958 0.12 0.941 -0.243 0.138 0.661 0.296 0.279 -8.68e-1\r## 10 1959 0.49 -0.055 -0.23 -0.0142 0.631 0.316 0.725 -7.62e-2\r## # ... with 58 more rows\rFinally we need to join both tables by year.\ndata_all \u0026lt;- left_join(pr_yr, telecon, by = \u0026quot;yr\u0026quot;)\rdata_all\r## # A tibble: 68 x 14\r## yr Bilbao Santiago Barcelona Madrid Valencia NAO WeMO EA\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA 0.49 0.555 -0.332\r## 2 1951 1306. 2344. 1072. 798. NA -0.07 0.379 -0.372\r## 3 1952 1355. 1973. 415. 524. NA -0.37 0.693 -0.688\r## 4 1953 1372. 973. 683. 365. NA 0.4 -0.213 -0.727\r## 5 1954 1428. 1348. 581. 246. NA 0.51 1.20 -0.912\r## 6 1955 1062. 1769. 530. 473. NA -0.64 0.138 -0.824\r## 7 1956 1254. 1533. 695. 480. NA 0.17 0.617 -1.29 ## 8 1957 968. 1599. 635. 424. NA -0.02 0.321 -0.952\r## 9 1958 1272. 2658. 479. 482. NA 0.12 0.941 -0.243\r## 10 1959 1450. 2847. 1006 665. NA 0.49 -0.055 -0.23 ## # ... with 58 more rows, and 5 more variables: `POL-EUAS` \u0026lt;dbl\u0026gt;,\r## # `EATL/WRUS` \u0026lt;dbl\u0026gt;, MO \u0026lt;dbl\u0026gt;, SCAND \u0026lt;dbl\u0026gt;, AO \u0026lt;dbl\u0026gt;\r\rCorrelation test\rA correlation test between paired samples can be done with the cor.test() function of R Base. In this case between the annual precipitation of Bilbao and the NAO index.\ncor_nao_bil \u0026lt;- cor.test(data_all$Bilbao, data_all$NAO,\rmethod = \u0026quot;spearman\u0026quot;)\r## Warning in cor.test.default(data_all$Bilbao, data_all$NAO, method =\r## \u0026quot;spearman\u0026quot;): Cannot compute exact p-value with ties\rcor_nao_bil\r## ## Spearman\u0026#39;s rank correlation rho\r## ## data: data_all$Bilbao and data_all$NAO\r## S = 44372, p-value = 0.2126\r## alternative hypothesis: true rho is not equal to 0\r## sample estimates:\r## rho ## 0.1531149\rstr(cor_nao_bil)\r## List of 8\r## $ statistic : Named num 44372\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## $ parameter : NULL\r## $ p.value : num 0.213\r## $ estimate : Named num 0.153\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ null.value : Named num 0\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ alternative: chr \u0026quot;two.sided\u0026quot;\r## $ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## $ data.name : chr \u0026quot;data_all$Bilbao and data_all$NAO\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;htest\u0026quot;\rWe see that the result is in an unmanageable and untidy format. It is a console summary of the correlation with all the statistical parameters necessary to get a conclusion about the relationship. The orginal structure is a list of vectors. However, the tidy() function of the {broom} package allows us to convert the result into a table format.\ntidy(cor_nao_bil)\r## # A tibble: 1 x 5\r## estimate statistic p.value method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.153 44372. 0.213 Spearman\u0026#39;s rank correlation rho two.sided\r\rApply the correlation test to multiple variables\rThe objective is to apply the correlation test to all weather stations and climate teleconnection indices.\nFirst, we must pass the table to the long format, that is, create a column/variable for the city and for the value of the corresponding precipitation. Then we repeat the same for the teleconnections indices.\ndata \u0026lt;- gather(data_all, city, pr, Bilbao:Valencia)%\u0026gt;%\rgather(telecon, index, NAO:AO)\rdata\r## # A tibble: 2,720 x 5\r## yr city pr telecon index\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Bilbao 1342 NAO 0.49\r## 2 1951 Bilbao 1306. NAO -0.07\r## 3 1952 Bilbao 1355. NAO -0.37\r## 4 1953 Bilbao 1372. NAO 0.4 ## 5 1954 Bilbao 1428. NAO 0.51\r## 6 1955 Bilbao 1062. NAO -0.64\r## 7 1956 Bilbao 1254. NAO 0.17\r## 8 1957 Bilbao 968. NAO -0.02\r## 9 1958 Bilbao 1272. NAO 0.12\r## 10 1959 Bilbao 1450. NAO 0.49\r## # ... with 2,710 more rows\rTo apply the test to all cities, we need the corresponding groupings. Therefore, we use the group_by() function for indicating the two groups: city and telecon. In addition, we apply the nest() function of the {tidyr} package ({tidyverse} collection) with the aim of creating lists of tables nested per row. In other words, in each row of each city and teleconnection index we will have a new table that contains the year, the precipitation value and the value of each teleconection, correspondingly.\ndata_nest \u0026lt;- group_by(data, city, telecon) %\u0026gt;% nest()\rdata_nest\r## # A tibble: 40 x 3\r## # Groups: city, telecon [40]\r## city telecon data\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026lt;df[,3]\u0026gt;\u0026gt;\r## 1 Bilbao NAO [68 x 3]\r## 2 Santiago NAO [68 x 3]\r## 3 Barcelona NAO [68 x 3]\r## 4 Madrid NAO [68 x 3]\r## 5 Valencia NAO [68 x 3]\r## 6 Bilbao WeMO [68 x 3]\r## 7 Santiago WeMO [68 x 3]\r## 8 Barcelona WeMO [68 x 3]\r## 9 Madrid WeMO [68 x 3]\r## 10 Valencia WeMO [68 x 3]\r## # ... with 30 more rows\rstr(slice(data_nest, 1))\r## Classes \u0026#39;grouped_df\u0026#39;, \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 40 obs. of 3 variables:\r## $ city : chr \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data : list\u0026lt;df[,3]\u0026gt; [1:40] ## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..@ ptype:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 0 obs. of 3 variables:\r## .. ..$ yr : num ## .. ..$ pr : num ## .. ..$ index: num ## - attr(*, \u0026quot;groups\u0026quot;)=Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 40 obs. of 3 variables:\r## ..$ city : chr \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows :List of 40\r## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..$ : int 7\r## .. ..$ : int 8\r## .. ..$ : int 9\r## .. ..$ : int 10\r## .. ..$ : int 11\r## .. ..$ : int 12\r## .. ..$ : int 13\r## .. ..$ : int 14\r## .. ..$ : int 15\r## .. ..$ : int 16\r## .. ..$ : int 17\r## .. ..$ : int 18\r## .. ..$ : int 19\r## .. ..$ : int 20\r## .. ..$ : int 21\r## .. ..$ : int 22\r## .. ..$ : int 23\r## .. ..$ : int 24\r## .. ..$ : int 25\r## .. ..$ : int 26\r## .. ..$ : int 27\r## .. ..$ : int 28\r## .. ..$ : int 29\r## .. ..$ : int 30\r## .. ..$ : int 31\r## .. ..$ : int 32\r## .. ..$ : int 33\r## .. ..$ : int 34\r## .. ..$ : int 35\r## .. ..$ : int 36\r## .. ..$ : int 37\r## .. ..$ : int 38\r## .. ..$ : int 39\r## .. ..$ : int 40\r## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi FALSE\rThe next step is to create a function, in which we define the correlation test and pass it to the clean format using the tidy() function, which we apply to each groupings.\ncor_fun \u0026lt;- function(df) cor.test(df$pr, df$index, method = \u0026quot;spearman\u0026quot;) %\u0026gt;% tidy()\rNow we only have to apply our function to the column that contains the tables for each combination between city and teleconnection. To do this, we use the map() function that applies another function to a vector or list. What we do is create a new column that contains the result, a statistical summary table, for each combination.\ndata_nest \u0026lt;- mutate(data_nest, model = map(data, cor_fun))\rdata_nest\r## # A tibble: 40 x 4\r## # Groups: city, telecon [40]\r## city telecon data model ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026lt;df[,3]\u0026gt;\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 2 Santiago NAO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 3 Barcelona NAO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 4 Madrid NAO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 5 Valencia NAO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 6 Bilbao WeMO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 7 Santiago WeMO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 8 Barcelona WeMO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 9 Madrid WeMO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## 10 Valencia WeMO [68 x 3] \u0026lt;tibble [1 x 5]\u0026gt;\r## # ... with 30 more rows\rstr(slice(data_nest, 1))\r## Classes \u0026#39;grouped_df\u0026#39;, \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 40 obs. of 4 variables:\r## $ city : chr \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data : list\u0026lt;df[,3]\u0026gt; [1:40] ## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 345 1072 415 683 581 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1342 1306 1355 1372 1428 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA 798 524 365 246 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num 1800 2344 1973 973 1348 ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.30083 -0.00667 0.06417 0.02333 0.4575 ...\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 68 obs. of 3 variables:\r## .. ..$ yr : num 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num NA NA NA NA NA NA NA NA NA NA ...\r## .. ..$ index: num 0.555 0.379 0.693 -0.213 1.196 ...\r## ..@ ptype:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 0 obs. of 3 variables:\r## .. ..$ yr : num ## .. ..$ pr : num ## .. ..$ index: num ## $ model :List of 40\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.00989\r## .. ..$ statistic : num 52912\r## .. ..$ p.value : num 0.936\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.295\r## .. ..$ statistic : num 67832\r## .. ..$ p.value : num 0.0147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.161\r## .. ..$ statistic : num 43966\r## .. ..$ p.value : num 0.19\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.255\r## .. ..$ statistic : num 65754\r## .. ..$ p.value : num 0.0361\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.0203\r## .. ..$ statistic : num 53460\r## .. ..$ p.value : num 0.869\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.178\r## .. ..$ statistic : num 43082\r## .. ..$ p.value : num 0.147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.161\r## .. ..$ statistic : num 43970\r## .. ..$ p.value : num 0.19\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.0292\r## .. ..$ statistic : num 50862\r## .. ..$ p.value : num 0.813\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.185\r## .. ..$ statistic : num 62070\r## .. ..$ p.value : num 0.131\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.256\r## .. ..$ statistic : num 65825\r## .. ..$ p.value : num 0.0348\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.0155\r## .. ..$ statistic : num 51584\r## .. ..$ p.value : num 0.9\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.0457\r## .. ..$ statistic : num 54788\r## .. ..$ p.value : num 0.711\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.153\r## .. ..$ statistic : num 44372\r## .. ..$ p.value : num 0.213\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.147\r## .. ..$ statistic : num 44670\r## .. ..$ p.value : num 0.23\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.357\r## .. ..$ statistic : num 33688\r## .. ..$ p.value : num 0.00296\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.404\r## .. ..$ statistic : num 31242\r## .. ..$ p.value : num 0.000706\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.313\r## .. ..$ statistic : num 65806\r## .. ..$ p.value : num 0.0102\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.304\r## .. ..$ statistic : num 65369\r## .. ..$ p.value : num 0.0123\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.0643\r## .. ..$ statistic : num 46893\r## .. ..$ p.value : num 0.605\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.497\r## .. ..$ statistic : num 75028\r## .. ..$ p.value : num 2.42e-05\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.291\r## .. ..$ statistic : num 64692\r## .. ..$ p.value : num 0.0169\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.0835\r## .. ..$ statistic : num 45930\r## .. ..$ p.value : num 0.501\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.306\r## .. ..$ statistic : num 34766\r## .. ..$ p.value : num 0.012\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.109\r## .. ..$ statistic : num 44660\r## .. ..$ p.value : num 0.38\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.443\r## .. ..$ statistic : num 75608\r## .. ..$ p.value : num 0.00018\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.01\r## .. ..$ statistic : num 52919\r## .. ..$ p.value : num 0.935\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.176\r## .. ..$ statistic : num 43170\r## .. ..$ p.value : num 0.151\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.19\r## .. ..$ statistic : num 62364\r## .. ..$ p.value : num 0.12\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.181\r## .. ..$ statistic : num 61902\r## .. ..$ p.value : num 0.139\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.0504\r## .. ..$ statistic : num 49752\r## .. ..$ p.value : num 0.682\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.44\r## .. ..$ statistic : num 29356\r## .. ..$ p.value : num 0.000203\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.332\r## .. ..$ statistic : num 35014\r## .. ..$ p.value : num 0.00594\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.211\r## .. ..$ statistic : num 19574\r## .. ..$ p.value : num 0.129\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.0672\r## .. ..$ statistic : num 26472\r## .. ..$ p.value : num 0.632\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.0542\r## .. ..$ statistic : num 23460\r## .. ..$ p.value : num 0.7\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.0478\r## .. ..$ statistic : num 25990\r## .. ..$ p.value : num 0.733\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.113\r## .. ..$ statistic : num 27600\r## .. ..$ p.value : num 0.422\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num 0.0971\r## .. ..$ statistic : num 22396\r## .. ..$ p.value : num 0.488\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.0795\r## .. ..$ statistic : num 26776\r## .. ..$ p.value : num 0.57\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ :Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1 obs. of 5 variables:\r## .. ..$ estimate : num -0.252\r## .. ..$ statistic : num 31056\r## .. ..$ p.value : num 0.0688\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## - attr(*, \u0026quot;groups\u0026quot;)=Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 40 obs. of 3 variables:\r## ..$ city : chr \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows :List of 40\r## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..$ : int 7\r## .. ..$ : int 8\r## .. ..$ : int 9\r## .. ..$ : int 10\r## .. ..$ : int 11\r## .. ..$ : int 12\r## .. ..$ : int 13\r## .. ..$ : int 14\r## .. ..$ : int 15\r## .. ..$ : int 16\r## .. ..$ : int 17\r## .. ..$ : int 18\r## .. ..$ : int 19\r## .. ..$ : int 20\r## .. ..$ : int 21\r## .. ..$ : int 22\r## .. ..$ : int 23\r## .. ..$ : int 24\r## .. ..$ : int 25\r## .. ..$ : int 26\r## .. ..$ : int 27\r## .. ..$ : int 28\r## .. ..$ : int 29\r## .. ..$ : int 30\r## .. ..$ : int 31\r## .. ..$ : int 32\r## .. ..$ : int 33\r## .. ..$ : int 34\r## .. ..$ : int 35\r## .. ..$ : int 36\r## .. ..$ : int 37\r## .. ..$ : int 38\r## .. ..$ : int 39\r## .. ..$ : int 40\r## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi FALSE\rHow can we undo the list of tables in each row of our table?\nFirst we eliminate the column with the data and then simply we can apply the unnest() function.\ncorr_pr \u0026lt;- select(data_nest, -data) %\u0026gt;% unnest()\r## Warning: `cols` is now required.\r## Please use `cols = c(model)`\rcorr_pr\r## # A tibble: 40 x 7\r## # Groups: city, telecon [40]\r## city telecon estimate statistic p.value method alternative\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Bilbao NAO 0.153 44372. 0.213 Spearman\u0026#39;s rank~ two.sided ## 2 Santia~ NAO -0.181 61902. 0.139 Spearman\u0026#39;s rank~ two.sided ## 3 Barcel~ NAO -0.0203 53460. 0.869 Spearman\u0026#39;s rank~ two.sided ## 4 Madrid NAO -0.291 64692. 0.0169 Spearman\u0026#39;s rank~ two.sided ## 5 Valenc~ NAO -0.113 27600. 0.422 Spearman\u0026#39;s rank~ two.sided ## 6 Bilbao WeMO 0.404 31242. 0.000706 Spearman\u0026#39;s rank~ two.sided ## 7 Santia~ WeMO 0.332 35014. 0.00594 Spearman\u0026#39;s rank~ two.sided ## 8 Barcel~ WeMO 0.0292 50862 0.813 Spearman\u0026#39;s rank~ two.sided ## 9 Madrid WeMO 0.109 44660 0.380 Spearman\u0026#39;s rank~ two.sided ## 10 Valenc~ WeMO -0.252 31056. 0.0688 Spearman\u0026#39;s rank~ two.sided ## # ... with 30 more rows\rThe result is a table in which we can see the correlations and their statistical significance for each city and teleconnection index.\n\rHeatmap of the results\rFinally, we make a heatmap of the obtained result. But, previously we create a column that indicates whether the correlation is significant with p-value less than 0.05.\ncorr_pr \u0026lt;- mutate(corr_pr, sig = ifelse(p.value \u0026lt;0.05, \u0026quot;Sig.\u0026quot;, \u0026quot;Non Sig.\u0026quot;))\rggplot()+\rgeom_tile(data = corr_pr,\raes(city, telecon, fill = estimate),\rsize = 1,\rcolour = \u0026quot;white\u0026quot;)+\rgeom_tile(data = filter(corr_pr, sig == \u0026quot;Sig.\u0026quot;),\raes(city, telecon),\rsize = 1,\rcolour = \u0026quot;black\u0026quot;,\rfill = \u0026quot;transparent\u0026quot;)+\rgeom_text(data = corr_pr,\raes(city, telecon, label = round(estimate, 2),\rfontface = ifelse(sig == \u0026quot;Sig.\u0026quot;, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)))+\rscale_fill_gradient2(breaks = seq(-1, 1, 0.2))+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, p.value = \u0026quot;\u0026quot;)+\rtheme_minimal()+\rtheme(panel.grid.major = element_blank(),\rpanel.border = element_blank(),\rpanel.background = element_blank(),\raxis.ticks = element_blank())\r\r","date":1555459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"80bb1a83759c286dad15b616f8eb9216","permalink":"/en/2019/tidy-correlation-tests-in-r/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/en/2019/tidy-correlation-tests-in-r/","section":"post","summary":"When we try to estimate the correlation coefficient between multiple variables, the task is more complicated in order to obtain a simple and tidy result. A simple solution is to use the ``tidy()`` function from the *{broom}* package. As an example, in this post we are going to estimate the correlation coefficients between the annual precipitation of several Spanish cities and climate teleconnections indices.","tags":["correlation","variables","tidy","tests"],"title":"Tidy correlation tests in R","type":"post"},{"authors":["M Lemus-Canovas","JA Lopez-Bustins","J Martin-Vide","D Royé"],"categories":null,"content":"","date":1555286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555286400,"objectID":"e180a1f5657a478e802087d80217785c","permalink":"/en/publication/synoptreg_2019/","publishdate":"2019-04-15T00:00:00Z","relpermalink":"/en/publication/synoptreg_2019/","section":"publication","summary":"Spatial knowledge of the climatic or environmental variables associated with the most frequent circulation types is essential with regard to developing strategies to address the risk of avalanches, floods, soil erosion, air pollution or other natural hazards. In order to derive an environmental regionalization, we present an Open Source R package known as synoptReg, which combines the spatialization of environmental variables based on the atmospheric circulation types. The synoptReg package contains a set of functions, which we will employ (1) to perform a PCA-based synoptic classification using an atmospheric variable; (2) to map the spatial distribution of the selected environmental variable based upon the circulation types; (3) to develop a spatial environmental regionalization based on the previous results. We illustrate the usefulness of the package for a case study in the Alps area.","tags":["Alps","Environmental regionalization","R package","synoptReg","Synoptic classification"],"title":"synoptReg: An R package for computing a synoptic climate classification and a spatial regionalization of environmental data","type":"publication"},{"authors":["D Royé","María T Zarrabeitia","Javier Riancho","Ana Santurtún"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"9f3d52d2c337856a95d57a1727a613c7","permalink":"/en/publication/ictus_madrid_2019/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/en/publication/ictus_madrid_2019/","section":"publication","summary":"The understanding of the role of environment on the pathogenesis of stroke is gaining importance in the context of climate change. This study analyzes the temporal pattern of ischemic stroke (IS) in Madrid, Spain, during a 13-year period (2001-2013), and the relationship between ischemic stroke (admissions and deaths) incidence and environmental factors on a daily scale by using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of air pollutants and Apparent Temperature (AT), a biometeorological index which represents human thermal comfort on IS, a lag non-linear model was fitted in a generalized additive model. The mortality rate followed a downward trend over the studied period, however admission rates progressively increased. Our results show that both increases and decreases in AT had a marked relationship with IS deaths, while hospital admissions were only associated with low AT. When analyzing the cumulative effects (for lag 0 to 14 days), with an AT of 1.7°C (percentile 5%) a RR of 1.20 (95% CI, 1.05-1.37) for IS mortality and a RR of 1.09 (95% CI, 0.91-1.29) for morbidity is estimated. Concerning gender differences, men show higher risks of mortality in low temperatures and women in high temperatures. No significant relationship was found between air pollutant concentrations and IS morbi mortality, but this result must be interpreted with caution, since there are strong spatial fluctuations of the former between nearby geographical areas that make it difficult to perform correlation analyses.","tags":["short‐term effects","Spain","Madrid","thermal environment","ischemic stroke","air pollutants","apparent temperature","mortality","hospital admissions"],"title":"A time series analysis of the relationship between Apparent Temperature, Air Pollutants and Ischemic Stroke in Madrid, Spain","type":"publication"},{"authors":null,"categories":["management","R","R:intermediate"],"content":"\rWe usually work with different data sources, and sometimes we can find tables distributed over several Excel sheets. In this post we are going to import the average daily temperature of Madrid and Berlin which is found in two Excel files with sheets for each year between 2000 and 2005: download.\nPackages\rIn this post we will use the following packages:\n\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rreadxl\rImport Excel files\r\r\r\r#install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;readxl\u0026quot;)) install.packages(\u0026quot;readxl\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(fs)\rlibrary(readxl)\rBy default, the read_excel() function imports the first sheet. To import a different sheet it is necessary to indicate the number or name with the argument sheet (second argument).\n#import first sheet\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;)\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#import third sheet\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;, 3)\r## # A tibble: 365 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2002-01-01 00:00:00 8.7 2002\r## 2 2002-01-02 00:00:00 7.4 2002\r## 3 2002-01-03 00:00:00 8.5 2002\r## 4 2002-01-04 00:00:00 9.2 2002\r## 5 2002-01-05 00:00:00 9.3 2002\r## 6 2002-01-06 00:00:00 7.3 2002\r## 7 2002-01-07 00:00:00 5.4 2002\r## 8 2002-01-08 00:00:00 5.6 2002\r## 9 2002-01-09 00:00:00 6.8 2002\r## 10 2002-01-10 00:00:00 6.1 2002\r## # ... with 355 more rows\rThe excel_sheets() function can extract the names of the sheets.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rpath %\u0026gt;%\rexcel_sheets()\r## [1] \u0026quot;2000\u0026quot; \u0026quot;2001\u0026quot; \u0026quot;2002\u0026quot; \u0026quot;2003\u0026quot; \u0026quot;2004\u0026quot; \u0026quot;2005\u0026quot;\rThe results are the sheet names and we find the years from 2000 to 2005. The most important function to read multiple sheets is map() of the {purrr} package, which is part of the {tidyverse] collection. map() allows you to apply a function to each element of a vector or list.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap(read_excel,\rpath = path)\rstr(mad)\r## List of 6\r## $ 2000:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 366 obs. of 3 variables:\r## ..$ date: POSIXct[1:366], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:366] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:366] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ 2001:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2001-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 8.2 8.8 7.5 9.2 10 9 5.5 4.6 3 7.9 ...\r## ..$ yr : num [1:365] 2001 2001 2001 2001 2001 ...\r## $ 2002:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2002-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 8.7 7.4 8.5 9.2 9.3 7.3 5.4 5.6 6.8 6.1 ...\r## ..$ yr : num [1:365] 2002 2002 2002 2002 2002 ...\r## $ 2003:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2003-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 9.4 10.8 9.7 9.2 6.3 6.6 3.8 6.4 4.3 3.4 ...\r## ..$ yr : num [1:365] 2003 2003 2003 2003 2003 ...\r## $ 2004:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 366 obs. of 3 variables:\r## ..$ date: POSIXct[1:366], format: \u0026quot;2004-01-01\u0026quot; ...\r## ..$ ta : num [1:366] 6.6 5.9 7.8 8.1 6.4 5.7 5.2 6.9 11.8 12.2 ...\r## ..$ yr : num [1:366] 2004 2004 2004 2004 2004 ...\r## $ 2005:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 365 obs. of 3 variables:\r## ..$ date: POSIXct[1:365], format: \u0026quot;2005-01-01\u0026quot; ...\r## ..$ ta : num [1:365] 7.1 7.8 6.4 5.6 4.4 6.8 7.4 6 5.2 4.2 ...\r## ..$ yr : num [1:365] 2005 2005 2005 2005 2005 ...\rThe result is a named list with the name of each sheet that contains the data.frame. Since it is the same table in all sheets, we could use the function bind_rows(), however, there is a variant of map() that directly joins all the tables by row: map_df(). If it were necessary to join by column, map_dfc() could be used.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path)\rmad\r## # A tibble: 2,192 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 2,182 more rows\rIn our case we have a column in each sheet (year, but also the date) that differentiates each table. If it were not the case, we should use the name of the sheets as a new column when joining all of them. In bind_rows() it can be done with the .id argument by assigning a name for the column. The same works for map_df().\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path,\r.id = \u0026quot;yr2\u0026quot;)\rstr(mad)\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 4 variables:\r## $ yr2 : chr \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; ...\r## $ date: POSIXct, format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## $ yr : num 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\rBut how do we import multiple Excel files?\nTo do this, first we must know the dir_ls() function from the {fs} package. Indeed, there is the dir() function of R Base, but the advantages of the recent package are several, especially the compatibility with the {tidyverse} collection.\ndir_ls()\r## berlin_temp.xlsx featured.png index.en.html index.en.Rmd ## madrid_temp.xlsx\r#we can filter the files that we want\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) \r## berlin_temp.xlsx madrid_temp.xlsx\rWe import the two Excel files.\n#without joining\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;%\rmap(read_excel)\r## $berlin_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 1.2 2000\r## 2 2000-01-02 00:00:00 3.6 2000\r## 3 2000-01-03 00:00:00 5.7 2000\r## 4 2000-01-04 00:00:00 5.1 2000\r## 5 2000-01-05 00:00:00 2.2 2000\r## 6 2000-01-06 00:00:00 1.8 2000\r## 7 2000-01-07 00:00:00 4.2 2000\r## 8 2000-01-08 00:00:00 4.2 2000\r## 9 2000-01-09 00:00:00 4.2 2000\r## 10 2000-01-10 00:00:00 1.7 2000\r## # ... with 356 more rows\r## ## $madrid_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#joining with a new id column\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;%\rmap_df(read_excel, .id = \u0026quot;city\u0026quot;)\r## # A tibble: 732 x 4\r## city date ta yr\r## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 berlin_temp.xlsx 2000-01-01 00:00:00 1.2 2000\r## 2 berlin_temp.xlsx 2000-01-02 00:00:00 3.6 2000\r## 3 berlin_temp.xlsx 2000-01-03 00:00:00 5.7 2000\r## 4 berlin_temp.xlsx 2000-01-04 00:00:00 5.1 2000\r## 5 berlin_temp.xlsx 2000-01-05 00:00:00 2.2 2000\r## 6 berlin_temp.xlsx 2000-01-06 00:00:00 1.8 2000\r## 7 berlin_temp.xlsx 2000-01-07 00:00:00 4.2 2000\r## 8 berlin_temp.xlsx 2000-01-08 00:00:00 4.2 2000\r## 9 berlin_temp.xlsx 2000-01-09 00:00:00 4.2 2000\r## 10 berlin_temp.xlsx 2000-01-10 00:00:00 1.7 2000\r## # ... with 722 more rows\rHowever, in this case we only import the first sheet of each Excel file. To solve this problem, we must create our own function. In this function we do what we previously did individually.\nread_multiple_excel \u0026lt;- function(path) {\rpath %\u0026gt;%\rexcel_sheets() %\u0026gt;% set_names() %\u0026gt;% map_df(read_excel, path = path)\r}\rWe apply our created function to import multiple sheets of several Excel files.\n#separately\rdata \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map(read_multiple_excel)\rstr(data)\r## List of 2\r## $ berlin_temp.xlsx:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 3 variables:\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:2192] 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ madrid_temp.xlsx:Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2192 obs. of 3 variables:\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; ...\r## ..$ ta : num [1:2192] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r#joining all data.frames\rdata_df \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map_df(read_multiple_excel,\r.id = \u0026quot;city\u0026quot;)\rstr(data_df)\r## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 4384 obs. of 4 variables:\r## $ city: chr \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; ...\r## $ date: POSIXct, format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## $ yr : num 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r\r","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"ca84d77cc554e05833562162d0d9260f","permalink":"/en/2019/import-excel-sheets-with-r/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/en/2019/import-excel-sheets-with-r/","section":"post","summary":"We usually work with different data sources, and sometimes we can find tables distributed over several Excel sheets. In this post we are going to import the average daily temperature of Madrid and Berlin which is found in two Excel files with sheets for each year between 2000 and 2005.","tags":["excel","sheets","import"],"title":"Import Excel sheets with R","type":"post"},{"authors":["D Royé","N Lorenzo","D Rasilla","A Martí"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"15c539a23e016cefebced36892b897d8","permalink":"/en/publication/cloudiness_ip_2018/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/en/publication/cloudiness_ip_2018/","section":"publication","summary":"This paper presents the first systematic study of the relationships between atmospheric circulation types (CT) and cloud fraction (CF) over the whole Iberian Peninsula, using satellite data from the MODIS (MOD09GA and MYD09GA) cloud mask for the period 2001--2017. The high level of detail, in combination with a classification for circulation patterns, provides us with relevant information about the spatio-temporal variability of cloudiness and the main mechanisms affecting the genesis of clouds. The results show that westerly CTs are the most influential, followed by cyclonic types, in cloudiness in the west of the Iberian Peninsula. Westerly flows, however, do not affect the Mediterranean coastline, which is dominated by easterly CTs, suggesting that local factors such as convective processes, orography and proximity to a body of warm water could play a major role in cloudiness processes. The Cantabrian Coast also has a particularly characteristic cloudiness dominated by northerly CTs. In general, the results found in this study are in line with the few studies that exist on cloudiness in the Iberian Peninsula. Furthermore, the results are geographically consistent, showing links to synoptic forcing in terms of atmospheric circulation patterns and the impact of the Iberian Peninsula's complex orography upon this element of the climate system.","tags":["cloudiness","circulation types","Iberian Peninsula","MODIS","weather","spatio-temporal patterns"],"title":"Spatio-temporal variations of cloud fraction based on circulation types in the Iberian Peninsula","type":"publication"},{"authors":null,"categories":["gis","R","R:elementary"],"content":"\rThe distance to the sea is a fundamental variable in geography, especially relevant when it comes to modeling. For example, in interpolations of air temperature, the distance to the sea is usually used as a predictor variable, since there is a casual relationship between the two that explains the spatial variation. How can we estimate the (shortest) distance to the coast in R?\nPackages\rIn this post we will use the following libraries:\n\r\rLibrary\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rraster\rImport, export and manipulate raster\r\rrnaturalearth\rSet of vector maps ‘natural earth’\r\rRColorBrewer\rColor palettes\r\r\r\r#install the libraries if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r#packages\rlibrary(rnaturalearth)\rlibrary(sf)\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(RColorBrewer)\r\rThe coast of Iceland as an example\rOur example in this post will be Iceland, and, as it is an island territory it will facilitate the tutorial showing the process in a simple manner. The rnaturalearth package allows you to import the boundaries of countries (with different administrative levels) from around the world. The data comes from the platform naturalearthdata.com. I recommend exploring the package, more info here. The ne_countries( ) function imports the country boundaries. In this case we indicate with the argument scale the resolution (10, 50 or 110m), with country we indicate the specific country of interest and with returnclass we determine which class we want (sf or sp), in our case sf (simple feature).\nworld \u0026lt;- ne_countries(scale = 50) #world map with 50m resolution\rplot(world) #sp class by default\r#import the limits of Iceland\riceland \u0026lt;- ne_countries(scale = 10, country = \u0026quot;Iceland\u0026quot;, returnclass = \u0026quot;sf\u0026quot;)\r#info of our spatial vector object\riceland\r## Simple feature collection with 1 feature and 94 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: -24.53991 ymin: 63.39671 xmax: -13.50292 ymax: 66.56415\r## epsg (SRID): 4326\r## proj4string: +proj=longlat +datum=WGS84 +no_defs\r## featurecla scalerank labelrank sovereignt sov_a3 adm0_dif level\r## 188 Admin-0 country 0 3 Iceland ISL 0 2\r## type admin adm0_a3 geou_dif geounit gu_a3 su_dif\r## 188 Sovereign country Iceland ISL 0 Iceland ISL 0\r## subunit su_a3 brk_diff name name_long brk_a3 brk_name brk_group\r## 188 Iceland ISL 0 Iceland Iceland ISL Iceland \u0026lt;NA\u0026gt;\r## abbrev postal formal_en formal_fr name_ciawf note_adm0\r## 188 Iceland IS Republic of Iceland \u0026lt;NA\u0026gt; Iceland \u0026lt;NA\u0026gt;\r## note_brk name_sort name_alt mapcolor7 mapcolor8 mapcolor9 mapcolor13\r## 188 \u0026lt;NA\u0026gt; Iceland \u0026lt;NA\u0026gt; 1 4 4 9\r## pop_est pop_rank gdp_md_est pop_year lastcensus gdp_year\r## 188 339747 10 16150 2017 NA 2016\r## economy income_grp wikipedia fips_10_\r## 188 2. Developed region: nonG7 1. High income: OECD NA IC\r## iso_a2 iso_a3 iso_a3_eh iso_n3 un_a3 wb_a2 wb_a3 woe_id woe_id_eh\r## 188 IS ISL ISL 352 352 IS ISL 23424845 23424845\r## woe_note adm0_a3_is adm0_a3_us adm0_a3_un adm0_a3_wb\r## 188 Exact WOE match as country ISL ISL NA NA\r## continent region_un subregion region_wb name_len\r## 188 Europe Europe Northern Europe Europe \u0026amp; Central Asia 7\r## long_len abbrev_len tiny homepart min_zoom min_label max_label\r## 188 7 7 NA 1 0 2 7\r## ne_id wikidataid name_ar name_bn name_de name_en name_es name_fr\r## 188 1159320917 Q189 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Island Iceland Islandia Islande\r## name_el name_hi name_hu name_id name_it name_ja name_ko name_nl\r## 188 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Izland Islandia Islanda \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; IJsland\r## name_pl name_pt name_ru name_sv name_tr name_vi name_zh\r## 188 Islandia Islândia \u0026lt;NA\u0026gt; Island Izlanda Iceland \u0026lt;NA\u0026gt;\r## geometry\r## 188 MULTIPOLYGON (((-14.56363 6...\r#here Iceland\rplot(iceland)\rBy default, the plot( ) function with the class sf creates as many facets of the map as there are variables in it. To limit this behavior we can use either a variable name plot(iceland[\"admin\"]) or the limit argument plot(iceland, max.plot = 1). With the argument max.plot = 1 the function uses the first available variable of the map.\nIn addition, we see in the information of the object sf that the projection is WGS84 with decimal degrees (EPSG code: 4326). For the calculation of distances it is more convenient to use meters instead of degrees. Because of this, the first thing we do is to transform the map of Iceland to UTM Zone 27 (EPSG code: 3055). More information about EPSG and projections here. For that purpose, we use the st_transform( ) function. We simply indicate the map and the EPSG code.\n#transform to UTM\riceland \u0026lt;- st_transform(iceland, 3055)\r\rCreate a fishnet of points\rWe still need the points where we want to know the distance. In our case it will be a regular fishnet of points in Iceland with a resolution of 5km. We do this with the function st_make_grid( ), indicating the resolution in the unit of the coordinate system (meters in our case) with the argument cellsize, and what geometry we would like to create what (polygons, centers or corners).\n#create the fishnet\rgrid \u0026lt;- st_make_grid(iceland, cellsize = 5000, what = \u0026quot;centers\u0026quot;)\r#our fishnet with the extension of Iceland\rplot(grid)\r#only extract the points in the limits of Iceland\rgrid \u0026lt;- st_intersection(grid, iceland) #our fishnet now\rplot(grid)\r\rCalculating the distance\rTo estimate the distance we use the st_distance( ) function that returns a vector of distances for all our points in the fishnet. But first it is necessary to transform the map of Iceland from a polygon shape (MULTIPOLYGON) to a line (MULTILINESTRING). More details with ?st_cast.\n#transform Iceland from polygon shape to line\riceland \u0026lt;- st_cast(iceland, \u0026quot;MULTILINESTRING\u0026quot;)\r#calculation of the distance between the coast and our points\rdist \u0026lt;- st_distance(iceland, grid)\r#distance with unit in meters\rhead(dist)\r## Units: [m]\r## [1] 790.7906 1151.4360 1270.7603 3128.9057 2428.5677 4197.7472\r\rPlotting the calculated distance\rOnce obtained the distance for our points, we can combine them with the coordinates and plot them in ggplot2. For this, we create a data.frame. The object dist is a matrix of one column, so we have to convert it to a vector with the function as.vector( ). In addition, we divide by 1000 to convert the distance in meters to km. The st_coordinates( ) function extracts the coordinates of our points. For the final visualization we use a vector of colors with the RdGy palette (more here).\n#create a data.frame with the distance and the coordinates of the points\rdf \u0026lt;- data.frame(dist = as.vector(dist)/1000,\rst_coordinates(grid))\r#structure\rstr(df)\r## \u0026#39;data.frame\u0026#39;: 4104 obs. of 3 variables:\r## $ dist: num 0.791 1.151 1.271 3.129 2.429 ...\r## $ X : num 608796 613796 583796 588796 593796 ...\r## $ Y : num 7033371 7033371 7038371 7038371 7038371 ...\r#colors col_dist \u0026lt;- brewer.pal(11, \u0026quot;RdGy\u0026quot;)\rggplot(df, aes(X, Y, fill = dist))+ #variables\rgeom_tile()+ #geometry\rscale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance\rlabs(fill = \u0026quot;Distance (km)\u0026quot;)+ #legend name\rtheme_void()+ #map theme\rtheme(legend.position = \u0026quot;bottom\u0026quot;) #legend position\r\rExport the distance as a raster\rTo be able to export the estimated distance to the sea of Iceland, we need to use the rasterize( ) function of the library raster.\nFirst, it is necessary to create an empty raster. In this raster we have to indicate the resolution, in our case it is of 5000m, the projection and the extension of the raster.\nWe can extract the projection from the information of the map of Iceland.\n\rThe extension can be extracted from our grid points with the function extent( ). However, this last function needs the class sp, so we pass the object grid in sf format, only for this time, to the class sp using the function as( ) and the argument “Spatial”.\n\r\rIn addition to the above, the data.frame df, that we created earlier, has to be converted into the sf class. Therefore, we apply the function st_as_sf( ) with the argument coords indicating the names of the coordinates. Additionally, we also define the coordinate system that we know.\n\r\r#get the extension\rext \u0026lt;- extent(as(grid, \u0026quot;Spatial\u0026quot;))\r#extent object\rext\r## class : Extent ## xmin : 338795.6 ## xmax : 848795.6 ## ymin : 7033371 ## ymax : 7383371\r#raster destination\rr \u0026lt;- raster(resolution = 5000, ext = ext, crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\r#convert the points to a spatial object class sf\rdist_sf \u0026lt;- st_as_sf(df, coords = c(\u0026quot;X\u0026quot;, \u0026quot;Y\u0026quot;)) %\u0026gt;%\rst_set_crs(3055)\r#create the distance raster\rdist_raster \u0026lt;- rasterize(dist_sf, r, \u0026quot;dist\u0026quot;, fun = mean)\r#raster\rdist_raster\r## class : RasterLayer ## dimensions : 70, 102, 7140 (nrow, ncol, ncell)\r## resolution : 5000, 5000 (x, y)\r## extent : 338795.6, 848795.6, 7033371, 7383371 (xmin, xmax, ymin, ymax)\r## crs : +proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs ## source : memory\r## names : layer ## values : 0.006124901, 115.1712 (min, max)\r#plot the raster\rplot(dist_raster)\r#export the raster\rwriteRaster(dist_raster, file = \u0026quot;dist_islandia.tif\u0026quot;, format = \u0026quot;GTiff\u0026quot;, overwrite = TRUE)\rThe rasterize( ) function is designed to create rasters from an irregular grid. In case we have a regular grid, like this one, we can use an easier alternative way. The rasterFromXYZ( ) function converts a data.frame with longitude, latitude and the variable Z into a raster object. It is important that the order should be longitude, latitude, variables.\nr \u0026lt;- rasterFromXYZ(df[, c(2:3, 1)], crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\rplot(r)\rWith the calculation of distance we can create art, as seen in the header of this post, which includes a world map only with the distance to the sea of all continents. A different perspective to our world (here more (spanish)) .\n\r","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"c7b36d9cdb7e5cf3a4e9369cbd9ba007","permalink":"/en/2019/calculating-the-distance-to-the-sea-in-r/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/en/2019/calculating-the-distance-to-the-sea-in-r/","section":"post","summary":"The distance to the sea is a fundamental variable in geography, especially relevant when it comes to modeling. For example, in interpolations of air temperature, the distance to the sea is usually used as a predictor variable, since there is a casual relationship between the two that explains the spatial variation. How can we estimate the (shortest) distance to the coast in R?","tags":["distance","raster","estimation","variable"],"title":"Calculating the distance to the sea in R","type":"post"},{"authors":["F Mori-Gamarra","L Moure-Rodríguez","X Sureda","C Carbiae","D Royé","A Montes-Martínez","F Cadaveira","F Caamaño-Isorna"],"categories":null,"content":"","date":1545350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545350400,"objectID":"0a110e00a459320b291d11a3782d372b","permalink":"/en/publication/alcohol_galicia2018/","publishdate":"2018-12-21T00:00:00Z","relpermalink":"/en/publication/alcohol_galicia2018/","section":"publication","summary":"Objective: To assess the influence that alcohol outlet density, off- and on-alcohol premises, and alcohol consumption wield on the consumption patterns of young pre-university students in Galicia (Spain). Method: A cross-sectional analysis of a cohort of students of the University of Santiago de Compostela (Compostela Cohort 2016) was carried out. Consumption prevalence were calculated for each of the municipalities from the first-cycle students’ home residence during the year prior to admission. The association with risky alcohol consumption (RC) and binge-drinking (BD) was assessed with a logistic model considering as independent variables the municipality population, alcohol outlet density of off- premises, density of off- and on- premises and total density of both types of premises in the municipality. Results: The prevalence of RC was 60.5% (95% confidence interval [95%CI]: 58.4-62.5) and the BD was 28.5% (95%CI: 26.7-30.2). A great variability was observed according to the municipality of provenance. The multivariate logistic model showed municipalities with a density of 8.42-9.34 of both types of premises per thousand inhabitants presented a higher risk of RC (odds ratio [OR]: 1,39; 95%CI: 1.09-1.78) and BD (OR: 1.29; 95%CI: 1.01-1.66). Conclusion: These data suggest the importance of including environmental information when studying alcohol consumption. Knowing our environment better could help plan policies that encourage healthier behaviour in the population.","tags":["Alcohol outlet density","Alcohol","Underage drinking","Adolescents"],"title":"Alcohol outlet density and alcohol consumption in Galician youth","type":"publication"},{"authors":[],"categories":null,"content":"   Probability of a summer day (maximum temperature greater than 25ºC) through the year in Australia. Data: SILO.\n   Probability of a frost day (minimum temperature less than 0ºC) through the year in Europe. Data: E-OBS 18e.\n   Probability of a frost day (minimum temperature less than 0ºC/32ºF) through the year in the Contiguous United States. Data: PRISM. Platform: Google Earth Engine.\n\rConcentration of daily precipitation in the contiguous United States 1981-2017. Data: PRISM. More details here.\r\r\rAverage of Heating and Cooling Degree Days (1950-2018) in Spain. As reference value I used 15.5ºC and 22ºC of average daily temperature, respectively. Data: ECA\u0026amp;D.\r\r\rWinter anomalies of temperature and precipitation in Bilbao. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in Zaragoza. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in Santander. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in Málaga. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in Sevilla. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in Santiago. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in Valencia. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in Vigo. Data: ECA\u0026amp;D\r\r\rWinter anomalies of temperature and precipitation in A Coruña. Data: ECA\u0026amp;D\r\r\rWinter anomalies of daily mean temperature in Madrid. Data: ECA\u0026amp;D. Time series were homogenized with climatol.\r\r\rWinter anomalies of daily mean temperature in Santander. Data: ECA\u0026amp;D. Time series were homogenized with climatol.\r\r\rWinter anomalies of daily mean temperature in Barcelona. Data: ECA\u0026amp;D. Time series were homogenized with climatol.\r\r\rWinter anomalies of daily mean temperature in Santiago. Data: ECA\u0026amp;D. Time series were homogenized with climatol.\r\r\rWinter anomalies of daily mean temperature in Bilbao. Data: ECA\u0026amp;D.Time series were homogenized with climatol.\r\r\rWinter anomalies of daily mean temperature in Sevilla. Data: ECA\u0026amp;D. Time series were homogenized with climatol\r\r\rWinter anomalies of daily mean temperature in A Coruña. Data: ECA\u0026amp;D. Time series were homogenized with climatol\r\r\rWinter anomalies of daily mean temperature in Málaga. Data: ECA\u0026amp;D. Time series were homogenized with climatol.\r\r\rAverage first day of summer in Europe (maximum temperature \u0026gt;25ºC). Data: E-OSB 18e.\r\r\rDistribution of the average first day of summer in Europe (maximum temperature \u0026gt;25ºC). Data: E-OSB 18e.\r\r\rWarming stripes for several Spanish cities. These graphs represent and communicate climate change in a very illustrative and effective way. Data: ECA\u0026amp;D. Time series were homogenized with climatol. More: post.\r\r\r62 years of annual anomalies of precipitation (%) in peninsular Spain in a single graphic. Data: SPREAD.\r\r\rSummer anomaly of temperature and precipitation in Barcelona 1914-2018. Data: ECA\u0026amp;D.\r\r\rSummer anomaly of temperature and precipitation in Madrid 1920-2018. Data: ECA\u0026amp;D.\r\r\rSummer anomaly of temperature and precipitation in Santiago de Compostela 1950-2018. Data: ECA\u0026amp;D.\r\r\rMonthly precipitation anomaly registered in Santiago de Compostela in 2017. Data: ECA\u0026amp;D.\r\r\rMonthly precipitation anomaly registered in Barcelona in 2018. Data: ECA\u0026amp;D, opendata.aemet.es.\r\r\rCloud fraction anomaly for Europe (Summer 2018). Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rTrends of first frost days in Barcelona 1950-2016. 12.1 days later each decade. Data: ECA\u0026amp;D.\r\r\rTrends of last frost days in Madrid 1920-2016. Data: ECA\u0026amp;D.\r\r\rTrends of first tropical nights in Barcelona 1950-2016. Data: ECA\u0026amp;D.\r\r\rTrends of last tropical nights in Barcelona 1950-2016. Data: ECA\u0026amp;D.\r\r\rTrends of first tropical night (minimum temperature \u0026gt; 20ºC) in Madrid 1920-2016. -2.3 days earlier each decade. Data: ECA\u0026amp;D.\r\r\rTrends of last tropical night (minimum temperature \u0026gt; 20ºC) in Madrid 1920-2016. 1.9 days later each decade. Data: ECA\u0026amp;D.\r\r\rTrends of frist days with more than 30ºC in several Spanish cities. Data: ECA\u0026amp;D.\r\r\rTrends of last days with more than 30ºC in several Spanish cities. Data: ECA\u0026amp;D.\r\r\rAverage of consecutive days without rainfall 1950-2012. Data: SPREAD.\r\r\rAverage of consecutive days without rainfall by seasons 1950-2012 in the Iberian Peninsula. Data: SPREAD.\r\r\rAnnual precipitation per inhabitant in Europe based on gridded population (1km resolution) and precipitation (0.25º) data. Data: ECA\u0026amp;D, SEDAC.\r\r\rAverage cloud fraction for summer 2018 and normal 2001-2018 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rLand Surface Temperature anomaly for summer 2018 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rAverage Land Surface Temperature for summer 2018 and normal 2001-2018 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rDistribution of temperature anomalies in autumn according to different decades in Barcelona. You can clearly see how the autumn is getting warmer due to global warming. Data: ECA\u0026amp;D.\r\r\rDistribution of temperature anomalies in autumn according to different decades in Santiago de Compostela. You can clearly see how the autumn is getting warmer due to global warming. Data: ECA\u0026amp;D.\r\r\rClimate circles for several Spanish cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: ECA\u0026amp;D.\r\r\rClimate circles for several European cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: ECA\u0026amp;D.\r\r\rGraphic definition of climate and weather. The difference between weather and climate is particularly a scale of time. Single atmospheric conditions over a short period of time is weather, and climate is the statistical description of all these single condicions over a relatively long period of time.\r\r\rClimate circles for several Chilean cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: explorador.cr2.cl.\r\r\rSummer months, mild winters, lot of sun and little wind, the climatic preferences for the Galician population. Map is based on survey results. More: article (galician).\r\r\rWhere is the lightning activity concentrated in a few days in Galicia? Values toward 1 indicate that a few days contribute much of all the lightning; instead, values toward 0 are places where more regularity is observed. More: article.\r\r\rHow is the lightning activity distributed annual and by seasons in Galicia? Data: meteogalicia. More: article.\r\r\rHow is the lightning activity distributed by month and hour in Galicia? Data: meteogalicia. More: article.\r\r\rSeasonal rainfall regime, i.e. ranking seasons according to average precipitation (1950-2017) in descending order in Europe. (P, spring; S, summer; A, autumn; W, winter). Data: ECA\u0026amp;D.\r\r\rSeasonal rainfall regime, i.e. ranking seasons according to average precipitation (1956-2006) in descending order in the contiguous United States. (P, spring; S, summer; A, autumn; W, winter). More: article, dataset.\r\r\rConcentration of Daily Precipitation (1956-2006) in the contiguous United States. (P, spring; S, summer; A, autumn; W, winter). The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. More: article, dataset.\r\r\rAverage cloud fraction for summer 2001-2018 in the contiguous United States. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for winter 2001-2018 in the contiguous United States. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) for different dates in Europe. Data: ECA\u0026amp;D.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) through the year in Europe. Data: ECA\u0026amp;D.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) through the year in the pensinular Spain. Data: STEAD from Research Group climayagua.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC) for different dates in the pensinular Spain. Data: STEAD from Research Group climayagua.\r\r\rSummer day probability (maximum temperature \u0026gt; 25ºC/77ºF) through the year in the Contiguous United States. Data: Maurer et al (2002).\r\r\rAnnual sunhours for Germany in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: ECA\u0026amp;D, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rWarming stripes for Lisboa. These graphs represent and communicate climate change in a very illustrative and effective way. Data: GISTEMP. More: post.\r\r\rWhere do we observe the trajectories of extratropical cyclones in Europe? Here the frequency for the months October to March between 1979-2010. Data: extra-tropical cyclone tracks.\r\r\rWarming stripes for Madrid. These graphs represent and communicate climate change in a very illustrative and effective way. Data: ECA\u0026amp;D. More: post.\r\r\rAverage cloud fraction for summer 2001-2018 in Germany. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for winter 2001-2018 in Germany. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAnnual sunhours for Galicia (Spain) in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: Meteogalicia, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rAnnual sunhours for Spain in 2017. Map is a result of a interpolation process based on sunhour registers and cloudiness from MODIS. Data: ECA\u0026amp;D, NASA/MODIS. Platform for MODIS: Google Earth Engine.\r\r\rPotential insolation (sun hours) in Barcelona\u0026rsquo;s city center for July 21 and December 22. The calculation is based on the Sky View Factor. Estimation made using SAGA-GIS. Data: LiDAR-IGN.\r\r\rAverage Diurnal Land Surface Temperature for July 2017 in Europe. Data: NASA/MODIS Platform: Google Earth Engine.\r\r\rAverage Night Land Surface Temperature for July 2017 in Europe. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rNumber of snow days on the ground in the Iberian Peninsula (2002-2017). The daily images with a binary code (condition: Snow_Cover_Daily_Tile == 200, and Fractional_Snow_Cover \u0026gt; 90) have been reclassified and than summed up and divided by the number of years. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rClimate circles for several American cities. For each day of the year the average of the maximum and minimum (bar length) and the average temperature (color) is indicated. Data: NCDC-CDO/NOAA.\r\r\rAverage cloud fraction for summer 2018 and normal 2001-2017 in the Iberian Peninsular. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAverage cloud fraction for march 2018 and march 2001-2018 in the Iberian Peninsular. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAccumulated precipitation of 2017 compared to other years in Valladolid. Data: ECA\u0026amp;D.\r\r\rAccumulated precipitation of 2017 compared to other years in Vigo. Data: ECA\u0026amp;D.\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544828400,"objectID":"0540a522c0da792daafe21d2d251604c","permalink":"/en/graphs/climate/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/en/graphs/climate/","section":"graphs","summary":" ","tags":["climate","weather","datavis","atmosphere","temperature"],"title":"Climate and Weather","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rLight pollution by municipality in 2015. What is the municipality that emits the most artificial light? The map shows the Coefficient of Variation (standard deviation/mean) of each municipalities. Data: VIIRS Nighttime Lights NOAA (2015).\r\r\rWhich coast has more light pollution in Peninsular Spain? The coast margins include a buffer of 5km. Data: VIIRS Nighttime Lights NOAA (2016).\r\r\rLight pollution by municipality in 2015. What is the municipality that emits the most artificial light? The map shows the Coefficient of Variation (standard deviation/mean) of each municipalities. Data: VIIRS Nighttime Lights NOAA (2015).\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in the Iberian Peninsula. Clearly stand out Barcelona and Madrid. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in Germany. Clearly stands out the Rhine-Ruhr metropolitan region. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rPollution spots from nitrogen dioxide (NO2) in autumn 2018 in the contiguous United States. Data: NASA/MODIS. Platform: Google Earth Engine.\r\r\rAir pollution in the Iberian Peninsula. Annual average of PM2.5 for 2016 seen by MODIS/MISR/SeaWiFS. Clearly visible are Madrid and Barcelona. Data: SEDAC.\r\r\rOur human footprint in the Iberian Peninsula, or rather, the pressure we exert on the terrestrial ecosystem. Data: Global terrestrial Human Footprint .\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544050800,"objectID":"0570828f5e1287a69c4e732cc5b69020","permalink":"/en/graphs/environment/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/en/graphs/environment/","section":"graphs","summary":" ","tags":["datavis","environment","nature","pollution","ecosystem"],"title":"Environment","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rSpatial patterns of cemeteries in Northwest Spain (number per 1,000 inhabitants) based on OpenStreetMaps. Data: OpenStreetMap, IGE More: post.\r\r\rSpatial patterns of cemeteries in Spain (number per 10,000 inhabitants) based on OpenStreetMaps. Data: OpenStreetMap, INE More: post.\r\r\rUrban growth seen by the year of construction for the six largest Spanish cities.. Data: Catastro.\r\r\rAnimation of urban growth seen by the year of construction in Valencia, Spain. Data: Catastro.\r\r\rUrban growth seen by the year of construction in Valencia, Spain. Data: Catastro.\r\r\rDistribution of the year of building construction in Spanish provincial capitals from 1850. Data: Catastro.\r\r\rDistribution of travel time to high-density urban centers via surface transport in Europe by NUTS-1 in 2015. Data: resourcewatch.org.\r\r\rDistribution of travel time to high-density urban centers via surface transport in Spain by province in 2015. Data: resourcewatch.org.\r\r\rConsumer Price Index: Rental of housing (annual change) 2002-2019 for Spanish provinces. Data: INE.\r\r\rTravel time to high-density urban centers via surface transport in Spain. Data: resourcewatch.org.\r\r\rRanking of travel time to high-density urban centers via surface transport by Spanish Provinces. Data: resourcewatch.org.\r\r\rInspired by the great work of @geo_coe I have created an elevation map for the meandering river Ebro, middle section between Logroño and Zaragoza in Spain. Data: DEM05 (Modelo Digital del Terreno - MDT05).\r\r\rInspired by the great work of @geo_coe I have created an elevation map for the meandering river Alagón, right tributary to the Tagus in Spain. Data: DEM05 (Modelo Digital del Terreno - MDT05).\r\r\rUrban growth of Santiag de Compostela from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rFlight routes of the ten busiest airports by passenger traffic in Europe based on 24 hour data for each airport. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rThe Sky View Factor is very useful urban spatial indicator for radiation and thermal environmental assessment. SVF describes how visible is the sky (0: the entire sky is blocked from view; 1: free view on the whole sky). Estimation made using SAGA-GIS. Data: LiDAR-IGN.\r\r\rEuropean flight density based on 24 hour data for each of the ten busiest airports by passenger. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of Frankfurt airport based on 24 hour data. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of the busiest airports in the Iberian Peninsula based on 24 hour data. Data: https://www.flightradar24.com/. More: article (spanish)\r\r\rFlight routes of Paris Charles de Gaulle airport based on 24 hour data. Data: https://www.flightradar24.com/. More: link\r\r\rUrban growth of Madrid from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rTotal hours of fishing activity per km2 for the year 2016 in the Iberian Peninsula. Data: https://globalfishingwatch.org/\r\r\rTotal hours of fishing activity per km2 for the year 2016 in the Mediterranean. Data: https://globalfishingwatch.org/\r\r\rDistribution of gas stations (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of drinking water (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (spanish),script\r\r\rDistribution of gas and charging stations (Point Of Interest) in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of drinking water (Point Of Interest) in the World, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (Spanish),script\r\r\rAnother perspective on the world. Distance to the sea (the more black, the further away is the sea). Euclidean distance estimation made with R. More: article (Spanish)\r\r\rNumber of bars per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: article (Spanish),script\r\r\rNumber of Cafes per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of building heights at 10 meter resolution in European capitals. Data: COPERNICUS\r\r\rDifferences of building heights at 10 meter resolution in European capitals. Data: COPERNICUS\r\r\rUrban growth of Barcelona from before 1800 until today. Data: Catastro INSPIRE QGIS-Plugin.\r\r\rNumber of published articles in ElPaís by year for the term \u0026lsquo;wildfire\u0026rsquo;. Data: elpais\r\r\rDistribution of fastfood restaurants in the contiguous United States, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of fastfood restaurants per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of pharmacies per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rSpain leads access to Open Data in the EU. Data: europendataportal\r\r\rNumber of pubs per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rDistribution of fastfood restaurants in Europe., extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of restaurants per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r\rNumber of Kebab restaurants per 10,000 inhabitants in Europe, extracted from the overpass API of OpenStreetMaps (June 2017). Data: OpenStreetMaps. More: script\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515193200,"objectID":"1de12ae97ed63c8a065f7603f22e9e1c","permalink":"/en/graphs/geography/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/en/graphs/geography/","section":"graphs","summary":" ","tags":["datavis","geography","distribution","human","physical"],"title":"Geography","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rEvolution of adult obesity in Europe between 1975 and 2016. Data: ourwoldindata, script.\r\r\rNational Overdose Death in the US by different drugs since 1999, showing a horrifying trend. Data: ourwoldindata.\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515193200,"objectID":"760deca28613896076d07e4c526494ad","permalink":"/en/graphs/health/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/en/graphs/health/","section":"graphs","summary":" ","tags":["datavis","health","population","disease","global"],"title":"Health","type":"graphs"},{"authors":[],"categories":null,"content":"\r\rPopulation point clouds of Ourense since 1975. Data: IGE\r\r\rPopulation pyramid of Galicia since 1975. Data: IGE\r\r\rPopulation pyramid of Galician provinces since 1975. Data: IGE\r\r\rPopulation pyramid of Spanish autonomous community since 1998. Data: INE\r\r\rMigration flows between Spanish autonomous communities in 2008. Data: INE\r\r ","date":1544828400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544050800,"objectID":"f995b7608a372052bdfa5bed0c14c213","permalink":"/en/graphs/population/","publishdate":"2018-12-15T00:00:00+01:00","relpermalink":"/en/graphs/population/","section":"graphs","summary":" ","tags":["datavis","demography","human","aging"],"title":"Population","type":"graphs"},{"authors":null,"categories":["datavis","R","R:elementary"],"content":"\rThis year, the so-called warming stripes, which were created by the scientist Ed Hawkins of the University of Reading, became very famous all over the world. These graphs represent and communicate climate change in a very illustrative and effective way.\nVisualising global temperature change since records began in 1850. Versions for USA, central England \u0026amp; Toronto available too: https://t.co/H5Hv9YgZ7v pic.twitter.com/YMzdySrr3A\n\u0026mdash; Ed Hawkins (@ed_hawkins) May 23, 2018  From his idea, I created strips for examples of Spain, like the next one in Madrid.\n#Temperatura anual en #MadridRetiro desde 1920 a 2017. #CambioClimatico #dataviz #ggplot2 (idea de @ed_hawkins 🙏) @Divulgameteo @edupenabad @climayagua @ClimaGroupUB @4gotas_com pic.twitter.com/wmLb5uczpT\n\u0026mdash; Dominic Royé (@dr_xeo) June 2, 2018  In this post I will show how you can create these strips in R with the library ggplot2. Although I must say that there are many ways in R that can lead us to the same result or to a similar one, even within ggplot2.\nData\rIn this case we will use the annual temperatures of Lisbon\rGISS Surface Temperature Analysis, homogenized time series, comprising the period from 1880 to 2018. Monthly temperatures or other time series could also be used. The file can be downloaded here. First, we should, as long as we have not done it, install the collection of tidyverse libraries that also include ggplot2. In addition, we will need the library lubridate for the treatment of dates. Then, we import the data of Lisbon in csv format.\n#install the lubridate and tidyverse libraries\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\r#packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(RColorBrewer)\r#import the annual temperatures\rtemp_lisboa \u0026lt;- read_csv(\u0026quot;temp_lisboa.csv\u0026quot;)\rstr(temp_lisboa)\r## Classes \u0026#39;spec_tbl_df\u0026#39;, \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 139 obs. of 18 variables:\r## $ YEAR : num 1880 1881 1882 1883 1884 ...\r## $ JAN : num 9.17 11.37 10.07 10.86 11.16 ...\r## $ FEB : num 12 11.8 11.9 11.5 10.6 ...\r## $ MAR : num 13.6 14.1 13.5 10.5 12.4 ...\r## $ APR : num 13.1 14.4 14 13.8 12.2 ...\r## $ MAY : num 15.7 17.3 15.6 14.6 16.4 ...\r## $ JUN : num 17 19.2 17.9 17.2 19.1 ...\r## $ JUL : num 19.1 21.8 20.3 19.5 21.4 ...\r## $ AUG : num 20.6 23.5 21 21.6 22.4 ...\r## $ SEP : num 20.7 20 18 18.8 19.5 ...\r## $ OCT : num 17.9 16.3 16.4 15.8 16.4 ...\r## $ NOV : num 12.5 14.7 13.7 13.5 12.5 ...\r## $ DEC : num 11.07 9.97 10.66 9.46 10.25 ...\r## $ D-J-F : num 10.7 11.4 10.6 11 10.4 ...\r## $ M-A-M : num 14.1 15.2 14.3 12.9 13.6 ...\r## $ J-J-A : num 18.9 21.5 19.7 19.4 20.9 ...\r## $ S-O-N : num 17 17 16 16 16.1 ...\r## $ metANN: num 15.2 16.3 15.2 14.8 15.3 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. YEAR = col_double(),\r## .. JAN = col_double(),\r## .. FEB = col_double(),\r## .. MAR = col_double(),\r## .. APR = col_double(),\r## .. MAY = col_double(),\r## .. JUN = col_double(),\r## .. JUL = col_double(),\r## .. AUG = col_double(),\r## .. SEP = col_double(),\r## .. OCT = col_double(),\r## .. NOV = col_double(),\r## .. DEC = col_double(),\r## .. `D-J-F` = col_double(),\r## .. `M-A-M` = col_double(),\r## .. `J-J-A` = col_double(),\r## .. `S-O-N` = col_double(),\r## .. metANN = col_double()\r## .. )\rWe see in the columns that we have monthly and seasonal values, and the annual temperature value. But before proceeding to visualize the annual temperature, we must replace the missing values 999.9 with NA, using the ifelse( ) function that evaluates a condition and perform the given argument corresponding to true and false.\n#select only the annual temperature and year column\rtemp_lisboa_yr \u0026lt;- select(temp_lisboa, YEAR, metANN)\r#rename the temperature column\rtemp_lisboa_yr \u0026lt;- rename(temp_lisboa_yr, ta = metANN)\r#missing values 999.9\rsummary(temp_lisboa_yr) \r## YEAR ta ## Min. :1880 Min. : 14.53 ## 1st Qu.:1914 1st Qu.: 15.65 ## Median :1949 Median : 16.11 ## Mean :1949 Mean : 37.38 ## 3rd Qu.:1984 3rd Qu.: 16.70 ## Max. :2018 Max. :999.90\rtemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, ta = ifelse(ta == 999.9, NA, ta))\rWhen we use the year as a variable, we do not usually convert it into a date object, however it is advisable. This allows us to use the date functions of the library lubridate and the support functions inside of ggplot2. The str_c( ) function of the library stringr, part of the collection of tidyverse, is similar to paste( ) of R Base that allows us to combine characters by specifying a separator (sep = “-”). The ymd( ) (year month day) function of the lubridate library converts a date character into a Date object. It is possible to combine several functions\rusing the pipe operator %\u0026gt;% that helps to chain without assigning the result to a new object. Its use is very extended especially with the library tidyverse. If you want to know more about its use, here you have a tutorial.\ntemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, date = str_c(YEAR, \u0026quot;01-01\u0026quot;, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd())\r\rCreating the strips\rFirst, we create the style of the graph, specifying all the arguments of the theme we want to adjust. We start with the default style of theme_minimal( ). In addition, we assign\rthe colors from RColorBrewer to an object col_srip. More information about the colors used here.\ntheme_strip \u0026lt;- theme_minimal()+\rtheme(axis.text.y = element_blank(),\raxis.line.y = element_blank(),\raxis.title = element_blank(),\rpanel.grid.major = element_blank(),\rlegend.title = element_blank(),\raxis.text.x = element_text(vjust = 3),\rpanel.grid.minor = element_blank(),\rplot.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;)\r)\rcol_strip \u0026lt;- brewer.pal(11, \u0026quot;RdBu\u0026quot;)\rbrewer.pal.info\r## maxcolors category colorblind\r## BrBG 11 div TRUE\r## PiYG 11 div TRUE\r## PRGn 11 div TRUE\r## PuOr 11 div TRUE\r## RdBu 11 div TRUE\r## RdGy 11 div FALSE\r## RdYlBu 11 div TRUE\r## RdYlGn 11 div FALSE\r## Spectral 11 div FALSE\r## Accent 8 qual FALSE\r## Dark2 8 qual TRUE\r## Paired 12 qual TRUE\r## Pastel1 9 qual FALSE\r## Pastel2 8 qual FALSE\r## Set1 9 qual FALSE\r## Set2 8 qual TRUE\r## Set3 12 qual FALSE\r## Blues 9 seq TRUE\r## BuGn 9 seq TRUE\r## BuPu 9 seq TRUE\r## GnBu 9 seq TRUE\r## Greens 9 seq TRUE\r## Greys 9 seq TRUE\r## Oranges 9 seq TRUE\r## OrRd 9 seq TRUE\r## PuBu 9 seq TRUE\r## PuBuGn 9 seq TRUE\r## PuRd 9 seq TRUE\r## Purples 9 seq TRUE\r## RdPu 9 seq TRUE\r## Reds 9 seq TRUE\r## YlGn 9 seq TRUE\r## YlGnBu 9 seq TRUE\r## YlOrBr 9 seq TRUE\r## YlOrRd 9 seq TRUE\rFor the final graphic we use the geometry geom_tile( ). Since the data does not have a specific value for the Y axis, we need a dummy value, here I used 1. Also, I adjust the width of the color bar in the legend.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile()+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_continuous(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rguides(fill = guide_colorbar(barwidth = 1))+\rlabs(title = \u0026quot;LISBOA 1880-2018\u0026quot;,\rcaption = \u0026quot;Datos: GISS Surface Temperature Analysis\u0026quot;)+\rtheme_strip\rIn case we want to get only the strips, we can use theme_void( ) and the argument show.legend = FALSE in geom_tile( ) to remove all style elements. We can also change the color for the NA values, including the argument na.value = “gray70” in the scale_fill_gradientn( ) function.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile(show.legend = FALSE)+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_discrete(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rtheme_void()\r\r","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"42602d7478f5a984c3ad25eb3362ff7c","permalink":"/en/2018/how-to-create-warming-stripes-in-r/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/en/2018/how-to-create-warming-stripes-in-r/","section":"post","summary":"This year, the so-called warming stripes, which were created by the scientist Ed Hawkins of the University of Reading, became very famous all over the world. These graphs represent and communicate climate change in a very illustrative and effective way.","tags":["ggplot2","warming stripes","global warming","visualization"],"title":"How to create 'Warming Stripes' in R","type":"post"},{"authors":null,"categories":["visualization","R:elementary","R","mapping"],"content":"\rThe database of Open Street Maps\rRecently I created a map of the distribution of gas stations and electric charging stations in Europe.\nPopulation density through the number of gas stations in Europe. #dataviz @AGE_Oficial @mipazos @simongerman600 @openstreetmap pic.twitter.com/eIUx2yn7ej\n\u0026mdash; Dominic Royé (@dr_xeo) February 25, 2018  How can you obtain this data?\nWell, in this case I used points of interest (POIs) from the database of Open Street Maps (OSM). Obviously OSM not only contains streets and highways, but also information that can be useful when we use a map such as locations of hospitals or gas stations. To avoid downloading the entire OSM and extracting the required information, you can use an overpass API, which allows us to query the OSM database with our own criteria.\nAn easy way to access an overpass API is through overpass-turbo.eu, which even includes a wizard to build a query and display the results on a interactive map. A detailed explanation of the previous web can be found here.\rHowever, we have at our disposal a package osmdata that allows us to create and make queries directly from the R environment. Nevertheless, the use of the overpass-turbo.eu can be useful when we are not sure what we are looking for or when we have some difficulty in building the query.\n\rAccessing the overpass API from R\rThe first step is to install several packages, in case they are not installed. In almost all my scripts I use tidyverse which is a fundamental collection of different packages, including dplyr (data manipulation), ggplot2 (visualization), etc. The sf package is the new standard for working with spatial data and is compatible with ggplot2 and dplyr. Finally, ggmap makes it easier for us to create maps.\n#install the osmdata, sf, tidyverse and ggmap package\rif(!require(\u0026quot;osmdata\u0026quot;)) install.packages(\u0026quot;osmdata\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggmap\u0026quot;)) install.packages(\u0026quot;ggmap\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(osmdata)\rlibrary(sf)\rlibrary(ggmap)\r\rBuild a query\rBefore creating a query, we need to know what we can filter. The available_features( ) function returns a list of available OSM features that have different tags. More details are available in the OSM wiki here.\rFor example, the feature shop contains several tags among others supermarket, fishing, books, etc.\n#the first five features\rhead(available_features())\r## [1] \u0026quot;4wd only\u0026quot; \u0026quot;abandoned\u0026quot; \u0026quot;abutters\u0026quot; \u0026quot;access\u0026quot; \u0026quot;addr\u0026quot; \u0026quot;addr:city\u0026quot;\r#amenities\rhead(available_tags(\u0026quot;amenity\u0026quot;))\r## [1] \u0026quot;animal boarding\u0026quot; \u0026quot;animal shelter\u0026quot; \u0026quot;arts centre\u0026quot; \u0026quot;atm\u0026quot; ## [5] \u0026quot;baby hatch\u0026quot; \u0026quot;baking oven\u0026quot;\r#shops\rhead(available_tags(\u0026quot;shop\u0026quot;))\r## [1] \u0026quot;agrarian\u0026quot; \u0026quot;alcohol\u0026quot; \u0026quot;anime\u0026quot; \u0026quot;antiques\u0026quot; \u0026quot;appliance\u0026quot; \u0026quot;art\u0026quot;\rThe first query: Where are cinemas in Madrid?\rTo build the query, we use the pipe operator %\u0026gt;%, which helps to chain several functions without assigning the result to a new object. Its use is very extended especially within the tidyverse package collection. If you want to know more about its use, you can find here a tutorial.\nIn the first part of the query we need to indicate the place where we want to extract the information. The getbb( ) function creates a boundering box for a given place, looking for the name. The main function is opq( ) which build the final query. We add our filter criteria with the add_osm_feature( ) function. In this first query we will look for cinemas in Madrid. That’s why we use as key amenity and cinema as tag. There are several formats to obtain the resulting spatial data of the query. The osmdata_*( ) function sends the query to the server and, depending on the suffix * sf/sp/xml, returns a simple feature, spatial or XML format.\n#building the query\rq \u0026lt;- getbb(\u0026quot;Madrid\u0026quot;)%\u0026gt;%\ropq()%\u0026gt;%\radd_osm_feature(\u0026quot;amenity\u0026quot;, \u0026quot;cinema\u0026quot;)\rstr(q) #query structure\r## List of 4\r## $ bbox : chr \u0026quot;40.3119774,-3.8889539,40.6437293,-3.5179163\u0026quot;\r## $ prefix : chr \u0026quot;[out:xml][timeout:25];\\n(\\n\u0026quot;\r## $ suffix : chr \u0026quot;);\\n(._;\u0026gt;;);\\nout body;\u0026quot;\r## $ features: chr \u0026quot; [\\\u0026quot;amenity\\\u0026quot;=\\\u0026quot;cinema\\\u0026quot;]\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr [1:2] \u0026quot;list\u0026quot; \u0026quot;overpass_query\u0026quot;\rcinema \u0026lt;- osmdata_sf(q)\rcinema\r## Object of class \u0026#39;osmdata\u0026#39; with:\r## $bbox : 40.3119774,-3.8889539,40.6437293,-3.5179163\r## $overpass_call : The call submitted to the overpass API\r## $meta : metadata including timestamp and version numbers\r## $osm_points : \u0026#39;sf\u0026#39; Simple Features Collection with 196 points\r## $osm_lines : NULL\r## $osm_polygons : \u0026#39;sf\u0026#39; Simple Features Collection with 12 polygons\r## $osm_multilines : NULL\r## $osm_multipolygons : NULL\rWe see that the result is a list of different spatial objects. In our case, we are only interested in osm_points.\nHow can we visulise these points?\nThe advantage of sf objects is that for ggplot2 already exists a geometry function geom_sf( ). Furthermore, we can include a background map using ggmap. The get_map( ) function downloads the map for a given place. Alternatively, it can be an address, latitude/longitude or a bounding box. The maptype argument allows us to indicate the style or type of map. You can find more details in the help of the ?get_map function.\nWhen we build a graph with ggplot we usually start with ggplot( ). In this case, we start with ggmap( ) that includes the object with our background map. Then we add with geom_sf( ) the points of the cinemas in Madrid. It is important to indicate with the argument inherit.aes = FALSE that it has to use the aesthetic mappings of the spatial object osm_points. In addition, we change the color, fill, transparency (alpha), type and size of the circles.\n#our background map\rmad_map \u0026lt;- get_map(getbb(\u0026quot;Madrid\u0026quot;),maptype = \u0026quot;toner-background\u0026quot;)\r#final map\rggmap(mad_map)+\rgeom_sf(data=cinema$osm_points,\rinherit.aes =FALSE,\rcolour=\u0026quot;#238443\u0026quot;,\rfill=\u0026quot;#004529\u0026quot;,\ralpha=.5,\rsize=4,\rshape=21)+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)\r\rWhere can we find Mercadona supermarkets?\rInstead of obtaining a bounding box with the function getbb( ) we can build our own box. To do this, we create a matrix of two columns, the order has to be East/West/South/North. In the query we use two features: name and shop to filter supermarkets that are of this particular brand. Depending on the area or volume of the query, it is necessary to extend the waiting time. By default, the limit is set at 25 seconds (timeout).\nThe map, we create in this case, consists only of the supermarket points. Therefore, we use the usual grammar by adding the geometry geom_sf( ). The theme_void( ) function removes everything except for the points. By default, the object sf in ggplot creates reticles, that can be removed using the coord_sf( ) function with the argument datum = NA.\n#bounding box for the Iberian Peninsula\rm \u0026lt;- matrix(c(-10,5,30,46),ncol=2,byrow=TRUE)\rrow.names(m) \u0026lt;- c(\u0026quot;x\u0026quot;,\u0026quot;y\u0026quot;)\rnames(m) \u0026lt;- c(\u0026quot;min\u0026quot;,\u0026quot;max\u0026quot;)\r#building the query\rq \u0026lt;- m %\u0026gt;% opq (timeout=25*100) %\u0026gt;%\radd_osm_feature(\u0026quot;name\u0026quot;,\u0026quot;Mercadona\u0026quot;)%\u0026gt;%\radd_osm_feature(\u0026quot;shop\u0026quot;,\u0026quot;supermarket\u0026quot;)\r#query\rmercadona \u0026lt;- osmdata_sf(q)\r#final map\rggplot(mercadona$osm_points)+\rgeom_sf(colour=\u0026quot;#08519c\u0026quot;,\rfill=\u0026quot;#08306b\u0026quot;,\ralpha=.5,\rsize=1,\rshape=21)+\rcoord_sf(datum=NA)+\rtheme_void()\r\r\r","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"701016639d7073a67bf2e8d9a21ef1f3","permalink":"/en/2018/accessing-openstreetmap-data-with-r/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/en/2018/accessing-openstreetmap-data-with-r/","section":"post","summary":"Recently I created a map of the distribution of gas stations and electric charging stations in Europe. How can you obtain this data? Well, in this case I used points of interest (POIs) from the database of *Open Street Maps* (OSM). Obviously OSM not only contains the streets and highways, but also information that can be useful when we use a map such as locations of hospitals or gas stations.","tags":["database","overpass API","OSM","Point of interest"],"title":"Accessing OpenStreetMap data with R","type":"post"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","J Bech","FS Rodrigo"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"4de1ab5836dd086cdc9d0fa45b7fbc5c","permalink":"/en/publication/appliedgeophysics_2017/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/en/publication/appliedgeophysics_2017/","section":"publication","summary":"The Eastern Mediterranean is one of the most prominent hot spots of climate change in the world and extreme climatic phenomena in this region such as drought or extreme rainfall events are expected to become more frequent and intense. In this study climate extreme indices recommended by the joint World Meteorological Organization Expert Team on Climate Change Detection and Indices are calculated for daily precipitation data in 70 weather stations during 1961–2012. Observed trends and changes in daily precipitation extremes over the EM basin were analysed using the RClimDex package, which was developed by the Climate Research Branch of the Meteorological Service of Canada. Extreme and heavy precipitation events showed globally a statistically significant decrease in the Eastern Mediterranean and, in the southern parts, a significant decrease in total precipitation. The overall analysis of extreme precipitation indices reveals that decreasing trends are generally more frequent than increasing trends. We found statistically significant decreasing trends (reaching 74% of stations for extremely wet days) and increasing trends (reaching 36% of stations for number of very heavy precipitation days). Finally, most of the extreme precipitation indices have a statistically significant positive correlation with annual precipitation, particularly the number of heavy and very heavy precipitation days.","tags":["Eastern Mediterranean","extreme precipitation","trend","spatial temporal distribution"],"title":"Observed Changes in Daily Precipitation Extremes at Annual Timescale Over the Eastern Mediterranean During 1961–2012","type":"publication"},{"authors":null,"categories":["R","R:intermediate"],"content":"\r\r1 Introduction\r2 NCEP\r2.1 Packages\r2.2 Data download\r2.3 Monthly average\r2.4 Visualization\r\r3 ERA-Interim\r3.1 Installation\r3.2 Connection and download with the ECMWF API\r3.3 Processing ncdf\r\r4 Update for accessing ERA-5\r\r\rA friend advised me to introduce R levels as categories. An idea that I now add to each blog post. There are three levels: elementary, intermediate, and advanced. I hope it will help the reader and the R user.\n1 Introduction\rIn this post, I will show how we can download and work directly with data from climatic reanalysis in R. These kind of datasets are a combination of forcast models and data assimilation systems, which allows us to create corrected global grids of recent history of the atmosphere, land surface, and oceans. The two most used reanalyses are NCEP-DO (Reanalysis II) from the NOAA/OAR/ESRL, an improved version of NCEP-NCAR (Reanalysis I), and ERA-Interim from the ECMWF. Since NCEP-DO is the first generation, it is recommended to use third-generation climate reanalysis, especially ERA-Interim. An overview of the current atmospheric reanalysis can be found here. First, let’s see how to access the NCEP data through an R library on CRAN that facilitates the download and handling of the data. Then we will do the same with the ERA-Interim, however, to access this last reanalysis dataset it is necessary to use python and the corresponding API of the ECMWF.\n\r2 NCEP\rTo access the NCEP reanalysis it is required to install the corresponding package RNCEP. The main function is NCEP.gather( ). The resolution of the NCEP reanalysis is 2.5º X 2.5º.\n2.1 Packages\r#install the RNCEP, lubridate and tidyverse packages\rif(!require(\u0026quot;RNCEP\u0026quot;)) install.packages(\u0026quot;RNCEP\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\r#load the packages\rlibrary(RNCEP)\rlibrary(lubridate) #date and time manipulation\rlibrary(tidyverse) #data manipulation and visualization\rlibrary(RColorBrewer) #color schemes\rlibrary(sf) #to import a spatial object and to work with geom_sf in ggplot2\r\r2.2 Data download\rWe will download the air temperature of the 850haPa pressure level for the year 2016. The variables and pressure levels can be found in the details of the function ?NCEP.gather. The reanalysis2 argument allows us to download both version I and version II, being by default FALSE, that is, we access reanalysis I. In all the requests we will obtain data of every 6 hours (00:00, 06:00, 12:00 and 18:00). This supposes a total of 1464 values for the year 2016.\n#define the necessary arguments\rmonth_range \u0026lt;- c(1,12) #period of months\ryear_range \u0026lt;- c(2016,2016) #period of years\rlat_range \u0026lt;- c(30,60) #latitude range\rlon_range \u0026lt;- c(-30,50) #longitude range\rdata \u0026lt;- NCEP.gather(\u0026quot;air\u0026quot;, #name of the variable\r850, #pressure level 850hPa\rmonth_range,year_range,\rlat_range,lon_range,\rreturn.units = TRUE,\rreanalysis2=TRUE)\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r#dimensions dim(data) \r## [1] 13 33 1464\r#we find lon, lat and time with dimnames()\r#date and time\rdate_time \u0026lt;- dimnames(data)[[3]]\rdate_time \u0026lt;- ymd_h(date_time)\rhead(date_time)\r## [1] \u0026quot;2016-01-01 00:00:00 UTC\u0026quot; \u0026quot;2016-01-01 06:00:00 UTC\u0026quot;\r## [3] \u0026quot;2016-01-01 12:00:00 UTC\u0026quot; \u0026quot;2016-01-01 18:00:00 UTC\u0026quot;\r## [5] \u0026quot;2016-01-02 00:00:00 UTC\u0026quot; \u0026quot;2016-01-02 06:00:00 UTC\u0026quot;\r#longitude and latitude\rlat \u0026lt;- dimnames(data)[[1]]\rlon \u0026lt;- dimnames(data)[[2]]\rhead(lon);head(lat)\r## [1] \u0026quot;-30\u0026quot; \u0026quot;-27.5\u0026quot; \u0026quot;-25\u0026quot; \u0026quot;-22.5\u0026quot; \u0026quot;-20\u0026quot; \u0026quot;-17.5\u0026quot;\r## [1] \u0026quot;60\u0026quot; \u0026quot;57.5\u0026quot; \u0026quot;55\u0026quot; \u0026quot;52.5\u0026quot; \u0026quot;50\u0026quot; \u0026quot;47.5\u0026quot;\r\r2.3 Monthly average\rWe see that the downloaded data is an array of three dimensions with [lat, lon, time]. As above mentioned, we extracted latitude, longitude and time. The temperature is given in Kelvin. The objective in the next section will be to show two maps comparing January and July.\n#create our grouping variable\rgroup \u0026lt;- month(date_time) #estimate the average temperature by month data_month \u0026lt;- aperm(\rapply(\rdata, #our data\rc(1,2), #apply to each time series 1:row, 2:column a the mean( ) function\rby, #group by\rgroup, #months\rfunction(x)ifelse(all(is.na(x)),NA,mean(x))),\rc(2,3,1)) #reorder to get an array like the original\rdim(data_month) #850haPa temperature per month January to December\r## [1] 13 33 12\r\r2.4 Visualization\rOnce we got here, we can visualize the 850hPa temperature of January and July with ggplot2. In this example, I use geom_sf( ) from the library sf, which makes the work easier to visualize spatial objects in ggplot (in the near future I will make a post about sf and ggplot). In the dimension of latitude and longitude we saw that it only indicates a value for each row and column. But we need the coordinates of all the cells in the matrix. To create all combinations between two variables we can use the expand.grid( ) function.\n#first we create all the combinations of lon-lat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#as lonlat was a row/column name, it is character, that\u0026#39;s why we convert it into numeric\rlonlat \u0026lt;- apply(lonlat,2,as.numeric)\r#lon and lat are not in the order as we expect\r#row=lon; column=lat\rdata_month \u0026lt;- aperm(data_month,c(2,1,3))\r#subtract 273.15K to convert K to ºC.\rdf \u0026lt;- data.frame(lonlat,\rTa01=as.vector(data_month[,,1])-273.15,\rTa07=as.vector(data_month[,,7])-273.15)\rBefore we can make the map with ggplot2, we have to adapt the table. The shapefile with the countries limits can be downloaded here.\n#convert the wide table into a long one\rdf \u0026lt;- gather(df,month,Ta,Ta01:Ta07)%\u0026gt;%\rmutate(month=factor(month,unique(month),c(\u0026quot;Jan\u0026quot;,\u0026quot;Jul\u0026quot;)))\r#import the countries limits\rlimit \u0026lt;- st_read(\u0026quot;CNTR_RG_03M_2014.shp\u0026quot;)\r## Reading layer `CNTR_RG_03M_2014\u0026#39; from data source `C:\\Users\\xeo19\\Documents\\GitHub\\blogR_update\\content\\post\\en\\2018-09-15-access-to-climate-reanalysis-data-from-r\\CNTR_RG_03M_2014.shp\u0026#39; using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 256 features and 3 fields\r## geometry type: MULTIPOLYGON\r## dimension: XY\r## bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.66068\r## epsg (SRID): NA\r## proj4string: +proj=longlat +ellps=GRS80 +no_defs\r#color scheme\rcolbr \u0026lt;- brewer.pal(11,\u0026quot;RdBu\u0026quot;)\rggplot(df)+\rgeom_tile(aes(lon,lat,fill=Ta))+ #temperature data\rgeom_sf(data=limit,fill=NA,size=.5)+ #limits scale_fill_gradientn(colours=rev(colbr))+\rcoord_sf(ylim=c(30,60),xlim=c(-30,50))+\rscale_x_continuous(breaks=seq(-30,50,10),expand=c(0,0))+\rscale_y_continuous(breaks=seq(30,60,5),expand=c(0,0))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Ta 850hPa (ºC)\u0026quot;)+\rfacet_grid(month~.)+ #plot panels by month\rtheme_bw()\r\r\r3 ERA-Interim\rThe ECMWF offers access to its public databases from a pyhton-API. It is required to be registered on the ECMWF website. You can register here. When dealing with another programming language, in R we have to use an interface between both which allows the library reticulate. We must also have installed a pyhton distribution (version 2.x or 3.x). In the case of Windows we can use anaconda.\nRecently a new package called ecmwfr has been published that facilitates accessing the Copernicus and ECMWF APIs. The major advantage is that it is not necessary to install python. More details here.\n 3.1 Installation\rif(!require(\u0026quot;reticulate\u0026quot;)) install.packages(\u0026quot;reticulate\u0026quot;)\rif(!require(\u0026quot;ncdf4\u0026quot;)) install.packages(\u0026quot;ncdf4\u0026quot;) #to manage netCDF format\r#load packages\rlibrary(reticulate)\rlibrary(ncdf4)\rOnce we have installed anaconda and the package reticulate, we can install the library python ecmwfapi. We can carry out the installation, or through the Windows CMD using the command conda install -c conda-forge ecmwf-api-client, or with the R function py_install( ) from the reticulate package. The same function allows us to install any python library from R.\n#install the python ECMWF API\rpy_install(\u0026quot;ecmwf-api-client\u0026quot;)\r\r3.2 Connection and download with the ECMWF API\rIn order to access the API, it is required to create a file with the user’s information.\nThe “.ecmwfapirc” file must contain the following information:\n{\r\u0026quot;url\u0026quot; : \u0026quot;https://api.ecmwf.int/v1\u0026quot;,\r\u0026quot;key\u0026quot; : \u0026quot;XXXXXXXXXXXXXXXXXXXXXX\u0026quot;,\r\u0026quot;email\u0026quot; : \u0026quot;john.smith@example.com\u0026quot;\r}\rThe key can be obtained with the user account here.\nThe file can be created with the Windows notebook.\nWe create a document “ecmwfapirc.txt”.\rRename this file to “.ecmwfapirc.”\r\rThe last point disappears automatically. Then we save this file in “C:/USERNAME/.ecmwfapirc” or “C:/USERNAME/Documents/.ecmwfapirc”.\n#import the python library ecmwfapi\recmwf \u0026lt;- import(\u0026#39;ecmwfapi\u0026#39;)\r#for this step there must exist the file .ecmwfapirc\rserver = ecmwf$ECMWFDataServer() #start the connection\rOne we get here, how do we create a query? The easiest thing is to go to the website of ECMWF, where we choose the database, in this case ERA-Interim surface, to create a script with all the necessary data. More details about the syntax can be found here. When we proceed on the website, we only have to click on “View MARS Request”. This step takes us to the script in python.\nWith the syntax of the script from the MARS Request, we can create the query in R.\n#we create the query\rquery \u0026lt;-r_to_py(list(\rclass=\u0026#39;ei\u0026#39;,\rdataset= \u0026quot;interim\u0026quot;, #dataset\rdate= \u0026quot;2017-01-01/to/2017-12-31\u0026quot;, #time period\rexpver= \u0026quot;1\u0026quot;,\rgrid= \u0026quot;0.125/0.125\u0026quot;, #resolution\rlevtype=\u0026quot;sfc\u0026quot;,\rparam= \u0026quot;167.128\u0026quot;, # air temperature (2m)\rarea=\u0026quot;45/-10/30/5\u0026quot;, #N/W/S/E\rstep= \u0026quot;0\u0026quot;,\rstream=\u0026quot;oper\u0026quot;,\rtime=\u0026quot;00:00:00/06:00:00/12:00:00/18:00:00\u0026quot;, #hours\rtype=\u0026quot;an\u0026quot;,\rformat= \u0026quot;netcdf\u0026quot;, #format\rtarget=\u0026#39;ta2017.nc\u0026#39; #file name\r))\r#query to get the ncdf\rserver$retrieve(query)\rThe result is a netCDF file that we can process with the library ncdf4.\n\r3.3 Processing ncdf\rIn the next section, the objective will be the extraction of a time serie from the closest coordinate to a given one. We will use the coordinates of Madrid (40.418889, -3.691944).\n#load packages\rlibrary(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\r#open the connection with the ncdf file\rnc \u0026lt;- nc_open(\u0026quot;ta2017.nc\u0026quot;)\r#extract lon and lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r## [1] 121\r## [1] 121\r#extract the time\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#time unit: hours since 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r## $units\r## [1] \u0026quot;hours since 1900-01-01 00:00:00.0\u0026quot;\r## ## $long_name\r## [1] \u0026quot;time\u0026quot;\r## ## $calendar\r## [1] \u0026quot;gregorian\u0026quot;\r#convert the hours into date + hour\r#as_datetime() function of the lubridate package needs seconds\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#import the data\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#close the conection with the ncdf file\rnc_close(nc)\rIn this next section we use the sf package, which is replacing the well known sp and rgdal packages.\n#create all the combinations of lon-lat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#we must convert the coordinates in a spatial object sf\r#we also indicate the coordinate system in EPSG code\rcoord \u0026lt;- st_as_sf(lonlat,coords=c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;))%\u0026gt;%\rst_set_crs(4326)\r#we do the same with our coordinate of Madrid\rpsj \u0026lt;- st_point(c(-3.691944,40.418889))%\u0026gt;%\rst_sfc()%\u0026gt;%\rst_set_crs(4326)\r#plot all points\rplot(st_geometry(coord))\rplot(psj,add=TRUE,pch = 3, col = \u0026#39;red\u0026#39;)\rIn the next steps we calculate the distance of our reference point to all the grid points. Then we look for the one with less distance.\n#add the distance to the points\rcoord \u0026lt;- mutate(coord,dist=st_distance(coord,psj))\r#create a distance matrix with the same dimensions as our data\rdist_mat \u0026lt;- matrix(coord$dist,dim(data)[-3])\r#the arrayInd function is useful to obtain the row and column indexes\rmat_index \u0026lt;- as.vector(arrayInd(which.min(dist_mat), dim(dist_mat)))\r#we extract the time serie and change the unit from K to ºC\r#we convert the time in date + hour\rdf \u0026lt;- data.frame(ta=data[mat_index[1],mat_index[2],],time=timestamp)%\u0026gt;%\rmutate(ta=ta-273.15,time=ymd_hms(time))\rFinally, we visualize our time series.\nggplot(df,\raes(time,ta))+\rgeom_line()+\rlabs(y=\u0026quot;Temperature (ºC)\u0026quot;,\rx=\u0026quot;\u0026quot;)+\rtheme_bw()\r\r\r4 Update for accessing ERA-5\rRecently the new reanalysis ERA-5 with single level or pressure level was made available to users. It is the fifth generation of the European Center for Medium-Range Weather Forecasts (ECMWF) and accessible through a new Copernicus API. The ERA-5 reanalysis has a temporary coverage from 1950 to the present at a horizontal resolution of 30km worldwide, with 137 levels from the surface to a height of 80km. An important difference with respect to the previous ERA-Interim is the temporal resolution with hourly data.\nThe access changes to the Climate Data Store (CDS) infrastructure with its own API. It is possible to download directly from the web or using the Python API in a similar way to the one already presented in this post. However, there are slight differences which I will explain below.\nIt is necessary to have a Copernicus CDS account link\rAgain, you need a account key link\rThere are changes in the Python library and in some arguments of the query.\r\r#load libraries library(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\rlibrary(reticulate)\r#install the CDS API\rconda_install(\u0026quot;r-reticulate\u0026quot;,\u0026quot;cdsapi\u0026quot;, pip=TRUE)\rTo be able to access the API, a requirement is to create a file with the user’s information.\nThe “.cdsapirc” file must contain the following information:\n\rurl: https://cds.climate.copernicus.eu/api/v2\rkey: {uid}:{api-key}\r\rThe key can be obtained with the user account in the User profile.\nThe file can be created in the same way as it has been explained for ERA-Interim.\n#import python CDS-API\rcdsapi \u0026lt;- import(\u0026#39;cdsapi\u0026#39;)\r#for this step there must exist the file .cdsapirc\rserver = cdsapi$Client() #start the connection\rWith the syntax of the script from the Show API request single level, we can create the query in R.\n#we create the query\rquery \u0026lt;- r_to_py(list(\rvariable= \u0026quot;2m_temperature\u0026quot;,\rproduct_type= \u0026quot;reanalysis\u0026quot;,\ryear= \u0026quot;2018\u0026quot;,\rmonth= \u0026quot;07\u0026quot;, #formato: \u0026quot;01\u0026quot;,\u0026quot;01\u0026quot;, etc.\rday= str_pad(1:31,2,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;), time= str_c(0:23,\u0026quot;00\u0026quot;,sep=\u0026quot;:\u0026quot;)%\u0026gt;%str_pad(5,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;),\rformat= \u0026quot;netcdf\u0026quot;,\rarea = \u0026quot;45/-20/35/5\u0026quot; # North, West, South, East\r))\r#query to get the ncdf\rserver$retrieve(\u0026quot;reanalysis-era5-single-levels\u0026quot;,\rquery,\r\u0026quot;era5_ta_2018.nc\u0026quot;)\rIt is possible that the first time an error message is received, given that the required terms and conditions have not yet been accepted. Simply, the indicated link should be followed.\nError in py_call_impl(callable, dots$args, dots$keywords) : Exception: Client has not agreed to the required terms and conditions.. To access this resource, you first need to accept the termsof \u0026#39;Licence to Use Copernicus Products\u0026#39; at https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products\rFrom here we can follow the same steps as with ERA-Interim.\n#open the connection with the file\rnc \u0026lt;- nc_open(\u0026quot;era5_ta_2018.nc\u0026quot;)\r#extract lon, lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r## [1] 41\r## [1] 101\r#extract time\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#time unit: hours from 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r## $units\r## [1] \u0026quot;hours since 1900-01-01 00:00:00.0\u0026quot;\r## ## $long_name\r## [1] \u0026quot;time\u0026quot;\r## ## $calendar\r## [1] \u0026quot;gregorian\u0026quot;\r#we convert the hours into date+time #as_datetime from lubridate needs seconds\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#temperatures in K from july 2018\rhead(timestamp)\r## [1] \u0026quot;2018-07-01 00:00:00 UTC\u0026quot; \u0026quot;2018-07-01 01:00:00 UTC\u0026quot;\r## [3] \u0026quot;2018-07-01 02:00:00 UTC\u0026quot; \u0026quot;2018-07-01 03:00:00 UTC\u0026quot;\r## [5] \u0026quot;2018-07-01 04:00:00 UTC\u0026quot; \u0026quot;2018-07-01 05:00:00 UTC\u0026quot;\r#import temperature data\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#plot 2018-07-01\rfilled.contour(data[,,1])\r#time serie plot for a pixel\rplot(data.frame(date=timestamp,\rta=data[1,5,]),\rtype=\u0026quot;l\u0026quot;)\r#close the conection with the ncdf file\rnc_close(nc)\r\r","date":1537005584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537005584,"objectID":"216967a9625628bc1857d3728930e987","permalink":"/en/2018/access-to-climate-reanalysis-data-from-r/","publishdate":"2018-09-15T10:59:44+01:00","relpermalink":"/en/2018/access-to-climate-reanalysis-data-from-r/","section":"post","summary":"In this post, I will show how we can download and work directly with data from climatic reanalysis in R. These kind of datasets are combination of forcast models and data assimilation systems, which allows us to create corrected global grids of recent history of the atmosphere, land surface, and oceans.","tags":["reanalisis","interim","NCEP/NCAR","era","download","ncdf","access","api","python","ECMWF"],"title":"Access to climate reanalysis data from R","type":"post"},{"authors":null,"categories":["datavis","R","R:elementary"],"content":"\rWelcome to my blog! I am Dominic Royé, researcher and lecturer of physical geography at the University of Santiago de Compostela. One of my passions is R programming to visualize and analyze any type of data. Hence, my idea of this blog has its origin in my datavis publications I have been cooking in the last year on Twitter on different topics describing the world. In addition, I would like to take advantage of the blog and publish short introductions and explanation on data visualization, management and manipulation in R. I hope you like it. Any suggestion or ideas are welcomed.\nBackground\rI have always wanted to write about the use of the pie chart. The pie chart is widely used in research, teaching, journalism or technical reports. I do not know if it is due to Excel, but even worse than the pie chart itself, is its 3D version (the same for the bar chart). About the 3D versions, I only want to say that they are not recommended, since in these cases the third dimension does not contain any information and therefore it does not help to correctly read the information of the graphic. Regarding the pie chart, among many experts its use is not advised. But why?\nAlready in a study conducted by (???) they found that the interpretation and processing of angles is more difficult than that of linear forms. Mostly it is easier to read a bar chart than a pie chart. A problem that becomes very visible when we have; 1) too many categories 2) few differences between categories 3) a misuse of colors as legend or 4) comparisons between various pie charts.\nIn general, to decide what possible graphic representations exist for our data, I recommend using the website www.data-to-viz.com or the Financial Times Visual Vocabulary.\n\nWell, now what alternative ways can we use in R?\n\rAlternatives to the pie chart\rThe dataset we will use about the vaccination status of measles correspond to June 2018 in Europe and come from the ECDC.\n#packages\rlibrary(tidyverse)\rlibrary(scales)\rlibrary(RColorBrewer)\r#data\rmeasles \u0026lt;- data.frame(\rvacc_status=c(\u0026quot;Unvaccinated\u0026quot;,\u0026quot;1 Dose\u0026quot;,\r\u0026quot;\u0026gt;= 2 Dose\u0026quot;,\u0026quot;Unkown Dose\u0026quot;,\u0026quot;Unkown\u0026quot;),\rprop=c(0.75,0.091,0.05,0.012,0.096)\r)\r#we order from the highest to the lowest and fix it with a factor\rmeasles \u0026lt;- arrange(measles,\rdesc(prop))%\u0026gt;%\rmutate(vacc_status=factor(vacc_status,vacc_status))\r\r\rvacc_status\rprop\r\r\r\rUnvaccinated\r0.750\r\rUnkown\r0.096\r\r1 Dose\r0.091\r\r\u0026gt;= 2 Dose\r0.050\r\rUnkown Dose\r0.012\r\r\r\rBar plot or similar\rggplot(measles,aes(vacc_status,prop))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\rggplot(measles,aes(x=vacc_status,prop,ymin=0,ymax=prop))+\rgeom_pointrange()+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\r#custom themes definitions\rtheme_singlebar \u0026lt;- theme_bw()+\rtheme(\rlegend.position = \u0026quot;bottom\u0026quot;,\raxis.title = element_blank(),\raxis.ticks.y = element_blank(),\raxis.text.y = element_blank(),\rpanel.border = element_blank(),\rpanel.grid=element_blank(),\rplot.title=element_text(size=14, face=\u0026quot;bold\u0026quot;)\r)\r#plot\rmutate(measles,\rvacc_status=factor(vacc_status, #we change the order of the categories\rrev(levels(vacc_status))))%\u0026gt;%\rggplot(aes(1,prop,fill=vacc_status))+ #we put 1 in x to create a single bar\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent,\rlimits=c(0,1),\rexpand=c(.01,.01))+\rscale_x_continuous(expand=c(0,0))+\rscale_fill_brewer(\u0026quot;\u0026quot;,palette=\u0026quot;Set1\u0026quot;)+\rcoord_flip()+\rtheme_singlebar\r#we expand our data with numbers from Italy\rmeasles2 \u0026lt;- mutate(measles,\ritaly=c(0.826,0.081,0.053,0.013,0.027),\rvacc_status=factor(vacc_status,rev(levels(vacc_status))))%\u0026gt;%\rrename(europe=\u0026quot;prop\u0026quot;)%\u0026gt;%\rgather(region,prop,europe:italy)\r#plot\rggplot(measles2,aes(region,prop,fill=vacc_status))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,position=\u0026quot;stack\u0026quot;)+ #stack bar\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1),\rexpand=c(0,0))+\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;)+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Vaccination Status\u0026quot;)+\rtheme_minimal()\r\rWaffle plot\r#package\rlibrary(waffle)\r#the waffle function uses a vector with names\rval_measles \u0026lt;- round(measles$prop*100)\rnames(val_measles) \u0026lt;- measles$vacc_status\r#plot\rwaffle(val_measles, #data\rcolors=brewer.pal(5,\u0026quot;Set1\u0026quot;), #colors\rrows=5) #row number \rThe Waffle chart seems very interesting to me when we want to show a proportion of an individual category.\n#data\rmedida \u0026lt;- c(41,59) #data from the OECD 2015\rnames(medida) \u0026lt;- c(\u0026quot;Estudios Superiores\u0026quot;,\u0026quot;Otros estudios\u0026quot;)\r#plot\rwaffle(medida,\rcolors=c(\u0026quot;#377eb8\u0026quot;,\u0026quot;#bdbdbd\u0026quot;),\rrows=5)\r\rTreemap\r#package\rlibrary(treemap)\r#plot\rtreemap(measles,\rindex=\u0026quot;vacc_status\u0026quot;, #variable with categories\rvSize=\u0026quot;prop\u0026quot;, #values\rtype=\u0026quot;index\u0026quot;, #style more in ?treemap\rtitle=\u0026quot;\u0026quot;, palette = brewer.pal(5,\u0026quot;Set1\u0026quot;) #colors\r)\rPersonally, I think that all types of graphic representations have their advantages and disadvantages. However, we currently have a huge variety of alternatives to avoid using the pie chart. If you still want to make a pie chart, which I would not rule out either, I recommend following certain rules, which you can find very well summarized in a recent post by Lisa Charlotte Rost. For example, you should order from the highest to the lowest unless there is a natural order or use a maximum of five categories. Finally, I leave you a link to a cheat sheet from policyviz with basic rules of data visualization. A good reference on graphics using different programs from Excel to R can be found in the book Creating More Effective Graphs (???).\n\rReferences\r\r\r","date":1534933412,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534933412,"objectID":"3a5ca713880df90e42e9c9eabcbda7c3","permalink":"/en/2018/the-pie-chart/","publishdate":"2018-08-22T11:23:32+01:00","relpermalink":"/en/2018/the-pie-chart/","section":"post","summary":"Welcome to my blog! I am Dominic Royé, researcher and lecturer of physical geography at the University of Santiago de Compostela. One of my passions is R programming to visualize and analyze any type of data. Hence, my idea of this blog has its origin in my datavis publications I have been cooking in the last year on Twitter on different topics describing the world. In addition, I would like to take advantage of the blog and publish short introductions and explanation on data visualization, management and manipulation in R.","tags":["pie chart","data","circular","proportions","first post","treemap","waffle","bar"],"title":"the pie chart","type":"post"},{"authors":["D Royé","A Figueiras","M Taracido-Trunk"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"8c48be3d65e4bed0ce4fe5203eb06bb0","permalink":"/en/publication/pharma_coru%C3%B1a_2018/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/en/publication/pharma_coru%C3%B1a_2018/","section":"publication","summary":"The consumption of medication, especially over-the-counter (OTC) drugs, can reflect environmental exposure with a lesser degree of severity in terms of morbidity. The non-linear effects of maximum and minimum apparent temperature on respiratory drug sales in A Coruña from 2006 to 2010 were examined using a distributed lag non-linear model. In particular, low apparent temperatures proved to be associated with increased sales of respiratory drugs. The strongest consistent risk estimates were found for minimum apparent temperatures in respiratory drug sales with an increase of 33.4% (95% CI: 12.5-58.0%) when the temperature changed from 2.8 ºC to −1.4 ºC. These findings may serve to guide the planning of public health interventions in order to predict and manage the health effects of exposure to the thermal environment for lower degrees of morbidity. More precisely, significant increases in the use of measured OTC medication could be used to identify and anticipate influenza outbreaks due to a more sensitive degree of the data source.","tags":["drug sales","pharmacoepidemiology","respiratory cause","short‐term effects","Spain","thermal environment"],"title":"Short-term effects of heat and cold on respiratory drug use. A time-series epidemiological study in A Coruña, Spain","type":"publication"},{"authors":["D Royé","N Lorenzo","J Martin-Vide"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"fd71c0d62978df70b5d41813df70acdf","permalink":"/en/publication/lightning_galicia_2018/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/en/publication/lightning_galicia_2018/","section":"publication","summary":"The spatial-temporal patterns of cloud-to-ground (CG) lightning covering the period 2010-2015 over the northwest Iberian Peninsula were investigated. The analysis conducted employed three main methods: the circulation weather types developed by Jenkinson \u0026 Collison, the fit of a generalized additive model for geographic variables and the use of a concentration index for the ratio of lightning strikes and thunderstorm days. The main activity in the summer months can be attributed to situations with eastern or anticyclonic flow due to convection by insolation. In winter, lightning proves to have a frontal origin and is mainly associated with western or cyclonic flow situations which occur with advections of air masses of maritime origin. The largest number of CG discharges occurs under eastern flow and their hybrids with anticyclonic situations. Thunderstorms with greater CG lightning activity, highlighted by a higher Concentration Index, are located in areas with a higher density of lightning strikes, above all in mountainous areas away from the sea. The modeling of lightning density with geographic variables shows the positive influence of altitude and, particularly, distance to the sea, with nonlinear relationships due to the complex orography of the region. Likewise, areas with convex topography receive more lightning strikes than concave ones, a relation which has been demonstrated for the first time from a Generalized Additive Model (GAM).","tags":["thunderstorm","Iberian Peninsula","Concentration Index","weather types","Convexity Index","Generalized Additive Model","cloud-to-ground lightning"],"title":"Spatial–temporal patterns of cloud-to-ground lightning over the northwest Iberian Peninsula during the period 2010–2015","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"735acb4c3b633c5b3cfad91a943dfb61","permalink":"/en/publication/hotnights_bcn_2017/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/en/publication/hotnights_bcn_2017/","section":"publication","summary":"Heat-related effects on mortality have been widely analyzed using maximum and minimum temperatures as exposure variables. Nevertheless, the main focus is usually on the former with the minimum temperature being limited in use as far as human health effects are concerned. Therefore, new thermal indices were used in this research to describe the duration of night hours with air temperatures higher than the 95% percentile of the minimum temperature (Hot Night hours) and intensity as the summation of these air temperatures in degrees (Hot Night degrees). An exposure-response relationship between mortality due to natural, respiratory and cardiovascular causes and summer night temperatures was assessed using data from the Barcelona region between 2003 and 2013. The non-linear relationship between the exposure and response variables was modeled using a distributed lag non-linear model. The estimated associations for both exposure variables and mortality shows a relationship with high and medium values that persist significantly up to a lag of 1–2 days. In mortality due to natural causes an increase of 1.1% per 10% (CI95% 0.6–1.5) for Hot Night hours and 5.8% per each 10º (CI95% 3.5–8.2%) for Hot Night degrees is observed. The effects of Hot Night hours reach their maximum with 100% and leads to an increase by 9.2% (CI95% 5.3–13.1%). The hourly description of night heat effects reduced to a single indicator in duration and intensity is a new approach and shows a different perspective and significant heat-related effects on human health.","tags":["heat","mortality","tropical night","hot night","effects","human health","climate change"],"title":"The effects of hot nights on mortality in Barcelona, Spain","type":"publication"},{"authors":["D Royé","J Martin-Vide"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"83752c0287cd3c7c14fd70c98985fa6b","permalink":"/en/publication/usa_ci_2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/en/publication/usa_ci_2017/","section":"publication","summary":"The contiguous US exhibits a wide variety of precipitation regimes, first, because of the wide range of latitudes and altitudes. The physiographic units with a basic meridional configuration contribute to the differentiation between east and west in the country while generating some large interior continental spaces. The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. Positive exponential curves, which plot the cumulative percentages of days with precipitation against the cumulative percentage of the rainfall amounts that they contribute, can be evaluated through the Concentration Index. The Concentration Index has been applied to the contiguous United States using a gridded climate dataset of daily precipitation data, at a resolution of 0.25°, provided by CPC/NOAA/OAR/Earth System Research Laboratory, for the period between 1956 and 2006. At the same time, other rainfall indices and variables such as the annual coefficient of variation, seasonal rainfall regimes and the probabilities of a day with precipitation have been presented with a view to explaining spatial CI patterns. The spatial distribution of the CI in the contiguous United States is geographically consistent, reflecting the principal physiographic and climatic units of the country. Likewise, linear correlations have been established between the CI and geographical factors such as latitude, longitude and altitude. In the latter case the Pearson correlation coefficient (r) between this factor and the CI is −0.51 (p-value ","tags":["Concentration Index","Contiguous United States","daily precipitation","precipitation indices","spatial–temporal patterns"],"title":"Concentration of Daily Precipitation in the Contiguous United States","type":"publication"},{"authors":["P Fdez-Arroyabe","D Royé"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"389371f80a93c479ccb639ee58c2c7bf","permalink":"/en/publication/chapter_springer_2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/publication/chapter_springer_2017/","section":"publication","summary":"Co-creation of scientific knowledge based on new technologies and big data sources is one of the main challenges for the digital society in the XXI century. Data management and the analysis of patterns among datasets based on machine learning and artificial intelligence has become essential for many sectors nowadays. The development of real time health-related climate services represents an example where abundant structured and unstructured information and transdisciplinary research are needed. The study of the interactions between atmospheric processes and human health through a big data approach can reveal the hidden value of data. The Oxyalert technological platform is presented as an example of a digital biometeorological infrastructure able to forecast, at an individual level, oxygen changes impacts on human health.","tags":["co-creation","interdisciplinarity","transdisciplinarity","morbidity","climate services","digital divide","big data","health"],"title":"Co-creation and Participatory Design of Big Data Infrastructures on the Field of Human Health Related Climate Services","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"f87f687a4a74b8e7fc56e41604e72c84","permalink":"/en/more/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/en/more/","section":"","summary":"","tags":null,"title":"More","type":"page"},{"authors":["D Royé","J Taboada","A Ezpeleta-Martí","N Lorenzo"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"c3bdb9563e443f7fc44c7b7eeece88b4","permalink":"/en/publication/cwt_galicia_resp_2016/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/en/publication/cwt_galicia_resp_2016/","section":"publication","summary":"The link between various pathologies and atmospheric conditions has been a constant topic of study over recent decades in many places across the world; knowing more about it enables us to pre-empt the worsening of certain diseases, thereby optimizing medical resources. This study looked specifically at the connections in winter between respiratory diseases and types of atmospheric weather conditions (Circulation Weather Types, CWT) in Galicia, a region in the north-western corner of the Iberian Peninsula. To do this, the study used hospital admission data associated with these pathologies as well as an automatic classification of weather types. The main result obtained was that weather types giving rise to an increase in admissions due to these diseases are those associated with cold, dry weather, such as those in the east and south-east, or anticyclonic types. A second peak was associated with humid, hotter weather, generally linked to south-west weather types. In the future, this result may help to forecast the increase in respiratory pathologies in the region some days in advance.","tags":["weather type","respiratory diseases","hospital admissions","human health","Spain"],"title":"Winter circulation weather types and hospital admissions for respiratory diseases in Galicia, Spain","type":"publication"},{"authors":["D Royé","A Ezpeleta-Martí"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"48e40344ca1e4f7a2ab2a6c4510aa260","permalink":"/en/publication/hotnights_age_2015/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/en/publication/hotnights_age_2015/","section":"publication","summary":"Analysis of tropical nights on the Atlantic coast of the Iberian peninsula. A proposed methodology. This paper presents a new methodology for the study of warm nights, also called «tropical», in Galicia and Portugal in order to identify those nights where people can be affected by heat stress. The use of two indicators obtained through half-hourly data has allowed us to define in more detail the thermal characteristics of the nights between May and October, thereby being able to more accurately assess the risk to the health and well-being of the population. There is a significant increase in the frequency of tropical nights and warm nights on the Atlantic coast, from the north of Galicia to the south of Portugal. The lower latitude and proximity to the coastline are associated with greater persistence of heat and thermal stress during these nights. In inland areas the persistence is less. The warmest nights are more frequent and intense in centres of the cities, due to the effect of the urban heat island.","tags":["tropical night","thermic stress","heat island","Galicia","Portugal"],"title":"Analysis of tropical nights on the atlantic coast of the Iberian Peninsula. A proposed methodology","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"Used datasets are available for download here. Alternative datasets for Spain in ncdf format can be downloaded:\nAEMET\n Gridded 20km and 50km (precipitation and temperature)\n Gridded 5km (precipitation)\n  CSIC\n Gridded 5km (precipitation)  ","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"28a50037c73f6b009121b5250e4fe2d1","permalink":"/en/publication/ncdf_2015/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/en/publication/ncdf_2015/","section":"publication","summary":"A practical introduction in the use of netCDF in the environment of R Spatio-temporal data is currently key to many disciplines, especially to climatology and meteorology. A widespread format is netCDF allowing a multidimensional structure and an exchange of data machine independently. In this article, we introduce the use of these databases with the free software environment R. To do this, we will work with a grid of the maximum temperature of the Iberian Peninsula for the period 1971-2007. The goal is to read and visualize the netCDF format, and make some fist overall and specifi calculations. Finally the applicability is shown in a case study: the diurnal temperature variation in the Iberian Peninsula for January and August 2006. (Spanish)","tags":["netCDF","R","climatology","temperature","matrix","database"],"title":"The use of climate databases netCDF with array structure in the environment of R","type":"publication"}]