[{"authors":null,"categories":null,"content":"About me I am a climate scientist and actually, I am researcher and head of data science at the Climate Research Foundation (FIC), previously researcher at the University of Santiago de Compostela. I am originally from Grevenbroich, near by Cologne, in Germany. I graduated in Geography and Hispanic Philology at the University of Cologne and RWTH-Aachen University in 2010. After I met my wife in Galicia, I came to Santiago de Compostela, in northwest Spain, and made my Ph.D. in 2015 about the relationship between health and weather at the same University.\nThe main lines of research are, on the one hand, biometeorology and geographies of health, the relationship between human health and the atmospheric environment, and on the other hand, applied physical geography with a focus on atmospheric variables and their spatio-temporal behaviors.\nI am an enthusiastic R user, with a lot of curiosity for spatial analysis, data visualizations, management and manipulation, and GIS.\nI am a member of the Public Health Research Group at the University of Santiago de Compostela. In addition, I am a close collaborator of two other research groups, Geobiomet at the University of Cantabria and Climatology Group at the University of Barcelona. Since 2019 I am a member of the MCC Collaborative Research Network, an international research program on the associations between environmental stressors, climate, and health.\nFeel free to contact me.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dominicroye.github.io/en/author/dr.-dominic-roye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/dr.-dominic-roye/","section":"authors","summary":"About me I am a climate scientist and actually, I am researcher and head of data science at the Climate Research Foundation (FIC), previously researcher at the University of Santiago de Compostela.","tags":null,"title":"Dr. Dominic Royé","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Dr. Dominic Royé FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://dominicroye.github.io/en/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/en/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"📊 Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://dominicroye.github.io/en/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/en/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://dominicroye.github.io/en/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/en/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://dominicroye.github.io/en/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/en/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://dominicroye.github.io/en/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Yuan Gao","et al"],"categories":null,"content":"","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"95271db1ad48ce6507c80e707c94cd90","permalink":"https://dominicroye.github.io/en/publication/2024-cold-spells-mortality-lancet-planetary-health/","publishdate":"2024-02-01T00:00:00Z","relpermalink":"/en/publication/2024-cold-spells-mortality-lancet-planetary-health/","section":"publication","summary":"Background. Exposure to cold spells is associated with mortality. However, little is known about the global mortality burden of cold spells. Methods. A three-stage meta-analytical method was used to estimate the global mortality burden associated with cold spells by means of a time series dataset of 1960 locations across 59 countries (or regions). First, we fitted the location-specific, cold spell-related mortality associations using a quasi-Poisson regression with a distributed lag non-linear model with a lag period of up to 21 days. Second, we built a multivariate meta-regression model between location-specific associations and seven predictors. Finally, we predicted the global grid-specific cold spell-related mortality associations during 2000–19 using the fitted meta-regression model and the yearly grid-specific meta-predictors. We calculated the annual excess deaths, excess death ratio (excess deaths per 1000 deaths), and excess death rate (excess deaths per 100 000 population) due to cold spells for each grid across the world. Findings. Globally, 205 932 (95% empirical CI [eCI] 162 692–250 337) excess deaths, representing 3·81 (95% eCI 2·93–4·71) excess deaths per 1000 deaths (excess death ratio), and 3·03 (2·33–3·75) excess deaths per 100 000 population (excess death rate) were associated with cold spells per year between 2000 and 2019. The annual average global excess death ratio in 2016–19 increased by 0·12 percentage points and the excess death rate in 2016–19 increased by 0·18 percentage points, compared with those in 2000–03. The mortality burden varied geographically. The excess death ratio and rate were highest in Europe, whereas these indicators were lowest in Africa. Temperate climates had higher excess death ratio and rate associated with cold spells than other climate zones. Interpretation. Cold spells are associated with substantial mortality burden around the world with geographically varying patterns. Although the number of cold spells has on average been decreasing since year 2000, the public health threat of cold spells remains substantial. The findings indicate an urgency of taking local and regional measures to protect the public from the mortality burdens of cold spells.","tags":["cold waves","mortality","duration","spells"],"title":"Global, regional, and national burden of mortality associated with cold spells during 2000–19: a three-stage modelling study","type":"publication"},{"authors":["Lina Madaniyazi","et al"],"categories":null,"content":"","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"ef8531340dadeca3fb437eeed3d7f297","permalink":"https://dominicroye.github.io/en/publication/2024-seasonality-mortality-lancet-planetary-health/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/en/publication/2024-seasonality-mortality-lancet-planetary-health/","section":"publication","summary":"Background. Climate change can directly impact temperature-related excess deaths and might subsequently change the seasonal variation in mortality. In this study, we aimed to provide a systematic and comprehensive assessment of potential future changes in the seasonal variation, or seasonality, of mortality across different climate zones. Methods. In this modelling study, we collected daily time series of mean temperature and mortality (all causes or non-external causes only) via the Multi-Country Multi-City Collaborative (MCC) Research Network. These data were collected during overlapping periods, spanning from Jan 1, 1969 to Dec 31, 2020. We projected daily mortality from Jan 1, 2000 to Dec 31, 2099, under four climate change scenarios corresponding to increasing emissions (Shared Socioeconomic Pathways [SSP] scenarios SSP1-2.6, SSP2-4.5, SSP3-7.0, and SSP5-8.5). We compared the seasonality in projected mortality between decades by its shape, timings (the day-of-year) of minimum (trough) and maximum (peak) mortality, and sizes (peak-to-trough ratio and attributable fraction). Attributable fraction was used to measure the burden of seasonality of mortality. The results were summarised by climate zones. Findings. The MCC dataset included 126 809 537 deaths from 707 locations within 43 countries or areas. After excluding the only two polar locations (both high-altitude locations in Peru) from climatic zone assessments, we analysed 126 766 164 deaths in 705 locations aggregated in four climate zones (tropical, arid, temperate, and continental). From the 2000s to the 2090s, our projections showed an increase in mortality during the warm seasons and a decrease in mortality during the cold seasons, albeit with mortality remaining high during the cold seasons, under all four SSP scenarios in the arid, temperate, and continental zones. The magnitude of this changing pattern was more pronounced under the high-emission scenarios (SSP3-7.0 and SSP5-8.5), substantially altering the shape of seasonality of mortality and, under the highest emission scenario (SSP5-8.5), shifting the mortality peak from cold seasons to warm seasons in arid, temperate, and continental zones, and increasing the size of seasonality in all zones except the arid zone by the end of the century. In the 2090s compared with the 2000s, the change in peak-to-trough ratio (relative scale) ranged from 0·96 to 1·11, and the change in attributable fraction ranged from 0·002% to 0·06% under the SSP5-8.5 (highest emission) scenario. Interpretation. A warming climate can substantially change the seasonality of mortality in the future. Our projections suggest that health-care systems should consider preparing for a potentially increased demand during warm seasons and sustained high demand during cold seasons, particularly in regions characterised by arid, temperate, and continental climates.","tags":["seasonality","mortality","climate change","extreme temperatures"],"title":"Seasonality of mortality under climate change: a multicountry projection study","type":"publication"},{"authors":["D Royé","Carmen Iñiguez","Aurelio Tobías"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"003411fdee5e0e79a235c20ea19271ae","permalink":"https://dominicroye.github.io/en/publication/2024-air-pollution-reanalysis-comparison-environment-health/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/en/publication/2024-air-pollution-reanalysis-comparison-environment-health/","section":"publication","summary":"Air pollution poses a health hazard in all countries. However, complete data on ambient particulate matter (PM) concentrations are not available in all world regions. Reanalysis data is already a valuable source of exposure data in epidemiological studies examining the relationship between temperature and health. Nevertheless, the performance of reanalysis data in assessing the short-term health effects of particulate air pollution remains unclear. We assessed the performance of CAMS reanalysis (EAC4) data from the European Centre for Medium-Range Weather Forecasts, compared with daily PM concentrations from field monitoring stations, to estimate short-term exposure to PM with an aerodynamic diameter less than 10 μm (PM10) on daily mortality in 33 Spanish provincial capital cities using a two-stage time series regression design. The shape of the PM10 distribution varied substantially between PM observations and CAMS global reanalysis of atmospheric composition (EAC4) reanalysis data, with correlation ranging from 0.21 to 0.58. The pooled mortality risk for a 10 μg/m3 increase in PM10 showed similar estimates using PM concentrations {relative risks (RR) = 1.007, 95% confidence intervals (95% CI) = [1.002, 1.011]} and EAC4 reanalysis data (RR = 1.011, 95% CI = [1.006, 1.015]). However, the city-specific PM10 beta coefficients estimated using PM concentrations and EAC4 reanalysis data showed a low correlation (r = 0.22). The use of reanalysis data should be approached with caution when assessing the association between particulate matter air pollution and health outcomes, particularly in cities with small populations.","tags":["air pollution","atmospheric chemistry","environmental modeling","particulate matter","mortality","reanalysis"],"title":"Comparison of Air Pollution–Mortality Associations Using Observed Particulate Matter Concentrations and Reanalysis Data in 33 Spanish Cities","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"e711ea6a4fa53f42d72a2902ff51292f","permalink":"https://dominicroye.github.io/en/publication/2024-geoprocesamiento-nube-con-r/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/en/publication/2024-geoprocesamiento-nube-con-r/","section":"publication","summary":"El planteamiento de un problema basado en datos de diversos proveedores habitualmente implica la descarga de grandes volúmenes de datos. La actual proliferación de servicios de Open Data, despliegues de sensores y diversas fuentes, incluyendo los satélites, dificulta su procesamiento en equipos personales. El gran crecimiento en grandes volúmenes de datos espaciotemporales de tipo vectorial o raster lleva a la necesidad en trabajar con servicios en nube para ahorrar tiempo computacional y espacio de almacenamiento. En la actualidad existen diferentes servicios de geoprocesamiento en nube que ayudan a hacer análisis online sin necesidad de descargar los datos ni preocuparse por el rendimiento computacional. Uno de estos servicios es Google Earth Engine (GEE), donde se combina un catálogo de varios petabytes de imágenes satelitales y conjuntos de datos geoespaciales multidimensionales (vectorial y raster) de alta resolución con capacidades de análisis a escala planetaria. Este servicio gratuito para uso no comercial incluye incluso la posibilidad de crear aplicaciones.","tags":["geoprocessing","r","google earth engine","cloud","sea surface temperature"],"title":"Geoprocesamiento en nube","type":"publication"},{"authors":["Wenzhong Huang","et al"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"7d0244fab0ff67b726c80c755ae58e00","permalink":"https://dominicroye.github.io/en/publication/2024-tropical-cyclone-periods-of-concern-plos-medicine/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/en/publication/2024-tropical-cyclone-periods-of-concern-plos-medicine/","section":"publication","summary":"Background. More intense tropical cyclones (TCs) are expected in the future under a warming climate scenario, but little is known about their mortality effect pattern across countries and over decades. We aim to evaluate the TC-specific mortality risks, periods of concern (POC) and characterize the spatiotemporal pattern and exposure-response (ER) relationships on a multicountry scale. Methods and findings. Daily all-cause, cardiovascular, and respiratory mortality among the general population were collected from 494 locations in 18 countries or territories during 1980 to 2019. Daily TC exposures were defined when the maximum sustained windspeed associated with a TC was ≥34 knots using a parametric wind field model at a 0.5° × 0.5° resolution. We first estimated the TC-specific mortality risks and POC using an advanced flexible statistical framework of mixed Poisson model, accounting for the population changes, natural variation, seasonal and day of the week effects. Then, a mixed meta-regression model was used to pool the TC-specific mortality risks to estimate the overall and country-specific ER relationships of TC characteristics (windspeed, rainfall, and year) with mortality. Overall, 47.7 million all-cause, 15.5 million cardiovascular, and 4.9 million respiratory deaths and 382 TCs were included in our analyses. An overall average POC of around 20 days was observed for TC-related all-cause and cardiopulmonary mortality, with relatively longer POC for the United States of America, Brazil, and Taiwan (30 days). The TC-specific relative risks (RR) varied substantially, ranging from 1.04 to 1.42, 1.07 to 1.77, and 1.12 to 1.92 among the top 100 TCs with highest RRs for all-cause, cardiovascular, and respiratory mortality, respectively. At country level, relatively higher TC-related mortality risks were observed in Guatemala, Brazil, and New Zealand for all-cause, cardiovascular, and respiratory mortality, respectively. We found an overall monotonically increasing and approximately linear ER curve of TC-related maximum sustained windspeed and cumulative rainfall with mortality, with heterogeneous patterns across countries and regions. The TC-related mortality risks were generally decreasing from 1980 to 2019, especially for the Philippines, Taiwan, and the USA, whereas potentially increasing trends in TC-related all-cause and cardiovascular mortality risks were observed for Japan. Conclusions. The TC mortality risks and POC varied greatly across TC events, locations, and countries. To minimize the TC-related health burdens, targeted strategies are particularly needed for different countries and regions, integrating epidemiological evidence on region-specific POC and ER curves that consider across-TC variability.","tags":["tropical cyclones","periods of concern","mortality","risks","exposure-response"],"title":"Tropical cyclone-specific mortality risks and the periods of concern: A multicountry time-series study","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"d3370811075206f61e10050ed8dd7285","permalink":"https://dominicroye.github.io/en/publication/2024-una-nota-cambio-climatico-con-r/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/en/publication/2024-una-nota-cambio-climatico-con-r/","section":"publication","summary":"La temperatura media global en la superficie terrestre ha aumentado en 1,1 ºC desde la era preindustrial (1880-1900). A pesar de parecer un leve incremento en la temperatura, implica un aumento significativo en el calor acumulado del sistema Tierra. Cuando se combina el aumento de la temperatura en la superficie terrestre y en la superficie oceánica, la tasa de incremento promedio es de 0,08 ºC por década desde 1880. Sin embargo, la tasa promedio de aumento desde 1981 ha sido más del doble: 0,18 ºC por año. Los océanos se caracterizan por una menor tasa de calentamiento debido a su capacidad calorífica. No obstante, son los océanos los que absorben la mayoría del calor adicional del planeta debido al cambio climático...","tags":["climate change","sea surface temperature","Mediterranean Sea","ggplot2","data visualization"],"title":"Una nota sobre el cambio climático","type":"publication"},{"authors":["M Stafoggia","et al"],"categories":null,"content":"","date":1697068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697068800,"objectID":"7f0e58580d280c72c5ee43a925090ff7","permalink":"https://dominicroye.github.io/en/publication/2023-joint-effect-pollution-mortality-environment-international/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/en/publication/2023-joint-effect-pollution-mortality-environment-international/","section":"publication","summary":"Background: The epidemiological evidence on the interaction between heat and ambient air pollution on mortality is still inconsistent. Objectives: To investigate the interaction between heat and ambient air pollution on daily mortality in a large dataset of 620 cities from 36 countries. Methods: We used daily data on all-cause mortality, air temperature, particulate matter ≤ 10 μm (PM10), PM ≤ 2.5 μm (PM2.5), nitrogen dioxide (NO2), and ozone (O3) from 620 cities in 36 countries in the period 1995–2020. We restricted the analysis to the six consecutive warmest months in each city. City-specific data were analysed with over-dispersed Poisson regression models, followed by a multilevel random-effects meta-analysis. The joint association between air temperature and air pollutants was modelled with product terms between non-linear functions for air temperature and linear functions for air pollutants. Results: We analyzed 22,630,598 deaths. An increase in mean temperature from the 75th to the 99th percentile of city-specific distributions was associated with an average 8.9 % (95 % confidence interval: 7.1 %, 10.7 %) mortality increment, ranging between 5.3 % (3.8 %, 6.9 %) and 12.8 % (8.7 %, 17.0 %), when daily PM10 was equal to 10 or 90 μg/m3, respectively. Corresponding estimates when daily O3 concentrations were 40 or 160 μg/m3 were 2.9 % (1.1 %, 4.7 %) and 12.5 % (6.9 %, 18.5 %), respectively. Similarly, a 10 μg/m3 increment in PM10 was associated with a 0.54 % (0.10 %, 0.98 %) and 1.21 % (0.69 %, 1.72 %) increase in mortality when daily air temperature was set to the 1st and 99th city-specific percentiles, respectively. Corresponding mortality estimate for O3 across these temperature percentiles were 0.00 % (-0.44 %, 0.44 %) and 0.53 % (0.38 %, 0.68 %). Similar effect modification results, although slightly weaker, were found for PM2.5 and NO2. Conclusions: Suggestive evidence of effect modification between air temperature and air pollutants on mortality during the warm period was found in a global dataset of 620 cities.","tags":["air pollution","joint effect","heat","mortality","short-term association"],"title":"Joint effect of heat and air pollution on mortality in 620 cities of 36 countries","type":"publication"},{"authors":["A Tobías","C Iñiguez","D Royé"],"categories":null,"content":"","date":1696896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696896000,"objectID":"9cb3c3c537b785186fcf38ce98de5c86","permalink":"https://dominicroye.github.io/en/publication/2023-mace-shiny-app-environment-health/","publishdate":"2023-10-10T00:00:00Z","relpermalink":"/en/publication/2023-mace-shiny-app-environment-health/","section":"publication","summary":"Exposure to heat poses a major threat to high-risk populations by substantially contributing to increased mortality and morbidity. Heat-related mortality has been a significant concern since the summer of 2003, when Europe experienced a heatwave, leading to an excess of more than 70,000 deaths during the summer months, with 3,166 of those occurring in Spain. Heat-health early warning systems can reduce the burden of high ambient temperatures. However, the evidence of their effectiveness is limited. Therefore, developing innovative tools for real-time monitoring and forecast of health impacts from heat becomes essential for effective public health interventions and resource allocation strategies. We developed a user-friendly and accessible tool for monitoring heat-attributable mortality in Spain during the summer season between June and August. https://ficlima.shinyapps.io/mace/","tags":["mortality","application","extreme temperature","early warning systems","monitoring"],"title":"From Research to the Development of an Innovative Application for Monitoring Heat-Related Mortality in Spain","type":"publication"},{"authors":null,"categories":["gis","R","R:elementary","visualization"],"content":"\rToday I present a short post on how we can position an outermost territory near the main map or insert an orientation map. In this example we use the typical map of Spain where the Canary Islands are located in the southwest of the peninsula.\nPackages\r\r\r\r\rPaquete\rDescripción\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rmapSpain\rAdministrative boundaries of Spain at different levels\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rgiscoR\rAdministrative boundaries of the world\r\rpatchwork\rSimple grammar to combine separate ggplots into the same graphic\r\rrmapshaper\rmapshaper library client for geospatial operations\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;mapSpain\u0026quot;)) install.packages(\u0026quot;mapSpain\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;giscoR\u0026quot;)) install.packages(\u0026quot;giscoR\u0026quot;)\rif(!require(\u0026quot;patchwork\u0026quot;)) install.packages(\u0026quot;patchwork\u0026quot;)\rif(!require(\u0026quot;rmapshaper\u0026quot;)) install.packages(\u0026quot;rmapshaper\u0026quot;)\r# packages\rlibrary(sf)\rlibrary(giscoR)\rlibrary(mapSpain)\rlibrary(tidyverse)\rlibrary(patchwork)\rlibrary(rmapshaper)\r\rOption 1\rWe can easily find some administrative boundaries of states such as Spain, where the actual geographical position of a remote territory has been changed, such as the Canary Islands. The default mapSpain package shifts the islands to the southwest of the Iberian Peninsula, a common position we see in many maps. However, these vector boundaries with displacement cannot be used in all assumptions, as this is a false geographical position and is not suitable for spatial calculations or other projections.\nWe obtain the vector boundaries using the esp_get_prov() function for the provincial level with the projection code EPSG:4326 (WGS84). In the construction of the map via ggplot2 we simply add the object to the geom_sf() geometry specifically designed for handling vector objects of class sf.\n# province boundaries with Canary Islands displacement\resp \u0026lt;- esp_get_prov(epsg = 4326)\r# simple map\rggplot(esp) +\rgeom_sf(colour = \u0026quot;white\u0026quot;, linewidth = .2) +\rtheme_void()\rmapSpain also includes a function to get the separator box (esp_get_can_box()) in order to indicate the false location. With the gisco_get_countries() function we get the global country boundaries to add as geographical context, although we clipped it to the extent of the Iberian Peninsula. It may be surprising to see a curved cutout using the WGS84 projection, but this is because spherical geometry is used by default in all sf operations (sf_use_s2()).\n# we add the Canary Islands box and the boundaries of the environment\rcan_bx \u0026lt;- esp_get_can_box(epsg = 4326)\rentorno \u0026lt;- gisco_get_countries(resolution = \u0026quot;10\u0026quot;) %\u0026gt;% st_crop(xmin = -10, xmax = 5, ymin = 34, ymax = 45)\r# with displacement\rggplot(esp) +\rgeom_sf(data = entorno, fill = \u0026quot;grey70\u0026quot;, colour = NA) +\rgeom_sf(colour = \u0026quot;white\u0026quot;, linewidth = .2) +\rgeom_sf(data = can_bx, linewidth = .3, colour = \u0026quot;grey80\u0026quot;) +\rtheme_void()\r\rOption 2\rThe most correct way is to create an object for the inserted map, here the Canary Islands, and another one for the main map, mainland Spain and the Balearic Islands. In the esp_get_prov() function we must indicate that it returns the limits without displacement with the agrument moveCAN = FALSE. First, we build the map of the Canary Islands, filtering the autonomous community. The geometries geom_hline() and geom_vline() will draw the separation line to the peninsula. The second step is to create the main map excluding the Canary Islands. Then the map of the Canary Islands needs to be included as an object using the annotation_custom() function. The ggplot object must be converted to a grob with ggplotGrob() and the position area (the X and Y extreme points) must be indicated in the coordinate system of the main map. This form can be used for all types of maps.\n# boundaries of provinces without displacement of the Canary Islands\resp \u0026lt;- esp_get_prov(epsg = 4326, moveCAN = F)\r# Canary Island map\rcan \u0026lt;- filter(esp, nuts2.name == \u0026quot;Canarias\u0026quot;) %\u0026gt;%\rggplot() +\rgeom_vline(xintercept = -13.3, colour = \u0026quot;grey80\u0026quot;) +\rgeom_hline(yintercept = 29.5, colour = \u0026quot;grey80\u0026quot;) +\rgeom_sf(fill = \u0026quot;red\u0026quot;, colour = \u0026quot;white\u0026quot;) +\rcoord_sf(expand = F) +\rtheme_void() can\r# add ggplot map with annotation_custom() absolute position according to the SRC\rfilter(esp, nuts2.name != \u0026quot;Canarias\u0026quot;) %\u0026gt;%\rggplot() +\rgeom_sf(data = entorno, fill = \u0026quot;grey70\u0026quot;, colour = NA) +\rgeom_sf(colour = \u0026quot;white\u0026quot;, linewidth = .2) +\rannotation_custom(ggplotGrob(can),\rxmin = -14, xmax = -9,\rymin = 33, ymax = 38) +\rtheme_void()\rIf we want to project the main map, we only need to project the position area of the inserted map first.\n# position box with some adjustment pos \u0026lt;- c(xmin = -13.5, ymin = 32.5, xmax = -8.5, ymax = 37.5) class(pos) \u0026lt;- \u0026quot;bbox\u0026quot; # definimos como bbox\r# reproject to LAEA Europe EPSG:3035\rpos_prj \u0026lt;- st_as_sfc(pos) %\u0026gt;% st_set_crs(4326) %\u0026gt;%\rst_transform(3035) %\u0026gt;% st_bbox()\r# create the final map\rfilter(esp, nuts2.name != \u0026quot;Canarias\u0026quot;) %\u0026gt;%\rggplot() +\rgeom_sf(data = entorno, fill = \u0026quot;grey70\u0026quot;, colour = NA) +\rgeom_sf(colour = \u0026quot;white\u0026quot;, linewidth = .2) +\rannotation_custom(ggplotGrob(can),\rxmin = pos_prj[1], xmax = pos_prj[3],\rymin = pos_prj[2], ymax = pos_prj[4]) +\rcoord_sf(crs = 3035) +\rtheme_void()\r\rOption 3\rThe last option for inserting a secondary map is to use the inset_element() function of the patchwork package. The difference with the previous method is the relative position, which limits the use. In this case proportional symbols should not be represented as the relative insertion does not maintain the same dimensions as the main map.\n# provincial boundaries\resp \u0026lt;- esp_get_prov(epsg = 4326, moveCAN = F)\r# Canary Island map\rcan \u0026lt;- filter(esp, nuts2.name == \u0026quot;Canarias\u0026quot;) %\u0026gt;%\rggplot() +\rgeom_vline(xintercept = -13.3, colour = \u0026quot;grey80\u0026quot;) +\rgeom_hline(yintercept = 29.5, colour = \u0026quot;grey80\u0026quot;) +\rgeom_sf(fill = \u0026quot;red\u0026quot;, colour = \u0026quot;white\u0026quot;) +\rcoord_sf(expand = F) +\rtheme_void() # main map\rm \u0026lt;- filter(esp, nuts2.name != \u0026quot;Canarias\u0026quot;) %\u0026gt;%\rggplot() +\rgeom_sf(colour = \u0026quot;white\u0026quot;, linewidth = .2) +\rtheme_void()\r# insert with relative position m + inset_element(can, left = -.1, bottom = 0, right = .2, top = .2, align_to = \u0026quot;full\u0026quot;)\r\rEarth globe as inset map\rThe only difficulty here is the orthogonal projection while preserving the visible geometry of the earth.\rThe first step is the creation of the “ocean” using the radius of the earth from the point 0,0. Then we only have to cut out the visible part and reproject the boundaries. In the definition of the orthogonal projection it is possible to centre at different latitudes and longitudes by changing the +lat_0 and +lon_0 values. The ms_innerlines() functions of the rmapshaper package easily create the inner boundaries of polygons, which is recommended to avoid blurring small areas.\n# overall country boundaries\rwld \u0026lt;- gisco_get_countries(resolution = \u0026quot;20\u0026quot;)\r# definition of orthogonal projection\rortho_crs \u0026lt;-\u0026#39;+proj=ortho +lat_0=30 +lon_0=0.5 +x_0=0 +y_0=0 +R=6371000 +units=m +no_defs +type=crs\u0026#39;\r# creation of the ocean ocean \u0026lt;- st_point(x = c(0,0)) %\u0026gt;%\rst_buffer(dist = 6371000) %\u0026gt;% # radio Tierra\rst_sfc(crs = ortho_crs)\rplot(ocean)\r# cut out the visible land and reproject it\rworld \u0026lt;- st_intersection(wld, st_transform(ocean, 4326)) %\u0026gt;%\rst_transform(crs = ortho_crs) %\u0026gt;% mutate(dummy = ifelse(NAME_ENGL == \u0026quot;Spain\u0026quot;, \u0026quot;yes\u0026quot;, \u0026quot;no\u0026quot;))\rplot(world)\r# obtain only the inner limits\rworld_line \u0026lt;- ms_innerlines(world)\rplot(world_line)\r# main map of Spain wld_map \u0026lt;- ggplot(world) +\rgeom_sf(data = ocean, fill = \u0026quot;#deebf7\u0026quot;, linewidth = .2) +\rgeom_sf(aes(fill = dummy), colour = NA,\rshow.legend = F) +\rgeom_sf(data = world_line, linewidth = .05, colour = \u0026quot;white\u0026quot;) +\rscale_fill_manual(values = c(\u0026quot;grey50\u0026quot;, \u0026quot;red\u0026quot;)) + theme_void()\r# insert the globe marking the location of Spain m + inset_element(wld_map, left = 0.65, bottom = 0.82, right = 1.1, top = 1, align_to = \u0026quot;full\u0026quot;)\r\n\r","date":1696723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696723200,"objectID":"5d42f3d4da9e2e45eb1fc81b98c56444","permalink":"https://dominicroye.github.io/en/2023/inserted-maps-with-ggplot2/","publishdate":"2023-10-08T00:00:00Z","relpermalink":"/en/2023/inserted-maps-with-ggplot2/","section":"post","summary":"A short post on how we can position an outermost territory near the main map or insert an orientation map. In this example we use the typical map of Spain where the Canary Islands are located in the southwest of the peninsula.","tags":["inserted map","spain","canary","globe"],"title":"Inserted maps with ggplot2","type":"post"},{"authors":["Liu C","et al"],"categories":null,"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"41e0d9d0cb6226eb054cfb2c5b0894c9","permalink":"https://dominicroye.github.io/en/publication/2023-pm25-o3-mortality-bmj/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/en/publication/2023-pm25-o3-mortality-bmj/","section":"publication","summary":"Objective: To investigate potential interactive effects of fine particulate matter (PM2.5) and ozone (O3) on daily mortality at global level. Design: Two stage time series analysis. Setting: 372 cities across 19 countries and regions. Population: Daily counts of deaths from all causes, cardiovascular disease, and respiratory disease. Main outcome measure: Daily mortality data during 1994-2020. Stratified analyses by co-pollutant exposures and synergy index (1 denotes the combined effect of pollutants is greater than individual effects) were applied to explore the interaction between PM2.5 and O3 in association with mortality. Results: During the study period across the 372 cities, 19.3 million deaths were attributable to all causes, 5.3 million to cardiovascular disease, and 1.9 million to respiratory disease. The risk of total mortality for a 10 μg/m3 increment in PM2.5 (lag 0-1 days) ranged from 0.47% (95% confidence interval 0.26% to 0.67%) to 1.25% (1.02% to 1.48%) from the lowest to highest fourths of O3 concentration; and for a 10 μg/m3 increase in O3 ranged from 0.04% (−0.09% to 0.16%) to 0.29% (0.18% to 0.39%) from the lowest to highest fourths of PM2.5 concentration, with significant differences between strata (P for interaction ","tags":["pollution","PM2.5","Ozone","interactive effect","mortality"],"title":"Interactive effects of ambient fine particulate matter and ozone on daily mortality in 372 cities: two stage time series analysis","type":"publication"},{"authors":["Johannes Uhl","D Royé","Keith Burghardt","José Antonio Aldrey","Manuel Borobio","Stefan Leyk"],"categories":null,"content":"","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"c8de8efaa925c553c12c6e7f7dfd0bcd","permalink":"https://dominicroye.github.io/en/publication/2023-hisdac-es-dataset-essd/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/en/publication/2023-hisdac-es-dataset-essd/","section":"publication","summary":"Multi-temporal measurements quantifying the changes to the Earth’s surface are critical for understanding many natural, anthropogenic, and social processes. Researchers typically use remotely sensed earth observation data to quantify and characterize such changes in land use and land cover (LULC). However, such data sources are limited in their availability prior to the 1980s. While an observational window of 40 to 50 years is sufficient to study most recent LULC changes, processes such as urbanization, land development, and the evolution of urban, and coupled nature-human systems often operate over longer time periods covering several decades or even centuries. Thus, to quantify and better understand such processes, alternative historical-geospatial data sources are required that extend farther back in time. However, such data are rare and processing is labor-intensive, often involving manual work. To overcome the resulting lack in quantitative knowledge of urban systems and the built environment prior to the 1980s, we leverage cadastral data with rich thematic property attribution, such as building usage and construction year. We scraped, harmonized, and processed over 12,000,000 building footprints including construction years to create a multi-faceted series of gridded surfaces, describing the evolution of human settlements in Spain from 1900 to 2020, at 100 m spatial and 5 years temporal resolution. These surfaces include measures of building density, built-up intensity, and built-up land use. We evaluated our data against a variety of data sources including remotely sensed human settlement data and land cover data, model-based historical land use depictions, as well as historical maps and historical aerial imagery, and find high levels of agreement. This new data product, the Historical Settlement Data Compilation for Spain (HISDAC-ES), is publicly available (https://doi.org/10.6084/m9.figshare.22009643; Uhl et al., 2023a) and represents a rich source for quantitative, long-term analyses of the built environment and related processes over large spatial and temporal extents and at fine resolutions.","tags":["historical settlement","open data","Spain","urbanization","building footprints","catastro"],"title":"HISDAC-ES: Historical Settlement Data Compilation for Spain (1900 -2020)","type":"publication"},{"authors":["Díaz-Poso A","Lorenzo N","Marti A","Royé D"],"categories":null,"content":"","date":1695081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695081600,"objectID":"d78bc358c83c96dd604f3a77470340a6","permalink":"https://dominicroye.github.io/en/publication/2023-cold-wave-intensity-climate-projections-atmospheric-research/","publishdate":"2023-09-19T00:00:00Z","relpermalink":"/en/publication/2023-cold-wave-intensity-climate-projections-atmospheric-research/","section":"publication","summary":"In the context of global warming, cold waves have generated less interest in the scientific community than heat waves, despite their impacts on public health, transport infrastructures and energy consumption. The present study analyses climate change scenarios with simulations of the EURO-CORDEX project, using the Excess Cold Factor (ECF) index for the Iberian Peninsula and Balearic Islands (IPB). The dimensions of intensity, frequency, duration and spatial extent are evaluated for the near future (2021–2050) with respect to the historical period of reference (1971–2000). The projections show a significant overall decrease in all dimensions. The mean change in maximum cold wave intensity is −50% over most of the IPB as a whole in the near future (2021–2050). The largest changes occur in the interior of the Peninsula, where the decrease is around −100%. The annual mean number of cold wave days decreases for the IPB as a whole by −50% compared to 1971–2000, with the maximum extent decreasing by more than the mean, with decreases of between −2.4%/decade and − 5.5%/decade. Although a smaller number of cold waves suggests less human exposure, the acclimatisation of the population to higher temperatures will imply that cold waves will continue to pose a serious local threat.","tags":["cold waves","intensity","expansion","climate change"],"title":"Cold wave intensity on the Iberian Peninsula: Future climate projections","type":"publication"},{"authors":["Lüthi S","et al"],"categories":null,"content":"","date":1692835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692835200,"objectID":"fe11886db15e366f95f73db2adab30d0","permalink":"https://dominicroye.github.io/en/publication/2023-rapid-increase-risk-heat-mortality-nature-communication/","publishdate":"2023-08-24T00:00:00Z","relpermalink":"/en/publication/2023-rapid-increase-risk-heat-mortality-nature-communication/","section":"publication","summary":"Heat-related mortality has been identified as one of the key climate extremes posing a risk to human health. Current research focuses largely on how heat mortality increases with mean global temperature rise, but it is unclear how much climate change will increase the frequency and severity of extreme summer seasons with high impact on human health. In this probabilistic analysis, we combined empirical heat-mortality relationships for 748 locations from 47 countries with climate model large ensemble data to identify probable past and future highly impactful summer seasons. Across most locations, heat mortality counts of a 1-in-100 year season in the climate of 2000 would be expected once every ten to twenty years in the climate of 2020. These return periods are projected to further shorten under warming levels of 1.5°C and 2°C, where heat-mortality extremes of the past climate will eventually become commonplace if no adaptation occurs. Our findings highlight the urgent need for strong mitigation and adaptation to reduce impacts on human lives.","tags":["extreme events","return period","climate change","mortality"],"title":"Rapid increase in the risk of heat-related mortality","type":"publication"},{"authors":["Wenzhong Huang","et al"],"categories":null,"content":"","date":1690848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690848000,"objectID":"4412a6874415849bfed9016dc7b3356d","permalink":"https://dominicroye.github.io/en/publication/2023-mortality-cyclones-tropicales-lancet-planetary-health/","publishdate":"2023-08-01T00:00:00Z","relpermalink":"/en/publication/2023-mortality-cyclones-tropicales-lancet-planetary-health/","section":"publication","summary":"Background. The global spatiotemporal pattern of mortality risk and burden attributable to tropical cyclones is unclear. We aimed to evaluate the global short-term mortality risk and burden associated with tropical cyclones from 1980 to 2019. Methods. The wind speed associated with cyclones from 1980 to 2019 was estimated globally through a parametric wind field model at a grid resolution of 0·5°× 0·5°. A total of 341 locations with daily mortality and temperature data from 14 countries that experienced at least one tropical cyclone day (a day with maximum sustained wind speed associated with cyclones ≥17.5 m/s) during the study period were included. A conditional quasi-Poisson regression with distributed lag non-linear model was applied to assess the tropical cyclone–mortality association. A meta-regression model was fitted to evaluate potential contributing factors and estimate grid cell-specific tropical cyclone effects.Findings. Tropical cyclone exposure was associated with an overall 6% (95% CI 4–8) increase in mortality in the first 2 weeks following exposure. Globally, an estimate of 97 430 excess deaths (95% empirical CI [eCI] 71651–126438) per decade were observed over the 2 weeks following exposure to tropical cyclones, accounting for 20·7 (95% eCI 15·2–26·9) excess deaths per 100 000 residents (excess death rate) and 3·3 (95% eCI 2·4–4·3) excess deaths per 1000 deaths (excess death ratio) over 1980–2019. The mortality burden exhibited substantial temporal and spatial variation. East Asia and south Asia had the highest number of excess deaths during 1980–2019: 28744 (95% eCI 16863–42188) and 27267 (21157–34058) excess deaths per decade, respectively. In contrast, the regions with the highest excess death ratios and rates were southeast Asia and Latin America and the Caribbean. From 1980–99 to 2000–19, marked increases in tropical cyclone-related excess death numbers were observed globally, especially for Latin America and the Caribbean and south Asia. Grid cell-level and country-level results revealed further heterogeneous spatiotemporal patterns such as the high and increasing tropical cyclone-related mortality burden in Caribbean countries or regions. Interpretation. Globally, short-term exposure to tropical cyclones was associated with a significant mortality burden, with highly heterogeneous spatiotemporal patterns. In-depth exploration of tropical cyclone epidemiology for those countries and regions estimated to have the highest and increasing tropical cyclone-related mortality burdens is urgently needed to help inform the development of targeted actions against the increasing adverse health impacts of tropical cyclones under a changing climate.","tags":["tropical cyclones","short-term effects","hurricane","mortality"],"title":"Global short-term mortality risk and burden associated with tropical cyclones from 1980 to 2019: a multi-country time-series study","type":"publication"},{"authors":["Eunice Lo","et al"],"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"2a454cd7076c3a14ba9902bf52d7e3df","permalink":"https://dominicroye.github.io/en/publication/2023-heatstress-metric-mortality-climatology/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/en/publication/2023-heatstress-metric-mortality-climatology/","section":"publication","summary":"Combined heat and humidity is frequently described as the main driver of human heat-related mortality, more so than dry-bulb temperature alone. While based on physiological thinking, this assumption has not been robustly supported by epidemiological evidence. By performing the first systematic comparison of eight heat stress metrics (i.e., temperature combined with humidity and other climate variables) with warm-season mortality, in 604 locations over 39 countries, we find that the optimal metric for modelling mortality varies from country to country. Temperature metrics with no or little humidity modification associates best with mortality in ~40% of the studied countries. Apparent temperature (combined temperature, humidity and wind speed) dominates in another 40% of countries. There is no obvious climate grouping in these results. We recommend, where possible, that researchers use the optimal metric for each country. However, dry-bulb temperature performs similarly to humidity-based heat stress metrics in estimating heat-related mortality in present-day climate.","tags":["heat stress","metrics","modelling","mortality"],"title":"Optimal heat stress metric for modelling heat-related mortality varies from country to country","type":"publication"},{"authors":["Santiago Gestal","Adolfo Figueiras","D Royé"],"categories":null,"content":"","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"1cc04d490ff39de2c6892f74a239d488","permalink":"https://dominicroye.github.io/en/publication/2023-review-cardio-emgerncy-calls-environ-health/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/en/publication/2023-review-cardio-emgerncy-calls-environ-health/","section":"publication","summary":"Climate change has increased interest in the effects of the thermal environment on cardiovascular health. Most studies have focused on mortality data. However, pre-hospital care data are better able to evaluate these effects, as they can register the full spectrum of the disease in real time. This scoping review aims to synthesize the epidemiological evidence regarding the effects of the thermal environment on cardiovascular morbidity in the pre-hospital setting, evaluated through ambulance calls. A staged literature search was performed using the PubMed database for the period between 1st January 2000 and 30th March 2023, using the MeSH terms \"Weather\" AND \"Emergency Medical Services\". A total of 987 publications were identified that examined the correlation between the thermal environment and ambulance call-outs for cardiovascular causes. The studies were mostly ecological time series, with significant variability in the methodological aspects employed. An increase in the number of ambulance call-outs has been observed in association with low temperatures, both for overall cardiovascular pathologies and for certain pathological subtypes. For high temperatures, no effect has been observed in overall call-outs, although an increase has been observed during heat waves. The demand for ambulances for cardiac arrests is increased by both low and high temperatures and during heat waves. Ambulance call-outs for cardiovascular causes increase with low temperatures and heat waves, with no significant increase in the overall demand associated with high temperatures. Ambulance call-outs for cardiac arrests are the only subtype that is increased by high temperatures.","tags":["cardiovascular diseases","weather","cold exposure","heat exposure","ambulance call-out","emergency medical services"],"title":"Effect of Temperature on Emergency Ambulance Call-Outs for Cardiovascular Causes: A Scoping Review","type":"publication"},{"authors":["Edward O'Brien","Pierre Masselot","et al"],"categories":null,"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"0d30417eae76327d63ba8249976607df","permalink":"https://dominicroye.github.io/en/publication/2023-so2-mcc-ehp/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/en/publication/2023-so2-mcc-ehp/","section":"publication","summary":"Background: Epidemiological evidence on the health risks of sulfur dioxide (SO2) is more limited compared with other pollutants, and doubts remain on several aspects, such as the form of the exposure-response relationship, the potential role of copollutants, as well as the actual risk at low concentrations and possible temporal variation in risks. Objectives: Our aim was to assess the short-term association between exposure to SO2 and daily mortality in a large multilocation data set, using advanced study designs and statistical techniques. Methods: The analysis included 43,729,018 deaths that occurred in 399 cities within 23 countries between 1980 and 2018. A two-stage design was applied to assess the association between the daily concentration of SO2 and mortality counts, including first-stage time-series regressions and second-stage multilevel random-effect meta-analyses. Secondary analyses assessed the exposure-response shape and the lag structure using spline terms and distributed lag models, respectively, and temporal variations in risk using a longitudinal meta-regression. Bi-pollutant models were applied to examine confounding effects of particulate matter with an aerodynamic diameter of ≤10μm (PM10) and 2.5μm (PM2.5), ozone, nitrogen dioxide, and carbon monoxide. Associations were reported as relative risks (RRs) and fractions of excess deaths. Results: The average daily concentration of SO2 across the 399 cities was 11.7 μg/m3, with 4.7% of days above the World Health Organization (WHO) guideline limit (40 μg/m3, 24-h average), although the exceedances occurred predominantly in specific locations. Exposure levels decreased considerably during the study period, from an average concentration of 19.0 μg/m3 in 1980-1989 to 6.3 μg/m3 in 2010-2018. For all locations combined, a 10-μg/m3 increase in daily SO2 was associated with an RR of mortality of 1.0045 [95% confidence interval (CI): 1.0019, 1.0070], with the risk being stable over time but with substantial between-country heterogeneity. Short-term exposure to SO2 was associated with an excess mortality fraction of 0.50% [95% empirical CI (eCI): 0.42%, 0.57%] in the 399 cities, although decreasing from 0.74% (0.61%, 0.85%) in 1980-1989 to 0.37% (0.27%, 0.47%) in 2010-2018. There was some evidence of nonlinearity, with a steep exposure-response relationship at low concentrations and the risk attenuating at higher levels. The relevant lag window was 0-3 d. Significant positive associations remained after controlling for other pollutants. Discussion: The analysis revealed independent mortality risks associated with short-term exposure to SO2, with no evidence of a threshold. Levels below the current WHO guidelines for 24-h averages were still associated with substantial excess mortality, indicating the potential benefits of stricter air quality standards.","tags":["air pollution","sulfur dioxide","mortality","short-term association"],"title":"Short-Term Association between Sulfur Dioxide and Mortality: A Multicountry Analysis in 399 Cities","type":"publication"},{"authors":null,"categories":["gis","R","R:advanced","visualization"],"content":"\r\r\r\r\r\r\r\r\r\r\r1 Packages\r2 Part I. Geoprocessing with Google Earth Engine (GEE)\r\r2.1 Before using GEE in R\r2.2 Access the Global Forecast System\r2.3 Dynamic map via GEE\r2.4 Export multiple images\r\r3 Part II. Orthographic map\r\r3.1 Data\r3.2 Administrative boundaries and graticules\r3.3 Map construction\r\r3.3.1 Select tomorrow\r3.3.2 The shadow of the globe\r3.3.3 Adding other layers\r\r\r\r\rMy climate this week looks at the Arctic freeze that is sweeping across large parts of northern Asia, w/ @Emiliyadotcom https://t.co/u49jvHKxvK #dataviz #gistribe #cartography pic.twitter.com/6mX22bKZqF\n\u0026mdash; Chris Campbell (@digitalcampbell) January 28, 2023  A while back I saw Chris Campbell’s global maps from the Financial Times like in this Tweet and I thought I needed to do it in R. In this first post of 2023 we’ll see how we can access the GFS (Global Forecast System) data and visualize it with {ggplot2}, even though there are several ways, in this case we use the Google Earth Engine API via the {rgee} package for accessing the GFS data. We will select the most recent run and calculate the maximum temperature for the next few days.\n1 Packages\r\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rlubridate\rEasy manipulation of dates and times\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rterra\rImport, export and manipulate raster ({raster} successor package)\r\rrgee\rAccess to Google Earth Engine API\r\rgiscoR\rAdministrative boundaries of the world\r\rggshadow\rExtension to ggplot2 for shaded and glow geometries\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rggforce\rProvides missing functionality to ggplot2\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;rgee\u0026quot;)) install.packages(\u0026quot;rgee\u0026quot;)\rif(!require(\u0026quot;giscoR\u0026quot;)) install.packages(\u0026quot;giscoR\u0026quot;)\rif(!require(\u0026quot;ggshadow\u0026quot;)) install.packages(\u0026quot;ggshadow\u0026quot;)\rif(!require(\u0026quot;ggforce\u0026quot;)) install.packages(\u0026quot;ggforce\u0026quot;)\r# packages\rlibrary(rgee)\rlibrary(terra)\rlibrary(sf)\rlibrary(giscoR)\rlibrary(fs)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggshadow)\rlibrary(ggforce)\r\r2 Part I. Geoprocessing with Google Earth Engine (GEE)\r2.1 Before using GEE in R\rThe first step is to sign up at earthengine.google.com. In addition, it is necessary to install CLI of gcloud (https://cloud.google.com/sdk/docs/install?hl=es-419), you just have to follow the instructions in Google. Regarding the GEE language, many functions that are applied are similar to what is known from {tidyverse}. More help can be found at https://r-spatial.github.io/rgee/reference/rgee-package.html and on the GEE page itself.\nThe most essential of GEE’s native Javascript language is that it is characterized by the way of combining functions and variables using the dot, which is replaced by the $ in R. All GEE functions start with the prefix ee_* (ee_print( ), ee_image_to_drive()).\nOnce we have gcloud and the {rgee} package installed we can proceed to create the Python virtual environment. The ee_install() function takes care of installing Anaconda 3 and all necessary packages. To check the correct installation of Python, and particularly of the numpy and earthengine-api packages, we can use ee_check().\nee_install() # create python virtual environment\ree_check() # check if everything is correct\rBefore programming with GEE’s own syntax, GEE must be authenticated and initialized using the ee_Initialize() function.\nee_Initialize(drive = TRUE) \r## ── rgee 1.1.5 ─────────────────────────────────────── earthengine-api 0.1.339 ── ## ✔ user: not_defined\r## ✔ Google Drive credentials:\r✔ Google Drive credentials: FOUND\r## ✔ Initializing Google Earth Engine:\r✔ Initializing Google Earth Engine: DONE!\r## ✔ Earth Engine account: users/dominicroye ## ────────────────────────────────────────────────────────────────────────────────\r\r2.2 Access the Global Forecast System\rA time series of images or multidimensional data is called an ImageCollection in GEE. Each dataset is assigned an ID and we can access it by making the following call ee$ImageCollection('ID_IMAGECOLLECTION'). There are helper functions that allow conversion of purely R classes to Javascript, e.g. for dates rdate_to_eedate(). The first thing we do is to filter to the most recent date with the last run of the GFS model.\nWe have to know that, unlike R, only when GEE tasks are sent, the calculation are executed on the servers using all the created GEE objects. Most steps create only EarthEngine objects what you will see soon in this post.\n## GFS forecast\rdataset \u0026lt;- ee$ImageCollection(\u0026#39;NOAA/GFS0P25\u0026#39;)$filter(ee$Filter$date(rdate_to_eedate(today()-days(1)),\rrdate_to_eedate(today()+days(1))))\rdataset\r## EarthEngine Object: ImageCollection\rThe model runs every 6 hours (0, 6, 12, 18), so the ee_get_date_ic() function extracts the dates to choose the most recent one. This is the first time that calculations are run.\n# vector of unique run dates\rlast_run \u0026lt;- ee_get_date_ic(dataset)$time_start |\u0026gt; unique()\rlast_run\r## [1] \u0026quot;2023-02-22 00:00:00 GMT\u0026quot; \u0026quot;2023-02-22 06:00:00 GMT\u0026quot;\r## [3] \u0026quot;2023-02-22 12:00:00 GMT\u0026quot; \u0026quot;2023-02-22 18:00:00 GMT\u0026quot;\r## [5] \u0026quot;2023-02-23 00:00:00 GMT\u0026quot;\r# select the last one\rlast_run \u0026lt;- max(last_run)\rNext we filter the date of the last run and select the band of the air temperature at 2m. Forecast dates are attributes of each model run up to 336 hours (14 days) from the day of execution. When we want to make changes to each image in an ImageCollection we must make use of the map() function, similar to the one we know from the {purrr} package. In this case we redefine the date of each image (system:time_start: run date) by that of the forecast (forecast_time). It is important that the R function to apply is inside ee_utils_pyfunc(), which translates it into Python. Then we extract the dates from the 14 day forecast.\n# last run and variable selection\rtemp \u0026lt;- dataset$filter(ee$Filter$date(rdate_to_eedate(last_run)))$select(\u0026#39;temperature_2m_above_ground\u0026#39;)\r# define the forecast dates for each hour\rforcast_time \u0026lt;- temp$map(ee_utils_pyfunc(function(img) {\rreturn(ee$Image(img)$set(\u0026#39;system:time_start\u0026#39;,ee$Image(img)$get(\u0026quot;forecast_time\u0026quot;)))\r})\r)\r# get the forecast dates\rdate_forcast \u0026lt;- ee_get_date_ic(forcast_time)\rhead(date_forcast)\r## id time_start\r## 1 NOAA/GFS0P25/2023022300F000 2023-02-23 00:00:00\r## 2 NOAA/GFS0P25/2023022300F001 2023-02-23 01:00:00\r## 3 NOAA/GFS0P25/2023022300F002 2023-02-23 02:00:00\r## 4 NOAA/GFS0P25/2023022300F003 2023-02-23 03:00:00\r## 5 NOAA/GFS0P25/2023022300F004 2023-02-23 04:00:00\r## 6 NOAA/GFS0P25/2023022300F005 2023-02-23 05:00:00\rHere we could export the hourly temperature data, but it would also be possible to estimate the maximum or minimum daily temperature for the next 14 days. To achieve this we define the beginning and end of the period, and calculate the number of days. What we do in simple terms is map over the number of days to filter on each day and apply the max() function or any other similar function.\n# define start end of the period\rendDate \u0026lt;- rdate_to_eedate(round_date(max(date_forcast$time_start)-days(1), \u0026quot;day\u0026quot;))\rstartDate \u0026lt;- rdate_to_eedate(round_date(min(date_forcast$time_start), \u0026quot;day\u0026quot;))\r# number of days\rnumberOfDays \u0026lt;- endDate$difference(startDate, \u0026#39;days\u0026#39;)\r# calculate the daily maximum\rdaily \u0026lt;- ee$ImageCollection(\ree$List$sequence(0, numberOfDays$subtract(1))$\rmap(ee_utils_pyfunc(function (dayOffset) {\rstart = startDate$advance(dayOffset, \u0026#39;days\u0026#39;)\rend = start$advance(1, \u0026#39;days\u0026#39;)\rreturn(forcast_time$\rfilterDate(start, end)$\rmax()$ # alternativa: min(), mean()\rset(\u0026#39;system:time_start\u0026#39;, start$millis()))\r}))\r)\r# dates of the daily maximum\rhead(ee_get_date_ic(daily))\r## id time_start\r## 1 no_id 2023-02-23\r## 2 no_id 2023-02-24\r## 3 no_id 2023-02-25\r## 4 no_id 2023-02-26\r## 5 no_id 2023-02-27\r## 6 no_id 2023-02-28\r\r2.3 Dynamic map via GEE\rSince there is the possibility of adding images to a dynamic map in the GEE code editor, we can also do it from R using the GEE function Map.addLayer(). We simply select the first day with first(). In the other argument we define the range of the temperature values and the color ramp.\nMap$addLayer(\reeObject = daily$first(),\rvisParams = list(min = -45, max = 45,\rpalette = rev(RColorBrewer::brewer.pal(11, \u0026quot;RdBu\u0026quot;))),\rname = \u0026quot;GFS\u0026quot;) + Map$addLegend(\rlist(min = -45, max = 45, palette = rev(RColorBrewer::brewer.pal(11, \u0026quot;RdBu\u0026quot;))), name = \u0026quot;Maximum temperature\u0026quot;, position = \u0026quot;bottomright\u0026quot;, bins = 10)\r\r{\"x\":{\"options\":{\"minZoom\":1,\"maxZoom\":24,\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}},\"preferCanvas\":false,\"bounceAtZoomLimits\":false,\"maxBounds\":[[[-90,-370]],[[90,370]]]},\"calls\":[{\"method\":\"addProviderTiles\",\"args\":[\"CartoDB.Positron\",\"CartoDB.Positron\",\"CartoDB.Positron\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false,\"pane\":\"tilePane\",\"maxZoom\":24}]},{\"method\":\"addProviderTiles\",\"args\":[\"OpenStreetMap\",\"OpenStreetMap\",\"OpenStreetMap\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false,\"pane\":\"tilePane\",\"maxZoom\":24}]},{\"method\":\"addProviderTiles\",\"args\":[\"CartoDB.DarkMatter\",\"CartoDB.DarkMatter\",\"CartoDB.DarkMatter\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false,\"pane\":\"tilePane\",\"maxZoom\":24}]},{\"method\":\"addProviderTiles\",\"args\":[\"Esri.WorldImagery\",\"Esri.WorldImagery\",\"Esri.WorldImagery\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false,\"pane\":\"tilePane\",\"maxZoom\":24}]},{\"method\":\"addProviderTiles\",\"args\":[\"OpenTopoMap\",\"OpenTopoMap\",\"OpenTopoMap\",{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false,\"pane\":\"tilePane\",\"maxZoom\":24}]},{\"method\":\"addLayersControl\",\"args\":[[\"CartoDB.Positron\",\"OpenStreetMap\",\"CartoDB.DarkMatter\",\"Esri.WorldImagery\",\"OpenTopoMap\"],[],{\"collapsed\":true,\"autoZIndex\":true,\"position\":\"topleft\"}]},{\"method\":\"addScaleBar\",\"args\":[{\"maxWidth\":100,\"metric\":true,\"imperial\":true,\"updateWhenIdle\":true,\"position\":\"bottomleft\"}]},{\"method\":\"addTiles\",\"args\":[\"https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/a008b4a6531d66c1e0fee8a84fcedf1a-1bfc3a7d5d0add3872bf4326ef07976c/tiles/{z}/{x}/{y}\",\"GFS\",\"GFS\",{\"minZoom\":0,\"maxZoom\":24,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false}]},{\"method\":\"addLayersControl\",\"args\":[[\"CartoDB.Positron\",\"OpenStreetMap\",\"CartoDB.DarkMatter\",\"Esri.WorldImagery\",\"OpenTopoMap\"],\"GFS\",{\"collapsed\":true,\"autoZIndex\":true,\"position\":\"topleft\"}]},{\"method\":\"hideGroup\",\"args\":[null]},{\"method\":\"addLegend\",\"args\":[{\"colors\":[\"#053061 , #154D8A 5.55555555555556%, #3984BB 16.6666666666667%, #82BAD8 27.7777777777778%, #CAE1EE 38.8888888888889%, #F7F7F7 50%, #FDD5BF 61.1111111111111%, #EE9676 72.2222222222222%, #CA4C41 83.3333333333333%, #900C26 94.4444444444444%, #67001F \"],\"labels\":[\"-40\",\"-30\",\"-20\",\"-10\",\"0\",\"10\",\"20\",\"30\",\"40\"],\"na_color\":null,\"na_label\":\"NA\",\"opacity\":1,\"position\":\"bottomright\",\"type\":\"numeric\",\"title\":\"Maximum temperature\",\"extra\":{\"p_1\":0.0555555555555556,\"p_n\":0.944444444444444},\"layerId\":null,\"className\":\"info legend\",\"group\":null}]}],\"setView\":[[0,0],1,[]]},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el, x, data) {\\n return (\\n function(el, x, data) {\\n // get the leaflet map\\n var map = this; //HTMLWidgets.find('#' + el.id);\\n // we need a new div element because we have to handle\\n // the mouseover output separately\\n // debugger;\\n function addElement () {\\n // generate new div Element\\n var newDiv = $(document.createElement('div'));\\n // append at end of leaflet htmlwidget container\\n $(el).append(newDiv);\\n //provide ID and style\\n newDiv.addClass('lnlt');\\n newDiv.css({\\n 'position': 'relative',\\n 'bottomleft': '0px',\\n 'background-color': 'rgba(255, 255, 255, 0.7)',\\n 'box-shadow': '0 0 2px #bbb',\\n 'background-clip': 'padding-box',\\n 'margin': '0',\\n 'padding-left': '5px',\\n 'color': '#333',\\n 'font': '9px/1.5 \\\"Helvetica Neue\\\", Arial, Helvetica, sans-serif',\\n 'z-index': '700',\\n });\\n return newDiv;\\n }\\n\\n\\n // check for already existing lnlt class to not duplicate\\n var lnlt = $(el).find('.lnlt');\\n\\n if(!lnlt.length) {\\n lnlt = addElement();\\n\\n // grab the special div we generated in the beginning\\n // and put the mousmove output there\\n\\n map.on('mousemove', function (e) {\\n if (e.originalEvent.ctrlKey) {\\n if (document.querySelector('.lnlt') === null) lnlt = addElement();\\n lnlt.text(\\n ' lon: ' + (e.latlng.lng).toFixed(5) +\\n ' | lat: ' + (e.latlng.lat).toFixed(5) +\\n ' | zoom: ' + map.getZoom() +\\n ' | x: ' + L.CRS.EPSG3857.project(e.latlng).x.toFixed(0) +\\n ' | y: ' + L.CRS.EPSG3857.project(e.latlng).y.toFixed(0) +\\n ' | epsg: 3857 ' +\\n ' | proj4: +proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +no_defs ');\\n } else {\\n if (document.querySelector('.lnlt') === null) lnlt = addElement();\\n lnlt.text(\\n ' lon: ' + (e.latlng.lng).toFixed(5) +\\n ' | lat: ' + (e.latlng.lat).toFixed(5) +\\n ' | zoom: ' + map.getZoom() + ' ');\\n }\\n });\\n\\n // remove the lnlt div when mouse leaves map\\n map.on('mouseout', function (e) {\\n var strip = document.querySelector('.lnlt');\\n if( strip !==null) strip.remove();\\n });\\n\\n };\\n\\n //$(el).keypress(67, function(e) {\\n map.on('preclick', function(e) {\\n if (e.originalEvent.ctrlKey) {\\n if (document.querySelector('.lnlt') === null) lnlt = addElement();\\n lnlt.text(\\n ' lon: ' + (e.latlng.lng).toFixed(5) +\\n ' | lat: ' + (e.latlng.lat).toFixed(5) +\\n ' | zoom: ' + map.getZoom() + ' ');\\n var txt = document.querySelector('.lnlt').textContent;\\n console.log(txt);\\n //txt.innerText.focus();\\n //txt.select();\\n setClipboardText('\\\"' + txt + '\\\"');\\n }\\n });\\n\\n }\\n ).call(this.getMap(), el, x, data);\\n}\",\"data\":null},{\"code\":\"function(el, x, data) {\\n return (function(el,x,data){\\n var map = this;\\n\\n map.on('keypress', function(e) {\\n console.log(e.originalEvent.code);\\n var key = e.originalEvent.code;\\n if (key === 'KeyE') {\\n var bb = this.getBounds();\\n var txt = JSON.stringify(bb);\\n console.log(txt);\\n\\n setClipboardText('\\\\'' + txt + '\\\\'');\\n }\\n })\\n }).call(this.getMap(), el, x, data);\\n}\",\"data\":null}]}}\r\r2.4 Export multiple images\rThe {rgee} package has a very useful function for exporting an ImageCollection: ee_imagecollection_to_local(). Before using it, we need to set a region, the one that is intended to be exported. In this case, we export the entire globe with a rectangle covering the whole Earth.\n# earth extension\rgeom \u0026lt;- ee$Geometry$Polygon(coords = list(\rc(-180, -90), c(180, -90),\rc(180, 90),\rc(-180, 90),\rc(-180, -90)\r),\rproj = \u0026quot;EPSG:4326\u0026quot;,\rgeodesic = FALSE)\rgeom # EarthEngine object of type geometry\r## EarthEngine Object: Geometry\r# temporary download folder\rtmp \u0026lt;- tempdir()\r# run tasks and download each day\ric_drive_files_2 \u0026lt;- ee_imagecollection_to_local(\ric = daily$filter(ee$Filter$date(rdate_to_eedate(today()), rdate_to_eedate(today()+days(2)))), # we choose only the next 2 days\rregion = geom,\rscale = 20000,# resolution lazy = FALSE,\rdsn = path(tmp, \u0026quot;rast_\u0026quot;), # name of each raster\radd_metadata = TRUE\r)\r## ───────────────────────────────────── Downloading ImageCollection - via drive ──- region parameters\r## sfg : POLYGON ((-180 -90, 180 -90, 180 90, -180 90, -180 -90)) ## CRS : GEOGCRS[\u0026quot;WGS 84\u0026quot;,\r## DATUM[\u0026quot;World Geodetic System 1984\u0026quot;,\r## ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, ..... ## geodesic : FALSE ## evenOdd : TRUE ## ## Downloading: C:/Users/xeo19/AppData/Local/Temp/Rtmp8UpV6S/rast_0.tif\r## Downloading: C:/Users/xeo19/AppData/Local/Temp/Rtmp8UpV6S/rast_1.tif\r## ────────────────────────────────────────────────────────────────────────────────\r\r\r3 Part II. Orthographic map\r3.1 Data\rOf course, the first step is to import the data with the help of rast(). We also define the name of each layer according to its temporal dimension correctly.\n# paths to downloaded data\rforecast_world \u0026lt;- dir_ls(tmp, regexp = \u0026quot;tif\u0026quot;)\r# guarantee the file order file_ord \u0026lt;- str_extract(forecast_world, \u0026quot;_[0-9]{1,2}\u0026quot;) |\u0026gt; parse_number()\rforecast_rast \u0026lt;- rast(forecast_world[order(file_ord)]) # import\rforecast_rast\r## class : SpatRaster ## dimensions : 1002, 2004, 2 (nrow, ncol, nlyr)\r## resolution : 0.1796631, 0.1796631 (x, y)\r## extent : -180.0224, 180.0224, -90.01119, 90.01119 (xmin, xmax, ymin, ymax)\r## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## sources : rast_0.tif ## rast_1.tif ## names : temperature_2m_above_ground, temperature_2m_above_ground\r# define the temporal dimension as the name of each layer\rnames(forecast_rast) \u0026lt;- seq(today(), today() + days(1), \u0026quot;day\u0026quot;)\rforecast_rast\r## class : SpatRaster ## dimensions : 1002, 2004, 2 (nrow, ncol, nlyr)\r## resolution : 0.1796631, 0.1796631 (x, y)\r## extent : -180.0224, 180.0224, -90.01119, 90.01119 (xmin, xmax, ymin, ymax)\r## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## sources : rast_0.tif ## rast_1.tif ## names : 2023-02-23, 2023-02-24\r# plot\rplot(forecast_rast)\rNow we can define the orthographic projection indicating with +lat_0 and +lon_0 the center of the projection. We then reproject and convert the raster to a data.frame.\n# projection definition\rortho_crs \u0026lt;-\u0026#39;+proj=ortho +lat_0=51 +lon_0=0.5 +x_0=0 +y_0=0 +R=6371000 +units=m +no_defs +type=crs\u0026#39;\r# reproject the raster\rras_ortho \u0026lt;- project(forecast_rast, ortho_crs)\r# convert the raster to a data.frame of xyz\rforecast_df \u0026lt;- as.data.frame(ras_ortho, xy = TRUE)\r# transform to a long format\rforecast_df \u0026lt;- pivot_longer(forecast_df, 3:length(forecast_df), names_to = \u0026quot;date\u0026quot;, values_to = \u0026quot;ta\u0026quot;)\r\r3.2 Administrative boundaries and graticules\rWe import the administrative boundaries with gisco_get_countries() which we need prepare for the orthographic projection. In the same way we create the graticule using st_graticule(). In order to preserve the geometry, it will be necessary to cut to only the visible part. The ocean is created starting from a point at 0.0 with the radius of the earth. Using the st_intersection() function we reduce to the visible part and reproject the boundaries.\n# obtain the administrative limits\rworld_poly \u0026lt;- gisco_get_countries(year = \u0026quot;2016\u0026quot;, epsg = \u0026quot;4326\u0026quot;, resolution = \u0026quot;10\u0026quot;) # get the global graticule\rgrid \u0026lt;- st_graticule()\r# define what would be ocean\rocean \u0026lt;- st_point(x = c(0,0)) |\u0026gt;\rst_buffer(dist = 6371000) |\u0026gt; # earth radius\rst_sfc(crs = ortho_crs)\rplot(ocean)\r# select only visible from the boundaries and reproject\rworld \u0026lt;- world_poly |\u0026gt;\rst_intersection(st_transform(ocean, 4326)) |\u0026gt;\rst_transform(crs = ortho_crs) # plot(world)\rFor the graticules we must repeat the same selection, although we previously limit the grid of lines to the ocean. The ocean boundary is used to create the globe’s shadow, but to use it in geom_glowpath() you need to convert it to a data.frame.\n# eliminate the lines that pass over the continents\rgrid_crp \u0026lt;- st_difference(grid, st_union(world_poly))\r# select the visible part\rgrid_crp \u0026lt;- st_intersection(grid_crp, st_transform(ocean, 4326)) |\u0026gt;\rst_transform(crs = ortho_crs)\rplot(grid_crp)\r# convert the boundary of the globe into a data.frame\rocean_df \u0026lt;- st_cast(ocean, \u0026quot;LINESTRING\u0026quot;) |\u0026gt; st_coordinates() |\u0026gt; as.data.frame()\r\r3.3 Map construction\r3.3.1 Select tomorrow\rFirst we select the day of tomorrow, in my case when I write this post it is February 22, 2023. In addition, we limit the temperature range to -45ºC and +45ºC.\nforecast_tomorrow \u0026lt;- filter(forecast_df, date == today() + days(1)) |\u0026gt;\rmutate(ta_limit = case_when(ta \u0026gt; 45 ~ 45,\rta \u0026lt; -45 ~ -45,\rTRUE ~ ta))\r\r3.3.2 The shadow of the globe\rWe create the shadow effect using the geom_glowpath() function from the {ggshadow} package. Aiming for a more smooth transition I duplicate this layer with different transparency and shadow settings.\n# build a simple shadow\rggplot() + geom_glowpath(data = ocean_df, aes(X, Y, group = \u0026quot;L1\u0026quot;),\rshadowcolor=\u0026#39;grey90\u0026#39;,\rcolour = \u0026quot;white\u0026quot;,\ralpha = .01,\rshadowalpha=0.05,\rshadowsize = 1.5) +\rgeom_glowpath(data = ocean_df, aes(X, Y, group = \u0026quot;L1\u0026quot;),\rshadowcolor=\u0026#39;grey90\u0026#39;,\rcolour = \u0026quot;white\u0026quot;,\ralpha = .01,\rshadowalpha=0.01,\rshadowsize = 1) +\rcoord_sf() +\rtheme_void()\r# combining several layers of shadow\rg \u0026lt;- ggplot() +\rgeom_glowpath(data = ocean_df, aes(X, Y, group = \u0026quot;L1\u0026quot;),\rshadowcolor=\u0026#39;grey90\u0026#39;,\rcolour = \u0026quot;white\u0026quot;,\ralpha = .01,\rshadowalpha=0.05,\rshadowsize = 1.8) +\rgeom_glowpath(data = ocean_df, aes(X, Y, group = \u0026quot;L1\u0026quot;),\rshadowcolor=\u0026#39;grey90\u0026#39;,\rcolour = \u0026quot;white\u0026quot;,\ralpha = .01,\rshadowalpha=0.02,\rshadowsize = 1) +\rgeom_glowpath(data = ocean_df, aes(X, Y, group = \u0026quot;L1\u0026quot;),\rshadowcolor=\u0026#39;grey90\u0026#39;,\rcolour = \u0026quot;white\u0026quot;,\ralpha = .01,\rshadowalpha=0.01,\rshadowsize = .5) \r\r3.3.3 Adding other layers\rIn the next step we add the temperature layer and both vector layers.\ng2 \u0026lt;- g + geom_raster(data = forecast_tomorrow, aes(x, y, fill = ta_limit)) +\rgeom_sf(data = grid_crp, colour = \u0026quot;white\u0026quot;, linewidth = .2) +\rgeom_sf(data = world, fill = NA,\rcolour = \u0026quot;grey10\u0026quot;,\rlinewidth = .2) \rWhat we need to add are the last definitions of the color, the legend and the general style of the map.\ng2 + scale_fill_distiller(palette = \u0026quot;RdBu\u0026quot;, limits = c(-45, 45),\rbreaks = c(-45, -25, 0, 25, 45)) +\rguides(fill = guide_colourbar(barwidth = 15, barheight = .5, title.position = \u0026quot;top\u0026quot;,\rtitle.hjust = .5)) +\rcoord_sf() +\rlabs(fill = str_wrap(\u0026quot;Maximum temperature at 2 meters for February 14\u0026quot;, 35)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;,\rlegend.title = element_text(size = 7),\rplot.margin = margin(10, 10, 10, 10)) \rIf we wanted to add labels for the points with the lowest and highest temperatures, we would need to filter the extremes from our table.\nlabeling \u0026lt;- slice(forecast_tomorrow, which.min(ta), which.max(ta))\rlabeling\r## # A tibble: 2 × 5\r## x y date ta ta_limit\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 -1397101. 3923352. 2023-02-24 -42.5 -42.5\r## 2 2119554. -4121598. 2023-02-24 43.9 43.9\rThe geom_mark_circle() function allows you to include a circle label at any position. We create the label using str_glue() where the variable will be replaced by each temperature of both extremes, at the same time we can define the format of the number with number() from the {scales} package.\ng2 + geom_mark_circle(data = labeling, aes(x, y, description = str_glue(\u0026#39;{scales::number(ta, accuracy = .1, decimal.mark = \u0026quot;.\u0026quot;, style_positive = \u0026quot;plus\u0026quot;, suffix = \u0026quot;ºC\u0026quot;)}\u0026#39;)\r), expand = unit(1, \u0026quot;mm\u0026quot;), label.buffer = unit(4, \u0026quot;mm\u0026quot;),\rlabel.margin = margin(1, 1, 1, 1, \u0026quot;mm\u0026quot;),\rcon.size = 0.3,\rlabel.fontsize = 8,\rlabel.fontface = \u0026quot;bold\u0026quot;,\rcon.type = \u0026quot;straight\u0026quot;,\rlabel.fill = alpha(\u0026quot;white\u0026quot;, .5)) +\rscale_fill_distiller(palette = \u0026quot;RdBu\u0026quot;, limits = c(-45, 45),\rbreaks = c(-45, -25, 0, 25, 45)) +\rguides(fill = guide_colourbar(barwidth = 15, barheight = .5, title.position = \u0026quot;top\u0026quot;,\rtitle.hjust = .5)) +\rcoord_sf(crs = ortho_crs) +\rlabs(fill = str_wrap(\u0026quot;Maximum temperature at 2 meters for February 14\u0026quot;, 35)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;,\rlegend.title = element_text(size = 7),\rplot.margin = margin(10, 10, 10, 10)) \r\n\r\r\r","date":1676851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676851200,"objectID":"8cc17fe2fbd8fc3e97488679a8bb13f0","permalink":"https://dominicroye.github.io/en/2023/tomorrows-weather/","publishdate":"2023-02-20T00:00:00Z","relpermalink":"/en/2023/tomorrows-weather/","section":"post","summary":"A while back I saw Chris Campbell's global maps from the Financial Times like in this Tweet and I thought I needed to do it in R. In this first post of 2023 we'll see how we can access the GFS (Global Forecast System) data and visualize it with `{ggplot2}`, even though there are several ways, in this case we use the Google Earth Engine API via the `{rgee}` package for accessing the GFS data. We will select the most recent run and calculate the maximum temperature for the next few days.","tags":["raster","gfs","forecast","globe"],"title":"Tomorrow's weather","type":"post"},{"authors":["Barrak Alahmad","Haitham Khraishah","D Royé","et al"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"ae11c433986d09ad365847c3ed0f9a58","permalink":"https://dominicroye.github.io/en/publication/2023-subcardiovascular-mortality-circulation/","publishdate":"2022-12-12T00:00:00Z","relpermalink":"/en/publication/2023-subcardiovascular-mortality-circulation/","section":"publication","summary":"Background: Cardiovascular disease is the leading cause of death worldwide. Existing studies on the association between temperatures and cardiovascular deaths have been limited in geographic zones and have generally considered associations with total cardiovascular deaths rather than cause-speciﬁc cardiovascular deaths. Methods: We used uniﬁed data collection protocols within the Multi-Country Multi-City Collaborative Network to assemble a database of daily counts of speciﬁc cardiovascular causes of death from 567 cities in 27 countries across 5 continents in overlapping periods ranging from 1979 to 2019. City-speciﬁc daily ambient temperatures were obtained from weather stations and climate reanalysis models. To investigate cardiovascular mortality associations with extreme hot and cold temperatures, we ﬁt case-crossover models in each city and then used a mixed-effects meta-analytic framework to pool individual city estimates. Extreme temperature percentiles were compared with the minimum mortality temperature in each location. Excess deaths were calculated for a range of extreme temperature days. Results: The analyses included deaths from any cardiovascular cause (32 154 935), ischemic heart disease (11 745 880), stroke (9 351 312), heart failure (3 673 723), and arrhythmia (670 859). At extreme temperature percentiles, heat (99th percentile) and cold (1st percentile) were associated with higher risk of dying from any cardiovascular cause, ischemic heart disease, stroke, and heart failure as compared to the minimum mortality temperature, which is the temperature associated with least mortality. Across a range of extreme temperatures, hot days (above 97.5th percentile) and cold days (below 2.5th percentile) accounted for 2.2 (95% empirical CI [eCI], 2.1–2.3) and 9.1 (95% eCI, 8.9–9.2) excess deaths for every 1000 cardiovascular deaths, respectively. Heart failure was associated with the highest excess deaths proportion from extreme hot and cold days with 2.6 (95% eCI, 2.4–2.8) and 12.8 (95% eCI, 12.2–13.1) for every 1000 heart failure deaths, respectively. Conclusions: Across a large, multinational sample, exposure to extreme hot and cold temperatures was associated with a greater risk of mortality from multiple common cardiovascular conditions. The intersections between extreme temperatures and cardiovascular health need to be thoroughly characterized in the present day—and especially under a changing climate.","tags":["extreme temperatures","cardiovascular diseases","modification effect","stroke","mortality"],"title":"Associations Between Extreme Temperatures and Cardiovascular Cause-Specific Mortality: Results From 27 Countries","type":"publication"},{"authors":["A Tobias","D Royé","C Iñiguez"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"f3ccd44e6625eb67fee61daf522e2cb6","permalink":"https://dominicroye.github.io/en/publication/2023-mortality-estimation-spain-summer-epidemiology/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/en/publication/2023-mortality-estimation-spain-summer-epidemiology/","section":"publication","summary":"In July 2022, Spain experienced an excessive heat episode during which many cities broke temperature records. According to the Spanish Meteorological Agency, it has been the warmest month in Spain ever, at least since 1961. We estimated the mortality attributable to heat using the observed daily mortality by the Spanish daily mortality surveillance system (MoMo) and the nationwide mean temperature averaged from the 52 capital cities by calculating the temperature–mortality association during the summer months (June to August) between 1999-2018. The cumulative deaths estimated to be attributable to moderate heat were 1,798 in June, 2,588 in July, and 2,352 in August. Excess high temperatures and heatwaves are relevant events due to their role in heat related-mortality, social vulnerability, and economic impact in the agriculture and energy sectors. Moreover, one in three current heat-related deaths is already known to be related to human-induced climate change. Our findings shed light on the increasing burden of heat-related mortality among vulnerable populations.","tags":["extreme temperatures","summer","heatwave","mortality"],"title":"Heat-attributable mortality in the summer of 2022 in Spain","type":"publication"},{"authors":["A Poso-Díaz","N Lorenzo","D Royé"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"7ad10c6ce8f454e30f164df7db38e5d0","permalink":"https://dominicroye.github.io/en/publication/2023-ehf-iberian-peninsula-tendencias-environmental-research/","publishdate":"2022-11-25T00:00:00Z","relpermalink":"/en/publication/2023-ehf-iberian-peninsula-tendencias-environmental-research/","section":"publication","summary":"In the current climate change scenario, heat waves have become one of the most concerning extreme climatic events, both because of their implications for human health and the economy, and because of their increase in intensity and frequency in recent decades. This work presents for the first time a climatological analysis of heat waves in the Iberian Peninsula and Balearic Archipelago (IPB) using the Excess Heat Factor index (EHF). This index considers the factor of intensity and the acclimatization process of human body in the study of heat waves. We focused on the intensity (also called severity), duration, frequency and spatial extension of heat waves in the IPB in the 1950–2020 period. The exceptional heat wave of August 2018 was approached in a similar way to further explore the usefulness of the EHF index. We found that the EHF index identified heat wave conditions 2 days earlier than indices that used only maximum temperatures. Results showed a significant increase in intensity, duration, frequency and spatial extension of heat waves for the whole IPB for 1950–2020 period. The average extent of heat waves increased by 4.0% per decade and the maximum extent by 4.1% per decade. This trend suggested a significant increase in human exposure, droughts, fire risk and energy demand in this region in the last decades.","tags":["intensity","spatial extent","heat wave","Iberian Peninsula","trend"],"title":"Spatio-temporal evolution of heat waves severity and expansion across the Iberian Peninsula and Balearic islands","type":"publication"},{"authors":["L Nottmeyer","B Armstrong","R Lowe","et al"],"categories":null,"content":"","date":1662508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662508800,"objectID":"8e945f1ae3dfa57e3795327b7379a321","permalink":"https://dominicroye.github.io/en/publication/2022-covid19-meteo-global-multicity-analysis-stoten/","publishdate":"2022-09-07T00:00:00Z","relpermalink":"/en/publication/2022-covid19-meteo-global-multicity-analysis-stoten/","section":"publication","summary":"Background and aim: The associations between COVID-19 transmission and meteorological factors are scientifically debated. Several studies have been conducted worldwide, with inconsistent findings. However, often these studies had methodological issues, e.g., did not exclude important confounding factors, or had limited geographic or temporal resolution. Our aim was to quantify associations between temporal variations in COVID-19 incidence and meteorological variables globally. Methods: We analysed data from 455 cities across 20 countries from 3 February to 31 October 2020. We used a time-series analysis that assumes a quasi-Poisson distribution of the cases and incorporates distributed lag non-linear modelling for the exposure associations at the city-level while considering effects of autocorrelation, long-term trends, and day of the week. The confounding by governmental measures was accounted for by incorporating the Oxford Governmental Stringency Index. The effects of daily mean air temperature, relative and absolute humidity, and UV radiation were estimated by applying a meta-regression of local estimates with multi-level random effects for location, country, and climatic zone. Results: We found that air temperature and absolute humidity influenced the spread of COVID-19 over a lag period of 15 days. Pooling the estimates globally showed that overall low temperatures (7.5 °C compared to 17.0 °C) and low absolute humidity (6.0 g/m³ compared to 11.0 g/m³) were associated with higher COVID-19 incidence (RR temp =1.33 with 95%CI: 1.08; 1.64 and RR AH =1.33 with 95%CI: 1.12; 1.57). RH revealed no significant trend and for UV some evidence of a positive association was found. These results were robust to sensitivity analysis. However, the study results also emphasise the heterogeneity of these associations in different countries. Conclusion: Globally, our results suggest that comparatively low temperatures and low absolute humidity were associated with increased risks of COVID-19 incidence. However, this study underlines regional heterogeneity of weather-related effects on COVID-19 transmission.","tags":["Temperature","Humidity","UV radiation","COVID-19","Distributed lag non-linear modelling","Global analysis"],"title":"The association of COVID-19 incidence with temperature, humidity, and UV radiation – A global multi-city analysis","type":"publication"},{"authors":["HM Choi","W Lee","D Royé","et al"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"adb5c6b08a74b35c686a8cf73342e39f","permalink":"https://dominicroye.github.io/en/publication/2022-greeness-mortality-heat-ebiomedicine/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/en/publication/2022-greeness-mortality-heat-ebiomedicine/","section":"publication","summary":"Background: Identifying how greenspace impacts the temperature-mortality relationship in urban environments is crucial, especially given climate change and rapid urbanization. However, the effect modification of greenspace on heat-related mortality has been typically focused on a localized area or single country. This study examined the heat-mortality relationship among different greenspace levels in a global setting. Methods: We collected daily ambient temperature and mortality data for 452 locations in 24 countries and used Enhanced Vegetation Index (EVI) as the greenspace measurement. We used distributed lag non-linear model to estimate the heat-mortality relationship in each city and the estimates were pooled adjusting for city-specific average temperature, city-specific temperature range, city-specific population density, and gross domestic product (GDP). The effect modification of greenspace was evaluated by comparing the heat-related mortality risk for different greenspace groups (low, medium, and high), which were divided into terciles among 452 locations. Findings: Cities with high greenspace value had the lowest heat-mortality relative risk of 1·19 (95% CI: 1·13, 1·25), while the heat-related relative risk was 1·46 (95% CI: 1·31, 1·62) for cities with low greenspace when comparing the 99th temperature and the minimum mortality temperature. A 20% increase of greenspace is associated with a 9·02% (95% CI: 8·88, 9·16) decrease in the heat-related attributable fraction, and if this association is causal (which is not within the scope of this study to assess), such a reduction could save approximately 933 excess deaths per year in 24 countries.","tags":["greenspace","heat","mortality","effect modification"],"title":"Effect modification of greenness on the association between heat and mortality: A multi-city multi-country study","type":"publication"},{"authors":null,"categories":["gis","R","R:intermediate","visualization"],"content":"\rIt is very common to see relief maps with shadow effects, also known as ‘hillshade’, which generates visual depth. How can we create these effects in R and how to include them in ggplot2?\nPackages\r\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\relevatr\rAccess to elevation data from various APIs\r\rterra\rImport, export and manipulate raster ({raster} successor package)\r\rwhitebox\rAn R interface to the ‘WhiteboxTools’ library, which is an advanced geospatial data analysis platform\r\rtidyterra\rHelper functions for working with {terra}\r\rgiscoR\rAdministrative boundaries of the world\r\rggnewscale\rExtension for ggplot2 of multiple ‘scales’\r\rggblend\rExtension for mixing colors in ggplot graphs\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;elevatr\u0026quot;)) install.packages(\u0026quot;elevatr\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;whitebox\u0026quot;)) install.packages(\u0026quot;whitebox\u0026quot;)\rif(!require(\u0026quot;tidyterra\u0026quot;)) install.packages(\u0026quot;tidyterra\u0026quot;)\rif(!require(\u0026quot;giscoR\u0026quot;)) install.packages(\u0026quot;giscoR\u0026quot;)\rif(!require(\u0026quot;ggnewscale\u0026quot;)) install.packages(\u0026quot;ggnewscale\u0026quot;)\rif(!require(\u0026quot;ggblend\u0026quot;)) install.packages(\u0026quot;ggblend\u0026quot;)\r# packages\rlibrary(sf)\rlibrary(elevatr)\rlibrary(tidyverse)\rlibrary(terra)\rlibrary(whitebox)\rlibrary(ggnewscale)\rlibrary(tidyterra)\rlibrary(giscoR)\rlibrary(units)\rlibrary(ggblend)\r\rData\rAs an area of interest, we use Switzerland in this example. Except for lake boundaries download, the necessary data is obtained through APIs using different packages. For example, the giscoR package allows you to get country boundaries with different resolutions.\nsuiz \u0026lt;- gisco_get_countries(country = \u0026quot;Switzerland\u0026quot;, resolution = \u0026quot;03\u0026quot;)\rplot(suiz)\rThe lake boundaries correspond to a layer of digital cartographic models (DKM500) provided by swisstopo. The objective is to keep only the largest lakes; therefore, we exclude all those with less than 50 km2 and also those located entirely in Italian territory. Remember that with the units package, we can indicate units and thus do calculations.\n# import the lakes boundaries\rsuiz_lakes \u0026lt;- st_read(\u0026quot;22_DKM500_GEWAESSER_PLY.shp\u0026quot;)\r## Reading layer `22_DKM500_GEWAESSER_PLY\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\en\\post\\2022-07-19-hillshade-effect\\22_DKM500_GEWAESSER_PLY.shp\u0026#39; ## using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 596 features and 14 fields\r## Geometry type: POLYGON\r## Dimension: XY\r## Bounding box: xmin: 2480000 ymin: 1062000 xmax: 2865000 ymax: 1302000\r## Projected CRS: CH1903+ / LV95\r# filter the largest ones\rsuiz_lakes \u0026lt;- mutate(suiz_lakes, areakm = set_units(SHP_AREA, \u0026quot;m2\u0026quot;) %\u0026gt;% set_units(\u0026quot;km2\u0026quot;)) %\u0026gt;% filter(areakm \u0026gt; set_units(50, \u0026quot;km2\u0026quot;),\r!NAMN1 %in% c(\u0026quot;Lago di Como / Lario\u0026quot;,\r\u0026quot;Lago d\u0026#39;Iseo\u0026quot;,\r\u0026quot;Lago di Garda\u0026quot;))\rplot(suiz_lakes)\r## Warning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\r## all\r\rDigital Elevation Model (DEM)\rThe get_elev_raster() function allows us to download a DEM from any region of the world through different providers in raster format. By default, it uses AWS. An essential argument is the latitude-dependent resolution, which can be specified as the zoom level (see function help). For example, we use level 10, which at a latitude of 45º would correspond to approximately 100 m.\nAfter obtaining the DEM from Switzerland, we must mask the country’s boundaries. The object’s class is RasterLayer from the raster package, however, the new standard is terra with the class SpatRaster. That’s why we convert it and then apply the mask. Finally, we reproject to the Swiss coordinate system obtained from the vector data.\n# get the DEM with\rmdt \u0026lt;- get_elev_raster(suiz, z = 10)\r## Please note that rgdal will be retired during October 2023,\r## plan transition to sf/stars/terra functions using GDAL and PROJ\r## at your earliest convenience.\r## See https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\r## rgdal: version: 1.6-7, (SVN revision 1203)\r## Geospatial Data Abstraction Library extensions to R successfully loaded\r## Loaded GDAL runtime: GDAL 3.6.2, released 2023/01/02\r## Path to GDAL shared files: C:/Users/xeo19/AppData/Local/R/win-library/4.3/rgdal/gdal\r## GDAL does not use iconv for recoding strings.\r## GDAL binary built with GEOS: TRUE ## Loaded PROJ runtime: Rel. 9.2.0, March 1st, 2023, [PJ_VERSION: 920]\r## Path to PROJ shared files: C:/Users/xeo19/AppData/Local/R/win-library/4.3/rgdal/proj\r## PROJ CDN enabled: FALSE\r## Linking to sp version:1.6-1\r## To mute warnings of possible GDAL/OSR exportToProj4() degradation,\r## use options(\u0026quot;rgdal_show_exportToProj4_warnings\u0026quot;=\u0026quot;none\u0026quot;) before loading sp or rgdal.\r## Mosaicing \u0026amp; Projecting\r## Note: Elevation units are in meters.\rmdt # old RasterLayer class\r## class : RasterLayer ## dimensions : 3869, 7913, 30615397 (nrow, ncol, ncell)\r## resolution : 0.0006219649, 0.0006219649 (x, y)\r## extent : 5.625, 10.54661, 45.58354, 47.98992 (xmin, xmax, ymin, ymax)\r## crs : +proj=longlat +datum=WGS84 +no_defs ## source : file5fe02e084cd2.tif ## names : file5fe02e084cd2\rplot(mdt)\r# convert to terra and mask area of interest\rmdt \u0026lt;- rast(mdt) %\u0026gt;% mask(vect(suiz)) # reproject\rmdt \u0026lt;- project(mdt, crs(suiz_lakes))\r# reproject vect\rsuiz \u0026lt;- st_transform(suiz, st_crs(suiz_lakes))\rBefore calculating the shadow effect, we create a simple relief map. In ggplot2, we use the geom_raster() geometry, indicating the longitude, latitude and the variable to define the color. We add the boundaries of the lakes using geom_sf() since it is an sf object. Here we only indicate the fill color with a light blue. Then, with the help of scale_fill_hypso_tint_c(), we apply a range of colors corresponding to the relief, also called hypsometric tinting, and we define the breaks in the legend. We make appearance adjustments in the legend and the graph’s style in the rest of the functions.\n# convert the raster into a data.frame of xyz\rmdtdf \u0026lt;- as.data.frame(mdt, xy = TRUE)\rnames(mdtdf)[3] \u0026lt;- \u0026quot;alt\u0026quot;\r# map\rggplot() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt)) +\rgeom_sf(data = suiz_lakes,\rfill = \u0026quot;#c6dbef\u0026quot;, colour = NA) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\r\rCalculate the hillshade\rLet’s remember that the hillshade effect is nothing more than adding a hypothetical illumination with respect to a position of a light source to gain depth. Shadows depend on two variables, azimuth, the angle from the orientation on the surface of a sphere, and elevation, the angle from the height of the source.\nThe information required to simulate lighting is the digital elevation model. The slope and aspect can be derived from the DEM using the terrain() function from the terra package. The unit must be radians. Once we have all the data, we can use the shade() function to indicate the angle (elevation) and direction (azimuth). The result is a raster with values between 0 and 255, which shows shadows with low values, being 0 black and 255 white.\n# estimate the slope\rsl \u0026lt;- terrain(mdt, \u0026quot;slope\u0026quot;, unit = \u0026quot;radians\u0026quot;)\rplot(sl)\r# estimate the aspect or orientation\rasp \u0026lt;- terrain(mdt, \u0026quot;aspect\u0026quot;, unit = \u0026quot;radians\u0026quot;)\rplot(asp)\r# calculate the hillshade effect with 45º of elevation\rhill_single \u0026lt;- shade(sl, asp, angle = 45, direction = 300,\rnormalize= TRUE)\r# final hillshade plot(hill_single, col = grey(1:100/100))\r\rCombine the relief and shadow effect\rThe problem with adding both the relief with its hypsometric tints and the hillshade effect inside ggplot2 is that we have two different fills or scales for each layer.\rThe solution is to use the ggnewscale extension, which allows you to add multiple scales of the same argument. First, we add the hillshade with geom_raster(), then we define the grey tones, and before adding the altitude, we include the new_scale_fill() function to mark a different fill. To achieve the effect, it is necessary to give a degree of transparency to the relief layer; in this case, it is 70%. The choice of direction is important, which is why we must always take into account the place and the apparent path of the sun (sunearthtools).\n# convert the hillshade to xyz\rhilldf_single \u0026lt;- as.data.frame(hill_single, xy = TRUE)\r# map ggplot() +\rgeom_raster(data = hilldf_single,\raes(x, y, fill = hillshade),\rshow.legend = FALSE) +\rscale_fill_distiller(palette = \u0026quot;Greys\u0026quot;) +\rnew_scale_fill() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt),\ralpha = .7) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rgeom_sf(data = suiz_lakes,\rfill = \u0026quot;#c6dbef\u0026quot;, colour = NA) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\r\rMultidirectional shadows\rWe have seen a unidirectional effect; although it is the most common, we can create a smoother and even more realistic effect by combining several directions.\nWe map onto a vector of various directions to which the shade() function is applied with a fixed elevation angle. We then convert the raster list to a multi-layered object to reduce them by adding all the layers.\n# pass multiple directions to shade()\rhillmulti \u0026lt;- map(c(270, 15, 60, 330), function(dir){ shade(sl, asp, angle = 45, direction = dir,\rnormalize= TRUE)}\r)\r## |---------|---------|---------|---------|\r=========================================\r\r# create a multidimensional raster and reduce it by summing up\rhillmulti \u0026lt;- rast(hillmulti) %\u0026gt;% sum()\r## |---------|---------|---------|---------|\r=========================================\r\r# multidirectional\rplot(hillmulti, col = grey(1:100/100))\r# unidirectional\rplot(hill_single, col = grey(1:100/100))\rWe do the same as before to visualize the relief with multidirectional shadows.\n# convert the hillshade to xyz\rhillmultidf \u0026lt;- as.data.frame(hillmulti, xy = TRUE)\r# map\rggplot() +\rgeom_raster(data = hillmultidf,\raes(x, y, fill = sum),\rshow.legend = FALSE) +\rscale_fill_distiller(palette = \u0026quot;Greys\u0026quot;) +\rnew_scale_fill() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt),\ralpha = .7) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rgeom_sf(data = suiz_lakes,\rfill = \u0026quot;#c6dbef\u0026quot;, colour = NA) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;)\rThe color blending technique is very useful to obtain remarkable results in the shading effect. Recently the ggblend package offers this possibility. In order to blend several layers, it is necessary to insert the geom_raster() and the scale_fill_*() objects in a comma-separated list. Then follows the pipe with the blend(\"mix_type\") function to which we add the other ggplot2 objects. In this case we apply multiplication as a form of blending.\n# map\rm \u0026lt;- ggplot() +\rlist(\rgeom_raster(data = hillmultidf,\raes(x, y, fill = sum),\rshow.legend = FALSE),\rscale_fill_distiller(palette = \u0026quot;Greys\u0026quot;),\rnew_scale_fill(),\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt),\ralpha = .7),\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000))\r) %\u0026gt;% blend(\u0026quot;multiply\u0026quot;) +\rgeom_sf(data = suiz_lakes,\rfill = \u0026quot;#c6dbef\u0026quot;, colour = NA) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;)\rggsave(\u0026quot;mdt_hillshade_blend.png\u0026quot;, m, width = 10, height = 8, unit = \u0026quot;in\u0026quot;,\rdevice = png, type = \u0026quot;cairo\u0026quot;,\rbg = \u0026quot;white\u0026quot;)\r\rAnother alternative for multidirectional shadows\rWith less control over the directions, it would also be possible to apply the wbt_multidirectional_hillshade() function from the whitebox package. WhiteboxTool contains many tools as an advanced geospatial data analysis platform. The disadvantage is that we lose control over the directions and that it is also necessary to export the DEM to geotiff to obtain another raster with the shadows.\nWe first install the library with the install_whitebox() function.\n# instal whitebox\rinstall_whitebox()\r# export the DEM\rwriteRaster(mdt, \u0026quot;mdt.tiff\u0026quot;, overwrite = TRUE)\r# launch whitebox\rwbt_init()\r# create the hillshade\rwbt_multidirectional_hillshade(\u0026quot;mdt.tiff\u0026quot;,\r\u0026quot;hillshade.tiff\u0026quot;)\r# re-import the hillshade\rhillwb \u0026lt;- rast(\u0026quot;hillshade.tiff\u0026quot;)\rplot(hillwb)\r# remask hillwb \u0026lt;- mask(hillwb, vect(suiz))\r## Warning: [mask] CRS do not match\rplot(hillwb)\r# convert the hillshade to xyz\rhillwbdf \u0026lt;- as.data.frame(hillwb, xy = TRUE)\r# map\rggplot() +\rgeom_raster(data = hillwbdf,\raes(x, y, fill = hillshade),\rshow.legend = FALSE) +\rscale_fill_distiller(palette = \u0026quot;Greys\u0026quot;) +\rnew_scale_fill() +\rgeom_raster(data = mdtdf,\raes(x, y, fill = alt),\ralpha = .7) +\rscale_fill_hypso_tint_c(breaks = c(180, 250, 500, 1000,\r1500, 2000, 2500,\r3000, 3500, 4000)) +\rguides(fill = guide_colorsteps(barwidth = 20,\rbarheight = .5,\rtitle.position = \u0026quot;right\u0026quot;)) +\rlabs(fill = \u0026quot;m\u0026quot;) +\rcoord_sf() +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;)\r\n\r","date":1658275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658275200,"objectID":"817ca9851e347d11d5e83112158f367c","permalink":"https://dominicroye.github.io/en/2022/hillshade-effects/","publishdate":"2022-07-20T00:00:00Z","relpermalink":"/en/2022/hillshade-effects/","section":"post","summary":"It is very common to see relief maps with shadow effects, also known as 'hillshade', which generates visual depth. How can we create these effects in R and how to include them in ggplot2?","tags":["raster","hillshade","DEM","shadow","elevation"],"title":"Hillshade effects","type":"post"},{"authors":["Yao Wu","Shanshan Li","et al"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"eec262e7bde3c91585c338b442a56200","permalink":"https://dominicroye.github.io/en/publication/2022-temperature-variability-lancet-planetary-health/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/en/publication/2022-temperature-variability-lancet-planetary-health/","section":"publication","summary":"Background Increased mortality risk is associated with short-term temperature variability. However, to our knowledge, there has been no comprehensive assessment of the temperature variability-related mortality burden worldwide. In this study, using data from the MCC Collaborative Research Network, we first explored the association between temperature variability and mortality across 43 countries or regions. Then, to provide a more comprehensive picture of the global burden of mortality associated with temperature variability, global gridded temperature data with a resolution of 0·5° × 0·5° were used to assess the temperature variability-related mortality burden at the global, regional, and national levels. Furthermore, temporal trends in temperature variability-related mortality burden were also explored from 2000–19. Methods In this modelling study, we applied a three-stage meta-analytical approach to assess the global temperature variability-related mortality burden at a spatial resolution of 0·5° × 0·5° from 2000–19. Temperature variability was calculated as the SD of the average of the same and previous days’ minimum and maximum temperatures. We first obtained location-specific temperature variability related-mortality associations based on a daily time series of 750 locations from the Multi-country Multi-city Collaborative Research Network. We subsequently constructed a multivariable meta-regression model with five predictors to estimate grid-specific temperature variability related-mortality associations across the globe. Finally, percentage excess in mortality and excess mortality rate were calculated to quantify the temperature variability-related mortality burden and to further explore its temporal trend over two decades. Findings An increasing trend in temperature variability was identified at the global level from 2000 to 2019. Globally, 1 753 392 deaths (95% CI 1 159 901–2 357 718) were associated with temperature variability per year, accounting for 3·4% (2·2–4·6) of all deaths. Most of Asia, Australia, and New Zealand were observed to have a higher percentage excess in mortality than the global mean. Globally, the percentage excess in mortality increased by about 4·6% (3·7–5·3) per decade. The largest increase occurred in Australia and New Zealand (7·3%, 95% CI 4·3–10·4), followed by Europe (4·4%, 2·2–5·6) and Africa (3·3, 1·9–4·6). Interpretation Globally, a substantial mortality burden was associated with temperature variability, showing geographical heterogeneity and a slightly increasing temporal trend. Our findings could assist in raising public awareness and improving the understanding of the health impacts of temperature variability.","tags":["temperature variability","heat","modification effect","global, regional, national","burden"],"title":"Global, regional, and national burden of mortality associated with short-term temperature variability from 2000-19: a three-stage modelling study","type":"publication"},{"authors":["Fantina Tedim","Vittorio Leone","et al"],"categories":null,"content":"","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"6c16604b09cb7b54b26e562e0406fc92","permalink":"https://dominicroye.github.io/en/publication/2022-forest-fire-causes-forests/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/en/publication/2022-forest-fire-causes-forests/","section":"publication","summary":"Forest fires causes and motivations are poorly understood in southern and south-eastern Europe. This research aims to identify how experts perceive the different causes of forest fires as defined in the classification proposed by the European Commission in 2013. A panel of experts (N = 271) was gathered from the EU Southern Member States (France, Greece, Italy, Portugal, and Spain) and from Central (Switzerland) and south-eastern Europe (Croatia, Serbia, Bosnia and Herzegovina, Republic of North Macedonia, and Turkey). Experts were asked to answer a questionnaire to score the importance of the 29 fire causes using a five point (1–5) Likert Scale. Agricultural burnings received the highest score, followed by Deliberate fire for profit, and Vegetation management. Most of the events stem from negligence, whereas malicious fire setting is arguably overestimated although there are differences among the countries. This research demonstrates the importance of different techniques to enhance the knowledge of the causes of the complex anthropogenic phenomenon of forest fire occurrence.","tags":["wildfires","forest fires","causes","motivations","Europe","perception","experts"],"title":"Forest Fire Causes and Motivations in Southern and South-Eastern Europe through the Perception of Experts Contribution to Enhance the Current Policies","type":"publication"},{"authors":["Malcolm Mistry","Rochelle Schneider","Pierre Masselot","Dominic Royé","et al."],"categories":null,"content":"","date":1646870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646870400,"objectID":"1a2e9f34223fb351a08a69b76fa38d40","permalink":"https://dominicroye.github.io/en/publication/2022-era5land-global-scientific-reports/","publishdate":"2022-03-10T00:00:00Z","relpermalink":"/en/publication/2022-era5land-global-scientific-reports/","section":"publication","summary":"Epidemiological analyses of health risks associated with non-optimal temperature are traditionally based on ground observations from weather stations that offer limited spatial and temporal coverage. Climate reanalysis represents an alternative option that provide complete spatio-temporal exposure coverage, and yet are to be systematically explored for their suitability in assessing temperature-related health risks at a global scale. Here we provide the first comprehensive analysis over multiple regions to assess the suitability of the most recent generation of reanalysis datasets for health impact assessments and evaluate their comparative performance against traditional station-based data. Our findings show that reanalysis temperature from the last ERA5 products generally compare well to station observations, with similar non-optimal temperature-related risk estimates. However, the analysis offers some indication of lower performance in tropical regions, with a likely underestimation of heat-related excess mortality. Reanalysis data represent a valid alternative source of exposure variables in epidemiological analyses of temperature-related risk.","tags":["temperature","ERA5 Land Reanalysis","mortality"],"title":"Comparison of weather station and climate reanalysis data for modelling temperature-related mortality","type":"publication"},{"authors":null,"categories":["gis","R","R:advanzed","visualization"],"content":"\r\rInitial considerations\rSpace-time information is vital in many disciplines, especially in climatology or meteorology, and this makes it necessary to have a format that allows a multidimensional structure. It is also important that this format has a high degree of interchange compatibility and can store a large number of data. These characteristics led to the development of the open standard netCDF (NetworkCommon Data Form). The netCDF format is an open multi-dimensional scientific data exchange standard used with observational or model data, primarily in disciplines such as climatology, meteorology, and oceanography. The netCDF convention is managed by Unidata (https://www.unidata.ucar.edu/software/netcdf/). It is a space-time format with a regular or irregular grid. The multidimensional structure in the form of an array allows the use of space-time and multivariable data. The general characteristics of netCDF refer to the use of an n-dimensional coordinate system, multiple variables, and a regular or irregular grid. In addition, metadata describing the contents are included. The extension of the netCDF format is “nc”.\nI recently used drought data from Spain in netCDF format with a resolution of 1 km to represent the state of drought for each year since 1960 (https://monitordesequia.csic.es/historico/). The SPEI index (Standardized Precipitation-Evapotranspiration Index) is widely used to describe the drought with different time intervals (3, 6, 12 months, etc.).\n{{\u0026lt; tweet 1490260694851362821 \u0026gt;}}\nI have been asked on several occasions about handling the netCDF format. For this reason, in this post, we will use a subset of these same data, the year 2017 of the SPEI 12 months.\n\rPackages\rData handling in netCDF format is possible through various packages directly or indirectly. The specifically designed {ncdf4} package stands out, which is also used by other packages, although we don’t see it. Handling with {ncdf4} is somewhat complex, mainly because of the need to manage RAM when dealing with large datasets or also because of the way to handle the array class. Another very powerful package is {terra}, which we know when working with raster data and allows us to use its functions also for handling the netCDF format.\n\r\r\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rlubridate\rEasy manipulation of dates and times\r\rterra\rImport, export and manipulate raster (raster successor package)\r\rmapSpain\rSpanish administrative limits\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;mapSpain\u0026quot;)) install.packages(\u0026quot;mapSpain\u0026quot;)\r# load packages\rlibrary(tidyverse)\rlibrary(sf)\rlibrary(terra)\rlibrary(lubridate)\rlibrary(mapSpain)\rFor those less experienced with tidyverse, I recommend the brief introduction on this blog post.\n\rData\rFirst, we download the data here. Then, we import the SPEI-12 index data for 2017 using the rast() function. Actually, in this step, we have only created a reference to the file without importing all the data into memory. We see in the metadata the number of layers available. The SPEI-12 index is calculated weekly with four weeks per month. If we look at the metadata, the definition of the coordinate system is missing, so we define it by assigning the code EPSG:25830 (ETRS89/UTM 30N).\n# import\rspei \u0026lt;- rast(\u0026quot;spei12_2017.nc\u0026quot;)\r# metadata\rspei\r## class : SpatRaster ## dimensions : 834, 1115, 48 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ## source : spei12_2017.nc ## names : spei1~017_1, spei1~017_2, spei1~017_3, spei1~017_4, spei1~017_5, spei1~017_6, ... ## time : 2017-01-01 to 2017-12-23\r# define the coordinate system\rcrs(spei) \u0026lt;- \u0026quot;EPSG:25830\u0026quot;\r# map first weeks\rplot(spei)\r\rExtract metadata\rThere are different functions to access metadata, such as dates, layer names or variable names. Remember that netCDF files can also contain several variables.\n# time\rt \u0026lt;- time(spei)\rhead(t)\r## [1] \u0026quot;2017-01-01 UTC\u0026quot; \u0026quot;2017-01-09 UTC\u0026quot; \u0026quot;2017-01-16 UTC\u0026quot; \u0026quot;2017-01-23 UTC\u0026quot;\r## [5] \u0026quot;2017-02-01 UTC\u0026quot; \u0026quot;2017-02-09 UTC\u0026quot;\r# layer names\rnames(spei) %\u0026gt;% head()\r## [1] \u0026quot;spei12_2017_1\u0026quot; \u0026quot;spei12_2017_2\u0026quot; \u0026quot;spei12_2017_3\u0026quot; \u0026quot;spei12_2017_4\u0026quot;\r## [5] \u0026quot;spei12_2017_5\u0026quot; \u0026quot;spei12_2017_6\u0026quot;\r# variable names\rvarnames(spei)\r## [1] \u0026quot;spei12_2017\u0026quot;\r\rTime-series extraction\rOne possibility that netCDF data allows is time-series extraction, either from points or areas. For example, we will create here the SPEI-12 time series for the city of Zaragoza and the average for the entire autonomous community of Aragon.\n# Zaragoza coordinates\rzar \u0026lt;- st_point(c(-0.883333, 41.65)) %\u0026gt;% st_sfc(crs = 4326) %\u0026gt;% st_as_sf() %\u0026gt;% st_transform(25830)\rThe {terra} package only accepts its vector class SpatVector, so it is necessary to convert the point of class sf with the vect() function. To extract the time series we use the extract() function. The extracted data is given back in the form of a table, each row is an element of the vector data, and each column is a layer. In our case, it is only a single row corresponding to the city of Zaragoza.\n Some functions may conflict with the names of other packages; to avoid this, we can write the package’s name in front of the function we want to use, separated by the colon symbol written twice (package_name::function_name).   # extract time series\rspei_zar \u0026lt;- terra::extract(spei, vect(zar))\r# dimensions\rdim(spei_zar)\r## [1] 1 49\r# create a data.frame\rspei_zar \u0026lt;- tibble(date = t, zar = unlist(spei_zar)[-1])\rhead(spei_zar)\r## # A tibble: 6 x 2\r## date zar\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2017-01-01 00:00:00 0.280\r## 2 2017-01-09 00:00:00 0.25 ## 3 2017-01-16 00:00:00 0.220\r## 4 2017-01-23 00:00:00 0.210\r## 5 2017-02-01 00:00:00 0.350\r## 6 2017-02-09 00:00:00 0.220\rWe obtain the average of the autonomous community of Aragon using the polygon geometry and indicating the type of function with which we want to summarize the area. The esp_get_ccaa() function of the mapSpain() package is very useful when importing Spanish administrative boundaries of different levels. In the extraction, we must pass the na.rm = TRUE argument to the mean() function to exclude pixels with no value.\n# boundaries of Aragon\raragon \u0026lt;- esp_get_ccaa(\u0026quot;Aragon\u0026quot;) %\u0026gt;% st_transform(25830)\r# extract the average values of the SPEI-12\rspei_arag \u0026lt;- terra::extract(spei, vect(aragon), fun = \u0026quot;mean\u0026quot;, na.rm = TRUE)\r# add the new values to our data.frame\rspei_zar \u0026lt;- mutate(spei_zar, arag = unlist(spei_arag)[-1])\rIn the next step, we transform the table to the long format with pivot_longer(), merging the value of the SPEI index of Zaragoza and Aragon. We will also add a column with the interpretation of the index and change the labels.\nspei_zar \u0026lt;- pivot_longer(spei_zar, 2:3, names_to = \u0026quot;reg\u0026quot;, values_to = \u0026quot;spei\u0026quot;) %\u0026gt;%\rmutate(sign = case_when(spei \u0026lt; -0.5 ~ \u0026quot;drought\u0026quot;, spei \u0026gt; 0.5 ~ \u0026quot;wet\u0026quot;,\rTRUE ~ \u0026quot;normal\u0026quot;),\rdate = as_date(date),\rreg = factor(reg, c(\u0026quot;zar\u0026quot;, \u0026quot;arag\u0026quot;), c(\u0026quot;Zaragoza\u0026quot;, \u0026quot;Aragon\u0026quot;)))\rNow it remains to build the graph in which we compare the SPEI-12 of Zaragoza with the average of Aragon. The geom_rect() function helps us draw different background rectangles to mark drought and normal state.\n# time series graph\rggplot(spei_zar) +\rgeom_rect(aes(xmin = min(date), xmax = max(date), ymin = -0.5, ymax = 0.5), fill = \u0026quot;#41ab5d\u0026quot;) +\rgeom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1, ymax = -0.5), fill = \u0026quot;#ffffcc\u0026quot;) +\rgeom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1.5, ymax = -1), fill = \u0026quot;#F3641D\u0026quot;) +\rgeom_hline(yintercept = 0, size = 1, colour = \u0026quot;white\u0026quot;) +\rgeom_line(aes(date, spei, linetype = reg), size = 1, alpha = .7) +\rscale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rlabs(linetype = \u0026quot;\u0026quot;, y = \u0026quot;SPEI-12\u0026quot;, x = \u0026quot;\u0026quot;) +\rcoord_cartesian(expand = FALSE) +\rtheme_minimal() +\rtheme(legend.position = c(.25, .9),\rpanel.grid.minor = element_blank(),\rpanel.ontop = TRUE)\r\rDrought map\rSpain\rTo create a map of drought severity in 2017, we must first make some modifications. With the subset() function, we obtain a layer or several as a subset. Here we select the last one to see the state of drought for the whole year.\nWe replace all values greater than -0.5 with NA in the next step. Drought is considered when the SPEI index is below -0.5 and, on the other hand, if it is above 0.5, we would speak of a wet period.\nThe raster class is not directly compatible with ggplot, so we convert it to an xyz table with longitude, latitude and the variable. When we do the same conversion of multiple layers, each column will represent one layer. Finally, we rename our index column and add a new column with different levels of drought severity.\n# extract layer(s) with their index\rspei_anual \u0026lt;- subset(spei, 48) # substitute non-drought values with NA\rspei_anual[spei_anual \u0026gt; -0.5] \u0026lt;- NA\r# convert our raster into an xyz table\rspei_df \u0026lt;- as.data.frame(spei_anual, xy = TRUE)\rhead(spei_df)\r## x y spei12_2017_48\r## 38096 123100 4858900 -1.48\r## 39195 105500 4857800 -1.59\r## 39197 107700 4857800 -1.40\r## 39211 123100 4857800 -1.47\r## 39212 124200 4857800 -1.50\r## 40310 105500 4856700 -1.63\r# change the name of the variable\rnames(spei_df)[3] \u0026lt;- \u0026quot;spei\u0026quot;\r# categorize the index and fix the order of the factor\rspei_df \u0026lt;- mutate(spei_df, spei_cat = case_when(spei \u0026gt; -0.9 ~ \u0026quot;slight\u0026quot;,\rspei \u0026gt; -1.5 \u0026amp; spei \u0026lt; -0.9 ~ \u0026quot;moderate\u0026quot;,\rspei \u0026gt; -2 \u0026amp; spei \u0026lt;= -1.5 ~ \u0026quot;severe\u0026quot;,\rTRUE ~ \u0026quot;extreme\u0026quot;) %\u0026gt;% fct_relevel(c(\u0026quot;slight\u0026quot;, \u0026quot;moderate\u0026quot;, \u0026quot;severe\u0026quot;, \u0026quot;extreme\u0026quot;)))\rWe can create a raster map with the geom_tile() geometry indicating longitude, latitude and the fill of the pixels with our categorized variable.\n# boundaries\rccaa \u0026lt;- esp_get_ccaa() %\u0026gt;% filter(!ine.ccaa.name %in% c(\u0026quot;Canarias\u0026quot;, \u0026quot;Ceuta\u0026quot;, \u0026quot;Melilla\u0026quot;)) %\u0026gt;% st_transform(25830)\r# mapa\rggplot(spei_df) +\rgeom_tile(aes(x , y, fill = spei_cat)) +\rgeom_sf(data = ccaa, fill = NA, size = .1, colour = \u0026quot;white\u0026quot;, alpha = .4) +\rscale_fill_manual(values = c(\u0026quot;#ffffcc\u0026quot;, \u0026quot;#F3641D\u0026quot;, \u0026quot;#DE2929\u0026quot;, \u0026quot;#8B1A1A\u0026quot;),\rna.value = NA) +\rguides(fill = guide_legend(keywidth = 2, keyheight = .3, label.position = \u0026quot;bottom\u0026quot;,\rtitle.position = \u0026quot;top\u0026quot;)) +\rcoord_sf() +\rlabs(fill = \u0026quot;DROUGHT\u0026quot;) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 0.2,\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;, colour = NA),\rlegend.title = element_text(colour = \u0026quot;white\u0026quot;, size = 20, hjust = .5),\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;),\rplot.margin = margin(t = 10))\r\rAragon\rIn this last map example, we select the drought situation 12 months ahead, at the beginning and end of the year. The main function we use is crop() that cuts to the extent of a spatial object; in our case, it is Aragon, then we apply the mask() function that masks all those pixels within limits leaving the others in NA.\n# subset first and last week 2017\rspei_sub \u0026lt;- subset(spei, c(1, 48)) # crop and mask Aragon\rspei_arag \u0026lt;- crop(spei_sub, aragon) %\u0026gt;% mask(vect(aragon)) # convert the data to xyz\rspei_df_arag \u0026lt;- as.data.frame(spei_arag, xy = TRUE)\r# rename layers\rnames(spei_df_arag)[3:4] \u0026lt;- c(\u0026quot;January\u0026quot;, \u0026quot;December\u0026quot;)\r# changing to the long table format by merging both months\rspei_df_arag \u0026lt;- pivot_longer(spei_df_arag, 3:4, names_to = \u0026quot;mo\u0026quot;, values_to = \u0026quot;spei\u0026quot;) %\u0026gt;% mutate(mo = fct_relevel(mo, c(\u0026quot;January\u0026quot;, \u0026quot;December\u0026quot;)))\rWe will make the two maps in the same way as the one for whole Spain. The main difference is that we use the SPEI index directly as a continuous variable. Also, to create two maps as facets in one row, we add the facet_grid() function. Finally, the index shows negative and positive values; therefore, a divergent range of colours is necessary. To centre the midpoint at 0, we must rescale the index values using the rescale() function from the scales package.\n# map of Aragon\rggplot(spei_df_arag) +\rgeom_tile(aes(x , y, fill = spei)) +\rgeom_sf(data = aragon, fill = NA, size = .1, colour = \u0026quot;white\u0026quot;, alpha = .4) +\rscale_fill_distiller(palette = \u0026quot;RdYlGn\u0026quot;, direction = 1, values = scales::rescale(c(-2.1, 0, 0.9)),\rbreaks = seq(-2, 1, .5)) +\rguides(fill = guide_colorbar(barwidth = 8, barheight = .3, label.position = \u0026quot;bottom\u0026quot;)) +\rfacet_grid(. ~ mo) +\rcoord_sf() +\rlabs(fill = \u0026quot;SPEI-12\u0026quot;, title = \u0026quot;Aragon\u0026quot;) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 0.5,\rlegend.title = element_text(colour = \u0026quot;white\u0026quot;, vjust = 1.1),\rstrip.text = element_text(colour = \u0026quot;white\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;, colour = NA),\rplot.title = element_text(colour = \u0026quot;white\u0026quot;, size = 20, hjust = .5, vjust = 2.5,\rmargin = margin(b = 10, t = 10)),\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;),\rplot.margin = margin(10, 10, 10, 10))\r\r\rMore possibilities\rIt is possible to regroup the different layers by applying a function. For example, using the months of each week of the SPEI-12 we can calculate the monthly average in 2017. To do this, we use the tapp() function, which in turn applies another function to each group. It is crucial that the group is either a factor or the index of each layer. Both tapp() and app() functions have an argument to process in parallel using more than one core.\n# months as factor\rmo \u0026lt;- month(t, label = TRUE)\rmo\r## [1] ene ene ene ene feb feb feb feb mar mar mar mar abr abr abr abr may may may\r## [20] may jun jun jun jun jul jul jul jul ago ago ago ago sep sep sep sep oct oct\r## [39] oct oct nov nov nov nov dic dic dic dic\r## 12 Levels: ene \u0026lt; feb \u0026lt; mar \u0026lt; abr \u0026lt; may \u0026lt; jun \u0026lt; jul \u0026lt; ago \u0026lt; sep \u0026lt; ... \u0026lt; dic\r# average by month\rspei_mo \u0026lt;- tapp(spei, mo, mean)\rspei_mo\r## class : SpatRaster ## dimensions : 834, 1115, 12 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## names : ene, feb, mar, abr, may, jun, ... ## min values : -1.2800, -1.4675, -2.2400, -2.6500, -2.5775, -2.4675, ... ## max values : 1.3875, 1.9175, 1.7475, 1.8375, 1.7500, 1.7000, ...\r# maps\rplot(spei_mo)\rThe mean() function used directly on a multidimensional SpatRaster class object returns the average per cell. The same result can be obtained with the app() function that applies any function. The number of resulting layers depends on the function; for example, using range() results in two layers, one for the minimum value and one for the maximum value. Finally, the global() function summarizes each layer in the form of a table with the indicated function.\n# average over layers\rspei_mean \u0026lt;- mean(spei)\rspei_mean\r## class : SpatRaster ## dimensions : 834, 1115, 1 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## name : mean ## min value : -2.127083 ## max value : 1.568542\r# map\rplot(spei_mean)\r# alternative\rspei_min \u0026lt;- app(spei, min)\rspei_min\r## class : SpatRaster ## dimensions : 834, 1115, 1 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## name : min ## min value : -3.33 ## max value : 0.29\rspei_range \u0026lt;- app(spei, range)\rnames(spei_range) \u0026lt;- c(\u0026quot;min\u0026quot;, \u0026quot;max\u0026quot;)\rspei_range\r## class : SpatRaster ## dimensions : 834, 1115, 2 (nrow, ncol, nlyr)\r## resolution : 1100, 1100 (x, y)\r## extent : -80950, 1145550, 3979450, 4896850 (xmin, xmax, ymin, ymax)\r## coord. ref. : ETRS89 / UTM zone 30N (EPSG:25830) ## source : memory ## names : min, max ## min values : -3.33, -1.06 ## max values : 0.29, 2.02\r# map\rplot(spei_range)\r# statistical summary by layer\rglobal(spei, \u0026quot;mean\u0026quot;, na.rm = TRUE) %\u0026gt;% head()\r## mean\r## spei12_2017_1 -0.03389126\r## spei12_2017_2 -0.17395742\r## spei12_2017_3 -0.13228593\r## spei12_2017_4 -0.07536089\r## spei12_2017_5 0.06718260\r## spei12_2017_6 -0.03461822\r\n\r","date":1646697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646697600,"objectID":"866470535c55bb27847b5cd2414ab005","permalink":"https://dominicroye.github.io/en/2022/use-of-multidimensional-spatial-data/","publishdate":"2022-03-08T00:00:00Z","relpermalink":"/en/2022/use-of-multidimensional-spatial-data/","section":"post","summary":"Space-time information is vital in many disciplines, especially in climatology or meteorology, and this makes it necessary to have a format that allows a multidimensional structure. It is also important that this format has a high degree of interchange compatibility and can store a large number of data.","tags":["ncdf","cube","drought","spain","raster"],"title":"Use of multidimensional spatial data","type":"post"},{"authors":["Yao Wu","Bo Wen","et al"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"3fbb3b84e73313b56657d0a5d4a39016","permalink":"https://dominicroye.github.io/en/publication/2022-fluctuating-temperature-the-innovation/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/en/publication/2022-fluctuating-temperature-the-innovation/","section":"publication","summary":"Studies have investigated the effects of heat and temperature variability (TV) on mortality. However, few assessed whether TV modifies the heat-mortality association. Data on daily temperature and mortality in the warm season were collected from 717 locations across 36 countries. TV was calculated as the standard deviation of the average of the same and previous days’ minimum and maximum temperatures. We first used location-specific quasi-Poisson regression models with an interaction term between the cross-basis term for mean temperature and quartiles of TV to obtain heat-mortality associations under each quartile of TV, then pooled estimates at the country, regional, and global levels. Results show the increased risk in heat-related mortality with increments in TV, accounting for 0.70% (95% confidence interval [CI], -0.33–1.69), 1.34% (95% CI: -0.14–2.73), 1.99% (95% CI: 0.29–3.57), and 2.73% (95% CI: 0.76–4.50) of total deaths for Q1–Q4 (1st quartile–4th quartile) of TV. The modification effects of TV varied geographically. Central Europe had the highest attributable fractions (AFs), corresponding to 7.68% (95% CI: 5.25–9.89) of total deaths for Q4 of TV, while the lowest AFs were observed in North America, with the values for Q4 of 1.74% (95% CI: -0.09–3.39). TV had a significant modification effect on the heat-mortality association, causing a higher heat-related mortality burden with increments of TV. Implementing targeted strategies against heat exposure and fluctuant temperatures simultaneously would benefit public health.","tags":["temperature variability","heat","modification effect","mortality"],"title":"Fluctuating temperature modifies heat-mortality association in the globe","type":"publication"},{"authors":null,"categories":["gis","R","R:intermediate","visualization"],"content":"\r\rIn April of this year, I made an animation of the 24-hour average temperature of January 2020, also showing the day-night cycle.\nThe average temperature of 24 hours in January 2020 with the day/night cycle. You can see a lot of geographic patterns. I love this kind of hypnotic temperature gifs. #rstats #rspatial #dataviz #climate pic.twitter.com/NA5haUlnie\n\u0026mdash; Dr. Dominic Royé (@dr_xeo) April 17, 2021  My biggest problem was finding a way to project correctly the area at night without breaking the geometry. The easiest solution I found was rasterising the night polygon and then reprojecting it. Indeed, a vector approach could be used, but I have preferred to use raster data here.\nPackages\rWe will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rlubridate\rEasy manipulation of dates and times\r\rhms\rProvides a simple class to store durations or time of day values and display them in hh:mm:ss format\r\rterra\rImport, export and manipulate raster (raster successor package)\r\rlwgeom\rAccess to the liblwgeom library with additional vector functions for sf\r\rrnaturalearth\rVector maps of the world ‘Natural Earth’\r\rgifski\rCreating animations in gif format\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;hms\u0026quot;)) install.packages(\u0026quot;hms\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;lwgeom\u0026quot;)) install.packages(\u0026quot;lwgeom\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\rif(!require(\u0026quot;gifski\u0026quot;)) install.packages(\u0026quot;gifski\u0026quot;)\r# packages\rlibrary(rnaturalearth)\rlibrary(tidyverse)\rlibrary(lwgeom)\rlibrary(sf)\rlibrary(terra)\rlibrary(lubridate)\rlibrary(hms)\rlibrary(gifski)\r To use the 50 and 10 m resolution of the {rnaturalearth} package it is necessary to install the following additional packages. The {devtools} package must be installed. devtools::install_github(\u0026ldquo;ropensci/rnaturalearthdata\u0026rdquo;) devtools::install_github(\u0026ldquo;ropensci/rnaturalearthhires\u0026rdquo;)   \rPreparation\rExternal functions\rThe functions to estimate the separator line between day and night are based on a javascript L.Terminator.js from the {Leaflet} package I found on stackoverflow. You can download the script with the functions here or access it on github.\nsource(\u0026quot;terminator.R\u0026quot;) # import the functions\r\rCustom functions\rThe primary function terminator() based on the javascript of {Leaflet} needs as arguments: the date-time, the minimum and maximum extension, as well as the resolution or the interval of longitude.\nt0 \u0026lt;- Sys.time() # date and time of our operating system\rt0\r## [1] \u0026quot;2022-03-27 11:36:26 CEST\u0026quot;\rcoord_nightday \u0026lt;- terminator(t0, -180, 180, 0.2) # estimate the day-night line\r# convert it into a spatial object of class sf\rline_nightday \u0026lt;- st_linestring(as.matrix(coord_nightday)) %\u0026gt;% st_sfc(crs = 4326) # plot\rplot(line_nightday)\rIn the next step, we obtain the polygons corresponding to the day and the night that separates the previously estimated line. To do this, we create a rectangle covering the entire planet and use the st_split() function from the {lwgeom} package that divides the rectangle.\n# rectangle\rwld_bbx \u0026lt;- st_bbox(c(xmin = -180, xmax = 180,\rymin = -90, ymax = 90), crs = 4326) %\u0026gt;% st_as_sfc()\r# division with the day-night line\rpoly_nightday \u0026lt;- st_split(wld_bbx, line_nightday) %\u0026gt;% st_collection_extract(c(\u0026quot;POLYGON\u0026quot;)) %\u0026gt;% st_sf() # plot\rplot(poly_nightday)\rThe question now arises which of the two polygons corresponds to the night and which to the day. That will depend on what day of the year we are, given the changes in the Earth’s position concerning the Sun. Between the first summer equinox and the autumn equinox, it corresponds to the first polygon, when we can also observe the polar day at the north pole, and in the opposite case, it would be the second. The {terra} package only accepts its vector class called SpatVector, so we convert the vector object sf with the vect() function.\n# select the second polygon\rpoly_nightday \u0026lt;- slice(poly_nightday, 2) %\u0026gt;% mutate(daynight = 1)\r# create the raster with a resolution of 0.5º and the extent of the world\rr \u0026lt;- rast(vect(wld_bbx), resolution = .5)\r# rasterize the night polygon night_rast \u0026lt;- rasterize(vect(poly_nightday), r) # result in raster format\rplot(night_rast)\rIn the last step we reproject the raster to Mollweide.\n# define the raster projection (WGS84)\rcrs(night_rast) \u0026lt;- \u0026quot;EPSG:4326\u0026quot;\r# reproject\rnight_rast_prj \u0026lt;- project(night_rast, \u0026quot;ESRI:54009\u0026quot;, mask = TRUE, method = \u0026quot;near\u0026quot;)\r# map\rplot(night_rast_prj)\rFinally we include the individual steps that we have done in a custom function.\nrast_determiner \u0026lt;- function(x_min, date, res) {\r# create date with time adding the number of minutes\rt0 \u0026lt;- as_date(date) + minutes(x_min) # estimate the coordinates of the line that separates day and night\rnight_step \u0026lt;- terminator(t0, -180, 180, 0.2) %\u0026gt;% as.matrix()\r# pass the points to line\rnight_line \u0026lt;- st_linestring(night_step) %\u0026gt;% st_sfc(crs = 4326)\r# define the rectangle of the planet\rwld_bbx \u0026lt;- st_bbox(c(xmin = -180, xmax = 180,\rymin = -90, ymax = 90), crs = 4326) %\u0026gt;% st_as_sfc()\r# divide the polygon with the day-night line\rpoly_nightday \u0026lt;- st_split(wld_bbx, night_line) %\u0026gt;% st_collection_extract(c(\u0026quot;POLYGON\u0026quot;)) %\u0026gt;% st_sf() # select the polygon according to the date\rif(date \u0026lt;= make_date(year(date), 3, 20) | date \u0026gt;= make_date(year(date), 9, 23)) {\rpoly_nightday \u0026lt;- slice(poly_nightday, 2) %\u0026gt;% mutate(daynight = 1)\r} else {\rpoly_nightday \u0026lt;- slice(poly_nightday, 1) %\u0026gt;% mutate(daynight = 1)\r}\r# create the raster with the resolution given in the argument res\rr \u0026lt;- rast(vect(wld_bbx), resolution = res)\r# rasterize the night polygon\rnight_rast \u0026lt;- rasterize(vect(poly_nightday), r) return(night_rast)\r}\rSince we want to obtain the area at night for different day hours, we construct a second function to apply the first one at different day intervals (in minutes).\nnight_determinator \u0026lt;- function(time_seq, # minutes\rdate = Sys.Date(), # date (system default)\rres = .5) { # raster resolution 0.5º\r# apply the first function on a vector of day intervals\rnight_raster \u0026lt;- map(time_seq, rast_determiner,\rdate = date, res = res)\r# convert the raster into an object with as many layers as day intervals\rnight_raster \u0026lt;- rast(night_raster)\r# define the WGS84 projection\rcrs(night_raster) \u0026lt;- \u0026quot;EPSG:4326\u0026quot;\rreturn(night_raster)\r}\r\r\rCreate a day-night cycle\rFirst, we create the area of nights for the day of our operating system with intervals of 30 minutes. Then we reproject it to Winkel II.\n# apply our function for a 24 hour day in 30 minute intervals\rnight_rast \u0026lt;- night_determinator(seq(0, 1410, 30), Sys.Date(), res = .5)\r# reproject to Winkel II\rnight_raster_winkel \u0026lt;- project(night_rast, \u0026quot;ESRI:54019\u0026quot;, mask = TRUE,\rmethod = \u0026quot;near\u0026quot;)\r# map of the first 5 intervals\rplot(night_raster_winkel, maxnl = 5)\r\rAnimation of the day-night cycle\rPreparation\rTo create a 24-hour animation showing the movement of the night on the Earth, we must do a few previous steps. First we get the world boundaries with the ne_countries() function and reproject them to the new Winkel II projection. Then we convert the raster data into a data.frame indicating to keep missing values. We can see that each layer of the raster (of each 30-minute interval) is a column in the data.frame. We rename the columns and convert the table into a long format using the pivot_longer() function. What we do is to merge all the columns of the layers into a single one. As the last step, we exclude the missing values with the filter() function.\n# country boundaries\rwld \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;) %\u0026gt;% st_transform(\u0026quot;ESRI:54019\u0026quot;)\r# convert the raster to a data.frame with xyz\rdf_winkel \u0026lt;- as.data.frame(night_raster_winkel, xy = TRUE, na.rm = FALSE)\r# rename all the columns corresponding to the day intervals\rnames(df_winkel)[3:length(df_winkel)] \u0026lt;- str_c(\u0026quot;H\u0026quot;, as_hms(seq(0, 1410, 30)*60))\r# change to a long format\rdf_winkel \u0026lt;- pivot_longer(df_winkel, 3:length(df_winkel), names_to = \u0026quot;hour\u0026quot;, values_to = \u0026quot;night\u0026quot;) # exclude missing values to reduce table size\rdf_winkel \u0026lt;- filter(df_winkel, !is.na(night))\rIt only remains to create a graticule and obtain the extent of the world map.\n# graticule\rgrid \u0026lt;- st_graticule() %\u0026gt;% st_transform(\u0026quot;ESRI:54019\u0026quot;)\r# get the extension of the world\rbbx \u0026lt;- st_bbox(wld)\rNow we will build a map at a single interval with ggplot2, adding the vector geometry using the geom_sf() function (the boundaries and the graticule) and the raster data using the geom_raster() function. In the title, we are using a unicode symbol as a clock. We also define the map’s extent in coord_sf() to keet it constant over all maps in the animation. Finally, we make use of {{ }} from the {rlang} package within the filter()function to be able to filter our raster data in table form. So that our function can correctly evaluate the values that we pass in x (the intervals of the day) it is necessary to use this grammar of tidy evaluation due to data masking in tidyverse. Honestly, it is a topic for another post.\n# example 5 UTC\rx \u0026lt;- \u0026quot;H05:00:00\u0026quot;\r# map\rggplot() +\r# boundaries\rgeom_sf(data = wld,\rfill = \u0026quot;#74a9cf\u0026quot;, colour = \u0026quot;white\u0026quot;,\rsize = .1) +\r# graticule\rgeom_sf(data = grid, size = .1) +\r# filtered raster data geom_raster(data = filter(df_winkel, hour == {{x}}), aes(x, y), fill = \u0026quot;grey90\u0026quot;,\ralpha = .6) +\r# title\rlabs(title = str_c(\u0026quot;\\U1F551\u0026quot;, str_remove(x, \u0026quot;H\u0026quot;), \u0026quot; UTC\u0026quot;)) + # extension limits\rcoord_sf(xlim = bbx[c(1, 3)], ylim = bbx[c(2, 4)]) +\r# map style\rtheme_void() +\rtheme(plot.title = element_text(hjust = .1, vjust = .9))\r\rAnimation\rWe create the animation by applying the walk() function, which in turn will go through the interval vector to filter our data and map each step using ggplot.\nwalk(str_c(\u0026quot;H\u0026quot;, as_hms(seq(0, 1410, 30)*60)), function(step){\rg \u0026lt;- ggplot() +\rgeom_sf(data = wld,\rfill = \u0026quot;#74a9cf\u0026quot;, colour = \u0026quot;white\u0026quot;,\rsize = .1) +\rgeom_sf(data = grid,\rsize = .1) +\rgeom_raster(data = filter(df_winkel, hour == {{step}}), aes(x, y), fill = \u0026quot;grey90\u0026quot;,\ralpha = .6) +\rlabs(title = str_c(\u0026quot;\\U1F551\u0026quot;, str_remove(x, \u0026quot;H\u0026quot;), \u0026quot; UTC\u0026quot;)) + coord_sf(xlim = bbx[c(1, 3)], ylim = bbx[c(2, 4)]) +\rtheme_void() +\rtheme(plot.title = element_text(hjust = .1, vjust = .9))\rggsave(str_c(\u0026quot;wld_night_\u0026quot;, str_remove_all(step, \u0026quot;:\u0026quot;), \u0026quot;.png\u0026quot;), g,\rheight = 4.3, width = 8.4, bg = \u0026quot;white\u0026quot;, dpi = 300, units = \u0026quot;in\u0026quot;)\r})\rThe creation of the final gif is done with gifski() passing it the names of the images in the order as they should appear in the animation.\nfiles \u0026lt;- str_c(\u0026quot;wld_night_H\u0026quot;, str_remove_all(as_hms(seq(0, 1410, 30)*60), \u0026quot;:\u0026quot;), \u0026quot;.png\u0026quot;)\rgifski(files, \u0026quot;night_day.gif\u0026quot;, width = 807, height = 409, loop = TRUE, delay = 0.1)\r\n\r\r","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"2d7203b5c72c2d1515b98f57ed6f449a","permalink":"https://dominicroye.github.io/en/2021/visualize-the-day-night-cycle-on-a-world-map/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/en/2021/visualize-the-day-night-cycle-on-a-world-map/","section":"post","summary":"In April of this year, I made an animation of the 24-hour average temperature of January 2020, also showing the day-night cycle. My biggest problem was finding a way to project correctly the area at night without breaking the geometry. The easiest solution I found was rasterising the night polygon and then reprojecting it. Indeed, a vector approach could be used, but I have preferred to use raster data here.","tags":["world map","day-night","animation"],"title":"Visualize the day-night cycle on a world map","type":"post"},{"authors":["F Sera","B Armstrong","S Abbott","S Meakin","K O'Keilly","R von Borries","R Schneider","D Royé","M Hashizume","M Pascal","A Tobías","A Vicedo-Cabrera","MCC Collaborative Research Network","CMMID COVID-19 Working Group","A Gasparrini","R Lowe"],"categories":null,"content":"","date":1634169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634169600,"objectID":"d2bf4c0817addb47a5d983e8161edce6","permalink":"https://dominicroye.github.io/en/publication/2021-covid-nature-comm/","publishdate":"2021-10-14T00:00:00Z","relpermalink":"/en/publication/2021-covid-nature-comm/","section":"publication","summary":"There is conflicting evidence on the influence of weather on COVID-19 transmission. Our aim is to estimate weather-dependent signatures in the early phase of the pandemic, while controlling for socio-economic factors and non-pharmaceutical interventions. We identify a modest non-linear association between mean temperature and the effective reproduction number (Re) in 409 cities in 26 countries, with a decrease of 0.087 (95% CI: 0.025; 0.148) for a 10 °C increase. Early interventions have a greater effect on Re with a decrease of 0.285 (95% CI 0.223; 0.347) for a 5th - 95th percentile increase in the government response index. The variation in the effective reproduction number explained by government interventions is 6 times greater than for mean temperature. We find little evidence of meteorological conditions having influenced the early stages of local epidemics and conclude that population behaviour and government interventions are more important drivers of transmission.","tags":["COVID19","human health","temperature","pandemic","MCC Study"],"title":"A cross-sectional analysis of meteorological factors and SARS-CoV-2 transmission in 409 cities across 26 countries","type":"publication"},{"authors":["G Chen","et al"],"categories":null,"content":"","date":1631232000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631232000,"objectID":"387a305ab367b7af99ba32d3233f90b0","permalink":"https://dominicroye.github.io/en/publication/2021-forest-fire-mortality-lancet-planetary-health/","publishdate":"2021-09-10T00:00:00Z","relpermalink":"/en/publication/2021-forest-fire-mortality-lancet-planetary-health/","section":"publication","summary":"Background: Many regions of the world are now facing more frequent and unprecedentedly large wildfires. However, the association between wildfire-related PM2·5 and mortality has not been well characterised. We aimed to comprehensively assess the association between short-term exposure to wildfire-related PM2·5 and mortality across various regions of the world. Methods: For this time series study, data on daily counts of deaths for all causes, cardiovascular causes, and respiratory causes were collected from 749 cities in 43 countries and regions during 2000–16. Daily concentrations of wildfire-related PM2·5 were estimated using the three-dimensional chemical transport model GEOS-Chem at a 0·25° × 0·25° resolution. The association between wildfire-related PM2·5 exposure and mortality was examined using a quasi-Poisson time series model in each city considering both the current-day and lag effects, and the effect estimates were then pooled using a random-effects meta-analysis. Based on these pooled effect estimates, the population attributable fraction and relative risk (RR) of annual mortality due to acute wildfire-related PM2·5 exposure was calculated. Findings: 65·6 million all-cause deaths, 15·1 million cardiovascular deaths, and 6·8 million respiratory deaths were included in our analyses. The pooled RRs of mortality associated with each 10 μg/m3 increase in the 3-day moving average (lag 0–2 days) of wildfire-related PM2·5 exposure were 1·019 (95% CI 1·016–1·022) for all-cause mortality, 1·017 (1·012–1·021) for cardiovascular mortality, and 1·019 (1·013–1·025) for respiratory mortality. Overall, 0·62% (95% CI 0·48–0·75) of all-cause deaths, 0·55% (0·43–0·67) of cardiovascular deaths, and 0·64% (0·50–0·78) of respiratory deaths were annually attributable to the acute impacts of wildfire-related PM2·5 exposure during the study period. Interpretation: Short-term exposure to wildfire-related PM2·5 was associated with increased risk of mortality. Urgent action is needed to reduce health risks from the increasing wildfires.","tags":["climate change","wild fires","pollution","mortality","MCC Study","global","risk"],"title":"Mortality risk attributable to wildfire-related PM 2.5 pollution: a global time series study in 749 locations","type":"publication"},{"authors":null,"categories":["R","R:elementary","visualization"],"content":"\r\rThe climate of a place is usually presented through climographs that combine monthly precipitation and temperature in a single chart. However, it is also interesting to visualize the climate on a daily scale showing the thermal amplitude and the daily average temperature. To do this, the averages for each day of the year of daily minimums, maximums and means are calculated.\nThe annual climate cycle presents a good opportunity to use a radial or polar chart which allows us to clearly visualize seasonal patterns.\nPackages\rWe will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\rjanitor\rSimple functions to examine and clean data\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r# packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(fs)\rlibrary(janitor)\r\rPreparation\rData\rWe download the temperature data for a selection of US cities here. You can access other cities of the entire world through the WMO or GHCN datasets at NCDC/NOAA.\n\rImport\rTo import the temperature time series of each city, which we find in several files, we apply the read_csv() function using map_df(). The dir_ls() function of the fs package returns the list of files with csv extension. The suffix df of map() indicates that we want to join all imported tables into a single one. For those with less experience with tidyverse, I recommend a short introduction on this blog post.\nThen we obtain the names of the weather stations and define a new vector with the new city names.\n# import data\rmeteo \u0026lt;- dir_ls(regexp = \u0026quot;.csv$\u0026quot;) %\u0026gt;% map_df(read_csv)\rmeteo\r## # A tibble: 211,825 x 12\r## STATION NAME LATITUDE LONGITUDE ELEVATION DATE TAVG TMAX TMIN\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-01 6.8 NA NA\r## 2 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-02 8.4 NA NA\r## 3 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-03 11 NA NA\r## 4 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-04 -7.2 NA NA\r## 5 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-05 -10.2 NA NA\r## 6 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-06 -4.6 NA NA\r## 7 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-07 -7.1 NA NA\r## 8 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-08 -5.8 NA NA\r## 9 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-09 2.9 NA NA\r## 10 USW00094846 CHICAG~ 42.0 -87.9 202. 1950-01-10 3.9 NA NA\r## # ... with 211,815 more rows, and 3 more variables: TAVG_ATTRIBUTES \u0026lt;chr\u0026gt;,\r## # TMAX_ATTRIBUTES \u0026lt;chr\u0026gt;, TMIN_ATTRIBUTES \u0026lt;chr\u0026gt;\r# station names\rstats_names \u0026lt;- unique(meteo$NAME)\rstats_names\r## [1] \u0026quot;CHICAGO OHARE INTERNATIONAL AIRPORT, IL US\u0026quot; ## [2] \u0026quot;LAGUARDIA AIRPORT, NY US\u0026quot; ## [3] \u0026quot;MIAMI INTERNATIONAL AIRPORT, FL US\u0026quot; ## [4] \u0026quot;HOUSTON INTERCONTINENTAL AIRPORT, TX US\u0026quot; ## [5] \u0026quot;ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPORT, GA US\u0026quot;\r## [6] \u0026quot;SAN FRANCISCO INTERNATIONAL AIRPORT, CA US\u0026quot; ## [7] \u0026quot;SEATTLE TACOMA AIRPORT, WA US\u0026quot; ## [8] \u0026quot;DENVER INTERNATIONAL AIRPORT, CO US\u0026quot; ## [9] \u0026quot;MCCARRAN INTERNATIONAL AIRPORT, NV US\u0026quot;\r# new city names\rcities \u0026lt;- c(\u0026quot;CHICAGO\u0026quot;, \u0026quot;NEW YORK\u0026quot;, \u0026quot;MIAMI\u0026quot;, \u0026quot;HOUSTON\u0026quot;, \u0026quot;ATLANTA\u0026quot;, \u0026quot;SAN FRANCISCO\u0026quot;, \u0026quot;SEATTLE\u0026quot;, \u0026quot;DENVER\u0026quot;, \u0026quot;LAS VEGAS\u0026quot;)\r\rModify\rIn the first step, we will modify the original data, 1) selecting only the columns of interest, 2) filtering the period 1991-2020, 3) defining the new city names, 4) calculating the average temperature where it is absent, 5) cleaning the column names, and 6) creating a new variable with the days of the year. The clean_names() function of the janitor package is very useful for getting clean column names.\nmeteo \u0026lt;- select(meteo, NAME, DATE, TAVG:TMIN) %\u0026gt;% filter(DATE \u0026gt;= \u0026quot;1991-01-01\u0026quot;, DATE \u0026lt;= \u0026quot;2020-12-31\u0026quot;) %\u0026gt;% mutate(NAME = factor(NAME, stats_names, cities),\rTAVG = ifelse(is.na(TAVG), (TMAX+TMIN)/2, TAVG),\ryd = yday(DATE)) %\u0026gt;% clean_names()\rIn the next step, we calculate the daily maximum, minimum and mean temperature for each day of the year. It now only remains to convert the days of the year into a dummy date. Here we use the year 2000 since it is a leap year, and we have a total of 366 days.\n# estimate the daily averages\rmeteo_yday \u0026lt;- group_by(meteo, name, yd) %\u0026gt;% summarise(ta = mean(tavg, na.rm = TRUE),\rtmx = mean(tmax, na.rm = TRUE),\rtmin = mean(tmin, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;name\u0026#39;. You can override using the\r## `.groups` argument.\rmeteo_yday\r## # A tibble: 3,294 x 5\r## # Groups: name [9]\r## name yd ta tmx tmin\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 CHICAGO 1 -3.77 0.537 -7.86\r## 2 CHICAGO 2 -2.64 1.03 -6.68\r## 3 CHICAGO 3 -2.88 0.78 -6.93\r## 4 CHICAGO 4 -2.86 0.753 -7.10\r## 5 CHICAGO 5 -4.13 -0.137 -8.33\r## 6 CHICAGO 6 -4.50 -1.15 -8.05\r## 7 CHICAGO 7 -4.70 -0.493 -8.57\r## 8 CHICAGO 8 -3.97 0.147 -8.02\r## 9 CHICAGO 9 -3.47 0.547 -7.49\r## 10 CHICAGO 10 -3.41 1.09 -7.64\r## # ... with 3,284 more rows\r# convert the days of the year into a dummy date\rmeteo_yday \u0026lt;- mutate(meteo_yday, yd = as_date(yd, origin = \u0026quot;1999-12-31\u0026quot;))\r\r\rCreating the climate circles\rPredefinitions\rWe define a divergent vector of various hues.\ncol_temp \u0026lt;- c(\u0026quot;#cbebf6\u0026quot;,\u0026quot;#a7bfd9\u0026quot;,\u0026quot;#8c99bc\u0026quot;,\u0026quot;#974ea8\u0026quot;,\u0026quot;#830f74\u0026quot;,\r\u0026quot;#0b144f\u0026quot;,\u0026quot;#0e2680\u0026quot;,\u0026quot;#223b97\u0026quot;,\u0026quot;#1c499a\u0026quot;,\u0026quot;#2859a5\u0026quot;,\r\u0026quot;#1b6aa3\u0026quot;,\u0026quot;#1d9bc4\u0026quot;,\u0026quot;#1ca4bc\u0026quot;,\u0026quot;#64c6c7\u0026quot;,\u0026quot;#86cabb\u0026quot;,\r\u0026quot;#91e0a7\u0026quot;,\u0026quot;#c7eebf\u0026quot;,\u0026quot;#ebf8da\u0026quot;,\u0026quot;#f6fdd1\u0026quot;,\u0026quot;#fdeca7\u0026quot;,\r\u0026quot;#f8da77\u0026quot;,\u0026quot;#fcb34d\u0026quot;,\u0026quot;#fc8c44\u0026quot;,\u0026quot;#f85127\u0026quot;,\u0026quot;#f52f26\u0026quot;,\r\u0026quot;#d10b26\u0026quot;,\u0026quot;#9c042a\u0026quot;,\u0026quot;#760324\u0026quot;,\u0026quot;#18000c\u0026quot;)\rWe create a table with the x-axis grid lines.\ngrid_x \u0026lt;- tibble(x = seq(ymd(\u0026quot;2000-01-01\u0026quot;), ymd(\u0026quot;2000-12-31\u0026quot;), \u0026quot;month\u0026quot;), y = rep(-10, 12), xend = seq(ymd(\u0026quot;2000-01-01\u0026quot;), ymd(\u0026quot;2000-12-31\u0026quot;), \u0026quot;month\u0026quot;), yend = rep(41, 12))\rWe define all the style elements of the graph in our own theme theme_cc().\ntheme_cc \u0026lt;- function(){ theme_minimal(base_family = \u0026quot;Montserrat\u0026quot;) %+replace%\rtheme(plot.title = element_text(hjust = 0.5, colour = \u0026quot;white\u0026quot;, size = 30, margin = margin(b = 20)),\rplot.caption = element_text(colour = \u0026quot;white\u0026quot;, size = 9, hjust = .5, vjust = -30),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.margin = margin(1, 1, 2, 1, unit = \u0026quot;cm\u0026quot;),\raxis.text.x = element_text(face = \u0026quot;italic\u0026quot;, colour = \u0026quot;white\u0026quot;),\raxis.title.y = element_blank(),\raxis.text.y = element_blank(),\rlegend.title = element_text(colour = \u0026quot;white\u0026quot;),\rlegend.position = \u0026quot;bottom\u0026quot;,\rlegend.justification = 0.5,\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;),\rstrip.text = element_text(colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 14),\rpanel.spacing.y = unit(1, \u0026quot;lines\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rpanel.grid = element_blank()\r) }\r\rGraph\rWe start by building a chart for New York City only. We will use geom_linerange() to define line range with the daily maximum and minimum temperature. Also, we will draw the range line colour based on the mean temperature. Finally, we can adjust alpha and size to get a nicer look.\n# filter New York\rny_city \u0026lt;- filter(meteo_yday, name == \u0026quot;NEW YORK\u0026quot;) # graph\rggplot(ny_city) + geom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion()) +\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;,\rdate_labels = \u0026quot;%b\u0026quot;) +\rlabs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) \rTo get the polar graph it would only be necessary to add the coord_polar() function.\n# polar chart\rggplot(ny_city) + geom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion()) +\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;,\rdate_labels = \u0026quot;%b\u0026quot;) +\rcoord_polar() +\rlabs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) \rIn the final graph, we add the grid defining the lines on the y-axis with geom_hline() and those on the x-axis with geom_segement(). The most important thing here is the facet_wrap() function, which allows multiple facets of charts. The formula format is used to specify how the facets are created: row ~ column. If we do not have a second variable, a point . is indicated in the formula. In addition, we make changes to the appearance of the colour bar with guides() and guide_colourbar(), and we include the theme_cc() style.\nggplot(meteo_yday) + geom_hline(yintercept = c(-10, 0, 10, 20, 30, 40), colour = \u0026quot;white\u0026quot;, size = .4) +\rgeom_segment(data = grid_x , aes(x = x, y = y, xend = xend, yend = yend), linetype = \u0026quot;dashed\u0026quot;, colour = \u0026quot;white\u0026quot;, size = .2) +\rgeom_linerange(aes(yd, ymax = tmx, ymin = tmin, colour = ta),\rsize=0.5, alpha = .7) + scale_y_continuous(breaks = seq(-30, 50, 10), limits = c(-11, 42), expand = expansion())+\rscale_colour_gradientn(colours = col_temp, limits = c(-12, 35), breaks = seq(-12, 34, 5)) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rguides(colour = guide_colourbar(barwidth = 15,\rbarheight = 0.5, title.position = \u0026quot;top\u0026quot;)\r) +\rfacet_wrap(~name, nrow = 3) +\rcoord_polar() + labs(title = \u0026quot;CLIMATE CIRCLES\u0026quot;, colour = \u0026quot;Daily average temperature\u0026quot;) +\rtheme_cc()\r\n\r\r","date":1630713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630713600,"objectID":"cd985bb602720b738965f25d570bacb9","permalink":"https://dominicroye.github.io/en/2021/climate-circles/","publishdate":"2021-09-04T00:00:00Z","relpermalink":"/en/2021/climate-circles/","section":"post","summary":"The climate of a place is usually presented through climographs that combine monthly precipitation and temperature in a single chart. However, it is also interesting to visualize the climate on a daily scale showing the thermal amplitude and the daily average temperature. To do this, the averages for each day of the year of the daily minimums, maximums and averages are calculated. The annual climate cycle presents a good opportunity to use a radial or polar which allows us to clearly visualize seasonal patterns.","tags":["climate","polar","temperatures"],"title":"Climate circles","type":"post"},{"authors":["A Tobías","et al"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"52683671c3ed79a24a56c67b0f13c303","permalink":"https://dominicroye.github.io/en/publication/2021-minimum-mortality-temperature-environmental-epidemiology/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/en/publication/2021-minimum-mortality-temperature-environmental-epidemiology/","section":"publication","summary":"Background: Minimum mortality temperature (MMT) is an important indicator to assess the temperature-mortality association, indicating long-term adaptation to local climate. Limited evidence about the geographical variability of the MMT is available at a global scale. Methods: We collected data from 658 communities in 43 countries under different climates. We estimated temperature-mortality associations to derive the MMT for each community using Poisson regression with distributed lag nonlinear models. We investigated the variation in MMT by climatic zone using a mixed-effects meta-analysis and explored the association with climatic and socioeconomic indicators. Results: The geographical distribution of MMTs varied considerably by country between 14.2 and 31.1 ºC decreasing by latitude. For climatic zones, the MMTs increased from alpine (13.0 ºC) to continental (19.3 ºC), temperate (21.7 ºC), arid (24.5 ºC), and tropical (26.5 ºC). The MMT percentiles (MMTPs) corresponding to the MMTs decreased from temperate (79.5th) to continental (75.4th), arid (68.0th), tropical (58.5th), and alpine (41.4th). The MMTs indreased by 0.8 ºC for a 1 ºC rise in a communitys annual mean temperature, and by 1ºC for a 1ºC rise in its SD. While the MMTP decreased by 0.3 centile points for a 1 ºC rise in a communitys annual mean temperature and by 1.3 for a 1 ºC rise in its SD. Conclusions: The geographical distribution of the MMTs and MMTPs is driven mainly by the mean annual temperature, which seems to be a valuable indicator of overall adaptation across populations. Our results suggest that populations have adapted to the average temperature, although there is still more room for adaptation.","tags":["minimum mortality temperature","mortality","global","MCC Study","temperature","climate","human health"],"title":"Geographical Variations of the Minimum Mortality Temperature at a Global Scale","type":"publication"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide"],"categories":null,"content":"","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"281bad00093611848241c8aa3afad58c","permalink":"https://dominicroye.github.io/en/publication/2021-mediterranean-droughts-atmosphere/","publishdate":"2021-07-22T00:00:00Z","relpermalink":"/en/publication/2021-mediterranean-droughts-atmosphere/","section":"publication","summary":"Drought is one of the most complex climate-related phenomena and is expected to progressively affect our lives by causing very serious environmental and socioeconomic damage by the end of the 21st century. In this study, we have extracted a dataset of exceptional meteorological drought events between 1975 and 2019 at the country and subregional scales. Each drought event was described by its start and end date, intensity, severity, duration, areal extent, peak month and peak area. To define such drought events and their characteristics, separate analyses based on three drought indices were performed at 12-month timescale: the Standardized Precipitation Index (SPI), the Standardized Precipitation Evapotranspiration Index (SPEI), and the Reconnaissance Drought Index (RDI). A multivariate combined drought index (DXI) was developed by merging the previous three indices for more understanding of droughts’ features at the country and subregional levels. Principal component analysis (PCA) was used to identify five different drought subregions based on DXI-12 values for 312 Mediterranean stations and a new special score was defined to classify the multi-subregional exceptional drought events across the Mediterranean Basin (MED). The results indicated that extensive drought events occurred more frequently since the late 1990s, showing several drought hotspots in the last decades in the southeastern Mediterranean and northwest Africa. In addition, the results showed that the most severe events were more detected when more than single drought index was used. The highest percentage area under drought was also observed through combining the variations of three drought indices. Furthermore, the drought area in both dry and humid areas in the MED has also experienced a remarkable increase since the late 1990s. Based on a comparison of the drought events during the two periods—1975–1996 and 1997–2019—we find that the current dry conditions in the MED are more severe, intense, and frequent than the earlier period; moreover, the strongest dry conditions occurred in last two decades. The SPEI-12 and RDI-12 have a higher capacity in providing a more comprehensive description of the dry conditions because of the inclusion of temperature or atmospheric evaporative demand in their scheme. A complex range of atmospheric circulation patterns, particularly the Western Mediterranean Oscillation (WeMO) and East Atlantic/West Russia (EATL/WRUS), appear to play an important role in severe, intense and region-wide droughts, including the two most severe droughts, 1999–2001 and 2007–2012, with lesser influence of the NAO, ULMO and SCAND.","tags":["climate change","drought event","mediterranean basin","meteorological drought","SPEI"],"title":"Mediterranean-Scale Drought: Regional Datasets for Exceptional Meteorological Drought Events during 1975-2019","type":"publication"},{"authors":["D Royé","A Tobias","A Figueiras","S Gestal","A Santurtun","C Iñiguez"],"categories":null,"content":"","date":1626480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626480000,"objectID":"6a7c8555cac8ad123801a79619ac00d8","permalink":"https://dominicroye.github.io/en/publication/2021-medical-prescriptions-environmental-research/","publishdate":"2021-07-17T00:00:00Z","relpermalink":"/en/publication/2021-medical-prescriptions-environmental-research/","section":"publication","summary":"Background: The increased risk of mortality during periods of high and low temperatures has been well established. However, most of the studies used daily counts of deaths or hospitalisations as health outcomes, although they are the ones at the top of the health impact pyramid reflecting only a limited proportion of patients with the most severe cases. Objectives: This study evaluates the relationship between short-term exposure to the daily mean temperature and medication prescribed for the respiratory system in five Spanish cities. Methods: We fitted time series regression models to cause-specific medical prescriptions, including different respiratory subgroups and age groups. We included a distributed lag non-linear model with lags up to 14 days for daily mean temperature. City-specific associations were summarised as overall-cumulative exposure-response curves. Results: We found a positive association between cause-specific medical prescriptions and daily mean temperature with a non-linear inverted J- or V-shaped relationship in most cities. Between 0.3% and 0.6% of all respiratory prescriptions were attributed to cold for Madrid, Zaragoza and Pamplona, while in cities with only cold effects the attributable fractions were estimated as 19.2% for Murcia and 13.5% for Santander. Heat effects in Madrid, Zaragoza and Pamplona showed higher fractions between 8.7% and 17.2%. The estimated costs are in general higher for heat effects, showing annual values ranging between €191,905 and €311,076 for heat per 100,000 persons. Conclusions: This study provides novel evidence of the effects of the thermal environment on the prescription of medication for respiratory disorders in Spain, showing that low and high temperatures lead to an increase in the number of such prescriptions. The consumption of medication can reflect exposure to the environment with a lesser degree of severity in terms of morbidity.","tags":["Spain","drugs","medical prescriptions","respiratory","temperature"],"title":"Temperature-related effects on respiratory medical prescriptions in Spain","type":"publication"},{"authors":["A Martí","D Royé"],"categories":null,"content":"","date":1626307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626307200,"objectID":"9bc813f85710711b29872462c9195db4","permalink":"https://dominicroye.github.io/en/publication/2021-hotnights-madrid-urbclim-geographicalia/","publishdate":"2021-07-15T00:00:00Z","relpermalink":"/en/publication/2021-hotnights-madrid-urbclim-geographicalia/","section":"publication","summary":"In this work, a new methodology is applied to study hot nights, also called tropical nights, in the metropolitan area of Madrid. To evaluate hot nights from a temporal and spatial perspective in which the population may be affected by thermal stress, high resolution gridded hourly temperature data were used. The use of two indicators obtained through hourly data, together with the climatic information provided by the UrbClim model, allowed to evaluate the thermal night characteristics of July between 2008 and 2017 at a detailed scale. Hence we were able to estimate precisely the risk to the well-being and health of the population. The results show great interurban variability in terms of intensity and duration of heat stress and a significant correlation between heat island intensities and excess heat. Likewise, a close relationship between the typologies of land uses and urban structures defined in the Urban Atlas and the indices of night heat excess has been established.","tags":["Madrid","hot nights","urban climate","heat stress","night temperature"],"title":"Intensity and duration of heat stress in summer in the urban area of Madrid","type":"publication"},{"authors":["Q Zhao","et al"],"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625961600,"objectID":"c50b219bb70615ee0dffebce44592b69","permalink":"https://dominicroye.github.io/en/publication/2021-gridded-mortality-head-cold-lancet-planetary-health/","publishdate":"2021-07-11T00:00:00Z","relpermalink":"/en/publication/2021-gridded-mortality-head-cold-lancet-planetary-health/","section":"publication","summary":"Background Exposure to cold or hot temperatures is associated with premature deaths. We aimed to evaluate the global, regional, and national mortality burden associated with non-optimal ambient temperatures. Methods In this modelling study, we collected time-series data on mortality and ambient temperatures from 750 locations in 43 countries and five meta-predictors at a grid size of 0·5° × 0·5° across the globe. A three-stage analysis strategy was used. First, the temperature–mortality association was fitted for each location by use of a time-series regression. Second, a multivariate meta-regression model was built between location-specific estimates and meta-predictors. Finally, the grid-specific temperature–mortality association between 2000 and 2019 was predicted by use of the fitted meta-regression and the grid-specific meta-predictors. Excess deaths due to non-optimal temperatures, the ratio between annual excess deaths and all deaths of a year (the excess death ratio), and the death rate per 100 000 residents were then calculated for each grid across the world. Grids were divided according to regional groupings of the UN Statistics Division. Findings Globally, 5 083 173 deaths (95% empirical CI [eCI] 4 087 967–5 965 520) were associated with non-optimal temperatures per year, accounting for 9·43% (95% eCI 7·58–11·07) of all deaths (8·52% [6·19–10·47] were cold-related and 0·91% [0·56–1·36] were heat-related). There were 74 temperature-related excess deaths per 100 000 residents (95% eCI 60–87). The mortality burden varied geographically. Of all excess deaths, 2 617 322 (51·49%) occurred in Asia. Eastern Europe had the highest heat-related excess death rate and Sub-Saharan Africa had the highest cold-related excess death rate. From 2000–03 to 2016–19, the global cold-related excess death ratio changed by −0·51 percentage points (95% eCI −0·61 to −0·42) and the global heat-related excess death ratio increased by 0·21 percentage points (0·13–0·31), leading to a net reduction in the overall ratio. The largest decline in overall excess death ratio occurred in South-eastern Asia, whereas excess death ratio fluctuated in Southern Asia and Europe. Interpretation Non-optimal temperatures are associated with a substantial mortality burden, which varies spatiotemporally. Our findings will benefit international, national, and local communities in developing preparedness and prevention strategies to reduce weather-related impacts immediately and under climate change scenarios.","tags":["mortality","global","regional","non-optimal temperatures","MCC Study"],"title":"Global, regional, and national burden of mortality associated with non-optimal ambient temperatures from 2000 to 2019: a three-stage modelling study","type":"publication"},{"authors":["D Royé","F Sera","A Tobías","R Lowe","A Gasparrini","M Pascal","F de'Donato","B Nunes","JP Teixeira"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"7ba7d1ca7996cbb04df8057cf7f5f489","permalink":"https://dominicroye.github.io/en/publication/2021-hotnights-europe-epidemiology/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/en/publication/2021-hotnights-europe-epidemiology/","section":"publication","summary":"Background: There is strong evidence concerning the impact of heat stress on mortality, particularly from high temperatures. However, few studies to our knowledge emphasize the importance of hot nights, which may prevent necessary nocturnal rest. Objectives: In this study, we use hot-night duration and excess to predict daily cause-specific mortality in summer, using multiple cities across Southern Europe. Methods: We fitted time series regression models to summer cause-specific mortality, including natural, respiratory, and cardiovascular causes, in 11 cities across four countries. We included a distributed lag non-linear model with lags up to 7 days for hot night duration and excess adjusted by daily mean temperature. We summarized city-specific associations as overall-cumulative exposure–response curves at the country level using meta-analysis. Results: We found positive but generally non-linear associations between relative risk of cause-specific mortality and duration and excess of hot nights. Relative risk of duration associated with nonaccidental mortality in Portugal was 1.29 (95% CI, 1.07 to 1.54); other associations were imprecise, but we also found positive city-specific estimates for Rome and Madrid. Risk of hot-night excess ranged from 1.12 (95% CI, 1.05 to 1.20) for France to 1.37 (95% CI, 1.26 to 1.48) for Portugal. Risk estimates for excess were consistently higher than for duration. Conclusions: This study provides new evidence that, over a wider range of locations, hot night indices are strongly associated with cause-specific deaths. Modeling the impact of thermal characteristics during summer nights on mortality could improve decisionmaking for preventive public health strategies.","tags":["mortality","hot nights","human health","climate change","tropical nights","MCC Study"],"title":"Effects of hot nights on mortality in Southern Europe","type":"publication"},{"authors":null,"categories":["gis","R","R:intermediate","visualization"],"content":"\rCartography firefly\rFirefly maps are promoted and described by\rJohn Nelson who published a post in 2016 about its characteristics. However, these types of maps are linked to ArcGIS, which has led me to try to recreate them in R. The recent ggplot2 extension ggshadow facilitates the creation of this cartographic style. It is characterized by three elements 1) a dark and unsaturated basemap (eg satellite imagery) 2) a masked vignette and highlighted area and 3) a single bright thematic layer. The essential are the colors and the brightness that is achieved with cold colors, usually neon colors. John Nelson explains more details in this post.\nWhat is the firefly style for? In the words of John Nelson: “the map style that captures our attention and dutifully honors the First Law of Geography”. John refers to what was said by Waldo Tobler\r“everything is related to everything else, but near things are more related than distant things” (Tobler 1970).\nIn this post we will visualize all earthquakes recorded in southwestern Europe with a magnitude greater than 3.\n\rPackages\rWe will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rterra\rImport, export and manipulate raster (raster successor package)\r\rraster\rImport, export and manipulate raster\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rggshadow\rggplot2 extension for shaded and glow geometries\r\rggspatial\rggplot2 extension for spatial objects\r\rggnewscale\rggplot2 extension to create multiple scales\r\rjanitor\rSimple functions to examine and clean data\r\rrnaturalearth\rVector maps of the world ‘Natural Earth’\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;terra\u0026quot;)) install.packages(\u0026quot;terra\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;ggshadow\u0026quot;)) install.packages(\u0026quot;ggshadow\u0026quot;)\rif(!require(\u0026quot;ggspatial\u0026quot;)) install.packages(\u0026quot;ggspatial\u0026quot;)\rif(!require(\u0026quot;ggnewscale\u0026quot;)) install.packages(\u0026quot;ggnewscale\u0026quot;)\rif(!require(\u0026quot;janitor\u0026quot;)) install.packages(\u0026quot;janitor\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r# load packages\rlibrary(raster)\rlibrary(terra)\rlibrary(sf)\rlibrary(tidyverse)\rlibrary(ggshadow)\rlibrary(ggspatial)\rlibrary(ggnewscale)\rlibrary(janitor)\rlibrary(rnaturalearth)\r\rPreparation\rData\rFirst we download all the necessary data. For the base map we will use the Blue Marble imagery via the access to worldview.earthdata.nasa.gov where I have downloaded a selection of the area of interest in geoTiff format with a resolution of 1 km. It is important to adjust the resolution to the necessary detail of the map.\n\rBlue Marble selection via worldview.earthdata.nasa.gov ( ~ 66 MB) download\rRecords of historical earthquakes in southwestern Europe from IGN download\r\r\rImport\rThe first thing we do is to import the RGB Blue Marble raster and the earthquake data. To import the raster I use the new package terra which is the successor of the raster package. You can find a recent comparison here. Not all packages are yet compatible with the new SpatRaster class, so we also need the raster package.\n# earthquakes\rearthquakes \u0026lt;- read.csv2(\u0026quot;catalogoComunSV_1621713848556.csv\u0026quot;)\rstr(earthquakes)\r## \u0026#39;data.frame\u0026#39;:\t149724 obs. of 10 variables:\r## $ Evento : chr \u0026quot; 33\u0026quot; \u0026quot; 34\u0026quot; \u0026quot; 35\u0026quot; \u0026quot; 36\u0026quot; ...\r## $ Fecha : chr \u0026quot; 02/03/1373\u0026quot; \u0026quot; 03/03/1373\u0026quot; \u0026quot; 08/03/1373\u0026quot; \u0026quot; 19/03/1373\u0026quot; ...\r## $ Hora : chr \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; \u0026quot; 00:00:00\u0026quot; ...\r## $ Latitud : chr \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; \u0026quot; 42.5000\u0026quot; ...\r## $ Longitud : chr \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; \u0026quot; 0.7500\u0026quot; ...\r## $ Prof...Km. : int NA NA NA NA NA NA NA NA NA NA ...\r## $ Inten. : chr \u0026quot; VIII-IX\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ...\r## $ Mag. : chr \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ...\r## $ Tipo.Mag. : int NA NA NA NA NA NA NA NA NA NA ...\r## $ Localización: chr \u0026quot;Ribagorça.L\u0026quot; \u0026quot;Ribagorça.L\u0026quot; \u0026quot;Ribagorça.L\u0026quot; \u0026quot;Ribagorça.L\u0026quot; ...\r# Blue Marble RGB raster\rbm \u0026lt;- rast(\u0026quot;snapshot-2017-11-30T00_00_00Z.tiff\u0026quot;)\rbm # contains three layers (red, green, blue)\r## class : SpatRaster ## dimensions : 7156, 7156, 3 (nrow, ncol, nlyr)\r## resolution : 0.008789272, 0.008789272 (x, y)\r## extent : -33.49823, 29.39781, 15.77547, 78.67151 (xmin, xmax, ymin, ymax)\r## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : snapshot-2017-11-30T00_00_00Z.tiff ## colors RGB : 1, 2, 3 ## names : snapshot-2~0_00_00Z_1, snapshot-2~0_00_00Z_2, snapshot-2~0_00_00Z_3\r# plot\rplotRGB(bm)\r# country boundaries\rlimits \u0026lt;- ne_countries(scale = 50, returnclass = \u0026quot;sf\u0026quot;)\r\rEarthquakes\rIn this step we clean the imported earthquakes data. 1) We convert longitude, latitude and magnitude into numeric using the parse_number() function and clean the column names with the clean_names() function, 2) We create a spatial object sf and project it using the EPSG:3035 corresponding to ETRS89-extended/LAEA Europe.\n# we clean the data and create an sf object\rearthquakes \u0026lt;- earthquakes %\u0026gt;% clean_names() %\u0026gt;%\rmutate(across(c(mag, latitud, longitud), parse_number)) %\u0026gt;%\rst_as_sf(coords = c(\u0026quot;longitud\u0026quot;, \u0026quot;latitud\u0026quot;), crs = 4326) %\u0026gt;% st_transform(3035) # project to Laea\r\rBlue Marble background Map\rWe cropped the background map to a smaller extent, but we still haven’t limited to the final area yet.\n# clip to the desired area\rbm \u0026lt;- crop(bm, extent(-20, 10, 30, 50)) # W, E, S, N\rTo obtain an unsaturated version of the Blue Marble RGB raster, we must apply a function created for this purpose. In this, we use the colorize(), which helps us converting RGB to HSL and vice versa. The HSL model is defined by Hue, Saturation, Lightness. The last two parameters are expressed in ratio or percentage. The hue is defined on a color wheel from 0 to 360º. 0 is red, 120 is green, 240 is blue. To change the saturation we only have to reduce the value of S.\n# apply the function to unsaturate with 5%\rbm_desat \u0026lt;- colorize(bm, to = \u0026quot;hsl\u0026quot;)\rbm_desat[[2]] \u0026lt;- .05 # ratio of saturation\rset.RGB(bm_desat, 1:3, \u0026quot;hsl\u0026quot;)\rbm_desat \u0026lt;- colorize(bm_desat, to = \u0026quot;rgb\u0026quot;)\r# plot new RGB image\rplotRGB(bm_desat)\r# project bm_desat \u0026lt;- terra::project(bm_desat, \u0026quot;epsg:3035\u0026quot;)\r\r\rFirefly map construction\rBoundaries and graticules\rBefore starting to build the map, we create graticules and set the final map limits.\n# define the final map extent\rbx \u0026lt;- tibble(x = c(-13, 6.7), y = c(31, 47)) %\u0026gt;% st_as_sf(coords = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), crs = 4326) %\u0026gt;%\rst_transform(3035) %\u0026gt;% st_bbox()\r# create map graticules\rgrid \u0026lt;- st_graticule(earthquakes) \r\rMap with image background\rThe layer_spatial() function of ggspatial allows us to add an RGB raster without major problems, however, it still does not support the newSpatRaster class. Therefore, we must convert it to the stack class with the stack() function. It is also possible to use instead of geom_sf(), the layer_spatial() function for vector objects of class sf orsp.\nggplot() +\rlayer_spatial(data = stack(bm_desat)) + # blue marble background map\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) + # country boundaries\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void()\r\rMap with background and earthquakes\rTo create the glow effect on firefly maps, we use the geom_glowpoint() function from the ggshadow package. There is also the same function for lines. Since our data is of spatial class sf and the geometry sf is not directly supported, we must indicate as an argument stats = \"sf_coordinates\" and inside aes() indicate geometry = geometry. We will map the size of the points as a function of magnitude. In addition, we filter those earthquakes with a magnitude greater than 3.\nInside the geom_glowpoint() function, 1) we define the desired color for the point and the glow effect, 2) the degree of transparency with alpha either for the point or for the glow. Finally, in the scale_size() function we set the range (minimum, maximum) of the size that the points will have.\nggplot() +\rlayer_spatial(data = stack(bm_desat)) +\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = grid, colour = \u0026quot;white\u0026quot;, size = .1, alpha = .5) +\rgeom_glowpoint(data = filter(earthquakes, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .8,\rcolor = \u0026quot;#6bb857\u0026quot;,\rshadowcolour = \u0026quot;#6bb857\u0026quot;,\rshadowalpha = .1,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.1, 1.5)) +\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void()\r\rFinal map\rThe glow effect of firefly maps is characterized by having a white tone or a lighter tone in the center of the points. To achieve this, we must duplicate the previous created layer, changing only the color and make the glow points smaller.\nBy default, ggplot2 does not allow to use multiple scales for the same characteristic (size, color, etc) of different layers. But the ggnewscale package gives us the ability to incorporate multiple scales of a feature from different layers. The only important thing to achieve this is the order in which each layer (geom) and scale is added. First we must add the geometry and then its corresponding scale. We indicate with new_scale('size') that the next layer and scale is a new one independent of the previous one. If we used color or fill it would be done with new_scale_*().\nggplot() +\rlayer_spatial(data = stack(bm_desat)) +\rgeom_sf(data = limits, fill = NA, size = .3, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = grid, colour = \u0026quot;white\u0026quot;, size = .1, alpha = .5) +\rgeom_glowpoint(data = filter(earthquakes, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .8,\rcolor = \u0026quot;#6bb857\u0026quot;,\rshadowcolour = \u0026quot;#6bb857\u0026quot;,\rshadowalpha = .1,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.1, 1.5)) +\rnew_scale(\u0026quot;size\u0026quot;) +\rgeom_glowpoint(data = filter(earthquakes, mag \u0026gt; 3),\raes(geometry = geometry, size = mag), alpha = .6,\rshadowalpha = .05,\rcolor = \u0026quot;#ffffff\u0026quot;,\rstat = \u0026quot;sf_coordinates\u0026quot;,\rshow.legend = FALSE) +\rscale_size(range = c(.01, .7)) +\rlabs(title = \u0026quot;EARTHQUAKES\u0026quot;) +\rcoord_sf(xlim = bx[c(1, 3)], ylim = bx[c(2, 4)], crs = 3035,\rexpand = FALSE) +\rtheme_void() +\rtheme(plot.title = element_text(size = 50, vjust = -5, colour = \u0026quot;white\u0026quot;, hjust = .95))\rggsave(\u0026quot;firefly_map.png\u0026quot;, width = 15, height = 15, units = \u0026quot;in\u0026quot;, dpi = 300)\r\n\r\r","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"4c44e65e044bb791d0ee04c00fda76af","permalink":"https://dominicroye.github.io/en/2021/firefly-cartography/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/en/2021/firefly-cartography/","section":"post","summary":"*Firefly* maps are promoted and described by [John Nelson](https://twitter.com/John_M_Nelson) who published a [post](https://adventuresinmapping.com/2016/10/17/firefly-cartography/) in 2016 about its characteristics. However, these types of maps are linked to ArcGIS, which has led me to try to recreate them in R.","tags":["firefly","map","earthquake","cartography"],"title":"Firefly cartography","type":"post"},{"authors":["A Vicedo-Cabrera","N Scovronick","F Sera","D Royé","et al"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"f1ad4e7e93503a88f012c2248be8f453","permalink":"https://dominicroye.github.io/en/publication/2021-attribution-mortality-nature-climatechange/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/en/publication/2021-attribution-mortality-nature-climatechange/","section":"publication","summary":"Climate change affects human health; however, there have been no large-scale, systematic efforts to quantify the heat-related human health impacts that have already occurred due to climate change. Here, we use empirical data from 732 locations in 43 countries to estimate the mortality burdens associated with the additional heat exposure that has resulted from recent human-induced warming, during the period 1991–2018. Across all study countries, we find that 37.0% (range 20.5–76.3%) of warm-season heat-related deaths can be attributed to anthropogenic climate change and that increased mortality is evident on every continent. Burdens varied geographically but were of the order of dozens to hundreds of deaths per year in many locations. Our findings support the urgent need for more ambitious mitigation and adaptation strategies to minimize the public health impacts of climate change.","tags":["mortality","human health","climate change","attribution","MCC Study"],"title":"The burden of heat-related mortality attributable to recent human-induced climate change","type":"publication"},{"authors":["E de Schrijver","CL Folly","R Schneider","D Royé","OH Franco","A Gasparrini","AM Vicedo-Cabrera"],"categories":null,"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"8908a93adb7726c9f15b6513a326eb41","permalink":"https://dominicroye.github.io/en/publication/2021-mortality-gridded-datasets-switzerland-geohealth/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/en/publication/2021-mortality-gridded-datasets-switzerland-geohealth/","section":"publication","summary":"New gridded climate datasets (GCDs) on spatially‐resolved modelled weather data have recently been released to explore the impacts of climate change. GCDs have been suggested as potential alternatives to weather station data in epidemiological assessments on health impacts of temperature and climate change. These can be particularly useful for assessment in regions that have remained understudied due to limited or low quality weather station data. However to date, no study has critically evaluated the application of GCDs of variable spatial resolution in temperature‐mortality assessments across regions of different orography, climate and size. Here we explored the performance of population‐weighted daily mean temperature data from the global ERA5 reanalysis dataset in the 10 regions in the United Kingdom and the 26 cantons in Switzerland, combined with two local high‐resolution GCDs (Haduk‐grid UKPOC‐9 and MeteoSwiss‐grid‐product, respectively) and compared these to weather station data and unweighted homologous series. We applied quasi‐Poisson time series regression with distributed lag non‐linear models to obtain the GCD‐ and region‐specific temperature‐mortality associations and calculated the corresponding cold‐ and heat‐related excess mortality. Although the five exposure datasets yielded different average area‐level temperature estimates, these deviations did not result in substantial variations in the temperature‐mortality association or impacts. Moreover, local population‐weighted GCDs showed better overall performance, suggesting that they could be excellent alternatives to help advance knowledge on climate change impacts in remote regions with large climate and population distribution variability, which has remained largely unexplored in present literature due to the lack of reliable exposure data.","tags":["gridded climate dataset","spatiotemporal analysis","reanalysis","heat","cold","mortality","climate change"],"title":"A Comparative Analysis of the Temperature‐Mortality Risks Using Different Weather Datasets Across Heterogeneous Regions","type":"publication"},{"authors":["N Lorenzo","A Díaz-Poso","D Royé"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"69f0f0f60f7732338b3e84373a13bbfb","permalink":"https://dominicroye.github.io/en/publication/2021-heat-wave-ehf-peninsula-atmospheric-research/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/en/publication/2021-heat-wave-ehf-peninsula-atmospheric-research/","section":"publication","summary":"Heatwaves are the most relevant extreme climatic events, particularly in the context of global warming and the related increasing impacts on society and the natural environment. This work presents an analysis of climate change scenarios with simulations from the EURO-CORDEX project using the excess heat factor over the Iberian Peninsula. We focus on climate change projections of the heatwave intensity and spatial distribution, which are evaluated for the near future (2021–2050) relative to a reference past climate (1971–2000). Heatwave projections show a general significant increase in intensity, frequency, duration and spatial extent for the whole region. The average change in heatwave intensity is 104% for the whole Iberian Peninsula for the near future 2021–2050. The largest changes occur in the eastern-central region, rising to 150% for the Mediterranean coast and the Pyrenees. The greater spatial extent of heatwaves strongly suggests increased human exposure, increased energy demand, and implications for fire risk. This spatial trend is predicted to continue in the near future with increases in the maximum spatial heatwave extent ranging from 6% to 8% per decade.","tags":["heat wave","intensity","iberian peninsula","spatial extension","climate change","future scenarios"],"title":"Heatwave intensity on the Iberian Peninsula: Future climate projections","type":"publication"},{"authors":["BR Wright","B Laffineur","D Royé","G Armstrong","RJ Fensham"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"44f54c4d8304ac129455ebd3eda87f28","permalink":"https://dominicroye.github.io/en/publication/2021-megafires-australia-frontiers-ecology/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/en/publication/2021-megafires-australia-frontiers-ecology/","section":"publication","summary":"Large, high-severity wildfires, or ‘megafires’, occur periodically in arid Australian spinifex (Triodia spp.) grasslands after high rainfall periods that trigger fuel accumulation. It has been suggested that these fires are unprecedented in the modern era and were formerly constrained by Aboriginal patch-burning that kept landscape fuel levels low. This assumption deserves scrutiny, as evidence from fire-prone systems globally indicates that weather factors often the primary determinant behind megafire incidence, and that fuel management does not mitigate such fires during periods of climatic extreme. We reviewed explorer’s diaries, anthropologist’s reports, and remotely sensed data from the Australian Western Desert for evidence of large rainfall linked fires during the pre-contact period when traditional Aboriginal patch-burning was still being practiced. We used only observations that contained empiric estimates of fire sizes. Concurrently, we employed remote rainfall data and the Oceanic Niño Index to relate fire size to likely seasonal conditions at the time the observations were made. Numerous records were found of small fires during periods of average and below-average rainfall conditions, but no evidence of large-scale fires during these times. By contrast, there was strong evidence of large-scale wildfires during a high-rainfall period in the early 1870s, some of which are estimated to have burnt areas up to 700 000 ha. Our literature review also identified several Western Desert Aboriginal mythologies that refer to large-scale conflagrations. As oral traditions sometimes corroborate historic events, these myths may add further evidence that large fires are an inherent feature of spinifex grassland fire regimes. Overall, the results suggest that, contrary to predictions of the patch-burn mosaic hypothesis, traditional Aboriginal burning did not modulate spinifex fire size during periods of extreme-high arid zone rainfall. The mechanism behind this appears to be plant assemblages in seral spinifex vegetation that comprise highly flammable non-spinifex tussock grasses that rapidly accumulate high fuel loads under favorable precipitation conditions. Our finding that fuel management does not prevent megafires under extreme conditions in arid Australia has parallels with the primacy of climatic factors as drivers of megafires in the forests of temperate Australia.","tags":["arid vegetation","fire ecology","grass-fire feedbacks","patch-burning","indigenous land management"],"title":"Rainfall-linked megafires as innate fire regime elements in arid Australian spinifex (Triodia spp.) grasslands","type":"publication"},{"authors":null,"categories":["gis","R","R:advanced","visualization"],"content":"\r\rInitial considerations\rA disadvantage of choropleth maps is that they tend to distort the relationship between the true underlying geography and the represented variable. It is because the administrative divisions do not usually coincide with the geographical reality where people live. Besides, large areas appear to have a weight that they do not really have because of sparsely populated regions. To better reflect reality, more realistic population distributions are used, such as land use. With Geographic Information Systems techniques, it is possible to redistribute the variable of interest as a function of a variable with a smaller spatial unit.\nWith point data, the redistribution process is simply clipping points with population based on land use, usually classified as urban. We could also crop and mask with land use polygons when we have a vectorial polygon layer, but an interesting alternative is the same data in raster format. We will see how we can make a dasymetric map using raster data with a resolution of 100 m. This post will use data from census sections of the median income and the Gini index for Spain. We will make a dasymetric and bivariate map, representing both variables with two ranges of colours on the same map.\n\rPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rpatchwork\rSimple grammar to combine separate ggplots into the same graphic\r\rraster\rImport, export and manipulate raster\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rbiscale\rTools and Palettes for Bivariate Thematic Mapping\r\rsysfonts\rLoad fonts in R\r\rshowtext\rUse fonts more easily in R graphs\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;patchwork\u0026quot;)) install.packages(\u0026quot;patchwork\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;biscale\u0026quot;)) install.packages(\u0026quot;biscale\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# packages\rlibrary(tidyverse)\rlibrary(sf)\rlibrary(readxl)\rlibrary(biscale)\rlibrary(patchwork)\rlibrary(raster)\rlibrary(sysfonts)\rlibrary(showtext)\rlibrary(raster)\r\rPreparation\rData\rFirst we download all the necessary data. With the exception of the CORINE Land Cover (~ 200 MB), the data stored on this blog can be obtained directly via the indicated links.\n\rCORINE Land Cover 2018 (geotiff): COPERNICUS\rIncome data and Gini index (excel) [INE]: download\rCensus limits of Spain (vectorial) [INE]: download\r\r\rImport\rThe first thing we do is to import the land use raster, the income and Gini index data, and the census boundaries.\n# raster of CORINE LAND COVER 2018\rurb \u0026lt;- raster(\u0026quot;U2018_CLC2018_V2020_20u1.tif\u0026quot;)\r# income data and Gini index\rrenta \u0026lt;- read_excel(\u0026quot;30824.xlsx\u0026quot;)\rgini \u0026lt;- read_excel(\u0026quot;37677.xlsx\u0026quot;)\r# census boundaries\rlimits \u0026lt;- read_sf(\u0026quot;SECC_CE_20200101.shp\u0026quot;) \r\rLand uses\rIn this first step we filter the census sections to obtain those of the Autonomous Community of Madrid, and we create the municipal limits. To dissolve the polygons of census tracts we apply the function group_by() in combination with summarise().\n# filter the Autonomous Community of Madrid\rlimits \u0026lt;- filter(limits, NCA == \u0026quot;Comunidad de Madrid\u0026quot;)\r# obtain the municipal limits\rmun_limit \u0026lt;- group_by(limits, CUMUN) %\u0026gt;% summarise()\rIn the next step we cut the land use raster with the limits of Madrid. I recommend always using the crop() function first and then mask(), the first function crop to the required extent and the second mask the values. Subsequently, we remove all the cells that correspond to 1 or 2 (urban continuous, discontinuous). Finally, we project the raster.\n# project the limits\rlimits_prj \u0026lt;- st_transform(limits, projection(urb))\r# crop and mask urb_mad \u0026lt;- crop(urb, limits_prj) %\u0026gt;% mask(limits_prj)\r# remove non-urban pixels\rurb_mad[!urb_mad %in% 1:2] \u0026lt;- NA # plot the raster\rplot(urb_mad)\r# project\rurb_mad \u0026lt;- projectRaster(urb_mad, crs = CRS(\u0026quot;+proj=longlat +datum=WGS84 +no_defs\u0026quot;))\rIn this step, we convert the raster data into a point sf object.\n# transform the raster to xyz and a sf object\rurb_mad \u0026lt;- as.data.frame(urb_mad, xy = TRUE, na.rm = TRUE) %\u0026gt;%\rst_as_sf(coords = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), crs = 4326)\r# add the columns of the coordinates\rurb_mad \u0026lt;- urb_mad %\u0026gt;% rename(urb = 1) %\u0026gt;% cbind(st_coordinates(urb_mad))\r\rIncome data and Gini index\rThe format of the Excels does not coincide with the original of the INE, since I have cleaned the format before in order to make this post easier. What remains is to create a column with the codes of the census sections and exclude data that correspond to another administrative level.\n## income and Gini index data\rrenta_sec \u0026lt;- mutate(renta, NATCODE = str_extract(CUSEC, \u0026quot;[0-9]{5,10}\u0026quot;), nc_len = str_length(NATCODE),\rmun_name = str_remove(CUSEC, NATCODE) %\u0026gt;% str_trim()) %\u0026gt;%\rfilter(nc_len \u0026gt; 5)\rgini_sec \u0026lt;- mutate(gini, NATCODE = str_extract(CUSEC, \u0026quot;[0-9]{5,10}\u0026quot;), nc_len = str_length(NATCODE),\rmun_name = str_remove(CUSEC, NATCODE) %\u0026gt;% str_trim()) %\u0026gt;%\rfilter(nc_len \u0026gt; 5)\rIn the next step we join both tables with the census tracts using left_join() and convert columns of interest in numerical mode.\n# join both the income and Gini tables with the census limits\rmad \u0026lt;- left_join(limits, renta_sec, by = c(\u0026quot;CUSEC\u0026quot;=\u0026quot;NATCODE\u0026quot;)) %\u0026gt;% left_join(gini_sec, by = c(\u0026quot;CUSEC\u0026quot;=\u0026quot;NATCODE\u0026quot;))\r# convert selected columns to numeric\rmad \u0026lt;- mutate_at(mad, c(23:27, 30:31), as.numeric)\r\rBivariate variable\rTo create a bivariate map we must construct a single variable that combines different classes of two variables. Usually we make three classes of each variable which leads to nine combinations; in our case, the average income and the Gini index. The biscale package includes helper functions to carry out this process. With the bi_class() function we create the classification variable using quantiles as algorithm. Since in both variables we find missing values, we correct those combinations between both variables where an NA appears.\n# create bivariate classification\rmapbivar \u0026lt;- bi_class(mad, GINI_2017, RNMP_2017, style = \u0026quot;quantile\u0026quot;, dim = 3) %\u0026gt;% mutate(bi_class = ifelse(str_detect(bi_class, \u0026quot;NA\u0026quot;), NA, bi_class))\r# results\rhead(dplyr::select(mapbivar, GINI_2017, RNMP_2017, bi_class))\r## Simple feature collection with 6 features and 3 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: 415538.9 ymin: 4451487 xmax: 469341.7 ymax: 4552422\r## Projected CRS: ETRS89 / UTM zone 30N\r## # A tibble: 6 x 4\r## GINI_2017 RNMP_2017 bi_class geometry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;MULTIPOLYGON [m]\u0026gt;\r## 1 NA NA \u0026lt;NA\u0026gt; (((446007.9 4552348, 446133.7 4552288, 446207.8 ~\r## 2 31 13581 2-2 (((460243.8 4487756, 460322.4 4487739, 460279 44~\r## 3 30 12407 2-2 (((457392.5 4486262, 457391.6 4486269, 457391.1 ~\r## 4 34.3 13779 3-2 (((468720.8 4481374, 468695.5 4481361, 468664.6 ~\r## 5 33.5 9176 3-1 (((417140.2 4451736, 416867.5 4451737, 416436.8 ~\r## 6 26.2 10879 1-1 (((469251.9 4480826, 469268.1 4480797, 469292.6 ~\rWe finish by redistributing the inequality variable over the pixels of urban land use. The st_join() function joins the data with the land use points.\n# redistribute urban pixels to inequality\rmapdasi \u0026lt;- st_join(urb_mad, st_transform(mapbivar, 4326))\r\r\rMap building\rLegend and font\rBefore constructing both maps we must create the legend using the bi_legend() function. In the function we define the titles for each variable, the number of dimensions and the color scale. Finally, we add the Montserrat font for the final titles in the graphic.\n# bivariate legend\rlegend2 \u0026lt;- bi_legend(pal = \u0026quot;DkViolet\u0026quot;,\rdim = 3,\rxlab = \u0026quot;Higher inequality\u0026quot;,\rylab = \u0026quot;Higher income\u0026quot;,\rsize = 9)\r# download font\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\rshowtext_auto()\r\rDasymetric map\rWe build this map using geom_tile() for the pixels and geom_sf() for the municipal boundaries. In addition, it will be the map on the right where we also place the legend. To add the legend we use the annotation_custom() function indicating the position in the geographical coordinates of the map. The biscale package also helps us with the color definition via the bi_scale_fill() function.\np2 \u0026lt;- ggplot(mapdasi) + geom_tile(aes(X, Y, fill = bi_class), show.legend = FALSE) +\rgeom_sf(data = mun_limit, color = \u0026quot;grey80\u0026quot;, fill = NA, size = 0.2) +\rannotation_custom(ggplotGrob(legend2), xmin = -3.25, xmax = -2.65,\rymin = 40.55, ymax = 40.95) +\rbi_scale_fill(pal = \u0026quot;DkViolet\u0026quot;, dim = 3, na.value = \u0026quot;grey90\u0026quot;) +\rlabs(title = \u0026quot;dasymetric\u0026quot;, x = \u0026quot;\u0026quot;, y =\u0026quot;\u0026quot;) +\rbi_theme() +\rtheme(plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 30, face = \u0026quot;bold\u0026quot;)) +\rcoord_sf(crs = 4326)\r\rChoropleth map\rThe choropleth map is built in a similar way to the previous map with the difference that we use geom_sf().\np1 \u0026lt;- ggplot(mapbivar) + geom_sf(aes(fill = bi_class), colour = NA, size = .1, show.legend = FALSE) +\rgeom_sf(data = mun_limit, color = \u0026quot;white\u0026quot;, fill = NA, size = 0.2) +\rbi_scale_fill(pal = \u0026quot;DkViolet\u0026quot;, dim = 3, na.value = \u0026quot;grey90\u0026quot;) +\rlabs(title = \u0026quot;choropleth\u0026quot;, x = \u0026quot;\u0026quot;, y =\u0026quot;\u0026quot;) +\rbi_theme() +\rtheme(plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 30, face = \u0026quot;bold\u0026quot;)) +\rcoord_sf(crs = 4326)\r\rMerge both maps\rWith the help of the patchwork package, we combine both maps in a single row, first the choropleth map and on its right the dasymmetric map. More details of the grammar used for the combination of graphics here.\n# combine p \u0026lt;- p1 | p2\r# final map\rp\r\n\r\r","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"158a61691cc8516959fe965538fcd83d","permalink":"https://dominicroye.github.io/en/2021/bivariate-dasymetric-map/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/en/2021/bivariate-dasymetric-map/","section":"post","summary":"A disadvantage of choropleth maps is that they tend to distort the relationship between the true underlying geography and the represented variable. It is because the administrative divisions do not usually coincide with the geographical reality where people live. Besides, large areas appear to have a weight that they do not really have because of sparsely populated regions. To better reflect reality, more realistic population distributions are used, such as land use. With Geographic Information Systems techniques, it is possible to redistribute the variable of interest as a function of a variable with a smaller spatial unit.","tags":["bivariate","map","inequality","income","Madrid","urban"],"title":"Bivariate dasymetric map","type":"post"},{"authors":["P Fdez-Arroyabe","et al."],"categories":null,"content":"","date":1609977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609977600,"objectID":"c43c3d47ab5228aa3fec38fcdeb70898","permalink":"https://dominicroye.github.io/en/publication/2020-glossary-electricity-ij-biometeo/","publishdate":"2021-01-07T00:00:00Z","relpermalink":"/en/publication/2020-glossary-electricity-ij-biometeo/","section":"publication","summary":"There is an increasing interest to study the interactions between atmospheric electrical parameters and living organisms at multiple scales. So far, relatively few studies have been published that focus on possible biological effects of atmospheric electric and magnetic fields. To foster future work in this area of multidisciplinary research, here we present a glossary of relevant terms. Its main purpose is to facilitate the process of learning and communication among the different scientific disciplines working on this topic. While some definitions come from existing sources, other concepts have been re-defined to better reflect the existing and emerging scientific needs of this multidisciplinary and transdisciplinary area of research.","tags":["atmospheric electricity phenomena","atmospheric electric field","biological effects","biometeorological profile","glossary"],"title":"Glossary on atmospheric electricity and its effects on biology","type":"publication"},{"authors":null,"categories":["visualization","R","R:intermediate"],"content":"\r\rRecently I was looking for a visual representation to show the daily changes of temperature, precipitation and wind in an application xeo81.shinyapps.io/MeteoExtremosGalicia (in Spanish), which led me to use a heatmap in the form of a calendar. The shiny application is updated every four hours with new data showing calendars for each weather station. The heatmap as a calendar allows you to visualize any variable with a daily time reference.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rlubridate\rEasy manipulation of dates and times\r\rragg\rragg provides a set of high quality and high performance raster devices\r\r\r\r# instalamos los paquetes si hace falta\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ragg\u0026quot;)) install.packages(\u0026quot;ragg\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r# paquetes\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ragg)\rFor those with less experience with tidyverse, I recommend the short introduction on this blog post.\n\rData\rIn this example we will use the daily precipitation of Santiago de Compostela for this year 2020 (until December 20) download.\n# import the data\rdat_pr \u0026lt;- read_csv(\u0026quot;precipitation_santiago.csv\u0026quot;)\rdat_pr\r## # A tibble: 355 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-01-01 0 ## 2 2020-01-02 0 ## 3 2020-01-03 5.4\r## 4 2020-01-04 0 ## 5 2020-01-05 0 ## 6 2020-01-06 0 ## 7 2020-01-07 0 ## 8 2020-01-08 1 ## 9 2020-01-09 3.8\r## 10 2020-01-10 0 ## # ... with 345 more rows\r\rPreparation\rIn the first step we must 1) complement the time series from December 21 to December 31 with NA, 2) add the day of the week, the month, the week number and the day. Depending on whether we want each week to start on Sunday or Monday, we indicate it in the wday() function.\ndat_pr \u0026lt;- dat_pr %\u0026gt;% complete(date = seq(ymd(\u0026quot;2020-01-01\u0026quot;), ymd(\u0026quot;2020-12-31\u0026quot;), \u0026quot;day\u0026quot;)) %\u0026gt;%\rmutate(weekday = wday(date, label = T, week_start = 1), month = month(date, label = T, abbr = F),\rweek = isoweek(date),\rday = day(date))\rIn the next step we need to make a change in the week of the year, which is because in certain years there may be, for example, a few days at the end of the year as the first week of the following year. We also create two new columns. On the one hand, we categorize precipitation into 14 classes and on the other, we define a white text color for darker tones in the heatmap.\ndat_pr \u0026lt;- mutate(dat_pr, week = case_when(month == \u0026quot;December\u0026quot; \u0026amp; week == 1 ~ 53,\rmonth == \u0026quot;January\u0026quot; \u0026amp; week %in% 52:53 ~ 0,\rTRUE ~ week),\rpcat = cut(pr, c(-1, 0, .5, 1:5, 7, 9, 15, 20, 25, 30, 300)),\rtext_col = ifelse(pcat %in% c(\u0026quot;(15,20]\u0026quot;, \u0026quot;(20,25]\u0026quot;, \u0026quot;(25,30]\u0026quot;, \u0026quot;(30,300]\u0026quot;), \u0026quot;white\u0026quot;, \u0026quot;black\u0026quot;)) dat_pr \r## # A tibble: 366 x 8\r## date pr weekday month week day pcat text_col\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; ## 1 2020-01-01 0 Wed January 1 1 (-1,0] black ## 2 2020-01-02 0 Thu January 1 2 (-1,0] black ## 3 2020-01-03 5.4 Fri January 1 3 (5,7] black ## 4 2020-01-04 0 Sat January 1 4 (-1,0] black ## 5 2020-01-05 0 Sun January 1 5 (-1,0] black ## 6 2020-01-06 0 Mon January 2 6 (-1,0] black ## 7 2020-01-07 0 Tue January 2 7 (-1,0] black ## 8 2020-01-08 1 Wed January 2 8 (0.5,1] black ## 9 2020-01-09 3.8 Thu January 2 9 (3,4] black ## 10 2020-01-10 0 Fri January 2 10 (-1,0] black ## # ... with 356 more rows\r\rVisualization\rFirst we create a color ramp from Brewer colors.\n# color ramp\rpubu \u0026lt;- RColorBrewer::brewer.pal(9, \u0026quot;PuBu\u0026quot;)\rcol_p \u0026lt;- colorRampPalette(pubu)\rSecond, before building the chart, we define a custom theme as a function. To do this, we specify all the elements and their modifications with the help of the theme() function.\ntheme_calendar \u0026lt;- function(){\rtheme(aspect.ratio = 1/2,\raxis.title = element_blank(),\raxis.ticks = element_blank(),\raxis.text.y = element_blank(),\raxis.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rpanel.grid = element_blank(),\rpanel.background = element_blank(),\rstrip.background = element_blank(),\rstrip.text = element_text(family = \u0026quot;Montserrat\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 15),\rlegend.position = \u0026quot;top\u0026quot;,\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5),\rlegend.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 9, hjust = 1),\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = 1, size = 8),\rpanel.border = element_rect(colour = \u0026quot;grey\u0026quot;, fill=NA, size=1),\rplot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5, size = 26, face = \u0026quot;bold\u0026quot;, margin = margin(0,0,0.5,0, unit = \u0026quot;cm\u0026quot;)),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, hjust = .5, size = 16)\r)\r}\rFinally, we build the final chart using geom_tile() and specify the day of the week as the X axis and the week number as the Y axis. As you can see in the variable of the week number (-week), I change the sign so that the first day of each month is in the first row. With geom_text() we add the number of each day with its color according to what we defined previously. In guides we make the adjustments of the colorbar and in scale_fill/colour_manual() we define the corresponding colors. An important step is found in facet_wrap() where we specify the facets composition of each month. The facets should have free scales and the ideal would be a 4 x 3 facet distribution. It is possible to modify the position of the day number to another using the arguments nudge_* in geom_text() (eg bottom-right corner: nudge_x = .35, nudge_y = -.25).\n ggplot(dat_pr, aes(weekday, -week, fill = pcat)) +\rgeom_tile(colour = \u0026quot;white\u0026quot;, size = .4) + geom_text(aes(label = day, colour = text_col), size = 2.5) +\rguides(fill = guide_colorsteps(barwidth = 25, barheight = .4,\rtitle.position = \u0026quot;top\u0026quot;)) +\rscale_fill_manual(values = c(\u0026quot;white\u0026quot;, col_p(13)),\rna.value = \u0026quot;grey90\u0026quot;, drop = FALSE) +\rscale_colour_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;), guide = FALSE) + facet_wrap(~ month, nrow = 4, ncol = 3, scales = \u0026quot;free\u0026quot;) +\rlabs(title = \u0026quot;How is 2020 being in Santiago?\u0026quot;, subtitle = \u0026quot;Precipitation\u0026quot;,\rcaption = \u0026quot;Data: Meteogalicia\u0026quot;,\rfill = \u0026quot;mm\u0026quot;) +\rtheme_calendar()\rTo export we will use the ragg package, which provides higher performance and quality than the standard raster devices provided by grDevices.\nggsave(\u0026quot;pr_calendar.png\u0026quot;, height = 10, width = 8, device = agg_png())\rIn other heatmap calendars I have added the predominant wind direction of each day as an arrow using geom_arrow() from the metR package (it can be seen in the aforementioned application).\n\n\r","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"dbc17d19fd9e6d941106520f8a60f37c","permalink":"https://dominicroye.github.io/en/2020/a-heatmap-as-calendar/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/en/2020/a-heatmap-as-calendar/","section":"post","summary":"Recently I was looking for a visual representation to show the daily changes of temperature, precipitation and wind in an application [xeo81.shinyapps.io/MeteoExtremosGalicia](https://xeo81.shinyapps.io/MeteoExtremosGalicia/) (in Spanish), which led me to use a heatmap in the form of a calendar. The [shiny](https://shiny.rstudio.com/) application is updated every four hours with new data showing calendars for each weather station.","tags":["calendar","temperature","climate","heatmap"],"title":"A heatmap as calendar","type":"post"},{"authors":["C Iñiguez","D Royé","A Tobías"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"27e4ca05b49a88768f6b8aeb6904c20c","permalink":"https://dominicroye.github.io/en/publication/2021-morbi-mortality-spain-environmental-research/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/en/publication/2021-morbi-mortality-spain-environmental-research/","section":"publication","summary":"Background: Climate change is a severe public health challenge. Understanding to what extent fatal and non-fatal consequences of specific diseases are associated with temperature may help to improve the effectiveness of preventive public health efforts. This study examines the effects of temperature on deaths and hospital admissions by cardiovascular and respiratory diseases, empathizing the difference between mortality and morbidity. Methods: Daily counts for mortality and hospital admissions by cardiovascular and respiratory diseases were collected for the 52 provincial capital cities in Spain, between 1990 and 2014. The association with temperature in each city was investigated by means of distributed lag non-linear models using quasiPoisson regression. City-specific exposure-response curves were pooled by multivariate random-effects meta-analysis to obtain countrywide risk estimates of mortality and hospitalizations due to heat and cold, and attributable fractions were computed. Results: Heat and cold exposure were identified to be associated with increased risk of cardiovascular and respiratory mortality. Heat was not found to have an impact on hospital admissions. The estimated fraction of mortality attributable to cold was of greater magnitude in hospitalizations (17.5% for cardiovascular and 12.5% for respiratory diseases) compared to deaths (9% and 2.7%, respectively). Conclusions: There were noteworthy differences between temperature-related mortality and hospital admissions regarding cardiovascular and respiratory diseases, hence reinforcing the convenience of cause-specific measures to prevent temperature-related deaths.","tags":["temperature","mortality","hospital admissions","cardiovascular","respiratory","Spain","distributed lag non-linear models"],"title":"Contrasting patterns of temperature related mortality and hospitalization by cardiovascular and respiratory diseases in 52 Spanish cities","type":"publication"},{"authors":["S Gestal","D Royé","L Sánchez Santos","A Figueiras"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"3470b5a9635dbb70b0a9278f6afe7308","permalink":"https://dominicroye.github.io/en/publication/2020-emergency-calls-ij-public-health/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/en/publication/2020-emergency-calls-ij-public-health/","section":"publication","summary":"Introduction and objectives. The increase in mortality and hospital admissions associated with high and low temperatures is well established. However, less is known about the influence of extreme ambient temperature conditions on cardiovascular ambulance dispatches. This study seeks to evaluate the effects of minimum and maximum daily temperatures on cardiovascular morbidity in the cities of Vigo and A Coruña in North-West Spain, using emergency medical calls during the period 2005–2017. Methods. For the purposes of analysis, we employed a quasi-Poisson time series regression model, within a distributed non-linear lag model by exposure variable and city. The relative risks of cold- and heat-related calls were estimated for each city and temperature model. Results. A total of 70,537 calls were evaluated, most of which were associated with low maximum and minimum temperatures on cold days in both cities. At maximum temperatures, significant cold-related effects were observed at lags of 3–6 days in Vigo and 5–11 days in A Coruña. At minimum temperatures, cold-related effects registered a similar pattern in both cities, with significant relative risks at lags of 4 to 12 days in A Coruña. Heat-related effects did not display a clearly significant pattern. Conclusions. An increase in cardiovascular morbidity is observed with moderately low temperatures without extremes being required to establish an effect. Public health prevention plans and warning systems should consider including moderate temperature range in the prevention of cardiovascular morbidity.","tags":["emergency calls","extreme temperatures","cardiovascular","Galicia","Spain","distributed non-linear lag models","morbidity"],"title":"Impact of Extreme Temperatures on Ambulance Dispatches Due to Cardiovascular Causes in North-West Spain","type":"publication"},{"authors":null,"categories":["visualization","R","R:advanced"],"content":"\r\rIn the field of data visualization, the animation of spatial data in its temporal dimension can show fascinating changes and patterns. As a result of one of the last publications in the social networks that I have made, I was asked to make a post about how I created it. Well, here we go to start with an example of data from mainland Spain. You can find more animations in the graphics section of my blog.\nI couldn\u0026#39;t resist to make another animation. Smoothed daily maximum temperature throughout the year in Europe. #rstats #ggplot2 #dataviz #climate pic.twitter.com/ZC9L0vh3vR\n\u0026mdash; Dr. Dominic Royé (@dr_xeo) May 9, 2020  Packages\rIn this post we will use the following packages:\n\r\r\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rrnaturalearth\rVector maps of the world ‘Natural Earth’\r\rlubridate\rEasy manipulation of dates and times\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rraster\rImport, export and manipulate raster\r\rggthemes\rThemes for ggplot2\r\rgifski\rCreate gifs\r\rshowtext\rUse fonts more easily in R graphs\r\rsysfonts\rLoad fonts in R\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;ggthemes\u0026quot;)\rif(!require(\u0026quot;gifski\u0026quot;)) install.packages(\u0026quot;gifski\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# packages\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggthemes)\rlibrary(sf)\rlibrary(rnaturalearth)\rlibrary(extrafont)\rlibrary(showtext)\rlibrary(RColorBrewer)\rlibrary(gifski)\rFor those with less experience with tidyverse, I recommend the short introduction on this blog (post).\n\rPreparation\rData\rFirst, we need to download the STEAD dataset of the maximum temperature (tmax_pen.nc) in netCDF format from the CSIC repository here (the size of the data is 2 GB). It is a set of data with a spatial resolution of 5 km and includes daily maximum temperatures from 1901 to 2014. In climatology and meteorology, a widely used format is that of netCDF databases, which allow to obtain a multidimensional structure and to exchange data independently of the usued operating system. It is a space-time format with a regular or irregular grid. The multidimensional structure in the form of arrays or cubes can handle not only spatio-temporal data but also multivariate ones. In our dataset we will have an array of three dimensions: longitude, latitude and time of the maximum temperature.\nRoyé 2015. Sémata: Ciencias Sociais e Humanidades 27:11-37\n\r\rImport the dataset\rThe netCDF format with .nc extension can be imported via two main packages: 1) ncdf4 and 2) raster. Actually, the raster package use the first package to import the netCDF datasets. In this post we will use the raster package since it is somewhat easier, with some very useful and more universal functions for all types of raster format. The main import functions are: raster(), stack() and brick(). The first function only allows you to import a single layer, instead, the last two functions are used for multidimensional data. In our dataset we only have one variable, therefore it would not be necessary to use the varname argument.\n# import netCDF data\rtmx \u0026lt;- brick(\u0026quot;tmax_pen.nc\u0026quot;, varname = \u0026quot;tx\u0026quot;)\r## Loading required namespace: ncdf4\rtmx # metadata\r## class : RasterBrick ## dimensions : 190, 230, 43700, 41638 (nrow, ncol, ncell, nlayers)\r## resolution : 0.0585, 0.045 (x, y)\r## extent : -9.701833, 3.753167, 35.64247, 44.19247 (xmin, xmax, ymin, ymax)\r## crs : +proj=longlat +datum=WGS84 +no_defs ## source : tmax_pen.nc ## names : X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ... ## Time (days since 1901-01-01): 1, 41638 (min, max)\r## varname : tx\rThe RasterBrick object details show you all the necessary metadata: the resolution, the dimensions or the type of projection, or the name of the variable. It also tells us that it only points to the data source and has not imported them into the memory, which makes it easier to work with large datasets.\nTo access any layer we use [[ ]] with the corresponding index. So we can easily plot any day of the 41,638 days we have.\n# map any day\rplot(tmx[[200]], col = rev(heat.colors(7)))\r\rCalculate the average temperature\rIn this step the objective is to calculate the average maximum temperature for each day of the year. Therefore, the first thing we do is to create a vector, indicating the day of the year for the entire time series. In the raster package we have the stackApply() function that allows us to apply another function on groups of layers, or rather, indexes. Since our dataset is large, we include this function in parallelization functions.\nFor the parallelization we start and end always with the beginClusterr() and endCluster(). In the first function we must indicate the number of cores we want to use. In this case, I use 4 of 7 possible cores, however, the number must be changed according to the characteristics of each CPU, the general rule is n-1. So the clusterR function execute a function in parallel with multiple cores. The first argument corresponds to the raster object, the second to the used function, and as list argument we pass the arguments of the stackApply() function: the indexes that create the groups and the function used for each of the groups. Adding the argument progress = 'text' shows a progress bar of the calculation process.\n For the US dataset I did the preprocessing, the calculation of the average, in a cloud computing platform through Google Earth Engine, which makes the whole process faster. In the case of Australia the preprocessing was more complex as the dataset is separated in multiple netCDF files for each year.   # convert the dates between 1901 and 2014 to days of the year\rtime_days \u0026lt;- yday(seq(as_date(\u0026quot;1901-01-01\u0026quot;), as_date(\u0026quot;2014-12-31\u0026quot;), \u0026quot;day\u0026quot;))\r# calculate the average\rbeginCluster(4)\rtmx_mean \u0026lt;- clusterR(tmx, stackApply, args = list(indices = time_days, fun = mean))\rendCluster()\r\rSmooth the temperature variability\rBefore we start to smooth the time series of our RasterBrick, an example of why we do it. We extract a pixel from our dataset at coordinates -1º of longitude and 40º of latitude using the extract() function. Since the function with the same name appears in several packages, we must change to the form package_name::function_name. The result is a matrix with a single row corresponding to the pixel and 366 columns of the days of the year. The next step is to create a data.frame with a dummy date and the extracted maximum temperature.\n# extract a pixel\rpoint_ts \u0026lt;- raster::extract(tmx_mean, matrix(c(-1, 40), nrow = 1))\rdim(point_ts) # dimensions\r## [1] 1 366\r# create a data.frame\rdf \u0026lt;- data.frame(date = seq(as_date(\u0026quot;2000-01-01\u0026quot;), as_date(\u0026quot;2000-12-31\u0026quot;), \u0026quot;day\u0026quot;),\rtmx = point_ts[1,])\r# visualize the maximum temperature\rggplot(df, aes(date, tmx)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rlabs(y = \u0026quot;maximum temperature\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rThe graph clearly shows the still existing variability, which would cause an animation to fluctuate quite a bit. Therefore, we create a smoothing function based on a local polynomial regression fit (LOESS), more details can be found in the help of the loess() function. The most important argument is span, which determines the degree of smoothing, the smaller the value the less smooth the curve will be. I found the best result showed a value of 0.5.\ndaily_smooth \u0026lt;- function(x, span = 0.5){\rif(all(is.na(x))){\rreturn(x) } else {\rdf \u0026lt;- data.frame(yd = 1:366, ta = x)\rm \u0026lt;- loess(ta ~ yd, span = span, data = df)\rest \u0026lt;- predict(m, 1:366)\rreturn(est)\r}\r}\rWe apply our new smoothing function to the extracted time series and make some changes to be able to visualize the difference between the original and smoothed data.\n# smooth the temperature\rdf \u0026lt;- mutate(df, tmx_smoothed = daily_smooth(tmx)) %\u0026gt;% pivot_longer(2:3, names_to = \u0026quot;var\u0026quot;, values_to = \u0026quot;temp\u0026quot;)\r# visualize the difference\rggplot(df, aes(date, temp, colour = var)) + geom_line() + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(5, 28, 2)) +\rscale_colour_manual(values = c(\u0026quot;#f4a582\u0026quot;, \u0026quot;#b2182b\u0026quot;)) +\rlabs(y = \u0026quot;maximum temperature\u0026quot;, x = \u0026quot;\u0026quot;, colour = \u0026quot;\u0026quot;) +\rtheme_minimal()\rAs we see in the graph, the smoothed curve follows the original curve very well. In the next step we apply our function to the RasterBrick with the calc() function. The function returns as many layers as those returned by the function used for each of the time series.\n# smooth the RasterBrick\rtmx_smooth \u0026lt;- calc(tmx_mean, fun = daily_smooth)\r\r\rVisualization\rPreparation\rTo visualize the maximum temperatures throughout the year, first, we convert the RasterBrick to a data.frame, including longitude and latitude, but removing all time series without values (NA).\n# convert to data.frame\rtmx_mat \u0026lt;- as.data.frame(tmx_smooth, xy = TRUE, na.rm = TRUE)\r# rename the columns tmx_mat \u0026lt;- set_names(tmx_mat, c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, str_c(\u0026quot;D\u0026quot;, 1:366)))\rstr(tmx_mat[, 1:10])\r## \u0026#39;data.frame\u0026#39;: 20676 obs. of 10 variables:\r## $ lon: num -8.03 -7.98 -7.92 -7.86 -7.8 ...\r## $ lat: num 43.8 43.8 43.8 43.8 43.8 ...\r## $ D1 : num 10.5 10.3 10 10.9 11.5 ...\r## $ D2 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D3 : num 10.5 10.3 10.1 10.9 11.5 ...\r## $ D4 : num 10.6 10.4 10.1 10.9 11.5 ...\r## $ D5 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D6 : num 10.6 10.4 10.1 11 11.6 ...\r## $ D7 : num 10.6 10.4 10.2 11 11.6 ...\r## $ D8 : num 10.6 10.4 10.2 11 11.6 ...\rSecond, we import the administrative boundaries with the ne_countries() function from the rnaturalearth package, limiting the extension to the region of the Iberian Peninsula, southern France and northern Africa.\n# import global boundaries\rmap \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;) %\u0026gt;% st_cast(\u0026quot;MULTILINESTRING\u0026quot;)\r# limit the extension\rmap \u0026lt;- st_crop(map, xmin = -10, xmax = 5, ymin = 35, ymax = 44) \r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r# map of boundaries\rplot(map)\r## Warning: plotting the first 9 out of 94 attributes; use max.plot = 94 to plot\r## all\rThird, we create a vector with the day of the year as labels in order to include them later in the animation. In addition, we define the break points for the maximum temperature, adapted to the distribution of our data, to obtain a categorization with a total of 20 classes.\nFourth, we apply the cut() function with the breaks to all the columns with temperature data of each day of the year.\n# labels of day of the year\rlab \u0026lt;- as_date(0:365, \u0026quot;2000-01-01\u0026quot;) %\u0026gt;% format(\u0026quot;%d %B\u0026quot;)\r# breaks for the temperature data\rct \u0026lt;- c(-5, 0, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 40, 45)\r# categorized data with fixed breaks\rtmx_mat_cat \u0026lt;- mutate_at(tmx_mat, 3:368, cut, breaks = ct)\rFifth, we download the Montserrat font and define the colors corresponding to the created classes.\n# download font\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# use of showtext with 300 DPI\rshowtext_opts(dpi = 300)\rshowtext_auto()\r# define the color ramp\rcol_spec \u0026lt;- colorRampPalette(rev(brewer.pal(11, \u0026quot;Spectral\u0026quot;)))\r\rStatic map\rIn this first plot we make a map of May 29 (day 150). I am not going to explain all the details of the construction with ggplot2, however, it is important to note that I use the aes_string() function instead of aes() to use the column names in string format. With the geom_raster() function we add the gridded temperature data as the first layer of the graph and with geom_sf() the boundaries in sf class. Finally, the guide_colorsteps() function allows you to create a nice legend based on the classes created by the cut() function.\nggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = \u0026quot;D150\u0026quot;)) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Average maximum temperature during the year in Spain\u0026quot;, subtitle = lab[150], caption = \u0026quot;Reference period 1901-2014. Data: STEAD\u0026quot;,\rfill = \u0026quot;ºC\u0026quot;)\r\rAnimation of the whole year\rThe final animation consists of creating a gif from all the images of 366 days, in principle, the gganimate package could be used, but in my experience it is slower, since it requires a data.frame in long format. In this example a long table would have more than seven million rows. So what we do here is to use a loop over the columns and join all the created images with the gifski package that also uses gganimate for rendering.\nBefore looping we create a vector with the time steps or names of the columns, and another vector with the name of the images, including the name of the folder. In order to obtain a list of images ordered by their number, we must maintain three figures, filling the positions on the left with zeros.\ntime_step \u0026lt;- str_c(\u0026quot;D\u0026quot;, 1:366)\rfiles \u0026lt;- str_c(\u0026quot;./ta_anima/D\u0026quot;, str_pad(1:366, 3, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;), \u0026quot;.png\u0026quot;)\rLastly, we include the above plot construction in a for loop.\nfor(i in 1:366){\rggplot(tmx_mat_cat) + geom_raster(aes_string(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, fill = time_step[i])) +\rgeom_sf(data = map,\rcolour = \u0026quot;grey50\u0026quot;, size = 0.2) +\rcoord_sf(expand = FALSE) +\rscale_fill_manual(values = col_spec(20), drop = FALSE) +\rguides(fill = guide_colorsteps(barwidth = 30, barheight = 0.5,\rtitle.position = \u0026quot;right\u0026quot;,\rtitle.vjust = .1)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.justification = 1,\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, margin = margin(b = 5, t = 10, unit = \u0026quot;pt\u0026quot;)), plot.title = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot;, margin = margin(b = 2, t = 5, unit = \u0026quot;pt\u0026quot;)),\rlegend.text = element_text(family = \u0026quot;Montserrat\u0026quot;),\rplot.subtitle = element_text(family = \u0026quot;Montserrat\u0026quot;, size = 13, margin = margin(b = 10, t = 5, unit = \u0026quot;pt\u0026quot;))) +\rlabs(title = \u0026quot;Average maximum temperature during the year in Spain\u0026quot;, subtitle = lab[i], caption = \u0026quot;Reference period 1901-2014. Data: STEAD\u0026quot;,\rfill = \u0026quot;ºC\u0026quot;)\rggsave(files[i], width = 8.28, height = 7.33, type = \u0026quot;cairo\u0026quot;)\r}\rAfter having created images for each day of the year, we only have to create the gif.\ngifski(files, \u0026quot;tmx_spain.gif\u0026quot;, width = 800, height = 700, loop = FALSE, delay = 0.05)\r\n\r\r","date":1602374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602374400,"objectID":"58f33c7bf72d9521ae6a2427cfe257cb","permalink":"https://dominicroye.github.io/en/2020/climate-animation-of-maximum-temperatures/","publishdate":"2020-10-11T00:00:00Z","relpermalink":"/en/2020/climate-animation-of-maximum-temperatures/","section":"post","summary":"In the field of data visualization, the animation of spatial data in its temporal dimension can show fascinating changes and patterns. As a result of one of the last publications in the social networks that I have made, I was asked to make a post about how I created it. Well, here we go to start with an example of data from mainland Spain.","tags":["animation","temperature","climte","GIS"],"title":"Climate animation of maximum temperatures","type":"post"},{"authors":null,"categories":["gis","R","R:advanced"],"content":"\r\rI recently created a visualization of the distribution of river flow directions and also of coastal orientations. Following its publication in social networks (here), I was asked to make a post about how I did it. Well, here we go to start with an example of rivers, coastal orientation is somewhat more complex. I did the same for a selection of European rivers here in this tweet. However, originally I started with the orientation of the European coasts.\nHave you ever wondered where the European #coasts are oriented? #rstats #ggplot2 #geography #dataviz pic.twitter.com/tpWVxSoHlw\n\u0026mdash; Dr. Dominic Royé (@dr_xeo) May 26, 2020  Packages\rIn this post we will use the following packages:\n\r\r\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rremotes\rInstallation from remote repositories\r\rqgisprocess\rInterface between R and QGIS\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rggtext\rImproved text rendering support for ggplot2\r\rsysfonts\rLoad fonts in R\r\rshowtext\rUse fonts more easily in R graphs\r\rcircular\rFunctions for working with circular data\r\rgeosphere\rSpherical trigonometry for geographic applications\r\r\r\rIn the case of the qgisprocess package, it is necessary to install QIGS \u0026gt;= 3.16 here. I will explain the reason for using QGIS later.\n# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;remotes\u0026quot;)) install.packages(\u0026quot;remotes\u0026quot;)\rif(!require(\u0026quot;qgisprocess\u0026quot;)) remotes::install_github(\u0026quot;paleolimbot/qgisprocess\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggtext\u0026quot;)) install.packages(\u0026quot;ggtext\u0026quot;)\rif(!require(\u0026quot;circular\u0026quot;)) install.packages(\u0026quot;circular\u0026quot;)\rif(!require(\u0026quot;geosphere\u0026quot;)) install.packages(\u0026quot;geosphere\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\r# packages\rlibrary(sf)\rlibrary(tidyverse)\rlibrary(ggtext)\rlibrary(circular)\rlibrary(geosphere)\rlibrary(qgisprocess)\rlibrary(showtext)\rlibrary(sysfonts)\r\rInitial considerations\rAngles in vectorial lines are based on the angle between two vertices, and the number of vertices depends on the complexity, and therefore the resolution, of the vector data. Consequently, there can be differences in using different resolutions of a spatial line, either from the coast or from the river as in this example. A straight line is simply constructed with two points of longitude and latitude.\nRelated to this is fractality, an apparently irregular structure but that is repeated at different scales, known from coastlines or also from river. The most paradoxical feature is that the length of a coastline depends on the measurement scale, the smaller the measurement increment, the longer is the measured coastline.\nThere are two possibilities of obtaining the vertice angles. In the first one we calculate the angle between all consecutive vertices.\nFor example, imagine two points, Madrid (-3.71, 40.43) and Barcelona (2.14, 41.4).\nWhat is the angle of a straight line between both cities?\nbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4))\r## [1] 77.62391\rWe see that it is 77º, that is, northeast direction. But what if we go from Barcelona to Madrid?\nbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43))\r## [1] 257.6239\rThe angle is different because we move from the northeast to the southwest. We can easily invert the direction to get the opposite angle.\n# opposite angle of Barcelona -\u0026gt; Madrid\rbearingRhumb(c(2.14, 41.4), c(-3.71, 40.43)) - 180\r## [1] 77.62391\r# opposite angle of Madrid -\u0026gt; Barcelona\rbearingRhumb(c(-3.71, 40.43), c(2.14, 41.4)) + 180\r## [1] 257.6239\rThe direction in which we calculate the angles is important. In the case of rivers, it is expected to be the direction of flow from origin to the mouth, however, a problem may be that the vertices, which build the lines, are not geographically ordered in the attribute table. Another problem may be that the vertices start at the mouth which would give the reverse angle as we have seen before.\nHowever, there is an easier way. We can take advantage of the attributes of projected coordinate systems (Robinson projection, etc.) that include the angle between the vertices. We will use this last approach in this post. Still, we must pay close attention to the results as stated above.\n\rPreparation\rData\rWe download the central lines of the largest rivers in the world (here), also accessible in Zeenatul Basher et al. 2018.\n\rImport and project\rThe first thing we do is to import, project the spatial lines and delete the third dimension Z, chaining the following functions: st_read() helps us import any vector format, st_zm() delete the dimension Z or M of a geometry and st_transform() projects the vector data to the new projection in proj4 format. We combine the functions with the famous pipe (%\u0026gt;%) that facilitates the application of a sequence of functions on a data set, more details in this post. All functions in the sf package start with st_* with reference to the spatial character, similar to PostGIS. In the same style as PostGIS, verbs are used as function names.\nproj_rob \u0026lt;- \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m no_defs\u0026quot;\rriver_line \u0026lt;- st_read(\u0026quot;RiverHRCenterlinesCombo.shp\u0026quot;) %\u0026gt;% st_zm() %\u0026gt;% st_transform(proj_rob)\r## Reading layer `RiverHRCenterlinesCombo\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\en\\post\\2020-07-24-river-flow-directions\\RiverHRCenterlinesCombo.shp\u0026#39; ## using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 78 features and 6 fields\r## Geometry type: MULTILINESTRING\r## Dimension: XYZ\r## Bounding box: xmin: -164.7059 ymin: -36.97094 xmax: 151.5931 ymax: 72.64474\r## z_range: zmin: 0 zmax: 0\r## Geodetic CRS: WGS 84\r\rExtract the angles\rIn the next step we have to extract the vertice angles. Unfortunately, as far as I know, it is not possible to extract the attributes with some function from the sf package. Although the function st_coordinates() returns the coordinates, it does not include other attributes. Therefore, we must use another way, and that is the open software Quantum GIS in which we can find a tool to extract all the vertice attributes. We could import the vector data into QGIS Desktop and export the vertices from there, but it is also possible to access the QGIS tools from R directly.\nFor this, we need to have QGIS installed. The qgisprocess package allows us to use very easily all the tools of the software in R. First we use the qgis_configure() function to define all the necessary QGIS paths.\n# paths to QGIS\rqgis_configure()\r## getOption(\u0026#39;qgisprocess.path\u0026#39;) was not found.\r## Sys.getenv(\u0026#39;R_QGISPROCESS_PATH\u0026#39;) was not found.\r## Trying \u0026#39;qgis_process\u0026#39; on PATH\r## Error in processx::run(\u0026quot;cmd.exe\u0026quot;, c(\u0026quot;/c\u0026quot;, \u0026quot;call\u0026quot;, path, args), ...): System command \u0026#39;cmd.exe\u0026#39; failed, exit status: 1, stderr:\r## E\u0026gt; \u0026quot;qgis_process\u0026quot; no se reconoce como un comando interno o externo,\r## E\u0026gt; programa o archivo por lotes ejecutable.\r## Found 1 QGIS installation containing \u0026#39;qgis_process\u0026#39;:\r## C:/Program Files/QGIS 3.18/bin/qgis_process-qgis.bat\r## Trying command \u0026#39;C:/Program Files/QGIS 3.18/bin/qgis_process-qgis.bat\u0026#39;\r## Success!\r## QGIS version: 3.18.1-Zürich\r## Metadata of 986 algorithms queried and stored in cache.\r## Run `qgis_algorithms()` to see them.\rThe qgis_algorithms() function helps us to search for different QGIS tools. In addition the qgis_show_help() function specifies the way of usage with all the required parameters.\n# search tools\rqgis_algorithms()\r## # A tibble: 986 x 5\r## provider provider_title algorithm algorithm_id algorithm_title\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 3d QGIS (3D) 3d:tessellate tessellate Tessellate ## 2 gdal GDAL gdal:aspect aspect Aspect ## 3 gdal GDAL gdal:assignprojection assignproje~ Assign project~\r## 4 gdal GDAL gdal:buffervectors buffervecto~ Buffer vectors ## 5 gdal GDAL gdal:buildvirtualraster buildvirtua~ Build virtual ~\r## 6 gdal GDAL gdal:buildvirtualvector buildvirtua~ Build virtual ~\r## 7 gdal GDAL gdal:cliprasterbyextent cliprasterb~ Clip raster by~\r## 8 gdal GDAL gdal:cliprasterbymaskla~ cliprasterb~ Clip raster by~\r## 9 gdal GDAL gdal:clipvectorbyextent clipvectorb~ Clip vector by~\r## 10 gdal GDAL gdal:clipvectorbypolygon clipvectorb~ Clip vector by~\r## # ... with 976 more rows\r# usage of tool\rqgis_show_help(\u0026quot;native:extractvertices\u0026quot;)\r## Extract vertices (native:extractvertices)\r## ## ----------------\r## Description\r## ----------------\r## This algorithm takes a line or polygon layer and generates a point layer with points representing the vertices in the input lines or polygons. The attributes associated to each point are the same ones associated to the line or polygon that the point belongs to.\r## ## Additional fields are added to the point indicating the vertex index (beginning at 0), the vertex’s part and its index within the part (as well as its ring for polygons), distance along original geometry and bisector angle of vertex for original geometry.\r## ## ----------------\r## Arguments\r## ----------------\r## ## INPUT: Input layer\r## Argument type: source\r## Acceptable values:\r## - Path to a vector layer\r## OUTPUT: Vertices\r## Argument type: sink\r## Acceptable values:\r## - Path for new vector layer\r## ## ----------------\r## Outputs\r## ----------------\r## ## OUTPUT: \u0026lt;outputVector\u0026gt;\r## Vertices\rIn our case the tool to extract the vertices is simple and only has one input and one output. The function qgis_run_algorithm() executes a QGIS tool indicating the algorithm and its arguments. The advantage of using the algorithm directly from R is that we can pass objects of class sf (or sp) and raster that we have imported or created in R. As output we create a geojson, it could also be of another vector format, and we save it in a temporary folder. To obtain the QGIS output we need to use qgis_output() function.\nriver_vertices \u0026lt;- qgis_run_algorithm(alg = \u0026quot;native:extractvertices\u0026quot;,\rINPUT = river_line,\rOUTPUT = file.path(tempdir(), \u0026quot;rivers_world_vertices.geojson\u0026quot;))\r## Running cmd.exe /c call \\\r## \u0026quot;C:/Program Files/QGIS 3.18/bin/qgis_process-qgis.bat\u0026quot; run \\\r## \u0026quot;native:extractvertices\u0026quot; \\\r## \u0026quot;--INPUT=C:\\Users\\xeo19\\AppData\\Local\\Temp\\Rtmpkf37V1\\file1f54157824f7\\file1f54f432848.gpkg\u0026quot; \\\r## \u0026quot;--OUTPUT=C:\\Users\\xeo19\\AppData\\Local\\Temp\\Rtmpkf37V1/rivers_world_vertices.geojson\u0026quot;\r## ## ----------------\r## Inputs\r## ----------------\r## ## INPUT: C:\\Users\\xeo19\\AppData\\Local\\Temp\\Rtmpkf37V1\\file1f54157824f7\\file1f54f432848.gpkg\r## OUTPUT: C:\\Users\\xeo19\\AppData\\Local\\Temp\\Rtmpkf37V1/rivers_world_vertices.geojson\r## ## ## 0...10...20...30...40...50...60...70...80...90...\r## ----------------\r## Results\r## ----------------\r## ## OUTPUT: C:\\Users\\xeo19\\AppData\\Local\\Temp\\Rtmpkf37V1/rivers_world_vertices.geojson\rriver_vertices \u0026lt;- st_read(qgis_output(river_vertices, \u0026quot;OUTPUT\u0026quot;))\r## Reading layer `rivers_world_vertices\u0026#39; from data source ## `C:\\Users\\xeo19\\AppData\\Local\\Temp\\Rtmpkf37V1\\rivers_world_vertices.geojson\u0026#39; ## using driver `GeoJSON\u0026#39;\r## Simple feature collection with 339734 features and 12 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -12117400 ymin: -3953778 xmax: 13751910 ymax: 7507359\r## Geodetic CRS: WGS 84\r Currently on Windows there seem to be problems with the proj library. In principle, if the function ends up creating the river_vertices object, you should not worry.   \rSelection\rBefore continuing with the distribution estimation of the angles, we filter some rivers of interest. The functions of the tidyverse collection are compatible with the sf package. In the last post I made an introduction to tidyverse here.\nriver_vertices \u0026lt;- filter(river_vertices, NAME %in% c(\u0026quot;Mississippi\u0026quot;, \u0026quot;Colorado\u0026quot;, \u0026quot;Amazon\u0026quot;, \u0026quot;Nile\u0026quot;, \u0026quot;Orange\u0026quot;, \u0026quot;Ganges\u0026quot;, \u0026quot;Yangtze\u0026quot;, \u0026quot;Danube\u0026quot;,\r\u0026quot;Mackenzie\u0026quot;, \u0026quot;Lena\u0026quot;, \u0026quot;Murray\u0026quot;, \u0026quot;Niger\u0026quot;)\r) river_vertices \r## Simple feature collection with 94702 features and 12 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -10377520 ymin: -3953778 xmax: 13124340 ymax: 7507359\r## Geodetic CRS: WGS 84\r## First 10 features:\r## fid NAME SYSTEM name_alt scalerank rivernum Length_km vertex_index\r## 1 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 0\r## 2 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 1\r## 3 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 2\r## 4 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 3\r## 5 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 4\r## 6 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 5\r## 7 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 6\r## 8 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 7\r## 9 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 8\r## 10 6 Nile \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1 4 3343.871 9\r## vertex_part vertex_part_index distance angle geometry\r## 1 0 0 0.000 31.096005 POINT (3037149 1672482)\r## 2 0 1 1208.130 22.456672 POINT (3037772 1673517)\r## 3 0 2 2324.160 8.602259 POINT (3038039 1674600)\r## 4 0 3 3656.452 8.573580 POINT (3038118 1675930)\r## 5 0 4 5735.538 24.406889 POINT (3038612 1677950)\r## 6 0 5 6758.322 25.134763 POINT (3039200 1678787)\r## 7 0 6 10432.834 6.998982 POINT (3040164 1682333)\r## 8 0 7 14865.136 4.239641 POINT (3040070 1686764)\r## 9 0 8 16563.207 358.730530 POINT (3040356 1688438)\r## 10 0 9 18376.526 347.480822 POINT (3039972 1690210)\r\r\rEstimate the distribution\rTo visualize the distribution we can use either a histogram or a density graph. But in the case of estimating the probability density function, we find a mathematical problem when applying it to circular data. For circular data we should not use the density() standard function of R since in our data a direction of 360º is the same at 0º, which would cause errors in this range of values. It is a general problem for different statistical metrics. More statistical details are explained in the circular package. This package allows you to define the characteristics of circular data (unit, data type, rotation, etc.) as an object class in R.\nSo what we do is to build a function that estimates the density and returns a table with the angles (x) and the density estimates (y). Since rivers have different lengths, and we want to see differences regardless of that, we normalize the estimates using the maximum value. Unlike the density() function, in which the smoothing bandwidth bw is optimized, here it is required to indicate it manually. It is similar to defining the bar width in a histogram. There is an optimization function for the bandwidth, bw.nrd.circular() that could be used here.\ndens_circ \u0026lt;- function(x){\rdens \u0026lt;- density.circular(circular(x$angle, units = \u0026quot;degrees\u0026quot;),\rbw = 70, kernel = \u0026quot;vonmises\u0026quot;,\rcontrol.circular = list(units = \u0026quot;degrees\u0026quot;))\rdf \u0026lt;- data.frame(x = dens$x, y = dens$y/max(dens$y))\rreturn(df)\r}\rFinally, we estimate the density of each river in our selection. We use the split() function of R Base to get a table of each river in a list object. Then we apply our density estimation function to the list with the function map_df() from the purrr package. The suffix _df allows us to get a joined table, instead of a list with the results of each river. However, it is necessary to indicate the name of the column with the argument .id, which will contain the name of each river. Otherwise we would not know how to differentiate the results. Also here I recommend reading more details in the last post about tidyverse here.\ndens_river \u0026lt;- split(river_vertices, river_vertices$NAME) %\u0026gt;% map_df(dens_circ, .id = \u0026quot;river\u0026quot;)\r# results\rhead(dens_river)\r## river x y\r## 1 Amazon 0.000000 0.2399907\r## 2 Amazon 0.704501 0.2492548\r## 3 Amazon 1.409002 0.2585758\r## 4 Amazon 2.113503 0.2679779\r## 5 Amazon 2.818004 0.2774859\r## 6 Amazon 3.522505 0.2871232\r\rVisualization\rNow we only have to make the graph through the famous ggplot package. First we add a new font Montserrat for it use in this plot.\n# font download\rfont_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r# use of showtext\rshowtext_opts(dpi = 200)\rshowtext_auto() \rIn the next step we create two objects with the title and the plot caption. In the title we are using an html code to color part of the text instead of a legend. You can use html very easily with the ggtext package.\n# title with html\rtitle \u0026lt;- \u0026quot;Relative distribution of river \u0026lt;span style=\u0026#39;color:#011FFD;\u0026#39;\u0026gt;\u0026lt;strong\u0026gt;flow direction\u0026lt;/strong\u0026gt;\u0026lt;/span\u0026gt; in the world\u0026quot;\rcaption \u0026lt;- \u0026quot;Based on data from Zeenatul Basher, 20180215\u0026quot;\rThe background grid that creates ggplot by default for polar coordinates did not convince me, so we create a table with x axis background lines.\ngrid_x \u0026lt;- tibble(x = seq(0, 360 - 22.5, by = 22.5), y = rep(0, 16), xend = seq(0, 360 - 22.5, by = 22.5), yend = rep(Inf, 16))\rNext we define all the styles of the graph. The most important thing in this step is the element_textbox() function of the ggtext package to be able to interpret our html code incorporated into the title.\ntheme_polar \u0026lt;- function(){\rtheme_minimal() %+replace%\rtheme(axis.title.y = element_blank(),\raxis.text.y = element_blank(),\rlegend.title = element_blank(),\rplot.title = element_textbox(family = \u0026quot;Montserrat\u0026quot;, hjust = 0.5, colour = \u0026quot;white\u0026quot;, size = 15),\rplot.caption = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\raxis.text.x = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;),\rstrip.text = element_text(family = \u0026quot;Montserrat\u0026quot;, colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rpanel.grid = element_blank()\r)\r}\rFinally we build the graph: 1) We use the geom_hline() function with different y intersection points to create the background grid. The geom_segment() function creates the x grid. 2) We create the density area using the geom_area() function. 3) In scale_x_continous() we define a negative lower limit so that it does not collapse at a small point. The labels of the eight main directions are indicated in the scale_y_continous() function, and 4) Finally, we change to a polar coordinate system and set the variable to create facets.\nggplot() +\rgeom_hline(yintercept = c(0, .2, .4, .6, .8, 1), colour = \u0026quot;white\u0026quot;) +\rgeom_segment(data = grid_x , aes(x = x, y = y, xend = xend, yend = yend), linetype = \u0026quot;dashed\u0026quot;, col = \u0026quot;white\u0026quot;) +\rgeom_area(data = dens_river, aes(x = x, y = y, ymin = 0, ymax = y), alpha = .7, colour = NA, show.legend = FALSE,\rfill = \u0026quot;#011FFD\u0026quot;) + scale_y_continuous(limits = c(-.2, 1), expand = c(0, 0)) +\rscale_x_continuous(limits = c(0, 360), breaks = seq(0, 360 - 22.5, by = 22.5),\rminor_breaks = NULL,\rlabels = c(\u0026quot;N\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NE\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;E\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SE\u0026quot;, \u0026quot;\u0026quot;,\r\u0026quot;S\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;SW\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NW\u0026quot;, \u0026quot;\u0026quot;)) +\rcoord_polar() + facet_wrap(river ~ ., ncol = 4) +\rlabs(title = title, caption = caption, x = \u0026quot;\u0026quot;) +\rtheme_polar()\r## Warning: Ignoring unknown aesthetics: ymin, ymax\r\n\r","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595548800,"objectID":"9d75b104d90d7166a22838092da8ff2d","permalink":"https://dominicroye.github.io/en/2020/river-flow-directions/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/en/2020/river-flow-directions/","section":"post","summary":"I recently created a visualization of the distribution of river flow directions and also of coastal orientations. Following its publication in social networks, I was asked to make a post about how I did it. Well, here we go to start with an example of rivers, coastal orientation is somewhat more complex.","tags":["directions","river","fluvial","orientation","distribution"],"title":"River flow directions","type":"post"},{"authors":null,"categories":["tidyverse","R","R:elementary"],"content":"\r\r\r1 Tidyverse\r2 Style guide\r3 Pipe %\u0026gt;%\r4 Tidyverse packages\r\r4.1 Read and write data\r4.2 Character manipulations\r4.3 Management of dates and times\r4.4 Table and vector manipulation\r\r4.4.1 Select and rename\r4.4.2 Filter and sort\r4.4.3 Group and summarize\r4.4.4 Join tables\r4.4.5 Long and wide tables\r\r4.5 Visualize data\r\r4.5.1 Line and scatter plot\r4.5.2 Boxplot\r4.5.3 Heatmap\r\r4.6 Apply functions on vectors or lists\r\r\r\r1 Tidyverse\rThe tidyverse universe of packages, a collection of packages specially focused on data science, marked a milestone in R programming. In this post I am going to summarize very briefly the most essential to start in this world. The tidyverse grammar follows a common structure in all functions. The most essential thing is that the first argument is the object and then come the rest of the arguments. In addition, a set of verbs is provided to facilitate the use of the functions. The tidyverse philosophy and grammar of functions are also reflected in other packages that make its use compatible with the collection. For example, the sf package (simple feature) is a standardized way to encode spatial vector data and allows the use of multiple functions that we can find in the dplyr package.\nThe core of the tidyverse collection is made up of the following packages:\n\r\rPackage\rDescription\r\r\r\rggplot2\rGrammar for creating graphics\r\rpurrr\rR functional programming\r\rtibble\rModern and effective table system\r\rdplyr\rGrammar for data manipulation\r\rtidyr\rSet of functions to create tidy data\r\rstringr\rFunction set to work with characters\r\rreadr\rAn easy and fast way to import data\r\rforcats\rTools to easily work with factors\r\r\r\rIn addition to the mentioned packages, lubridate is also used very frequently to work with dates and times, and also readxl which allows us to import files in Excel format. To know all the available packages we can use the function tidyverse_packages().\n## [1] \u0026quot;broom\u0026quot; \u0026quot;cli\u0026quot; \u0026quot;crayon\u0026quot; \u0026quot;dbplyr\u0026quot; ## [5] \u0026quot;dplyr\u0026quot; \u0026quot;dtplyr\u0026quot; \u0026quot;forcats\u0026quot; \u0026quot;googledrive\u0026quot; ## [9] \u0026quot;googlesheets4\u0026quot; \u0026quot;ggplot2\u0026quot; \u0026quot;haven\u0026quot; \u0026quot;hms\u0026quot; ## [13] \u0026quot;httr\u0026quot; \u0026quot;jsonlite\u0026quot; \u0026quot;lubridate\u0026quot; \u0026quot;magrittr\u0026quot; ## [17] \u0026quot;modelr\u0026quot; \u0026quot;pillar\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;readr\u0026quot; ## [21] \u0026quot;readxl\u0026quot; \u0026quot;reprex\u0026quot; \u0026quot;rlang\u0026quot; \u0026quot;rstudioapi\u0026quot; ## [25] \u0026quot;rvest\u0026quot; \u0026quot;stringr\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;tidyr\u0026quot; ## [29] \u0026quot;xml2\u0026quot; \u0026quot;tidyverse\u0026quot;\rIt is very easy to get conflicts between functions, that is, that the same function name exists in several packages. To avoid this, we can write the name of the package in front of the function we want to use, separated by the colon symbol written twice (package_name::function_name).\nBefore I get started with the packages, I hope it will be a really short introduction, some comments on the style when programming in R.\n\r2 Style guide\rIn R there is no universal style guide, that is, in the R syntax it is not necessary to follow specific rules for our scripts. But it is recommended to work in a homogeneous, uniform, legible and clear way when writing scripts. The tidyverse collection has its own guide (https://style.tidyverse.org/).\nThe most important recommendations are:\n\rAvoid using more than 80 characters per line to allow reading the complete code.\rAlways use a space after a comma, never before.\rThe operators (==, +, -, \u0026lt;-,%\u0026gt;%, etc.) must have a space before and after.\rThere is no space between the name of a function and the first parenthesis, nor between the last argument and the final parenthesis of a function.\rAvoid reusing names of functions and common variables (c \u0026lt;- 5 vs. c())\rSort the script separating the parts with the comment form # Import data -----\rAvoid accent marks or special symbols in names, files, routes, etc.\rObject names must follow a constant structure: day_one, day_1.\r\rIt is advisable to use a correct indentation for multiple arguments of a function or functions chained by the pipe operator (%\u0026gt;%).\n\r3 Pipe %\u0026gt;%\rTo facilitate working in data management, manipulation and visualization, the magrittr package introduces the famous pipe operator in the form %\u0026gt;% with the aim of combining various functions without the need to assign the result to a new object. The pipe operator passes the output of a function applied to the first argument of the next function. This way of combining functions allows you to chain several steps simultaneously, to perform sequential tasks. In the very simple example below, we pass the vector 1:5 to the mean() function to calculate the average. You should know that there are a couple of other pipe operators in the same package.\n1:5 %\u0026gt;% mean()\r## [1] 3\r\r4 Tidyverse packages\r4.1 Read and write data\rThe readr package makes it easy to read or write multiple file formats using functions that start with read_* or write_*.\rIn comparison to R Base, readr functions are faster; they handle problematic column names, and dates are automatically converted. The imported tables are of class tibble (tbl_df), a modern version of data.frame from the tibble package. In the same sense, you can use the read_excel() function of the readxl package to import data from Excel sheets (more details also in this blog post). In the following example, we import the mobility data registered by Google (link) during the last months of the COVID-19 pandemic (download).\n\r\rFunction\rDescription\r\r\r\rread_csv() o read_csv2()\rcoma or semicolon (CSV)\r\rread_delim()\rgeneral separator\r\rread_table()\rwhitespace-separated\r\r\r\r# load package\rlibrary(tidyverse)\rgoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Rows: 516697 Columns: 13\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (6): country_region_code, country_region, sub_region_1, sub_region_2, i...\r## dbl (6): retail_and_recreation_percent_change_from_baseline, grocery_and_ph...\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rgoogle_mobility\r## # A tibble: 516,697 x 13\r## country_region_code country_region sub_region_1 sub_region_2 iso_3166_2_code\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 2 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 3 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 4 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 5 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 9 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 10 AE United Arab Em~ \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # ... with 516,687 more rows, and 8 more variables: census_fips_code \u0026lt;chr\u0026gt;,\r## # date \u0026lt;date\u0026gt;, retail_and_recreation_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # grocery_and_pharmacy_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # parks_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # transit_stations_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # workplaces_percent_change_from_baseline \u0026lt;dbl\u0026gt;,\r## # residential_percent_change_from_baseline \u0026lt;dbl\u0026gt;\rImportant is to take a look at the argument names, since they change in the readr functions. For example, the well-known header = TRUE argument of read.csv() is in this case col_names = TRUE. More details can be found in the Cheat-Sheet of readr.\n\r4.2 Character manipulations\rFor working with strings we use the stringr package, whose functions always start with str_* followed by a verb and the first argument.\nSome of these functions are as follows:\n\r\rFunction\rDescription\r\r\r\rstr_replace()\rreplace patterns\r\rstr_c()\rcombine characters\r\rstr_detect()\rdetect patterns\r\rstr_extract()\rextract patterns\r\rstr_sub()\rextract by position\r\rstr_length()\rlength of string\r\r\r\rRegular expressions are often used for character patterns. For example, the regular expression [aeiou] matches any single character that is a vowel. The use of square brackets [] corresponds to character classes. For example, [abc] corresponds to each letter regardless of its position. [a-z], [A-Z] or [0-9] each between a and z or 0 and 9. And finally, [:punct:] punctuation, etc. With curly braces “{}” we can indicate the number of the previous element, {2} would be twice, {1,2} between one and two, etc. Also with $ or ^ we can indicate if the pattern starts at the beginning or ends at the end. More details and patterns can be found in the Cheat-Sheet of stringr.\n# replace \u0026#39;er\u0026#39; at the end with empty space\rstr_replace(month.name, \u0026quot;er$\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;March\u0026quot; \u0026quot;April\u0026quot; \u0026quot;May\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;Septemb\u0026quot; \u0026quot;Octob\u0026quot; \u0026quot;Novemb\u0026quot; \u0026quot;Decemb\u0026quot;\rstr_replace(month.name, \u0026quot;^Ma\u0026quot;, \u0026quot;\u0026quot;)\r## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;rch\u0026quot; \u0026quot;April\u0026quot; \u0026quot;y\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;September\u0026quot; \u0026quot;October\u0026quot; \u0026quot;November\u0026quot; \u0026quot;December\u0026quot;\r# combine characters\ra \u0026lt;- str_c(month.name, 1:12, sep = \u0026quot;_\u0026quot;)\ra\r## [1] \u0026quot;January_1\u0026quot; \u0026quot;February_2\u0026quot; \u0026quot;March_3\u0026quot; \u0026quot;April_4\u0026quot; \u0026quot;May_5\u0026quot; ## [6] \u0026quot;June_6\u0026quot; \u0026quot;July_7\u0026quot; \u0026quot;August_8\u0026quot; \u0026quot;September_9\u0026quot; \u0026quot;October_10\u0026quot; ## [11] \u0026quot;November_11\u0026quot; \u0026quot;December_12\u0026quot;\r# collapse combination\rstr_c(month.name, collapse = \u0026quot;, \u0026quot;)\r## [1] \u0026quot;January, February, March, April, May, June, July, August, September, October, November, December\u0026quot;\r# detect patterns\rstr_detect(a, \u0026quot;_[1-5]{1}\u0026quot;)\r## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE\r# extract patterns\rstr_extract(a, \u0026quot;_[1-9]{1,2}\u0026quot;)\r## [1] \u0026quot;_1\u0026quot; \u0026quot;_2\u0026quot; \u0026quot;_3\u0026quot; \u0026quot;_4\u0026quot; \u0026quot;_5\u0026quot; \u0026quot;_6\u0026quot; \u0026quot;_7\u0026quot; \u0026quot;_8\u0026quot; \u0026quot;_9\u0026quot; \u0026quot;_1\u0026quot; \u0026quot;_11\u0026quot; \u0026quot;_12\u0026quot;\r# extract the characters between position 1 and 2\rstr_sub(month.name, 1, 2)\r## [1] \u0026quot;Ja\u0026quot; \u0026quot;Fe\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ap\u0026quot; \u0026quot;Ma\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Ju\u0026quot; \u0026quot;Au\u0026quot; \u0026quot;Se\u0026quot; \u0026quot;Oc\u0026quot; \u0026quot;No\u0026quot; \u0026quot;De\u0026quot;\r# string length of each month\rstr_length(month.name)\r## [1] 7 8 5 5 3 4 4 6 9 7 8 8\r# the \u0026#39;.\u0026#39; represents the object passed by the pipe operator %\u0026gt;%\rstr_length(month.name) %\u0026gt;% str_c(month.name, ., sep = \u0026quot;.\u0026quot;)\r## [1] \u0026quot;January.7\u0026quot; \u0026quot;February.8\u0026quot; \u0026quot;March.5\u0026quot; \u0026quot;April.5\u0026quot; \u0026quot;May.3\u0026quot; ## [6] \u0026quot;June.4\u0026quot; \u0026quot;July.4\u0026quot; \u0026quot;August.6\u0026quot; \u0026quot;September.9\u0026quot; \u0026quot;October.7\u0026quot; ## [11] \u0026quot;November.8\u0026quot; \u0026quot;December.8\u0026quot;\rA very useful function is str_glue() to interpolate characters.\nname \u0026lt;- c(\u0026quot;Juan\u0026quot;, \u0026quot;Michael\u0026quot;)\rage \u0026lt;- c(50, 80) date_today \u0026lt;- Sys.Date()\rstr_glue(\r\u0026quot;My name is {name}, \u0026quot;,\r\u0026quot;I\u0026#39;am {age}, \u0026quot;,\r\u0026quot;and my birth year is {format(date_today-age*365, \u0026#39;%Y\u0026#39;)}.\u0026quot;\r)\r## My name is Juan, I\u0026#39;am 50, and my birth year is 1972.\r## My name is Michael, I\u0026#39;am 80, and my birth year is 1942.\r\r4.3 Management of dates and times\rThe lubridate package is very powerful in handling dates and times. It allows us to create R recognized objects with functions (like ymd() or ymd_hms()) and we can even make calculations.\nWe only must know the following abbreviations:\n\rymd: represents y:year, m: month, d:day\rhms: represents h:hour, m:minutes, s:seconds\r\r# load package\rlibrary(lubridate)\r## ## Attaching package: \u0026#39;lubridate\u0026#39;\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## date, intersect, setdiff, union\r# date vector\rdat \u0026lt;- c(\u0026quot;1999/12/31\u0026quot;, \u0026quot;2000/01/07\u0026quot;, \u0026quot;2005/05/20\u0026quot;,\u0026quot;2010/03/25\u0026quot;)\r# date-time vector\rdat_time \u0026lt;- c(\u0026quot;1988-08-01 05:00\u0026quot;, \u0026quot;2000-02-01 22:00\u0026quot;)\r# convert to date class\rdat \u0026lt;- ymd(dat) dat\r## [1] \u0026quot;1999-12-31\u0026quot; \u0026quot;2000-01-07\u0026quot; \u0026quot;2005-05-20\u0026quot; \u0026quot;2010-03-25\u0026quot;\r# other date formats\rdmy(\u0026quot;05-02-2000\u0026quot;)\r## [1] \u0026quot;2000-02-05\u0026quot;\rymd(\u0026quot;20000506\u0026quot;)\r## [1] \u0026quot;2000-05-06\u0026quot;\r# convert to POSIXct\rdat_time \u0026lt;- ymd_hm(dat_time)\rdat_time\r## [1] \u0026quot;1988-08-01 05:00:00 UTC\u0026quot; \u0026quot;2000-02-01 22:00:00 UTC\u0026quot;\r# different date formats\rdat_mix \u0026lt;- c(\u0026quot;1999/12/05\u0026quot;, \u0026quot;05-09-2008\u0026quot;, \u0026quot;2000/08/09\u0026quot;, \u0026quot;25-10-2019\u0026quot;)\r# mixted formats with known convention found in ?strptime\rparse_date_time(dat_mix, order = c(\u0026quot;%Y/%m/%d\u0026quot;, \u0026quot;%d-%m-%Y\u0026quot;))\r## [1] \u0026quot;1999-12-05 UTC\u0026quot; \u0026quot;2008-09-05 UTC\u0026quot; \u0026quot;2000-08-09 UTC\u0026quot; \u0026quot;2019-10-25 UTC\u0026quot;\rMore useful functions:\n# extract the year\ryear(dat)\r## [1] 1999 2000 2005 2010\r# the month\rmonth(dat)\r## [1] 12 1 5 3\rmonth(dat, label = TRUE) # as label\r## [1] dic ene may mar\r## 12 Levels: ene \u0026lt; feb \u0026lt; mar \u0026lt; abr \u0026lt; may \u0026lt; jun \u0026lt; jul \u0026lt; ago \u0026lt; sep \u0026lt; ... \u0026lt; dic\r# the day of the week\rwday(dat)\r## [1] 6 6 6 5\rwday(dat, label = TRUE) # as label\r## [1] vi\\\\. vi\\\\. vi\\\\. ju\\\\.\r## Levels: do\\\\. \u0026lt; lu\\\\. \u0026lt; ma\\\\. \u0026lt; mi\\\\. \u0026lt; ju\\\\. \u0026lt; vi\\\\. \u0026lt; sá\\\\.\r# the hour\rhour(dat_time)\r## [1] 5 22\r# add 10 days\rdat + days(10)\r## [1] \u0026quot;2000-01-10\u0026quot; \u0026quot;2000-01-17\u0026quot; \u0026quot;2005-05-30\u0026quot; \u0026quot;2010-04-04\u0026quot;\r# add 1 month\rdat + months(1)\r## [1] \u0026quot;2000-01-31\u0026quot; \u0026quot;2000-02-07\u0026quot; \u0026quot;2005-06-20\u0026quot; \u0026quot;2010-04-25\u0026quot;\rFinally, the make_date() function is very useful to create dates from different date parts, such as the year, month, etc.\n# create date from its elements, here with year and month\rmake_date(2000, 5)\r## [1] \u0026quot;2000-05-01\u0026quot;\r# create date with time\rmake_datetime(2005, 5, 23, 5)\r## [1] \u0026quot;2005-05-23 05:00:00 UTC\u0026quot;\rMore details can be found in the Cheat-Sheet of lubridate.\n\r4.4 Table and vector manipulation\rThe dplyr and tidyr packages provide us with a data manipulation grammar, a set of useful verbs to solve common problems. The most important functions are:\n\r\rFunction\rDescription\r\r\r\rmutate()\radd new variables or modify existing ones\r\rselect()\rselect variables\r\rfilter()\rfilter\r\rsummarise()\rsummarize/reduce\r\rarrange()\rsort\r\rgroup_by()\rgroup\r\rrename()\rrename columns\r\r\r\rIn case you haven’t done it before, we import the mobility data.\ngoogle_mobility \u0026lt;- read_csv(\u0026quot;Global_Mobility_Report.csv\u0026quot;)\r## Rows: 516697 Columns: 13\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (6): country_region_code, country_region, sub_region_1, sub_region_2, i...\r## dbl (6): retail_and_recreation_percent_change_from_baseline, grocery_and_ph...\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r4.4.1 Select and rename\rWe can select or remove columns with the select() function, using the name or index of the column. To delete columns we make use of the negative sign. The rename function helps in renaming columns with either the same name or their index.\nresidential_mobility \u0026lt;- select(google_mobility, country_region_code:sub_region_1, date, residential_percent_change_from_baseline) %\u0026gt;% rename(resi = 5)\r\r4.4.2 Filter and sort\rTo filter data, we use filter() with logical operators (|, ==, \u0026gt;, etc) or functions that return a logical value (str_detect(), is.na() , etc.). The arrange() function sorts from least to greatest for one or multiple variables (with the negative sign - the order is reversed from greatest to least).\nfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;)\r## # A tibble: 304,648 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States \u0026lt;NA\u0026gt; 2020-02-15 -1\r## 2 US United States \u0026lt;NA\u0026gt; 2020-02-16 -1\r## 3 US United States \u0026lt;NA\u0026gt; 2020-02-17 5\r## 4 US United States \u0026lt;NA\u0026gt; 2020-02-18 1\r## 5 US United States \u0026lt;NA\u0026gt; 2020-02-19 0\r## 6 US United States \u0026lt;NA\u0026gt; 2020-02-20 1\r## 7 US United States \u0026lt;NA\u0026gt; 2020-02-21 0\r## 8 US United States \u0026lt;NA\u0026gt; 2020-02-22 -1\r## 9 US United States \u0026lt;NA\u0026gt; 2020-02-23 -1\r## 10 US United States \u0026lt;NA\u0026gt; 2020-02-24 0\r## # ... with 304,638 more rows\rfilter(residential_mobility, country_region_code == \u0026quot;US\u0026quot;, sub_region_1 == \u0026quot;New York\u0026quot;)\r## # A tibble: 7,068 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 US United States New York 2020-02-15 0\r## 2 US United States New York 2020-02-16 -1\r## 3 US United States New York 2020-02-17 9\r## 4 US United States New York 2020-02-18 3\r## 5 US United States New York 2020-02-19 2\r## 6 US United States New York 2020-02-20 2\r## 7 US United States New York 2020-02-21 3\r## 8 US United States New York 2020-02-22 -1\r## 9 US United States New York 2020-02-23 -1\r## 10 US United States New York 2020-02-24 0\r## # ... with 7,058 more rows\rfilter(residential_mobility, resi \u0026gt; 50) %\u0026gt;% arrange(-resi)\r## # A tibble: 32 x 5\r## country_region_code country_region sub_region_1 date resi\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 KW Kuwait Al Farwaniyah Governorate 2020-05-14 56\r## 2 KW Kuwait Al Farwaniyah Governorate 2020-05-21 55\r## 3 SG Singapore \u0026lt;NA\u0026gt; 2020-05-01 55\r## 4 KW Kuwait Al Farwaniyah Governorate 2020-05-28 54\r## 5 PE Peru Metropolitan Municipalit~ 2020-04-10 54\r## 6 EC Ecuador Pichincha 2020-03-27 53\r## 7 KW Kuwait Al Farwaniyah Governorate 2020-05-11 53\r## 8 KW Kuwait Al Farwaniyah Governorate 2020-05-13 53\r## 9 KW Kuwait Al Farwaniyah Governorate 2020-05-20 53\r## 10 SG Singapore \u0026lt;NA\u0026gt; 2020-04-10 53\r## # ... with 22 more rows\r\r4.4.3 Group and summarize\rWhere do we find greater variability between regions in each country on April 1, 2020?\nTo answer this question, we first filter the data and then we group by the country column. When we use the summarize() function after grouping, it allows us to summarize by these groups. Moreover, combining group_by() with the mutate() function modifies columns in each group separately. In summarize() we calculate the maximum, minimum value and the difference between both extremes creating new columns.\nresi_variability \u0026lt;- residential_mobility %\u0026gt;% filter(date == ymd(\u0026quot;2020-04-01\u0026quot;),\r!is.na(sub_region_1)) %\u0026gt;% group_by(country_region) %\u0026gt;% summarise(mx = max(resi, na.rm = TRUE), min = min(resi, na.rm = TRUE),\rrange = abs(mx)-abs(min))\rarrange(resi_variability, -range)\r## # A tibble: 94 x 4\r## country_region mx min range\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Nigeria 43 6 37\r## 2 United States 35 6 29\r## 3 India 36 15 21\r## 4 Malaysia 45 26 19\r## 5 Philippines 40 21 19\r## 6 Vietnam 28 9 19\r## 7 Colombia 41 24 17\r## 8 Ecuador 44 27 17\r## 9 Argentina 35 19 16\r## 10 Chile 30 14 16\r## # ... with 84 more rows\r\r4.4.4 Join tables\rHow can we filter the data to get a subset of Europe?\nTo do this, we import a spatial dataset with the country code and a column of regions. Detailed explanations about the sf (simple feature) package, I’ll leave for another post.\nlibrary(rnaturalearth) # package of spatial vectorial data\r# world limits\rwld \u0026lt;- ne_countries(returnclass = \u0026quot;sf\u0026quot;)\r# filter the countries with iso code and select the two columns of interest\rwld \u0026lt;- filter(wld, !is.na(iso_a2)) %\u0026gt;% select(iso_a2, subregion)\r# plot\rplot(wld)\rOther dplyr functions allow us to join tables: *_join (). Depending on which table (left or right) you want to join, the functions change: left_join(), right_join() or even full_join(). The by argument is not necessary as long as both tables have a column in common. However, in this case the variable names are different, so we use the following way: c(\"country_region_code\"=\"iso_a2\"). The forcats package of tidyverse has many useful functions for handling categorical variables (factors), variables that have a fixed and known set of possible values. All forcats functions have the prefix fct_*. For example, in this case we use fct_reorder() to reorder the country labels in order of the maximum based on the residential mobility records. Finally, we create a new column \"resi_real\" to change the reference value, the average or baseline, from 0 to 100.\nsubset_europe \u0026lt;- filter(residential_mobility, is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;%\rmutate(resi_real = resi + 100,\rregion = fct_reorder(country_region, resi, .fun = \u0026quot;max\u0026quot;, .desc = FALSE)) %\u0026gt;% select(-geometry, -sub_region_1)\rstr(subset_europe)\r## tibble [3,988 x 7] (S3: tbl_df/tbl/data.frame)\r## $ country_region_code: chr [1:3988] \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; \u0026quot;AT\u0026quot; ...\r## $ country_region : chr [1:3988] \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; \u0026quot;Austria\u0026quot; ...\r## $ date : Date[1:3988], format: \u0026quot;2020-02-15\u0026quot; \u0026quot;2020-02-16\u0026quot; ...\r## $ resi : num [1:3988] -2 -2 0 0 1 0 1 -2 0 -1 ...\r## $ subregion : chr [1:3988] \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; \u0026quot;Western Europe\u0026quot; ...\r## $ resi_real : num [1:3988] 98 98 100 100 101 100 101 98 100 99 ...\r## $ region : Factor w/ 35 levels \u0026quot;Belarus\u0026quot;,\u0026quot;Ukraine\u0026quot;,..: 18 18 18 18 18 18 18 18 18 18 ...\r\r4.4.5 Long and wide tables\rBefore we go to create graphics with ggplot2, it is very common to modify the table between two main formats, long and wide. A table is tidy when 1) each variable is a column 2) each observation/case is a row and 3) each type of observational unit forms a table.\n# subset\rmobility_selection \u0026lt;- select(subset_europe, country_region_code, date:resi)\rmobility_selection\r## # A tibble: 3,988 x 3\r## country_region_code date resi\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 AT 2020-02-15 -2\r## 2 AT 2020-02-16 -2\r## 3 AT 2020-02-17 0\r## 4 AT 2020-02-18 0\r## 5 AT 2020-02-19 1\r## 6 AT 2020-02-20 0\r## 7 AT 2020-02-21 1\r## 8 AT 2020-02-22 -2\r## 9 AT 2020-02-23 0\r## 10 AT 2020-02-24 -1\r## # ... with 3,978 more rows\r# wide table\rmobi_wide \u0026lt;- pivot_wider(mobility_selection, names_from = country_region_code,\rvalues_from = resi)\rmobi_wide\r## # A tibble: 114 x 36\r## date AT BA BE BG BY CH CZ DE DK EE ES\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 -2 -1 -1 0 -1 -1 -2 -1 0 0 -2\r## 2 2020-02-16 -2 -1 1 -3 0 -1 -1 0 1 0 -2\r## 3 2020-02-17 0 -1 0 -2 0 1 0 0 1 1 -1\r## 4 2020-02-18 0 -1 0 -2 0 1 0 1 1 1 0\r## 5 2020-02-19 1 -1 0 -1 -1 1 0 1 1 0 -1\r## 6 2020-02-20 0 -1 0 0 -1 0 0 1 1 0 -1\r## 7 2020-02-21 1 -2 0 -1 -1 1 0 2 1 1 -2\r## 8 2020-02-22 -2 -1 0 0 -2 -2 -3 0 1 0 -2\r## 9 2020-02-23 0 -1 0 -3 -1 -1 0 0 0 -2 -3\r## 10 2020-02-24 -1 -1 4 -1 0 0 0 4 0 16 0\r## # ... with 104 more rows, and 24 more variables: FI \u0026lt;dbl\u0026gt;, FR \u0026lt;dbl\u0026gt;, GB \u0026lt;dbl\u0026gt;,\r## # GR \u0026lt;dbl\u0026gt;, HR \u0026lt;dbl\u0026gt;, HU \u0026lt;dbl\u0026gt;, IE \u0026lt;dbl\u0026gt;, IT \u0026lt;dbl\u0026gt;, LT \u0026lt;dbl\u0026gt;, LU \u0026lt;dbl\u0026gt;,\r## # LV \u0026lt;dbl\u0026gt;, MD \u0026lt;dbl\u0026gt;, MK \u0026lt;dbl\u0026gt;, NL \u0026lt;dbl\u0026gt;, NO \u0026lt;dbl\u0026gt;, PL \u0026lt;dbl\u0026gt;, PT \u0026lt;dbl\u0026gt;,\r## # RO \u0026lt;dbl\u0026gt;, RS \u0026lt;dbl\u0026gt;, RU \u0026lt;dbl\u0026gt;, SE \u0026lt;dbl\u0026gt;, SI \u0026lt;dbl\u0026gt;, SK \u0026lt;dbl\u0026gt;, UA \u0026lt;dbl\u0026gt;\r# back to long table\rpivot_longer(mobi_wide,\r2:36,\rnames_to = \u0026quot;country_code\u0026quot;,\rvalues_to = \u0026quot;resi\u0026quot;)\r## # A tibble: 3,990 x 3\r## date country_code resi\r## \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020-02-15 AT -2\r## 2 2020-02-15 BA -1\r## 3 2020-02-15 BE -1\r## 4 2020-02-15 BG 0\r## 5 2020-02-15 BY -1\r## 6 2020-02-15 CH -1\r## 7 2020-02-15 CZ -2\r## 8 2020-02-15 DE -1\r## 9 2020-02-15 DK 0\r## 10 2020-02-15 EE 0\r## # ... with 3,980 more rows\rAnother group of functions you should take a look at are: separate(), case_when(), complete(). More details can be found in the Cheat-Sheet of dplyr.\n\r\r4.5 Visualize data\rggplot2 is a modern system for data visualization with a huge variety of options. Unlike the R Base graphic system, in ggplot2 a different grammar is used. The grammar of graphics (gg) consists of the sum of several independent layers or objects that are combined using + to construct the final graph. ggplot differentiates between data, what is displayed and how it is displayed.\n\rdata: our dataset (data.frame or tibble)\n\raesthetics: with the aes() function we indicate the variables that correspond to the x, y, z, … axes, or when it is intended to apply graphic parameters (color, size, shape) according to a variable. It is possible to include aes() in ggplot() or in the corresponding function to a geometry geom_ *.\n\rgeometries: are geom_ * objects that indicate the geometry to be used, (eg: geom_point(), geom_line(), geom_boxplot(), etc.).\n\rscales: are objects of type scales_ * (eg, scale_x_continous(), scale_colour_manual()) to manipulate axes, define colors, etc.\n\rstatistics: are stat_ * objects (eg, stat_density()) that allow to apply statistical transformations.\n\r\rMore details can be found in the Cheat-Sheet of ggplot2. ggplot is constantly supplemented by extensions for geometries or other graphical options (see https://exts.ggplot2.tidyverse.org/ggiraph.html), for graphical ideas have a look a the R Graph Gallery (https://www.r-graph-gallery.com/).\n4.5.1 Line and scatter plot\rWe create a subset of our mobility data for residences and parks, filtering the records for Italian regions. In addition, we divide the mobility values in percentage by 100 to obtain the fraction, since ggplot2 allows us to indicate the unit of percentage in the label argument (see last plot in this section).\n# create subset\rit \u0026lt;- filter(google_mobility, country_region == \u0026quot;Italy\u0026quot;, is.na(sub_region_1)) %\u0026gt;% mutate(resi = residential_percent_change_from_baseline/100, parks = parks_percent_change_from_baseline/100)\r# line plot\rggplot(it, aes(date, resi)) + geom_line()\r# scatter plot\rggplot(it, aes(parks, resi)) + geom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rTo modify the axes, we use the different scale_* functions that we must adapt to the scales of measurement (date, discrete, continuous, etc.). The labs() function helps us define the axis, legend and plot titles. Finally, we add the style of the graph with theme_light() (others are theme_bw(), theme_minimal(), etc.). We could also make changes to all graphic elements through theme().\n# time serie plot\rggplot(it, aes(date, resi)) + geom_line(colour = \u0026quot;#560A86\u0026quot;, size = 0.8) +\rscale_x_date(date_breaks = \u0026quot;10 days\u0026quot;, date_labels = \u0026quot;%d %b\u0026quot;) +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r# scatter plot\rggplot(it, aes(parks, resi)) + geom_point(alpha = .4, size = 2) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rscale_x_continuous(breaks = seq(-1, 1.4, 0.2), labels = scales::percent) +\rscale_y_continuous(breaks = seq(-1, 1, 0.1), labels = scales::percent) +\rlabs(x = \u0026quot;Park mobility\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;) +\rtheme_light()\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\r4.5.2 Boxplot\rWe can visualize different aspects of the mobility with other geometries. Here we will create boxplots for each European country representing the variability of mobility between and within countries during the COVID-19 pandemic.\n# subset\rsubset_europe_reg \u0026lt;- filter(residential_mobility, !is.na(sub_region_1),\r!is.na(resi)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;)) %\u0026gt;% mutate(resi = resi/100, country_region = fct_reorder(country_region, resi))\r# boxplot\rggplot(subset_europe_reg, aes(country_region, resi, fill = subregion)) + geom_boxplot() +\rscale_y_continuous(breaks = seq(-0.1, 1, 0.1), labels = scales::percent) +\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;) +\rcoord_flip() +\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;Residential mobility\u0026quot;,\rtitle = \u0026quot;Mobility during COVID-19\u0026quot;, fill = \u0026quot;\u0026quot;) +\rtheme_minimal()\r\r4.5.3 Heatmap\rTo visualize the mobility trend of all European countries it is recommended to use a heatmap instead of a bundle of lines. Before building the graph, we will create a vector of Sundays for the x-axis labels in the observation period.\n# sequence of dates\rdf \u0026lt;- data.frame(d = seq(ymd(\u0026quot;2020-02-15\u0026quot;), ymd(\u0026quot;2020-06-07\u0026quot;), \u0026quot;day\u0026quot;))\r# filter on Sundays sundays \u0026lt;- df %\u0026gt;% mutate(wd = wday(d, week_start = 1)) %\u0026gt;% filter(wd == 7) %\u0026gt;% pull(d)\rTo difference between European regions, we will use a color fill for the boxplots. We can set the color type with scale_fill_*, in this case, from the viridis scheme. In addition, the guides() function can modify the color bar of the legend. Finally, here we see the use of theme() with additional changes to theme_minimal().\n# headmap\rggplot(subset_europe, aes(date, region, fill = resi_real)) +\rgeom_tile() +\rscale_x_date(breaks = sundays,\rdate_labels = \u0026quot;%d %b\u0026quot;) +\rscale_fill_viridis_c(option = \u0026quot;A\u0026quot;, breaks = c(91, 146),\rlabels = c(\u0026quot;Less\u0026quot;, \u0026quot;More\u0026quot;), direction = -1) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;top\u0026quot;, title = element_text(size = 14),\rpanel.grid.major.x = element_line(colour = \u0026quot;white\u0026quot;, linetype = \u0026quot;dashed\u0026quot;),\rpanel.grid.minor.x = element_blank(),\rpanel.grid.major.y = element_blank(),\rpanel.ontop = TRUE,\rplot.margin = margin(r = 1, unit = \u0026quot;cm\u0026quot;)) +\rlabs(y = \u0026quot;\u0026quot;, x = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, title = \u0026quot;Mobility trends for places of residence\u0026quot;,\rcaption = \u0026quot;Data: google.com/covid19/mobility/\u0026quot;) +\rguides(fill = guide_colorbar(barwidth = 10, barheight = .5,\rlabel.position = \u0026quot;top\u0026quot;, ticks = FALSE)) +\rcoord_cartesian(expand = FALSE)\r\r\r4.6 Apply functions on vectors or lists\rThe purrr package contains a set of advanced functional programming functions for working with functions and vectors. The known lapply() family of R Base corresponds to the map() functions in this package. One of the biggest advantages is being able to reduce the use of loops (for, etc.).\n# list of two vectors\rvec_list \u0026lt;- list(x = 1:10, y = 50:70)\r# calculate the average for each one\rmap(vec_list, mean)\r## $x\r## [1] 5.5\r## ## $y\r## [1] 60\r# change the output type map_* (dbl, chr, lgl, etc.)\rmap_dbl(vec_list, mean)\r## x y ## 5.5 60.0\rFinally, a more complex example. We calculate the correlation coefficient between residential and park mobility in all European countries. To get a tidy summary of a model or test we use the tidy() function of the broom package.\nlibrary(broom) # tidy outputs\r# custom function\rcor_test \u0026lt;- function(x, formula) { df \u0026lt;- cor.test(as.formula(formula), data = x) %\u0026gt;% tidy()\rreturn(df)\r}\r# prepare the data\reurope_reg \u0026lt;- filter(google_mobility, !is.na(sub_region_1),\r!is.na(residential_percent_change_from_baseline)) %\u0026gt;%\rleft_join(wld, by = c(\u0026quot;country_region_code\u0026quot;=\u0026quot;iso_a2\u0026quot;)) %\u0026gt;% filter(subregion %in% c(\u0026quot;Northern Europe\u0026quot;,\r\u0026quot;Southern Europe\u0026quot;,\r\u0026quot;Western Europe\u0026quot;,\r\u0026quot;Eastern Europe\u0026quot;))\r# apply the function to each country creating a list\rcor_mobility \u0026lt;- europe_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;) cor_mobility[1:5]\r## $AT\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.360 -12.3 2.68e-32 1009 -0.413 -0.305 Pearson\u0026#39;~ two.sided ## ## $BE\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.312 -6.06 3.67e-9 340 -0.405 -0.213 Pears~ two.sided ## ## $BG\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pearson~ two.sided ## ## $CH\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0786 -2.91 0.00370 1360 -0.131 -0.0256 Pearson\u0026#39;s~ two.sided ## ## $CZ\r## # A tibble: 1 x 8\r## estimate statistic p.value parameter conf.low conf.high method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 -0.0837 -3.35 0.000824 1593 -0.132 -0.0347 Pearson\u0026#39;~ two.sided\rAs we’ve seen before, there are subfunctions of map_* to get an object of another class instead of a list, here for a bind data.frame.\ncor_mobility \u0026lt;- europe_reg %\u0026gt;%\rsplit(.$country_region_code) %\u0026gt;% map_df(cor_test, formula = \u0026quot;~ residential_percent_change_from_baseline + parks_percent_change_from_baseline\u0026quot;, .id = \u0026quot;country_code\u0026quot;)\rarrange(cor_mobility, estimate)\r## # A tibble: 27 x 9\r## country_code estimate statistic p.value parameter conf.low conf.high method\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 IT -0.831 -71.0 0 2250 -0.844 -0.818 Pears~\r## 2 ES -0.825 -65.4 0 2005 -0.839 -0.811 Pears~\r## 3 PT -0.729 -46.9 2.12e-321 1938 -0.749 -0.707 Pears~\r## 4 FR -0.698 -37.4 3.29e-216 1474 -0.723 -0.671 Pears~\r## 5 GR -0.692 -27.0 1.03e-114 796 -0.726 -0.654 Pears~\r## 6 BG -0.677 -37.8 1.47e-227 1694 -0.702 -0.650 Pears~\r## 7 RO -0.640 -56.0 0 4517 -0.657 -0.623 Pears~\r## 8 SI -0.627 -11.4 1.98e- 23 200 -0.704 -0.535 Pears~\r## 9 HR -0.579 -21.9 9.32e- 87 954 -0.620 -0.536 Pears~\r## 10 LV -0.544 -6.87 3.84e- 10 112 -0.662 -0.401 Pears~\r## # ... with 17 more rows, and 1 more variable: alternative \u0026lt;chr\u0026gt;\rOther practical examples here in this post or this other. More details can be found in the Cheat-Sheet of purrr.\n\n\r\r","date":1591401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591401600,"objectID":"cdcb21327c6f490be8096e3312c84d5b","permalink":"https://dominicroye.github.io/en/2020/a-very-short-introduction-to-tidyverse/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/en/2020/a-very-short-introduction-to-tidyverse/","section":"post","summary":"The tidyverse universe of packages, a collection of packages specially focused on data science, marked a milestone in R programming. In this post I am going to summarize very briefly the most essential to start in this world. The tidyverse grammar follows a common structure in all functions. The most essential thing is that the first argument is the object and then come the rest of the arguments. In addition, a set of verbs is provided to facilitate the use of the functions. The tidyverse philosophy and grammar of functions structure is also reflected in other packages that make its use compatible with the collection of tidyverse.","tags":["introduction","visualization","manipulation","data","COVID-19"],"title":"A very short introduction to Tidyverse","type":"post"},{"authors":["A Santurtún","R Almendra","P Fdez-Arroyabe","A Sanchez-Lorenzo","D Royé","MT Zarrabeitia","P Santana"],"categories":null,"content":"","date":1586822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586822400,"objectID":"b7804ed138a0b217293985653c0122c4","permalink":"https://dominicroye.github.io/en/publication/2020-hospital-admissions-cardio-thermal-indices-stoten/","publishdate":"2020-04-14T00:00:00Z","relpermalink":"/en/publication/2020-hospital-admissions-cardio-thermal-indices-stoten/","section":"publication","summary":"The natural environment has been considered an important determinant of cardiovascular morbidity. This work seeks to assess the impact of the winter thermal environment on hospital admissions from diseases of the circulatory system by using three biometeorological indices in five regions of the Iberian Peninsula. A theoretical index based on a thermophysiological model (Universal Thermal Climate Index [UTCI]) and two experimental biometeorological ones (Net Effective Temperature [NET] and Apparent Temperature [AT]) were estimated in two metropolitan areas of Portugal (Porto and Lisbon) and in three provinces of Spain (Madrid, Barcelona and Valencia). Subsequently, their relationship with hospital admissions, adjusted by NO2 concentration, time, and day of the week, was analyzed using a Generalized Additive Model. As the estimation method, a semi-parametric quasi-Poisson regression was used. Around 53% of the hospitalizations occurred during the cold periods. The admissions rate followed an upward trend over the 9-year period in both capitals (Madrid and Lisbon) as well as in Barcelona. An inverse and statistically significant relationship was found between thermal comfort and hospital admissions in the five regions (p","tags":["circulatory system diseases","air temperature","net effective temperature","apparent temperature","universal thermal climate index"],"title":"Predictive value of three thermal confort indices in low temperatures on cardiovascular morbidity in the Iberian Peninsula","type":"publication"},{"authors":null,"categories":["visualization","R","R:intermediate"],"content":"\r\rWhen we visualize precipitation and temperature anomalies, we simply use time series as bar graph indicating negative and positive values in red and blue. However, in order to have a better overview we need both anomalies in a single graph. In this way we could more easly answer the question of whether a particular season or month was dry-warm or wet-cold, and even compare these anomalies in the context of previous years.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rlubridate\rEasy manipulation of dates and times\r\rggrepel\rRepel overlapping text labels in ggplot2\r\r\r\r#we install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggrepel\u0026quot;)) install.packages(\u0026quot;ggrepel\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggrepel)\r\rPreparing the data\rFirst we import the daily precipitation and temperature data from the selected weather station (download). We will use the data from Tenerife South (Spain) [1981-2020] accessible through Open Data AEMET. In R there is a package called meteoland that facilitates the download with specific functions to access data from AEMET (Spanish State Meteorological Agency), Meteogalicia (Galician Meteorological Service) and Meteocat (Catalan Meteorological Service).\nStep 1: import the data\rWe import the data in csv format, the first column is the date, the second column the precipitation (pr) and the last column the average daily temperature (ta).\ndata \u0026lt;- read_csv(\u0026quot;meteo_tenerife.csv\u0026quot;) \r## Rows: 14303 Columns: 3\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (2): pr, ta\r## date (1): date\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rdata\r## # A tibble: 14,303 x 3\r## date pr ta\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1981-01-02 0 17.6\r## 2 1981-01-03 0 16.8\r## 3 1981-01-04 0 17.4\r## 4 1981-01-05 0 17.6\r## 5 1981-01-06 0 17 ## 6 1981-01-07 0 17.6\r## 7 1981-01-08 0 18.6\r## 8 1981-01-09 0 19.8\r## 9 1981-01-10 0 21.5\r## 10 1981-01-11 3.8 17.6\r## # ... with 14,293 more rows\r\rStep 2: preparing the data\rIn the second step we prepare the data to calculate the anomalies. To do this, we create three new columns: the month, the year, and the season of the year. Since our objective is to analyse winter anomalies, we cannot use the calendar year, because winter includes the month of December of one year and the months of January and February of the following. The custom function meteo_yr() extracts the year from a date indicating the starting month. The concept is similar to the hydrological year in which it starts on October 1.\nmeteo_yr \u0026lt;- function(dates, start_month = NULL) {\r# convert to POSIXlt\rdates.posix \u0026lt;- as.POSIXlt(dates)\r# year offset\roffset \u0026lt;- ifelse(dates.posix$mon \u0026gt;= start_month - 1, 1, 0)\r# new year\radj.year = dates.posix$year + 1900 + offset\rreturn(adj.year)\r}\rWe will use many functions of the package collection tidyverse (https://www.tidyverse.org/). The mutate() function helps to add new columns or change existing ones. To define the seasons, we use the case_when() function from the dplyr package, which has many advantages compared to a chain of ifelse(). In case_when() we use two-side formulas, on the one hand the condition and on the other the action when that condition is met. A two-sided formula in R consists of the operator ~. The binary operator %in% allows us to filter several values in a greater set.\ndata \u0026lt;- mutate(data, winter_yr = meteo_yr(date, 12),\rmonth = month(date), season = case_when(month %in% c(12,1:2) ~ \u0026quot;Winter\u0026quot;,\rmonth %in% 3:5 ~ \u0026quot;Spring\u0026quot;,\rmonth %in% 6:8 ~ \u0026quot;Summer\u0026quot;,\rmonth %in% 9:11 ~ \u0026quot;Autum\u0026quot;))\rdata\r## # A tibble: 14,303 x 6\r## date pr ta winter_yr month season\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1981-01-02 0 17.6 1981 1 Winter\r## 2 1981-01-03 0 16.8 1981 1 Winter\r## 3 1981-01-04 0 17.4 1981 1 Winter\r## 4 1981-01-05 0 17.6 1981 1 Winter\r## 5 1981-01-06 0 17 1981 1 Winter\r## 6 1981-01-07 0 17.6 1981 1 Winter\r## 7 1981-01-08 0 18.6 1981 1 Winter\r## 8 1981-01-09 0 19.8 1981 1 Winter\r## 9 1981-01-10 0 21.5 1981 1 Winter\r## 10 1981-01-11 3.8 17.6 1981 1 Winter\r## # ... with 14,293 more rows\r\rStep 3: estimate winter anomalies\rIn the next step we create a subset of the winter months. Then we group by the defined meteorological year and calculate the sum and average for precipitation and temperature, respectively. To facilitate the work, the magrittr package introduces the operator called pipe in the form %\u0026gt;% with the aim of combining several functions without the need to assign the result to a new object. The pipe operator passes the output of a function applied to the first argument of the next function. This way of combining functions allows you to chain several steps simultaneously. The %\u0026gt;% must be understood and pronounced as then.\ndata_inv \u0026lt;- filter(data, season == \u0026quot;Winter\u0026quot;) %\u0026gt;% group_by(winter_yr) %\u0026gt;%\rsummarise(pr = sum(pr, na.rm = TRUE),\rta = mean(ta, na.rm = TRUE))\rNow we only have to calculate the anomalies of precipitation and temperature. The columns pr_mean and ta_mean will contain the climate average, the reference for the anomalies with respect to the normal period 1981-2010. Therefore, we need to filter the values to the period before 2010, which we will do in the usual way of filtering vectors in R. Once we have the references we estimate the anomalies pr_anom and ta_anom. To facilitate the interpretation, in the case of precipitation we express the anomalies as percentage, with the average set at 0% instead of 100%.\nIn addition, we add three required columns with information for the creation of the graph: 1) labyr contains the year of each anomaly as long as it has been greater/less than -+10% or -+0.5ºC, respectively (this is for reducing the number of labels), 2) symb_point is a dummy variable in order to be able to create different symbols between the cases of (1), and 3) lab_font for highlighting in bold the year 2020.\ndata_inv \u0026lt;- mutate(data_inv, pr_mean = mean(pr[winter_yr \u0026lt;= 2010]), ta_mean = mean(ta[winter_yr \u0026lt;= 2010]),\rpr_anom = (pr*100/pr_mean)-100, ta_anom = ta-ta_mean,\rlabyr = case_when(pr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026lt; -10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026lt; -.5 ~ winter_yr,\rpr_anom \u0026gt; 10 \u0026amp; ta_anom \u0026gt; .5 ~ winter_yr),\rsymb_point = ifelse(!is.na(labyr), \u0026quot;yes\u0026quot;, \u0026quot;no\u0026quot;),\rlab_font = ifelse(labyr == 2020, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)\r)\r\r\rCreating the graph\rWe will build the chart adding layer by layer the distinctive elements: 1) the background with the different grids (Dry-Warm, Dry-Cold, etc.), 2) the points and labels, and 3) the style adjustments.\nPart 1\rThe idea is that the points with dry-warm anomalies are located in quadrant I (top-right) and those with wet-cold in quadrant III (bottom-left). Therefore, we must invert the sign in the precipitation anomalies. Then we create a data.frame with the label positions of the four quadrants. For the positions in x and y Inf and -Inf are used, which is equivalent to the maximum panel sides with respect to the data. However, it is necessary to adjust the position towards the extreme points within the panel with the known arguments of ggplot2: hjust and vjust.\ndata_inv_p \u0026lt;- mutate(data_inv, pr_anom = pr_anom * -1)\rbglab \u0026lt;- data.frame(x = c(-Inf, Inf, -Inf, Inf), y = c(Inf, Inf, -Inf, -Inf),\rhjust = c(1, 1, 0, 0), vjust = c(1, 0, 1, 0),\rlab = c(\u0026quot;Wet-Warm\u0026quot;, \u0026quot;Dry-Warm\u0026quot;,\r\u0026quot;Wet-Cold\u0026quot;, \u0026quot;Dry-Cold\u0026quot;))\rbglab\r## x y hjust vjust lab\r## 1 -Inf Inf 1 1 Wet-Warm\r## 2 Inf Inf 1 0 Dry-Warm\r## 3 -Inf -Inf 0 1 Wet-Cold\r## 4 Inf -Inf 0 0 Dry-Cold\r\rPart 2\rIn the second part we can start building the chart by adding all graphical elements. First we create the background with different colors of each quadrant. The function annotate() allows adding geometry layers without the use of variables within data.frames. With the geom_hline() and geom_vline() function we mark the quadrants horizontally and vertically using a dashed line. Finally, we draw the labels of each quadrant, using the function geom_text(). When we use other data sources than the main one used in ggplot(), we must indicate it with the argument data in the corresponding geometry function.\ng1 \u0026lt;- ggplot(data_inv_p, aes(pr_anom, ta_anom)) +\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = 0, ymax = Inf, fill = \u0026quot;#fc9272\u0026quot;, alpha = .6) + #wet-warm\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = 0, ymax = Inf, fill = \u0026quot;#cb181d\u0026quot;, alpha = .6) + #dry-warm\rannotate(\u0026quot;rect\u0026quot;, xmin = -Inf, xmax = 0, ymin = -Inf, ymax = 0, fill = \u0026quot;#2171b5\u0026quot;, alpha = .6) + #wet-cold\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = Inf, ymin = -Inf, ymax = 0, fill = \u0026quot;#c6dbef\u0026quot;, alpha = .6) + #dry-cold\rgeom_hline(yintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_vline(xintercept = 0,\rlinetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = bglab, aes(x, y, label = lab, hjust = hjust, vjust = vjust),\rfontface = \u0026quot;italic\u0026quot;, size = 5, angle = 90, colour = \u0026quot;white\u0026quot;)\rg1\r\rPart 3\rIn the third part we simply add the points of the anomalies and the labels of the years. The geom_text_repel() function is similar to the one known by default in ggplot2, geom_text(), but it repels overlapping text labels away from each other.\ng2 \u0026lt;- g1 + geom_point(aes(fill = symb_point, colour = symb_point),\rsize = 2.8, shape = 21, show.legend = FALSE) +\rgeom_text_repel(aes(label = labyr, fontface = lab_font),\rmax.iter = 5000, size = 3.5) g2\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\rPart 4\rIn the last part we adjust, in addition to the general style, the axes, the color type and the (sub)title. Remember that we changed the sign on precipitation anomalies. Hence, we must use the arguments breaks and labels in the function scale_x_continouous() to reverse the sign in the labels corresponding to the breaks.\ng3 \u0026lt;- g2 + scale_x_continuous(\u0026quot;Precipitation anomaly in %\u0026quot;,\rbreaks = seq(-100, 250, 10) * -1,\rlabels = seq(-100, 250, 10),\rlimits = c(min(data_inv_p$pr_anom), 100)) +\rscale_y_continuous(\u0026quot;Mean temperature anomaly in ºC\u0026quot;,\rbreaks = seq(-2, 2, 0.5)) +\rscale_fill_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;)) +\rscale_colour_manual(values = rev(c(\u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;))) +\rlabs(title = \u0026quot;Winter anomalies in Tenerife South\u0026quot;, caption = \u0026quot;Data: AEMET\\nNormal period 1981-2010\u0026quot;) +\rtheme_bw()\rg3\r## Warning: Removed 25 rows containing missing values (geom_text_repel).\r\n\r\r","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"2a3ac3bab08d4cf158760ff6e357e46f","permalink":"https://dominicroye.github.io/en/2020/visualize-climate-anomalies/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/en/2020/visualize-climate-anomalies/","section":"post","summary":"When we visualize precipitation and temperature anomalies, we simply use time series as bar graph indicating negative and positive values in red and blue. However, in order to have a better overview we need both anomalies in a single graph. In this way we could more easly answer the question of whether a particular season or month was dry-warm or wet-cold, and even compare these anomalies in the context of previous years.","tags":["anomaly","precipitation","temperature","climate","points"],"title":"Visualize climate anomalies","type":"post"},{"authors":["R Monjo","D Royé","J Martin-Vide"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"a7fd692ee43756f99192bf1657a2431c","permalink":"https://dominicroye.github.io/en/publication/2020-drought-fractality-essd/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/en/publication/2020-drought-fractality-essd/","section":"publication","summary":"Drought duration strongly depends on the definition thereof. In meteorology, dryness is habitually measured by means of fixed thresholds (e.g. 0.1 or 1 mm usually define dry spells) or climatic mean values (as is the case of the Standard-ised Precipitation Index), but this also depends on the aggregation time interval considered. However, robust measurements of drought duration are required for analysing the statistical significance of possible changes. Herein we have climatically classified the drought duration around the world according to their similarity to the voids of the Cantor set. Dryness time structure 5 can be concisely measured by the n-index (from the regular/irregular alternation of dry/wet spells), which is closely related to the Gini index and to a Cantor-based exponent. This enables the worlds climates to be classified into six large types based upon a new measure of drought duration. We performed the dry-spell analysis using the full global gridded daily Multi-Source Weighted-Ensemble Precipitation (MSWEP) dataset. The MSWEP combines gauge-, satellite-, and reanalysis-based data to provide reliable precipitation estimates. The study period comprises the years 1979-2016 (total of 45165 days), and a spatial 10 resolution of 0.5º, with a total of 259,197 grid points. Data set is publicly available at 10.5281/zenodo.3247041 (Monjo et al., 2019).","tags":["drought","classification","world","lacunarity","spatio-temporal patterns","dry spells"],"title":"Meteorological drought lacunarity around the world and its classification","type":"publication"},{"authors":["D Royé","F Tedim","J Martin-Vide","M Salis","J Vendrell","R Lovreglio","C Bouillon","V Leone"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"1f7b33fb7f23393295d2d37d98d160b3","permalink":"https://dominicroye.github.io/en/publication/2019-wildfire-ci-europe-land-degradation/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/en/publication/2019-wildfire-ci-europe-land-degradation/","section":"publication","summary":"The most widely used metrics to characterize wildfire regime and estimate the impact of wildfires are total burnt area (BA) and the number of fire events (FE). However, these are insufficient to analyse the threat to society of a new fire regime characterized by a higher occurrence of very large events. To overcome this weakness, we propose the use of a Concentration Index (CIB) which makes it possible to identify spatio-temporal patterns. The frequency distribution of BA follows a negative exponential distribution almost everywhere, in which a small minority of FE are responsible of the majority of BA. In this article, the spatio-temporal behaviour of BA is analysed in Western Mediterranean Europe, with particular focus on Portugal, Spain, France and Italy, using data from the European Forest Fire Information System and national wildfire databases. This is the first time that the CI has been applied to wildfire events. This research shows that, in most Mediterranean European countries, the amount of BA is increasingly related with a lower number of fires. The spatio-temporal distribution of CIB shows high variability in all of the countries analysed in Europe. Portugal and Spain show increasing significant trends of CIB +7.6% (p-value = 0.001) and +1.3% per decade (p-value = 0.003). Statistically significant correlations for Portugal, Spain and Italy are also found between the annual CIB and several teleconnection indices. The application of the CIB demonstrates its discriminatory ability, which is a key point in detecting vulnerable areas and temporal trends under climate change.","tags":["wildfire","concentration index","Europe","teleconnection","spatio-temporal patterns"],"title":"Wildfire burnt area patterns and trends in Western Mediterranean Europe via the application of a concentration index","type":"publication"},{"authors":["D Royé","A Tobías","C Iñiguez"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"d9a5421286714af3d1a6ee2d3589aab5","permalink":"https://dominicroye.github.io/en/publication/2020-era5-reanalysis-mortality-environmental-research/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/en/publication/2020-era5-reanalysis-mortality-environmental-research/","section":"publication","summary":"Background: Most studies use temperature observation data from weather stations near the analyzed region or city as the reference point for the exposure-response association. Climatic reanalysis data sets have already been used for climate studies, but are not yet used routinely in environmental epidemiology. Methods: We compared the mortality-temperature association using weather station temperature and ERA-5 reanalysis data for the 52 provincial capital cities in Spain, using time-series regression with distributed lag non-linear models. Results: The shape of temperature distribution is very close between the weather station and ERA-5 reanalysis data (correlation from 0.90 to 0.99). The overall cumulative exposure-response curves are very similar in their shape and risks estimates for cold and heat effects, although risk estimates for ERA-5 were slightly lower than for weather station temperature. Conclusions: Reanalysis data allow the estimation of the health effects of temperature, even in areas located far from weather stations or without any available.","tags":["Spain","temperature","reanalysis","ERA-5","mortality","weather stations"],"title":"Comparison of temperature-mortality associations using observed weather station and reanalysis data in 52 Spanish cities","type":"publication"},{"authors":null,"categories":["spatial analysis","R","R:elementary","gis"],"content":"\r\rThe first post of this year 2020, I will dedicate to a question that I was recently asked. The question was how to calculate the shortest distance between different points and how to know which is the closest point. When we work with spatial data in R, currently the easiest thing is to use the sf package in combination with the tidyverse collection of packages. We also use the units package which is very useful for working with units of measurement.\nPackages\r\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\runits\rSupport for measurement units in R vectors, matrices and arrays: propagation, conversion, derivation\r\rmaps\rDraw geographical maps\r\rrnaturalearth\rHold and facilitate interaction with Natural Earth map data\r\r\r\r# install the necessary packages\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;units\u0026quot;)) install.packages(\u0026quot;units\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;maps\u0026quot;)) install.packages(\u0026quot;maps\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r# load packages\rlibrary(maps)\rlibrary(sf) library(tidyverse)\rlibrary(units)\rlibrary(rnaturalearth)\r\rMeasurement units\rThe use of vectors and matrices with the units class allows us to calculate and transform units of measurement.\n# length\rl \u0026lt;- set_units(1:10, m)\rl\r## Units: [m]\r## [1] 1 2 3 4 5 6 7 8 9 10\r# convert units\rset_units(l, cm)\r## Units: [cm]\r## [1] 100 200 300 400 500 600 700 800 900 1000\r# sum different units\rset_units(l, cm) + l\r## Units: [cm]\r## [1] 200 400 600 800 1000 1200 1400 1600 1800 2000\r# area\ra \u0026lt;- set_units(355, ha)\rset_units(a, km2)\r## 3.55 [km2]\r# velocity\rvel \u0026lt;- set_units(seq(20, 50, 10), km/h)\rset_units(vel, m/s)\r## Units: [m/s]\r## [1] 5.555556 8.333333 11.111111 13.888889\r\rCapital cities of the world\rWe will use the capital cities of the whole world with the objective of calculating the distance to the nearest capital city and indicating the name/country.\n# set of world cities with coordinates\rhead(world.cities) # from the maps package\r## name country.etc pop lat long capital\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 31.31 34.34 0\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 31.32 34.35 0\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 30.55 72.11 0\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 29.36 47.98 0\r## 5 \u0026#39;Abud Palestine 2456 32.03 35.07 0\r## 6 \u0026#39;Abwein Palestine 3434 32.03 35.20 0\rTo convert points with longitude and latitude into a spatial object of class sf, we use the function st_as_sf(), indicating the coordinate columns and the coordinate reference system (WSG84, epsg: 4326).\n# convert the points into an sf object with CRS WSG84\rcities \u0026lt;- st_as_sf(world.cities, coords = c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;), crs = 4326)\rcities\r## Simple feature collection with 43645 features and 4 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -178.8 ymin: -54.79 xmax: 179.81 ymax: 78.93\r## Geodetic CRS: WGS 84\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Abasan al-Jadidah Palestine 5629 0 POINT (34.34 31.31)\r## 2 \u0026#39;Abasan al-Kabirah Palestine 18999 0 POINT (34.35 31.32)\r## 3 \u0026#39;Abdul Hakim Pakistan 47788 0 POINT (72.11 30.55)\r## 4 \u0026#39;Abdullah-as-Salam Kuwait 21817 0 POINT (47.98 29.36)\r## 5 \u0026#39;Abud Palestine 2456 0 POINT (35.07 32.03)\r## 6 \u0026#39;Abwein Palestine 3434 0 POINT (35.2 32.03)\r## 7 \u0026#39;Adadlay Somalia 9198 0 POINT (44.65 9.77)\r## 8 \u0026#39;Adale Somalia 5492 0 POINT (46.3 2.75)\r## 9 \u0026#39;Afak Iraq 22706 0 POINT (45.26 32.07)\r## 10 \u0026#39;Afif Saudi Arabia 41731 0 POINT (42.93 23.92)\rIn the next step, we filter by the capital cities encoded in the column capital with 1. The advantage of the sf package is the possibility of applying functions of the tidyverse collection to manipulate the attributes. In addition, we add a column with new labels using the str_c() function of the stringr package, which is similar to that of R Base paste().\n# filter the capital cities\rcapitals \u0026lt;- filter(cities, capital == 1)\r# create a new label combining name and country\rcapitals \u0026lt;- mutate(capitals, city_country = str_c(name, \u0026quot; (\u0026quot;, country.etc, \u0026quot;)\u0026quot;))\rcapitals \r## Simple feature collection with 230 features and 5 fields\r## Geometry type: POINT\r## Dimension: XY\r## Bounding box: xmin: -176.13 ymin: -51.7 xmax: 179.2 ymax: 78.21\r## Geodetic CRS: WGS 84\r## First 10 features:\r## name country.etc pop capital geometry\r## 1 \u0026#39;Amman Jordan 1303197 1 POINT (35.93 31.95)\r## 2 Abu Dhabi United Arab Emirates 619316 1 POINT (54.37 24.48)\r## 3 Abuja Nigeria 178462 1 POINT (7.17 9.18)\r## 4 Accra Ghana 2029143 1 POINT (-0.2 5.56)\r## 5 Adamstown Pitcairn 51 1 POINT (-130.1 -25.05)\r## 6 Addis Abeba Ethiopia 2823167 1 POINT (38.74 9.03)\r## 7 Agana Guam 1041 1 POINT (144.75 13.47)\r## 8 Algiers Algeria 2029936 1 POINT (3.04 36.77)\r## 9 Alofi Niue 627 1 POINT (-169.92 -19.05)\r## 10 Amsterdam Netherlands 744159 1 POINT (4.89 52.37)\r## city_country\r## 1 \u0026#39;Amman (Jordan)\r## 2 Abu Dhabi (United Arab Emirates)\r## 3 Abuja (Nigeria)\r## 4 Accra (Ghana)\r## 5 Adamstown (Pitcairn)\r## 6 Addis Abeba (Ethiopia)\r## 7 Agana (Guam)\r## 8 Algiers (Algeria)\r## 9 Alofi (Niue)\r## 10 Amsterdam (Netherlands)\r\rCalculate distances\rGeographical distance (Euclidean or greater circle) is calculated with the st_distance() function, either between two points, between one point and others or between all points. In the latter case we obtain a symmetric matrix of distances (NxN), taken pairwise between the elements of the capital city set. In the diagonal we find the combinations between the same points giving all null values.\n\r\r\rA\rB\rC\r\rA\r0\r255\r345\r\rB\r255\r0\r142\r\rC\r345\r142\r0\r\r\r\rFor instance, when we want to know the distance from Amsterdam to Abu Dhabi, Washington and Tokyo we pass two spatial objects.\n# calculate distance\rdist_amsterdam \u0026lt;- st_distance(slice(capitals, 10), slice(capitals, c(2, 220, 205)))\rdist_amsterdam # distance in meters\r## Units: [m]\r## [,1] [,2] [,3]\r## [1,] 5163124 6187634 9293710\rThe result is a matrix with a single row or column (depending on the order of the spatial objects) with a class of units. Thus it is possible to convert easily to another unit of measure. If we want to obtain a vector without class units, we only have to apply the function as.vector().\n# change from m to km\rset_units(dist_amsterdam, \u0026quot;km\u0026quot;)\r## Units: [km]\r## [,1] [,2] [,3]\r## [1,] 5163.124 6187.634 9293.71\r# units class to vector\ras.vector(dist_amsterdam)\r## [1] 5163124 6187634 9293710\rIn the second step, we estimate the distance matrix between all the capital cities. It is important to convert the null values to NA to subsequently obtain the correct matrix index.\n# calculate distance\rm_distance \u0026lt;- st_distance(capitals)\r# matrix\rdim(m_distance)\r## [1] 230 230\r# change m to km\rm_distance_km \u0026lt;- set_units(m_distance, km)\r# replace the distance of 0 m with NA\rm_distance_km[m_distance_km == set_units(0, km)] \u0026lt;- NA\r When the result is of the units class, it is necessary to use the same class to be able to make logical queries. For example, set_units(1, m) == set_units(1, m) vs. set_units(1, m) == 1.   To obtain the shortest distance, in addition to its position, we use the apply () function which in turn allows us to apply the function which.min() and min() on each row. It would also be possible to use the function on columns giving the same result. Finally, we add the results as new columns with the mutate() function. The indices in pos allow us to obtain the names of the nearest cities.\n# get the index (position) of the city and the distance\rpos \u0026lt;- apply(m_distance_km, 1, which.min)\rdist \u0026lt;- apply(m_distance_km, 1, min, na.rm = TRUE)\r# add the distance and get the name of the city\rcapitals \u0026lt;- mutate(capitals, nearest_city = city_country[pos], geometry_nearest = geometry[pos],\rdistance_city = dist)\r\rMap of distances to the next capital city\rFinally, we build a map representing the distance in proportional circles. To do this, we use the usual grammar of ggplot() by adding the geometry geom_sf(), first for the world map as background and then for the cities. In aes() we indicate, with the argument size = distance_city, the variable which we want to map proportionally. The theme_void() function removes all style elements. In addition, we define with the function coord_sf() a new projection indicating the proj4 format.\n# world map\rworld \u0026lt;- ne_countries(scale = 10, returnclass = \u0026quot;sf\u0026quot;)\r# map\rggplot(world) +\rgeom_sf(fill = \u0026quot;black\u0026quot;, colour = \u0026quot;white\u0026quot;) +\rgeom_sf(data = capitals, aes(size = distance_city),\ralpha = 0.7,\rfill = \u0026quot;#bd0026\u0026quot;,\rshape = 21,\rshow.legend = \u0026#39;point\u0026#39;) +\rcoord_sf(crs = \u0026quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\u0026quot;) +\rlabs(size = \u0026quot;Distance (km)\u0026quot;, title = \u0026quot;Distance to the next capital city\u0026quot;) +\rtheme_void()\r\n\r","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579392000,"objectID":"8a2c5ae507c17aff7d7769fa195c468f","permalink":"https://dominicroye.github.io/en/2020/geographic-distance/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/en/2020/geographic-distance/","section":"post","summary":"The first post of this year 2020, I will dedicate to a question that I was recently asked. The question was how to calculate the shortest distance between different points and how to know which is the closest point. When we work with spatial data in R, currently the easiest thing is to use the ``sf`` package in combination with the ``tidyverse`` collection of packages. We also use the ``units`` package which is very useful for working with units of measurement.","tags":["distance","points","cities"],"title":"Geographic distance","type":"post"},{"authors":["F Tedim","V Leone","M Coughlan","C Bouillon","G Xanthopoulos","D Royé","F.J.M. Correia","C Ferreira"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3a9da827765883a44fd563da52d3563f","permalink":"https://dominicroye.github.io/en/publication/2019-extreme-wildfire-definitions-elsevier/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/en/publication/2019-extreme-wildfire-definitions-elsevier/","section":"publication","summary":"Extreme wildfires events (EWEs) represent a minority among all wildfires but are a true challenge for societies as they exceed the current control capacity even in the best prepared regions of the world and they create destruction and a disproportionately number of fatalities. Recent events in Portugal, Chile, Greece, Australia, Canada, and the USA provide evidence that EWEs are an escalating worldwide problem, exceeding all previous records. Despite the challenges put by climate change, the occurrence of EWEs and disasters is not an ecological inevitability. In this chapter the rationale of the definition of EWEs and the integration of potential consequences on people and assets in a novel wildfire classification scheme are proposed and discussed. They are excellent instruments to enhance wildfire risk and crisis communication programs and to define appropriate prevention, mitigation, and response measures which are crucial to build up citizens safety.","tags":["control capacity","disaster extreme wildfire event (EWE)","fire intensity","mitigation","preparedness","prevention","rate of spread","socioeconomic system (SES)","wildfire classification"],"title":"Extreme wildfire events: the definition","type":"publication"},{"authors":["D Royé","R Codesido","A Tobías","M Taracido"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c7d58cef5a9424f0434cf5b5e6ddf591","permalink":"https://dominicroye.github.io/en/publication/2020-ehf-mortalidad-environmental-research/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/en/publication/2020-ehf-mortalidad-environmental-research/","section":"publication","summary":"In the current context of climate change, heat waves have become a significant problem for human health. This study assesses the effects of heat wave intensity on mortality (natural, respiratory and cardiovascular causes) in four of the largest cities of Spain (Barcelona, Bilbao, Madrid and Seville) during the period between 1990 and 2014. To model the heat wave severity the Excess Heat Factor (EHF) was used. The EHF is a two-component index. The first is the comparison of the three-day average daily mean temperature with the 95th percentile. The second component is a measure of the temperatures reached during the three-day period compared with the recent past (the previous 30 days). The city-specific exposure-response curves showed a non-linear J-shaped relationship between mortality and the EHF. Overall city-specific mortality risk estimates for 1th vs. 99th percentile increases range from the highest mortality risk with 2.73 (95% CI: 2.34-3.18) in Seville to a risk of 1.78 (95% CI: 1.62-1.97) and 1.78 (95% CI: 1.45-2.19) in Barcelona and Bilbao, respectively. When we compare our results with risk estimates for the analyzed Spanish cities in other studies, the heat wave related mortality risks seem to be clearly higher. Furthermore, it has been demonstrated that different heat wave days of the same event do not present the same degree of severity/intensity. Thus, the intensity of a heat wave is an important mortality risk indicator during heat wave days. Due to the low number of studies on the EHF as a heat wave intensity indicator and heat-related mortality and morbidity, further research is required to validate its application in other geographic areas and focus populations.","tags":["Spain","heat wave","heat excess factor","mortality","extreme temperature"],"title":"Heat wave intensity and daily mortality in four of the largest cities of Spain","type":"publication"},{"authors":null,"categories":["visualization","R","R:elementary","gis"],"content":"\r\rThe General Directorate for the Cadastre of Spain has spatial information of the all buildings except for the Basque Country and Navarra. This data set is part of the implementation of INSPIRE, the Space Information Infrastructure in Europe. More information can be found here. We will use the links (urls) in ATOM format, which is an RSS type for web feeds, allowing us to obtain the download link for each municipality.\n This blog post is a reduced version of the case study that you can find in our recent publication - Introduction to GIS with R - published by Dominic Royé and Roberto Serrano-Notivoli (in Spanish).   Packages\r\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\rfeedeR\rImport feeds RSS or ATOM\r\rtmap\rEasy creation of thematic maps\r\rclassInt\rCreate univariate class intervals\r\rsysfonts\rLoading system fonts and Google Fonts\r\rshowtext\rUsing fonts more easily in R graphs\r\r\r\r# install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;feedeR\u0026quot;)) install.packages(\u0026quot;feedeR\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;tmap\u0026quot;)) install.packages(\u0026quot;tmap\u0026quot;)\rif(!require(\u0026quot;classInt\u0026quot;)) install.packages(\u0026quot;classInt\u0026quot;)\rif(!require(\u0026quot;showtext\u0026quot;)) install.packages(\u0026quot;showtext\u0026quot;)\rif(!require(\u0026quot;sysfonts\u0026quot;)) install.packages(\u0026quot;sysfonts\u0026quot;)\rif(!require(\u0026quot;rvest\u0026quot;)) install.packages(\u0026quot;rvest\u0026quot;)\r# load packages\rlibrary(feedeR)\rlibrary(sf) library(fs)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(classInt)\rlibrary(tmap)\rlibrary(rvest)\r\rDownload links\rThe first url will give us access to a list of provinces, territorial headquarters (they do not always coincide with the oficial province), with new RSS links, which include the final download link for each municipality. In this case, we will download the buildings of Valencia. Cadastre data is updated every six months.\nurl \u0026lt;- \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.bu.atom.xml\u0026quot;\r# import RSS feed with provincial links\rprov_enlaces \u0026lt;- feed.extract(url)\rstr(prov_enlaces) # object is a list\r## List of 4\r## $ title : chr \u0026quot;Download service of Buildings. Territorial Office\u0026quot;\r## $ link : chr \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/ES.SDGC.BU.atom.xml\u0026quot;\r## $ updated: POSIXct[1:1], format: \u0026quot;2022-03-14\u0026quot;\r## $ items : tibble [52 x 5] (S3: tbl_df/tbl/data.frame)\r## ..$ title : chr [1:52] \u0026quot;Territorial office 02 Albacete\u0026quot; \u0026quot;Territorial office 03 Alicante\u0026quot; \u0026quot;Territorial office 04 Almería\u0026quot; \u0026quot;Territorial office 05 Avila\u0026quot; ...\r## ..$ date : POSIXct[1:52], format: \u0026quot;2022-03-14\u0026quot; \u0026quot;2022-03-14\u0026quot; ...\r## ..$ link : chr [1:52] \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/02/ES.SDGC.bu.atom_02.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/03/ES.SDGC.bu.atom_03.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/04/ES.SDGC.bu.atom_04.xml\u0026quot; \u0026quot;http://www.catastro.minhap.es/INSPIRE/buildings/05/ES.SDGC.bu.atom_05.xml\u0026quot; ...\r## ..$ description: chr [1:52] \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; \u0026quot;\\n\\n\\t\\t \u0026quot; ...\r## ..$ hash : chr [1:52] \u0026quot;d21ebb7975e59937\u0026quot; \u0026quot;bdba5e149f09e9d8\u0026quot; \u0026quot;03bcbcc7c5be2e17\u0026quot; \u0026quot;8a154202dd778143\u0026quot; ...\r# extract the table with the links\rprov_enlaces_tab \u0026lt;- as_tibble(prov_enlaces$items) %\u0026gt;% mutate(title = repair_encoding(title))\r## Warning: `html_encoding_repair()` was deprecated in rvest 1.0.0.\r## Instead, re-load using the `encoding` argument of `read_html()`\r## This warning is displayed once every 8 hours.\r## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\r## Best guess: UTF-8 (100% confident)\rprov_enlaces_tab\r## # A tibble: 52 x 5\r## title date link description hash ## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;\r## 1 \u0026quot;Territorial office 02 Albacete\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ d21e~\r## 2 \u0026quot;Territorial office 03 Alicante\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ bdba~\r## 3 \u0026quot;Territorial office 04 Almería\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 03bc~\r## 4 \u0026quot;Territorial office 05 Avila\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 8a15~\r## 5 \u0026quot;Territorial office 06 Badajoz\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 7d3f~\r## 6 \u0026quot;Territorial office 07 Baleares \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ 9c08~\r## 7 \u0026quot;Territorial office 08 Barcelona\u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ ff72~\r## 8 \u0026quot;Territorial office 09 Burgos \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ b431~\r## 9 \u0026quot;Territorial office 10 Cáceres \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ f79c~\r## 10 \u0026quot;Territorial office 11 Cádiz \u0026quot; 2022-03-14 00:00:00 http~ \u0026quot;\\n\\n\\t\\t ~ d702~\r## # ... with 42 more rows\rNow, we access and download the data from Valencia. To filter the final download link we use the filter() function of the dplyr package, searching for the name of the territorial headquarter and then the name of the municipality in capital letters with the str_detect() function of stringr. The pull() function allows us to extract a column from a data.frame.\n Currently the feed.extract() function does not import correctly in the encoding UTF-8 under Windows. For this reason, in some cities a bad codification of special characters may appear “CÃ¡diz”. To solve this problem we apply the repair_encoding() function of the rvest package. Nevertheless, problems can arise that have to be corrected manually.   # filter the province and get the RSS link\rval_atom \u0026lt;- filter(prov_enlaces_tab, str_detect(title, \u0026quot;Valencia\u0026quot;)) %\u0026gt;% pull(link)\r# import the RSS\rval_enlaces \u0026lt;- feed.extract(val_atom)\r# get the table with the download links\rval_enlaces_tab \u0026lt;- val_enlaces$items\rval_enlaces_tab \u0026lt;- mutate(val_enlaces_tab, title = repair_encoding(title),\rlink = repair_encoding(link)) \r## Best guess: UTF-8 (80% confident)\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r## Best guess: UTF-8 (80% confident)\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r## Warning in stringi::stri_conv(x, from): the Unicode code point \\U0000fffd cannot\r## be converted to destination encoding\r# filter the table with the name of the city\rval_link \u0026lt;- filter(val_enlaces_tab, str_detect(title, \u0026quot;VALENCIA\u0026quot;)) %\u0026gt;% pull(link)\rval_link\r## [1] \u0026quot;http://www.catastro.minhap.es/INSPIRE/Buildings/46/46900-VALENCIA/A.ES.SDGC.BU.46900.zip\u0026quot;\r\rData download\rThe download is done with the download.file() function that only has two main arguments, the download link and the path with the file name. In this case, we use the tempfile() function, which is useful for creating temporary files, that is, files that only exist in the memory for a certain time.\rThe file we download has the extension *.zip, so we must unzip it with another function (unzip()), which requires the name of the file and the name of the folder, where we want to unzip it. Finally, the URLencode() function encodes an URL address that contains special characters.\n# create a temporary file\rtemp \u0026lt;- tempfile()\r# download the data\rdownload.file(URLencode(val_link), temp)\r# unzip to a folder called buildings\runzip(temp, exdir = \u0026quot;buildings_valencia\u0026quot;) # change the name according to the city\r\rImport the data\rTo import the data we use the dir_ls() function of the fs package, which can obtain the files and folders of a specific path while filtering through a text pattern (regexp : regular expression). We apply the st_read() function of the sf package to the Geography Markup Language (GML) file.\n# get the path with the file\rfile_val \u0026lt;- dir_ls(\u0026quot;buildings_valencia\u0026quot;, regexp = \u0026quot;building.gml\u0026quot;) # change the folder if needed\r# import the data\rbuildings_val \u0026lt;- st_read(file_val)\r## Reading layer `Building\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\en\\post\\2019-11-01-visualize-urban-growth\\buildings_valencia\\A.ES.SDGC.BU.46900.building.gml\u0026#39; ## using driver `GML\u0026#39;\r## Simple feature collection with 36284 features and 24 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: 720570.9 ymin: 4351286 xmax: 734981.9 ymax: 4382906\r## Projected CRS: ETRS89 / UTM zone 30N\r\rData preparation\rWe only have to convert the column of the construction year (beginning) into a Date class. The date column contains some dates in --01-01 format, which does not correspond to any recognizable date. Therefore, we replace the first - with 0000.\nbuildings_val \u0026lt;- mutate(buildings_val, beginning = str_replace(beginning, \u0026quot;^-\u0026quot;, \u0026quot;0000\u0026quot;) %\u0026gt;% ymd_hms() %\u0026gt;% as_date()\r)\r## Warning: 4 failed to parse.\r\rDistribution chart\rBefore creating the maps of the construction years, which will reflect urban growth, we will make a graph of distribution of the beginning variable. We can clearly identify periods of urban expansion. We will use the ggplot2 package with the geometry of geom_density() for this purpose. The font_add_google() function of the sysfonts package allows us to download and include font families from Google.\n#font download\rsysfonts::font_add_google(\u0026quot;Montserrat\u0026quot;, \u0026quot;Montserrat\u0026quot;)\r#use showtext for fonts\rshowtext::showtext_auto() \r# limit the period after 1750\rfilter(buildings_val, beginning \u0026gt;= \u0026quot;1750-01-01\u0026quot;) %\u0026gt;%\rggplot(aes(beginning)) + geom_density(fill = \u0026quot;#2166ac\u0026quot;, alpha = 0.7) +\rscale_x_date(date_breaks = \u0026quot;20 year\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) +\rtheme_minimal(base_family = \u0026quot;Montserrat\u0026quot;) +\rlabs(y = \u0026quot;\u0026quot;,x = \u0026quot;\u0026quot;, title = \u0026quot;Evolution of urban development\u0026quot;)\r\rBuffer of 2,5 km for Valencia\rTo visualize better the distribution of urban growth, we limit the map to a radius of 2.5 km from the city center. Therefore, we use the geocode_OSM() function of the tmaptools package to obtain the coordinates of Valencia in class sf. Then we project the points to the system we use for the buildings (EPSG: 25830). The st_crs() function returns the coordinate system of a spatial object sf. Finally, we create with the function st_buffer() a buffer with 2500 m and the intersection with our building data. It is also possible to create a buffer in the form of a rectangle indicating the style with the argument endCapStyle =\" SQUARE \".\n# get the coordinates of Valencia\rciudad_point \u0026lt;- tmaptools::geocode_OSM(\u0026quot;Valencia\u0026quot;, as.sf = TRUE)\r# project the points\rciudad_point \u0026lt;- st_transform(ciudad_point, st_crs(buildings_val))\r# create the buffer\rpoint_bf \u0026lt;- st_buffer(ciudad_point, 2500) # radius of 2500 m\r# get the intersection between the buffer and the building\rbuildings_val25 \u0026lt;- st_intersection(buildings_val, point_bf)\r## Warning: attribute variables are assumed to be spatially constant throughout all\r## geometries\r\rPrepare data for mapping\rWe categorize the year into 15 groups using quartiles. It is also possible to modify the number of classes or the applied method (eg jenks, fisher, etc), you can find more details in the help ?classIntervals.\n# find 15 classes\rbr \u0026lt;- classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;)\r## Warning in classIntervals(year(buildings_val25$beginning), 15, \u0026quot;quantile\u0026quot;): var\r## has missing values, omitted in finding classes\r# create labels\rlab \u0026lt;- names(print(br, under = \u0026quot;\u0026lt;\u0026quot;, over = \u0026quot;\u0026gt;\u0026quot;, cutlabels = FALSE))\r## style: quantile\r## \u0026lt; 1890 1890 - 1912 1912 - 1925 1925 - 1930 1930 - 1940 1940 - 1950 ## 932 1350 947 594 1703 1054 ## 1950 - 1958 1958 - 1962 1962 - 1966 1966 - 1970 1970 - 1973 1973 - 1978 ## 1453 1029 1223 1158 1155 1190 ## 1978 - 1988 1988 - 1999 \u0026gt; 1999 ## 1149 1111 1244\r# categorize the year\rbuildings_val25 \u0026lt;- mutate(buildings_val25, yr_cl = cut(year(beginning), br$brks, labels = lab, include.lowest = TRUE))\r\rMap of Valencia\rFor the mapping, we will use the tmap package. It is an interesting alternative to ggplot2. It is a package of functions specialized in creating thematic maps. The philosophy of the package follows the same as in ggplot2, creating multiple layers with different functions, which always start with tm_ *and combine with +. Building a map with tmap always starts with tm_shape(), where the data, we want to draw, is defined. Then we add the corresponding geometry to the data type (tm_polygon(), tm_border(), tm_dots() or even tm_raster()). The tm_layout() function help us to configure the map style.\nWhen we need more colors than the maximum allowed by RColorBrewer, we can pass the colors to the colorRampPalette() function. This function interpolates a set of given colors.\n# colours\rcol_spec \u0026lt;- RColorBrewer::brewer.pal(11, \u0026quot;Spectral\u0026quot;)\r# colour ramp function\rcol_spec_fun \u0026lt;- colorRampPalette(col_spec)\r# create the final map\rtm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15), # adapt to the number of classes\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;) +\rtm_layout(bg.color = \u0026quot;black\u0026quot;,\router.bg.color = \u0026quot;black\u0026quot;,\rlegend.outside = TRUE,\rlegend.text.color = \u0026quot;white\u0026quot;,\rlegend.text.fontfamily = \u0026quot;Montserrat\u0026quot;, panel.label.fontfamily = \u0026quot;Montserrat\u0026quot;,\rpanel.label.color = \u0026quot;white\u0026quot;,\rpanel.label.bg.color = \u0026quot;black\u0026quot;,\rpanel.label.size = 5,\rpanel.label.fontface = \u0026quot;bold\u0026quot;)\rWe can export our map using the function tmap_save(\"name.png\", dpi = 300). I recommend using the dpi = 300 argument for a good image quality.\nAn alternative way to the tmap package is ggplot2.\n# create the final map\rggplot(buildings_val25) +\rgeom_sf(aes(fill = yr_cl), colour = \u0026quot;transparent\u0026quot;) +\rscale_fill_manual(values = col_spec_fun(15)) + # adapt to the number of classes\rlabs(title = \u0026quot;VALÈNCIA\u0026quot;, fill = \u0026quot;\u0026quot;) +\rguides(fill = guide_legend(keywidth = .7, keyheight = 2.7)) +\rtheme_void(base_family = \u0026quot;Montserrat\u0026quot;) +\rtheme(panel.background = element_rect(fill = \u0026quot;black\u0026quot;),\rplot.background = element_rect(fill = \u0026quot;black\u0026quot;),\rlegend.justification = .5,\rlegend.text = element_text(colour = \u0026quot;white\u0026quot;, size = 12),\rplot.title = element_text(colour = \u0026quot;white\u0026quot;, hjust = .5, size = 60,\rmargin = margin(t = 30)),\rplot.caption = element_text(colour = \u0026quot;white\u0026quot;,\rmargin = margin(b = 20), hjust = .5, size = 16),\rplot.margin = margin(r = 40, l = 40))\rTo export the result of ggplot we can use the function ggsave(\"name.png\").\n\rDynamic map with leaflet\rA very interesting advantage is the tmap_leaflet() function of the tmap package to easily pass a map created in the same frame to leaflet.\n# tmap object\rm \u0026lt;- tm_shape(buildings_val25) +\rtm_polygons(\u0026quot;yr_cl\u0026quot;, border.col = \u0026quot;transparent\u0026quot;,\rpalette = col_spec_fun(15), # adapt to the number of classes\rtextNA = \u0026quot;Without data\u0026quot;,\rtitle = \u0026quot;\u0026quot;)\r# dynamic map\rtmap_leaflet(m)\r\r\n\r","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"d741a3234b312561aff3984c44cc1e2e","permalink":"https://dominicroye.github.io/en/2019/visualize-urban-growth/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/en/2019/visualize-urban-growth/","section":"post","summary":"The General Directorate for the Cadastre of Spain has spatial information of the all buildings except for the Basque Country and Navarra. This data set is part of the implementation of [INSPIRE](https://inspire.ec.europa.eu/), the Space Information Infrastructure in Europe. More information can be found [here](http://www.catastro.meh.es/webinspire/index.html). We will use the links (*urls*) in *ATOM* format, which is an RSS type for web feeds, allowing us to obtain the download links for each municipality.","tags":["urban growth","city","urban geography"],"title":"Visualize urban growth","type":"post"},{"authors":["D Royé","R Serrano-Notivoli"],"categories":null,"content":"","date":1570406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570406400,"objectID":"5853d900e7df9f45aa72fc9f8af349c1","permalink":"https://dominicroye.github.io/en/publication/2019-manual-introduccion-sig-con-r-publicaciones-unizar/","publishdate":"2019-10-07T00:00:00Z","relpermalink":"/en/publication/2019-manual-introduccion-sig-con-r-publicaciones-unizar/","section":"publication","summary":"R tiene, como lenguaje de programación enfocado al análisis estadístico, todos los ingredientes para ser usado como herramienta de análisis espacial y representación cartográﬁca: es gratuito, permite personalizar, replicar y compartir los análisis de cualquier nivel de diﬁcultad y no tiene ninguna limitación en cuanto a cantidad de información a procesar o tipos de formato diferentes para gestionar. Esto le sitúa en una situación de ventaja que mejora día a día, gracias a su amplia comunidad de usuarios, respecto a un SIG (Sistema de Información Geográﬁca) convencional. Este manual explica, sin necesidad de conocimientos previos, cómo desarrollar con R todos los análisis disponibles en un SIG, con ejemplos sencillos y multitud de casos prácticos. Además, se muestran las enormes posibilidades de representación cartográﬁca, que van mucho más allá de la simple creación de mapas. R permite, desde exportar a cualquier formato de archivo, hasta crear mapas dinámicos para supublicación en Internet.","tags":["R","manual","visualisation","GIS"],"title":"Introducción a los SIG con R","type":"publication"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","A Benhamrouche"],"categories":null,"content":"","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"a769dca70eaff7cf8140b44abac4490f","permalink":"https://dominicroye.github.io/en/publication/2019-teleconnections-mediterranean-ij-climatology/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/en/publication/2019-teleconnections-mediterranean-ij-climatology/","section":"publication","summary":"This study has addressed the spatiotemporal distribution of the daily rainfall concentration and its relation to the teleconnection patterns across the Mediterranean (MR). Daily Concentration Index (CI) and the ordered n index () are used at annual time scale to reveal the statistical structure of precipitation across the MR based on 233 daily rainfall series for the period 1975–2015. Eight teleconnection patterns ,North Atlantic Oscillation (NAO), Mediterranean Oscillation (MO), Western Mediterranean Oscillation (WeMO), Upper Level Mediterranean Oscillation index (ULMO), East Atlantic (EA) pattern, East Atlantic/West Russia (EATL/WRUS) pattern, Scandinavia (SCAND) pattern and Southern Oscillation (SO) at annual time scale are selected. The spatiotemporal patterns in precipitation concentration indices, annual precipitation and their teleconnections with previous large-scale circulations are investigated. Results show a strong connection between the CI and the (r = 0.70, p","tags":["mediterranean","n-index","concentration index","teleconnection patterns","daily precipitation"],"title":"Spatiotemporal variability of daily precipitation concentration and its relationship to teleconnection patterns over the Mediterranean during 1975-2015","type":"publication"},{"authors":["D Royé","MT Zarrabeitia","P Fdez-Arroyabe","A Álvarez-Gutiérrez","A Santurtún"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"1f89cdb97a76af08cfdaa3e92bb4071d","permalink":"https://dominicroye.github.io/en/publication/2018-iam-cantabria-rev-esp-cardiologia/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/en/publication/2018-iam-cantabria-rev-esp-cardiologia/","section":"publication","summary":"Introduction and objectives. The role of the environment on cardiovascular health is becoming more prominent in the context of global change. The aim of this study was to analyze the relationship between apparent temperature (AT) and air pollutants and acute myocardial infarction (AMI) and to study the temporal pattern of this disease and its associated mortality. Methods. We performed a time-series study of admissions for AMI in Cantabria between 2001 and 2015. The association between environmental variables (including a biometeorological index, apparent AT) and AMI was analyzed using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of these variables on AMI, a lag non-linear model was fitted in a generalized additive model. Results. The incidence rate and the mortality followed a downward trend during the study period (CC=–0.714; P=.0002). An annual pattern was found in hospital admissions (P=.005), with the highest values being registered in winter; a weekly trend was also identified, reaching a minimum during the weekends (P=.000005). There was an inverse association between AT and the number of hospital admissions due to AMI and a direct association with particulate matter with a diameter smaller than 10 μm. Conclusions. Hospital admissions for AMI followed a downward trend between 2007 and 2015. Mortality associated with admissions due to this diagnosis has decreased. Predictive factors for this disease were AT and particulate matter with a diameter smaller than 10 μm.","tags":["acute myocardial infarction","apparent temperature","air pollutants","particulate matter"],"title":"Role of Apparent Temperature and Air Pollutants in Hospital Admissions for Acute Myocardial Infarction in the North of Spain","type":"publication"},{"authors":null,"categories":["visualization","R","R:intermediate"],"content":"\r\rNormally when we visualize monthly precipitation anomalies, we simply use a bar graph indicating negative and positive values with red and blue. However, it does not explain the general context of these anomalies. For example, what was the highest or lowest anomaly in each month? In principle, we could use a boxplot to visualize the distribution of the anomalies, but in this particular case they would not fit aesthetically, so we should look for an alternative. Here I present a very useful graphic form.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rreadr\rImport data\r\rggthemes\rThemes for ggplot2\r\rlubridate\rEasy manipulation of dates and times\r\rcowplot\rEasy creation of multiple graphics with ggplot2\r\r\r\r#we install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;ggthemes\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;cowplot\u0026quot;)) install.packages(\u0026quot;cowplot\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#packages\rlibrary(tidyverse) #include readr\rlibrary(ggthemes)\rlibrary(cowplot)\rlibrary(lubridate)\r\rPreparing the data\rFirst we import the daily precipitation of the selected weather station (download). We will use data from Santiago de Compostela (Spain) accessible through ECA\u0026amp;D.\nStep 1: import the data\rWe not only import the data in csv format, but we also make the first changes. We skip the first 21 rows that contain information about the weather station. In addition, we convert the date to the date class and replace missing values (-9999) with NA. The precipitation is given in 0.1 mm, therefore, we must divide the values by 10. Then we select the columns DATE and RR, and rename them.\ndata \u0026lt;- read_csv(\u0026quot;RR_STAID001394.txt\u0026quot;, skip = 21) %\u0026gt;%\rmutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10)) %\u0026gt;%\rselect(DATE:RR) %\u0026gt;% rename(date = DATE, pr = RR)\r## Rows: 27606 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rdata\r## # A tibble: 27,606 x 2\r## date pr\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1943-11-01 0.6\r## 2 1943-11-02 0 ## 3 1943-11-03 0 ## 4 1943-11-04 0 ## 5 1943-11-05 0 ## 6 1943-11-06 0 ## 7 1943-11-07 0 ## 8 1943-11-08 0 ## 9 1943-11-09 0 ## 10 1943-11-10 0 ## # ... with 27,596 more rows\r\rStep 2: creating monthly values\rIn the second step we calculate the monthly amounts of precipitation. To do this, a) we limit the period to the years after 1950, b) we add the month with its labels and the year as variables.\ndata \u0026lt;- mutate(data, mo = month(date, label = TRUE), yr = year(date)) %\u0026gt;%\rfilter(date \u0026gt;= \u0026quot;1950-01-01\u0026quot;) %\u0026gt;%\rgroup_by(yr, mo) %\u0026gt;% summarise(prs = sum(pr, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;yr\u0026#39;. You can override using the `.groups`\r## argument.\rdata\r## # A tibble: 833 x 3\r## # Groups: yr [70]\r## yr mo prs\r## \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Jan 55.6\r## 2 1950 Feb 349. ## 3 1950 Mar 85.8\r## 4 1950 Apr 33.4\r## 5 1950 May 272. ## 6 1950 Jun 111. ## 7 1950 Jul 35.4\r## 8 1950 Aug 76.4\r## 9 1950 Sep 85 ## 10 1950 Oct 53 ## # ... with 823 more rows\r\rStep 3: estimating anomalies\rNow we must estimate the normals of each month and join this table to our main data in order to calculate the monthly anomaly. We express the anomalies in percentage and subtract 100 to set the average to 0. In addition, we create a variable which indicates if the anomaly is negative or positive, and another with the date.\npr_ref \u0026lt;- filter(data, yr \u0026gt; 1981, yr \u0026lt;= 2010) %\u0026gt;%\rgroup_by(mo) %\u0026gt;%\rsummarise(pr_ref = mean(prs))\rdata \u0026lt;- left_join(data, pr_ref, by = \u0026quot;mo\u0026quot;)\rdata \u0026lt;- mutate(data, anom = (prs*100/pr_ref)-100, date = str_c(yr, as.numeric(mo), 1, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd(),\rsign= ifelse(anom \u0026gt; 0, \u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;) %\u0026gt;% factor(c(\u0026quot;pos\u0026quot;, \u0026quot;neg\u0026quot;)))\rWe can do a first test graph of anomalies (the classic one), for that we filter the year 2018. In this case we use a bar graph, remember that by default the function geom_bar() applies the counting of the variable. However, in this case we know y, hence we indicate with the argument stat = \"identity\" that it should use the given value in aes().\nfilter(data, yr == 2018) %\u0026gt;%\rggplot(aes(date, anom, fill = sign)) + geom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE) + scale_x_date(date_breaks = \u0026quot;month\u0026quot;, date_labels = \u0026quot;%b\u0026quot;) +\rscale_y_continuous(breaks = seq(-100, 100, 20)) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rlabs(y = \u0026quot;Precipitation anomaly (%)\u0026quot;, x = \u0026quot;\u0026quot;) +\rtheme_hc()\r\rStep 4: calculating the statistical metrics\rIn this last step we estimate the maximum, minimum value, the 25%/75% quantiles and the interquartile range per month of the entire time series.\ndata_norm \u0026lt;- group_by(data, mo) %\u0026gt;%\rsummarise(mx = max(anom),\rmin = min(anom),\rq25 = quantile(anom, .25),\rq75 = quantile(anom, .75),\riqr = q75-q25)\rdata_norm\r## # A tibble: 12 x 6\r## mo mx min q25 q75 iqr\r## \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Jan 193. -89.6 -43.6 56.3 99.9\r## 2 Feb 320. -96.5 -51.2 77.7 129. ## 3 Mar 381. -100 -40.6 88.2 129. ## 4 Apr 198. -93.6 -51.2 17.1 68.3\r## 5 May 141. -90.1 -45.2 17.0 62.2\r## 6 Jun 419. -99.3 -58.2 50.0 108. ## 7 Jul 311. -98.2 -77.3 27.1 104. ## 8 Aug 264. -100 -68.2 39.8 108. ## 9 Sep 241. -99.2 -64.9 48.6 113. ## 10 Oct 220. -99.0 -54.5 4.69 59.2\r## 11 Nov 137. -98.8 -44.0 39.7 83.7\r## 12 Dec 245. -91.8 -49.8 36.0 85.8\r\r\rCreating the graph\rTo create the anomaly graph with legend it is necessary to separate the main graph from the legends.\nPart 1\rIn this first part we are adding layer by layer the different elements: 1) the range of anomalies maximum-minimum 2) the interquartile range and 3) the anomalies of the year 2018.\n#range of anomalies maximum-minimum\rg1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#adding interquartile range\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#adding anomalies of the year 2018 g1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr == 2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE)\rg1.3\rFinally we change some last style settings.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Precipitation anomaly (%)\u0026quot;,\rbreaks = seq(-100, 500, 25),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Precipitation anomaly in Santiago de Compostela 2018\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Data: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\r\rPart 2\rWe still need a legend. First we create it for the normals.\n#legend data\rlegend \u0026lt;- filter(data_norm, mo == \u0026quot;Jan\u0026quot;)\rlegend_lab \u0026lt;- gather(legend, stat, y, mx:q75) %\u0026gt;%\rmutate(stat = factor(stat, stat, c(\u0026quot;maximum\u0026quot;,\r\u0026quot;minimum\u0026quot;,\r\u0026quot;Quantile 25%\u0026quot;,\r\u0026quot;Quantile 75%\u0026quot;)) %\u0026gt;%\ras.character())\r## Warning: attributes are not identical across measure variables;\r## they will be dropped\r#legend graph\rg2 \u0026lt;- legend %\u0026gt;% ggplot()+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;, width = 0.2) +\rgeom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;, width = 0.2) +\rgeom_text(data = legend_lab, aes(x = mo, y = y+c(12,-8,-10,12), label = stat), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.18, y = 40, label = \u0026quot;Period 1950-2018\u0026quot;, angle = 90, size = 3) +\rtheme_void() + theme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg2\rSecond, we create another legend for the current anomalies.\n#legend data\rlegend2 \u0026lt;- filter(data, yr == 1950, mo %in% c(\u0026quot;Jan\u0026quot;,\u0026quot;Feb\u0026quot;)) %\u0026gt;% ungroup() %\u0026gt;% select(mo, anom, sign)\rlegend2[2,1] \u0026lt;- \u0026quot;Jan\u0026quot;\rlegend_lab2 \u0026lt;- data.frame(mo = rep(\u0026quot;Jan\u0026quot;, 3), anom= c(110, 3, -70), label = c(\u0026quot;Positive anomaly\u0026quot;, \u0026quot;Average\u0026quot;, \u0026quot;Negative anomaly\u0026quot;))\r#legend graph\rg3 \u0026lt;- ggplot() + geom_bar(data = legend2,\raes(x = mo, y = anom, fill = sign),\ralpha = .6, colour = \u0026quot;NA\u0026quot;, stat = \u0026quot;identity\u0026quot;, show.legend = FALSE, width = 0.2) +\rgeom_segment(aes(x = .85, y = 0, xend = 1.15, yend = 0), linetype = \u0026quot;dashed\u0026quot;) +\rgeom_text(data = legend_lab2, aes(x = mo, y = anom+c(10,5,-13), label = label), fontface = \u0026quot;bold\u0026quot;, size = 2) +\rannotate(\u0026quot;text\u0026quot;, x = 1.25, y = 20, label =\u0026quot;Reference 1971-2010\u0026quot;, angle = 90, size = 3) +\rscale_fill_manual(values = c(\u0026quot;#99000d\u0026quot;, \u0026quot;#034e7b\u0026quot;)) +\rtheme_void() +\rtheme(plot.margin = unit(c(0, 0, 0, 0), \u0026quot;cm\u0026quot;))\rg3\r\rPart 3\rFinally, we only have to join the graph and the legends with the help of the cowplot package. The main function of cowplot is plot_grid() which is used for combining different graphs. However, in this case it is necessary to use more flexible functions to create less common formats. The ggdraw() function configures the basic layer of the graph, and the functions that are intended to operate on this layer start with draw_*.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .3, width = 1, height = 0.6) +\rdraw_plot(g2, x = 0, y = .15, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .15, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomaly2016_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\r\rMultiple facets\rIn this section we will make the same graph as in the previous one, but for several years.\nPart 1\rFirst we need to filter again by set of years, in this case from 2016 to 2018, using the operator %in%, we also add the function facet_grid() to ggplot, which allows us to plot the graph according to a variable. The formula used for the facet function is similar to the use in models: variable_by_row ~ variable_by_column. When we do not have a variable in the column, we should use the ..\n#range of anomalies maximum-minimum\rg1.1 \u0026lt;- ggplot(data_norm)+\rgeom_crossbar(aes(x = mo, y = 0, ymin = min, ymax = mx),\rfatten = 0, fill = \u0026quot;grey90\u0026quot;, colour = \u0026quot;NA\u0026quot;)\rg1.1\r#adding the interquartile range\rg1.2 \u0026lt;- g1.1 + geom_crossbar(aes(x = mo, y = 0, ymin = q25, ymax = q75),\rfatten = 0, fill = \u0026quot;grey70\u0026quot;)\rg1.2\r#adding the anomalies of the year 2016-2018\rg1.3 \u0026lt;- g1.2 + geom_crossbar(data = filter(data, yr %in% 2016:2018),\raes(x = mo, y = 0, ymin = 0, ymax = anom, fill = sign),\rfatten = 0, width = 0.7, alpha = .7, colour = \u0026quot;NA\u0026quot;,\rshow.legend = FALSE) +\rfacet_grid(yr ~ .)\rg1.3\rFinally we change some last style settings.\ng1 \u0026lt;- g1.3 + geom_hline(yintercept = 0)+\rscale_fill_manual(values=c(\u0026quot;#99000d\u0026quot;,\u0026quot;#034e7b\u0026quot;))+\rscale_y_continuous(\u0026quot;Anomalía de precipitación (%)\u0026quot;,\rbreaks = seq(-100, 500, 50),\rexpand = c(0, 5))+\rlabs(x = \u0026quot;\u0026quot;,\rtitle = \u0026quot;Anomalía de precipitación en Santiago de Compostela\u0026quot;,\rcaption=\u0026quot;Dominic Royé (@dr_xeo) | Datos: eca.knmi.nl\u0026quot;)+\rtheme_hc()\rg1\rWe use the same legend created for the previous graph.\n\r\rPart 2\rFinally, we join the graph and the legends with the help of the cowplot package. The only thing we must adjust here are the arguments in the draw_plot() function to correctly place the different parts.\np \u0026lt;- ggdraw() +\rdraw_plot(g1, x = 0, y = .18, width = 1, height = 0.8) +\rdraw_plot(g2, x = 0, y = .08, width = .2, height = .15) +\rdraw_plot(g3, x = 0.08, y = .08, width = .2, height = .15)\rp\rsave_plot(\u0026quot;pr_anomaly20162018_scq.png\u0026quot;, p, dpi = 300, base_width = 12.43, base_height = 8.42)\r\n\r","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562457600,"objectID":"280e92215b675beff1eedf9a9cc4df0c","permalink":"https://dominicroye.github.io/en/2019/visualize-monthly-precipitation-anomalies/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/en/2019/visualize-monthly-precipitation-anomalies/","section":"post","summary":"Normally when we visualize monthly precipitation anomalies, we simply use a bar graph indicating negative and positive values with red and blue. However, it does not explain the general context of these anomalies. For example, what was the highest or lowest anomaly in each month? In principle, we could use a *boxplot* to visualize the distribution of the anomalies, but in this particular case they would not fit aesthetically, so we should look for an alternative. Here I present a very useful graphic form.","tags":["anomalies","precipitation","climate","boxplot"],"title":"Visualize monthly precipitation anomalies","type":"post"},{"authors":["A Martí","J Taboada","D Royé","X Fonseca"],"categories":null,"content":"","date":1560384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560384000,"objectID":"4435d12996c0b605f0230622ff926476","permalink":"https://dominicroye.github.io/en/publication/2019-book-os-tempos-galicia-xerais/","publishdate":"2019-06-13T00:00:00Z","relpermalink":"/en/publication/2019-book-os-tempos-galicia-xerais/","section":"publication","summary":"Que récords climáticos se alcanzaron en Galicia? Cales son os lugares máis calorosos? E os máis fríos? Onde chove máis? Onde se rexistran máis días de precipitación? Que zonas gozan dun maior número de horas de sol? Cales teñen maior nebulosidade? Que lugares son os máis ventosos? Como está a afectar o cambio climático a Galicia? Neste libro atoparás as respostas a estas e a outras preguntas relacionadas co clima de Galicia e os diversos tipos de tempo que o caracterizan. Nas súas páxinas explícase como se producen os fenómenos meteorolóxicos máis habituais no noso territorio: as inversións térmicas, as néboas costeiras e orográficas, as illas de calor urbanas, os tipos de precipitación, o efecto foehn, as brisas mariñas, o arco da vella etc. A través de exemplos concretos, analízanse tamén os riscos climáticos que afectan regularmente a Galicia, como vagas de calor, temporais de neve, cicloxéneses explosivas e temporais de choiva e vento, tormentas, secas, tornados... Tamén poderás coñecer como está a cambiar o clima da nosa comunidade debido ao quecemento global e cales son os escenarios de futuro.","tags":["weather","Galicia","climate","dissemination"],"title":"Os tempos e o clima de Galicia","type":"publication"},{"authors":["A Vélez","J Martin-Vide","D Royé","O Santaella"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"e253b468309e82e9373188106dd0d2ef","permalink":"https://dominicroye.github.io/en/publication/2018-concentration-index-puerto-rico-applied-climatology/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/en/publication/2018-concentration-index-puerto-rico-applied-climatology/","section":"publication","summary":"The present study analyzes spatial patterns of precipitation Concentration Index (CI) in Puerto Rico considering the daily precipitation data of precipitation-gauging stations during 1971-2010. The South and East interior parts of Puerto Rico are characterized by higher CI and the West and North-West parts show lower CI. The annual CI and the rainy season CI show a gradient from South-East to North-West and the dry season CI shows a gradient from South to North. Another difference between the rainy season CI and dry season CI is that the former shows the lowest values of CI while the latter shows the highest values of CI. The different types of seasonal precipitation seem to play a major role on the spatial CI distribution. However, the local relief plays a major role in the spatial patterns due to the effect of the air circulation by the mountains. These findings can contribute to basin-scale water resource management (ooding, soil erosion, etc.) and conservation of the ecological environment.","tags":["concentration index","Puerto Rico","precipitation","spatial–temporal patterns"],"title":"Spatial Analysis of Daily Precipitation Concentration in Puerto Rico","type":"publication"},{"authors":null,"categories":["statistics","R","R:advanced"],"content":"\r\rWhen we try to estimate the correlation coefficient between multiple variables, the task is more complicated in order to obtain a simple and tidy result. A simple solution is to use the tidy() function from the {broom} package. In this post we are going to estimate the correlation coefficients between the annual precipitation of several Spanish cities and climate teleconnections indices: download. The data of the teleconnections are preprocessed, but can be downloaded directly from crudata.uea.ac.uk. The daily precipitation data comes from ECA\u0026amp;D.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackage\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rbroom\rConvert results of statistical functions (lm, t.test, cor.test, etc.) into tidy tables\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rlubridate\rEasy manipulation of dates and times\r\r\r\r#install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;broom\u0026quot;)) install.packages(\u0026quot;broom\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(broom)\rlibrary(fs)\rlibrary(lubridate)\r\rImport data\rFirst we have to import the daily precipitation of the selected weather stations.\nCreate a vector with all precipitation files using the function dir_ls() of the {fs} package.\rImport the data using the map_df() function of the {purrr} package that applies another function to a vector or list, and joins them together in a single data.frame.\rSelect the columns that interest us, b) Convert the date string into a date object using the ymd() function of the {lubridate} package, c) Create a new column yr with the years, d) Divide the precipitation values by 10 and reclassify absent values -9999 by NA, e) Finally, reclassify the ID of each weather station creating a factor with new labels.\r\r\rMore details about the use of the dir_ls() and map_df() functions can be found in this previous post.\n#precipitation files\rfiles \u0026lt;- dir_ls(regexp = \u0026quot;txt\u0026quot;)\rfiles\r## RR_STAID001393.txt RR_STAID001394.txt RR_STAID002969.txt RR_STAID003946.txt ## RR_STAID003969.txt\r#import all files and join them together\rpr \u0026lt;- files %\u0026gt;% map_df(read_csv, skip = 20)\r## Rows: 26329 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 27545 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 34729 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 24927 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r## Rows: 19813 Columns: 5\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (5): STAID, SOUID, DATE, RR, Q_RR\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rpr\r## # A tibble: 133,343 x 5\r## STAID SOUID DATE RR Q_RR\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1393 20611 19470301 0 0\r## 2 1393 20611 19470302 5 0\r## 3 1393 20611 19470303 0 0\r## 4 1393 20611 19470304 33 0\r## 5 1393 20611 19470305 15 0\r## 6 1393 20611 19470306 0 0\r## 7 1393 20611 19470307 85 0\r## 8 1393 20611 19470308 3 0\r## 9 1393 20611 19470309 0 0\r## 10 1393 20611 19470310 0 0\r## # ... with 133,333 more rows\r#create levels for the factor id \u0026lt;- unique(pr$STAID)\r#the corresponding labels\rlab \u0026lt;- c(\u0026quot;Bilbao\u0026quot;, \u0026quot;Santiago\u0026quot;, \u0026quot;Barcelona\u0026quot;, \u0026quot;Madrid\u0026quot;, \u0026quot;Valencia\u0026quot;)\r#first changes\rpr \u0026lt;- select(pr, STAID, DATE, RR) %\u0026gt;% mutate(DATE = ymd(DATE), RR = ifelse(RR == -9999, NA, RR/10), STAID = factor(STAID, id, lab), yr = year(DATE)) pr\r## # A tibble: 133,343 x 4\r## STAID DATE RR yr\r## \u0026lt;fct\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1947-03-01 0 1947\r## 2 Bilbao 1947-03-02 0.5 1947\r## 3 Bilbao 1947-03-03 0 1947\r## 4 Bilbao 1947-03-04 3.3 1947\r## 5 Bilbao 1947-03-05 1.5 1947\r## 6 Bilbao 1947-03-06 0 1947\r## 7 Bilbao 1947-03-07 8.5 1947\r## 8 Bilbao 1947-03-08 0.3 1947\r## 9 Bilbao 1947-03-09 0 1947\r## 10 Bilbao 1947-03-10 0 1947\r## # ... with 133,333 more rows\rWe still need to filter and calculate the annual amount of precipitation. Actually, it is not correct to sum up precipitation without taking into account that there are missing values, but it should be enough for this practice. Then, we change the table format with the spread() function, passing from a long to a wide table, that is, we want to obtain one column per weather station.\npr_yr \u0026lt;- filter(pr, DATE \u0026gt;= \u0026quot;1950-01-01\u0026quot;, DATE \u0026lt; \u0026quot;2018-01-01\u0026quot;) %\u0026gt;%\rgroup_by(STAID, yr)%\u0026gt;%\rsummarise(pr = sum(RR, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;STAID\u0026#39;. You can override using the\r## `.groups` argument.\rpr_yr\r## # A tibble: 324 x 3\r## # Groups: STAID [5]\r## STAID yr pr\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bilbao 1950 1342 ## 2 Bilbao 1951 1306.\r## 3 Bilbao 1952 1355.\r## 4 Bilbao 1953 1372.\r## 5 Bilbao 1954 1428.\r## 6 Bilbao 1955 1062.\r## 7 Bilbao 1956 1254.\r## 8 Bilbao 1957 968.\r## 9 Bilbao 1958 1272.\r## 10 Bilbao 1959 1450.\r## # ... with 314 more rows\rpr_yr \u0026lt;- spread(pr_yr, STAID, pr)\rpr_yr\r## # A tibble: 68 x 6\r## yr Bilbao Santiago Barcelona Madrid Valencia\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA\r## 2 1951 1306. 2344. 1072. 798. NA\r## 3 1952 1355. 1973. 415. 524. NA\r## 4 1953 1372. 973. 683. 365. NA\r## 5 1954 1428. 1348. 581. 246. NA\r## 6 1955 1062. 1769. 530. 473. NA\r## 7 1956 1254. 1533. 695. 480. NA\r## 8 1957 968. 1599. 635. 424. NA\r## 9 1958 1272. 2658. 479. 482. NA\r## 10 1959 1450. 2847. 1006 665. NA\r## # ... with 58 more rows\rThe next step is to import the climate teleconnection indices.\n#teleconnections\rtelecon \u0026lt;- read_csv(\u0026quot;teleconnections_indices.csv\u0026quot;)\r## Rows: 68 Columns: 9\r## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## dbl (9): yr, NAO, WeMO, EA, POL-EUAS, EATL/WRUS, MO, SCAND, AO\r## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\rtelecon\r## # A tibble: 68 x 9\r## yr NAO WeMO EA `POL-EUAS` `EATL/WRUS` MO SCAND AO\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 0.49 0.555 -0.332 0.0217 -0.0567 0.335 0.301 -0.199 ## 2 1951 -0.07 0.379 -0.372 0.402 -0.419 0.149 -0.00667 -0.365 ## 3 1952 -0.37 0.693 -0.688 -0.0117 -0.711 0.282 0.0642 -0.675 ## 4 1953 0.4 -0.213 -0.727 -0.0567 -0.0508 0.216 0.0233 -0.0164 ## 5 1954 0.51 1.20 -0.912 0.142 -0.318 0.386 0.458 -0.000583\r## 6 1955 -0.64 0.138 -0.824 -0.0267 0.154 0.134 0.0392 -0.362 ## 7 1956 0.17 0.617 -1.29 -0.197 0.0617 0.256 0.302 -0.163 ## 8 1957 -0.02 0.321 -0.952 -0.638 -0.167 0.322 -0.134 -0.342 ## 9 1958 0.12 0.941 -0.243 0.138 0.661 0.296 0.279 -0.868 ## 10 1959 0.49 -0.055 -0.23 -0.0142 0.631 0.316 0.725 -0.0762 ## # ... with 58 more rows\rFinally we need to join both tables by year.\ndata_all \u0026lt;- left_join(pr_yr, telecon, by = \u0026quot;yr\u0026quot;)\rdata_all\r## # A tibble: 68 x 14\r## yr Bilbao Santiago Barcelona Madrid Valencia NAO WeMO EA\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 1342 1800. 345 NA NA 0.49 0.555 -0.332\r## 2 1951 1306. 2344. 1072. 798. NA -0.07 0.379 -0.372\r## 3 1952 1355. 1973. 415. 524. NA -0.37 0.693 -0.688\r## 4 1953 1372. 973. 683. 365. NA 0.4 -0.213 -0.727\r## 5 1954 1428. 1348. 581. 246. NA 0.51 1.20 -0.912\r## 6 1955 1062. 1769. 530. 473. NA -0.64 0.138 -0.824\r## 7 1956 1254. 1533. 695. 480. NA 0.17 0.617 -1.29 ## 8 1957 968. 1599. 635. 424. NA -0.02 0.321 -0.952\r## 9 1958 1272. 2658. 479. 482. NA 0.12 0.941 -0.243\r## 10 1959 1450. 2847. 1006 665. NA 0.49 -0.055 -0.23 ## # ... with 58 more rows, and 5 more variables: `POL-EUAS` \u0026lt;dbl\u0026gt;,\r## # `EATL/WRUS` \u0026lt;dbl\u0026gt;, MO \u0026lt;dbl\u0026gt;, SCAND \u0026lt;dbl\u0026gt;, AO \u0026lt;dbl\u0026gt;\r\rCorrelation test\rA correlation test between paired samples can be done with the cor.test() function of R Base. In this case between the annual precipitation of Bilbao and the NAO index.\ncor_nao_bil \u0026lt;- cor.test(data_all$Bilbao, data_all$NAO,\rmethod = \u0026quot;spearman\u0026quot;)\r## Warning in cor.test.default(data_all$Bilbao, data_all$NAO, method = \u0026quot;spearman\u0026quot;):\r## Cannot compute exact p-value with ties\rcor_nao_bil\r## ## Spearman\u0026#39;s rank correlation rho\r## ## data: data_all$Bilbao and data_all$NAO\r## S = 44372, p-value = 0.2126\r## alternative hypothesis: true rho is not equal to 0\r## sample estimates:\r## rho ## 0.1531149\rstr(cor_nao_bil)\r## List of 8\r## $ statistic : Named num 44372\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## $ parameter : NULL\r## $ p.value : num 0.213\r## $ estimate : Named num 0.153\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ null.value : Named num 0\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## $ alternative: chr \u0026quot;two.sided\u0026quot;\r## $ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## $ data.name : chr \u0026quot;data_all$Bilbao and data_all$NAO\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;htest\u0026quot;\rWe see that the result is in an unmanageable and untidy format. It is a console summary of the correlation with all the statistical parameters necessary to get a conclusion about the relationship. The orginal structure is a list of vectors. However, the tidy() function of the {broom} package allows us to convert the result into a table format.\ntidy(cor_nao_bil)\r## # A tibble: 1 x 5\r## estimate statistic p.value method alternative\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.153 44372. 0.213 Spearman\u0026#39;s rank correlation rho two.sided\r\rApply the correlation test to multiple variables\rThe objective is to apply the correlation test to all weather stations and climate teleconnection indices.\nFirst, we must pass the table to the long format, that is, create a column/variable for the city and for the value of the corresponding precipitation. Then we repeat the same for the teleconnections indices.\ndata \u0026lt;- gather(data_all, city, pr, Bilbao:Valencia)%\u0026gt;%\rgather(telecon, index, NAO:AO)\rdata\r## # A tibble: 2,720 x 5\r## yr city pr telecon index\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1950 Bilbao 1342 NAO 0.49\r## 2 1951 Bilbao 1306. NAO -0.07\r## 3 1952 Bilbao 1355. NAO -0.37\r## 4 1953 Bilbao 1372. NAO 0.4 ## 5 1954 Bilbao 1428. NAO 0.51\r## 6 1955 Bilbao 1062. NAO -0.64\r## 7 1956 Bilbao 1254. NAO 0.17\r## 8 1957 Bilbao 968. NAO -0.02\r## 9 1958 Bilbao 1272. NAO 0.12\r## 10 1959 Bilbao 1450. NAO 0.49\r## # ... with 2,710 more rows\rTo apply the test to all cities, we need the corresponding groupings. Therefore, we use the group_by() function for indicating the two groups: city and telecon. In addition, we apply the nest() function of the {tidyr} package ({tidyverse} collection) with the aim of creating lists of tables nested per row. In other words, in each row of each city and teleconnection index we will have a new table that contains the year, the precipitation value and the value of each teleconection, correspondingly.\ndata_nest \u0026lt;- group_by(data, city, telecon) %\u0026gt;% nest()\rhead(data_nest)\r## # A tibble: 6 x 3\r## # Groups: city, telecon [6]\r## city telecon data ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt;\rstr(head(slice(data_nest, 1)))\r## grouped_df [6 x 3] (S3: grouped_df/tbl_df/tbl/data.frame)\r## $ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data :List of 6\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## - attr(*, \u0026quot;groups\u0026quot;)= tibble [6 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows : list\u0026lt;int\u0026gt; [1:6] ## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..@ ptype: int(0) ## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi TRUE\rThe next step is to create a function, in which we define the correlation test and pass it to the clean format using the tidy() function, which we apply to each groupings.\ncor_fun \u0026lt;- function(df) cor.test(df$pr, df$index, method = \u0026quot;spearman\u0026quot;) %\u0026gt;% tidy()\rNow we only have to apply our function to the column that contains the tables for each combination between city and teleconnection. To do this, we use the map() function that applies another function to a vector or list. What we do is create a new column that contains the result, a statistical summary table, for each combination.\ndata_nest \u0026lt;- mutate(data_nest, model = map(data, cor_fun))\rhead(data_nest)\r## # A tibble: 6 x 4\r## # Groups: city, telecon [6]\r## city telecon data model ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 Bilbao NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 2 Santiago NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 3 Barcelona NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 4 Madrid NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 5 Valencia NAO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\r## 6 Bilbao WeMO \u0026lt;tibble [68 x 3]\u0026gt; \u0026lt;tibble [1 x 5]\u0026gt;\rstr(head(slice(data_nest, 1)))\r## grouped_df [6 x 4] (S3: grouped_df/tbl_df/tbl/data.frame)\r## $ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## $ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## $ data :List of 6\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.199333 -0.364667 -0.674917 -0.016417 -0.000583 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.333 -0.372 -0.688 -0.727 -0.912 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] -0.0567 -0.4192 -0.7108 -0.0508 -0.3175 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.335 0.149 0.282 0.216 0.386 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.49 -0.07 -0.37 0.4 0.51 -0.64 0.17 -0.02 0.12 0.49 ...\r## ..$ : tibble [68 x 3] (S3: tbl_df/tbl/data.frame)\r## .. ..$ yr : num [1:68] 1950 1951 1952 1953 1954 ...\r## .. ..$ pr : num [1:68] 345 1072 415 683 581 ...\r## .. ..$ index: num [1:68] 0.0217 0.4025 -0.0117 -0.0567 0.1425 ...\r## $ model :List of 6\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.00989\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 52912\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.936\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.295\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 67832\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.161\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43966\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.19\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.255\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 65754\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.0361\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num -0.0203\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 53460\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.869\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## ..$ : tibble [1 x 5] (S3: tbl_df/tbl/data.frame)\r## .. ..$ estimate : Named num 0.178\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;rho\u0026quot;\r## .. ..$ statistic : Named num 43082\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;S\u0026quot;\r## .. ..$ p.value : num 0.147\r## .. ..$ method : chr \u0026quot;Spearman\u0026#39;s rank correlation rho\u0026quot;\r## .. ..$ alternative: chr \u0026quot;two.sided\u0026quot;\r## - attr(*, \u0026quot;groups\u0026quot;)= tibble [6 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ city : chr [1:6] \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; \u0026quot;Barcelona\u0026quot; ...\r## ..$ telecon: chr [1:6] \u0026quot;AO\u0026quot; \u0026quot;EA\u0026quot; \u0026quot;EATL/WRUS\u0026quot; \u0026quot;MO\u0026quot; ...\r## ..$ .rows : list\u0026lt;int\u0026gt; [1:6] ## .. ..$ : int 1\r## .. ..$ : int 2\r## .. ..$ : int 3\r## .. ..$ : int 4\r## .. ..$ : int 5\r## .. ..$ : int 6\r## .. ..@ ptype: int(0) ## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi TRUE\rHow can we undo the list of tables in each row of our table?\nFirst we eliminate the column with the data and then simply we can apply the unnest() function.\ncorr_pr \u0026lt;- select(data_nest, -data) %\u0026gt;% unnest()\r## Warning: `cols` is now required when using unnest().\r## Please use `cols = c(model)`\rcorr_pr\r## # A tibble: 40 x 7\r## # Groups: city, telecon [40]\r## city telecon estimate statistic p.value method alternative\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Bilbao NAO 0.153 44372. 0.213 Spearman\u0026#39;s rank co~ two.sided ## 2 Santiago NAO -0.181 61902. 0.139 Spearman\u0026#39;s rank co~ two.sided ## 3 Barcelona NAO -0.0203 53460. 0.869 Spearman\u0026#39;s rank co~ two.sided ## 4 Madrid NAO -0.291 64692. 0.0169 Spearman\u0026#39;s rank co~ two.sided ## 5 Valencia NAO -0.113 27600. 0.422 Spearman\u0026#39;s rank co~ two.sided ## 6 Bilbao WeMO 0.404 31242 0.000706 Spearman\u0026#39;s rank co~ two.sided ## 7 Santiago WeMO 0.332 35014 0.00594 Spearman\u0026#39;s rank co~ two.sided ## 8 Barcelona WeMO 0.0292 50862 0.813 Spearman\u0026#39;s rank co~ two.sided ## 9 Madrid WeMO 0.109 44660 0.380 Spearman\u0026#39;s rank co~ two.sided ## 10 Valencia WeMO -0.252 31056 0.0688 Spearman\u0026#39;s rank co~ two.sided ## # ... with 30 more rows\rThe result is a table in which we can see the correlations and their statistical significance for each city and teleconnection index.\n\rHeatmap of the results\rFinally, we make a heatmap of the obtained result. But, previously we create a column that indicates whether the correlation is significant with p-value less than 0.05.\ncorr_pr \u0026lt;- mutate(corr_pr, sig = ifelse(p.value \u0026lt;0.05, \u0026quot;Sig.\u0026quot;, \u0026quot;Non Sig.\u0026quot;))\rggplot()+\rgeom_tile(data = corr_pr,\raes(city, telecon, fill = estimate),\rsize = 1,\rcolour = \u0026quot;white\u0026quot;)+\rgeom_tile(data = filter(corr_pr, sig == \u0026quot;Sig.\u0026quot;),\raes(city, telecon),\rsize = 1,\rcolour = \u0026quot;black\u0026quot;,\rfill = \u0026quot;transparent\u0026quot;)+\rgeom_text(data = corr_pr,\raes(city, telecon, label = round(estimate, 2),\rfontface = ifelse(sig == \u0026quot;Sig.\u0026quot;, \u0026quot;bold\u0026quot;, \u0026quot;plain\u0026quot;)))+\rscale_fill_gradient2(breaks = seq(-1, 1, 0.2))+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;, p.value = \u0026quot;\u0026quot;)+\rtheme_minimal()+\rtheme(panel.grid.major = element_blank(),\rpanel.border = element_blank(),\rpanel.background = element_blank(),\raxis.ticks = element_blank())\r\n\r","date":1555459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"4e3fd56428046da415479e92ecd4bb6e","permalink":"https://dominicroye.github.io/en/2019/tidy-correlation-tests-in-r/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/en/2019/tidy-correlation-tests-in-r/","section":"post","summary":"When we try to estimate the correlation coefficient between multiple variables, the task is more complicated in order to obtain a simple and tidy result. A simple solution is to use the ``tidy()`` function from the *{broom}* package. As an example, in this post we are going to estimate the correlation coefficients between the annual precipitation of several Spanish cities and climate teleconnections indices.","tags":["correlation","variables","tidy","tests"],"title":"Tidy correlation tests in R","type":"post"},{"authors":["M Lemus-Canovas","JA Lopez-Bustins","J Martin-Vide","D Royé"],"categories":null,"content":"","date":1555286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555286400,"objectID":"27583a24ed658f26c0b96abd4fe4a35a","permalink":"https://dominicroye.github.io/en/publication/2019-rpackage-synoptreg-environmental-modelling/","publishdate":"2019-04-15T00:00:00Z","relpermalink":"/en/publication/2019-rpackage-synoptreg-environmental-modelling/","section":"publication","summary":"Spatial knowledge of the climatic or environmental variables associated with the most frequent circulation types is essential with regard to developing strategies to address the risk of avalanches, floods, soil erosion, air pollution or other natural hazards. In order to derive an environmental regionalization, we present an Open Source R package known as synoptReg, which combines the spatialization of environmental variables based on the atmospheric circulation types. The synoptReg package contains a set of functions, which we will employ (1) to perform a PCA-based synoptic classification using an atmospheric variable; (2) to map the spatial distribution of the selected environmental variable based upon the circulation types; (3) to develop a spatial environmental regionalization based on the previous results. We illustrate the usefulness of the package for a case study in the Alps area.","tags":["Alps","environmental regionalization","R package","synoptReg","synoptic classification"],"title":"synoptReg: An R package for computing a synoptic climate classification and a spatial regionalization of environmental data","type":"publication"},{"authors":["D Royé","María T Zarrabeitia","Javier Riancho","Ana Santurtún"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"3f3a0498b1f87d0adea14706995bea90","permalink":"https://dominicroye.github.io/en/publication/2019-ictus-madrid-environmental-research/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/en/publication/2019-ictus-madrid-environmental-research/","section":"publication","summary":"The understanding of the role of environment on the pathogenesis of stroke is gaining importance in the context of climate change. This study analyzes the temporal pattern of ischemic stroke (IS) in Madrid, Spain, during a 13-year period (2001-2013), and the relationship between ischemic stroke (admissions and deaths) incidence and environmental factors on a daily scale by using a quasi-Poisson regression model. To assess potential delayed and non-linear effects of air pollutants and Apparent Temperature (AT), a biometeorological index which represents human thermal comfort on IS, a lag non-linear model was fitted in a generalized additive model. The mortality rate followed a downward trend over the studied period, however admission rates progressively increased. Our results show that both increases and decreases in AT had a marked relationship with IS deaths, while hospital admissions were only associated with low AT. When analyzing the cumulative effects (for lag 0 to 14 days), with an AT of 1.7°C (percentile 5%) a RR of 1.20 (95% CI, 1.05-1.37) for IS mortality and a RR of 1.09 (95% CI, 0.91-1.29) for morbidity is estimated. Concerning gender differences, men show higher risks of mortality in low temperatures and women in high temperatures. No significant relationship was found between air pollutant concentrations and IS morbi mortality, but this result must be interpreted with caution, since there are strong spatial fluctuations of the former between nearby geographical areas that make it difficult to perform correlation analyses.","tags":["short‐term effects","Spain","Madrid","thermal environment","ischemic stroke","air pollutants","apparent temperature","mortality","hospital admissions"],"title":"A time series analysis of the relationship between Apparent Temperature, Air Pollutants and Ischemic Stroke in Madrid, Spain","type":"publication"},{"authors":null,"categories":["management","R","R:intermediate"],"content":"\r\rWe usually work with different data sources, and sometimes we can find tables distributed over several Excel sheets. In this post we are going to import the average daily temperature of Madrid and Berlin which is found in two Excel files with sheets for each year between 2000 and 2005: download.\nPackages\rIn this post we will use the following packages:\n\r\r\r\rPackages\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, purrr, etc.\r\rfs\rProvides a cross-platform, uniform interface to file system operations\r\rreadxl\rImport Excel files\r\r\r\r#install the packages if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;fs\u0026quot;)) install.packages(\u0026quot;fs\u0026quot;)\rif(!require(\u0026quot;readxl\u0026quot;)) install.packages(\u0026quot;readxl\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(fs)\rlibrary(readxl)\rBy default, the read_excel() function imports the first sheet. To import a different sheet it is necessary to indicate the number or name with the argument sheet (second argument).\n#import first sheet\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;)\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#import third sheet\rread_excel(\u0026quot;madrid_temp.xlsx\u0026quot;, 3)\r## # A tibble: 365 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2002-01-01 00:00:00 8.7 2002\r## 2 2002-01-02 00:00:00 7.4 2002\r## 3 2002-01-03 00:00:00 8.5 2002\r## 4 2002-01-04 00:00:00 9.2 2002\r## 5 2002-01-05 00:00:00 9.3 2002\r## 6 2002-01-06 00:00:00 7.3 2002\r## 7 2002-01-07 00:00:00 5.4 2002\r## 8 2002-01-08 00:00:00 5.6 2002\r## 9 2002-01-09 00:00:00 6.8 2002\r## 10 2002-01-10 00:00:00 6.1 2002\r## # ... with 355 more rows\rThe excel_sheets() function can extract the names of the sheets.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rpath %\u0026gt;%\rexcel_sheets()\r## [1] \u0026quot;2000\u0026quot; \u0026quot;2001\u0026quot; \u0026quot;2002\u0026quot; \u0026quot;2003\u0026quot; \u0026quot;2004\u0026quot; \u0026quot;2005\u0026quot;\rThe results are the sheet names and we find the years from 2000 to 2005. The most important function to read multiple sheets is map() of the {purrr} package, which is part of the {tidyverse] collection. map() allows you to apply a function to each element of a vector or list.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap(read_excel,\rpath = path)\rstr(mad)\r## List of 6\r## $ 2000: tibble [366 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:366], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## ..$ ta : num [1:366] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:366] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ 2001: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2001-01-01\u0026quot; \u0026quot;2001-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 8.2 8.8 7.5 9.2 10 9 5.5 4.6 3 7.9 ...\r## ..$ yr : num [1:365] 2001 2001 2001 2001 2001 ...\r## $ 2002: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2002-01-01\u0026quot; \u0026quot;2002-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 8.7 7.4 8.5 9.2 9.3 7.3 5.4 5.6 6.8 6.1 ...\r## ..$ yr : num [1:365] 2002 2002 2002 2002 2002 ...\r## $ 2003: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2003-01-01\u0026quot; \u0026quot;2003-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 9.4 10.8 9.7 9.2 6.3 6.6 3.8 6.4 4.3 3.4 ...\r## ..$ yr : num [1:365] 2003 2003 2003 2003 2003 ...\r## $ 2004: tibble [366 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:366], format: \u0026quot;2004-01-01\u0026quot; \u0026quot;2004-01-02\u0026quot; ...\r## ..$ ta : num [1:366] 6.6 5.9 7.8 8.1 6.4 5.7 5.2 6.9 11.8 12.2 ...\r## ..$ yr : num [1:366] 2004 2004 2004 2004 2004 ...\r## $ 2005: tibble [365 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:365], format: \u0026quot;2005-01-01\u0026quot; \u0026quot;2005-01-02\u0026quot; ...\r## ..$ ta : num [1:365] 7.1 7.8 6.4 5.6 4.4 6.8 7.4 6 5.2 4.2 ...\r## ..$ yr : num [1:365] 2005 2005 2005 2005 2005 ...\rThe result is a named list with the name of each sheet that contains the data.frame. Since it is the same table in all sheets, we could use the function bind_rows(), however, there is a variant of map() that directly joins all the tables by row: map_df(). If it were necessary to join by column, map_dfc() could be used.\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path)\rmad\r## # A tibble: 2,192 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 2,182 more rows\rIn our case we have a column in each sheet (year, but also the date) that differentiates each table. If it were not the case, we should use the name of the sheets as a new column when joining all of them. In bind_rows() it can be done with the .id argument by assigning a name for the column. The same works for map_df().\npath \u0026lt;- \u0026quot;madrid_temp.xlsx\u0026quot;\rmad \u0026lt;- path %\u0026gt;%\rexcel_sheets() %\u0026gt;%\rset_names() %\u0026gt;%\rmap_df(read_excel,\rpath = path,\r.id = \u0026quot;yr2\u0026quot;)\rstr(mad)\r## tibble [2,192 x 4] (S3: tbl_df/tbl/data.frame)\r## $ yr2 : chr [1:2192] \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; \u0026quot;2000\u0026quot; ...\r## $ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num [1:2192] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## $ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\rBut how do we import multiple Excel files?\nTo do this, first we must know the dir_ls() function from the {fs} package. Indeed, there is the dir() function of R Base, but the advantages of the recent package are several, especially the compatibility with the {tidyverse} collection.\ndir_ls()\r## berlin_temp.xlsx featured.png index.en.html index.en.Rmd ## index.en.Rmd.lock~ index.en_files madrid_temp.xlsx\r#we can filter the files that we want\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) \r## berlin_temp.xlsx madrid_temp.xlsx\rWe import the two Excel files.\n#without joining\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;%\rmap(read_excel)\r## $berlin_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 1.2 2000\r## 2 2000-01-02 00:00:00 3.6 2000\r## 3 2000-01-03 00:00:00 5.7 2000\r## 4 2000-01-04 00:00:00 5.1 2000\r## 5 2000-01-05 00:00:00 2.2 2000\r## 6 2000-01-06 00:00:00 1.8 2000\r## 7 2000-01-07 00:00:00 4.2 2000\r## 8 2000-01-08 00:00:00 4.2 2000\r## 9 2000-01-09 00:00:00 4.2 2000\r## 10 2000-01-10 00:00:00 1.7 2000\r## # ... with 356 more rows\r## ## $madrid_temp.xlsx\r## # A tibble: 366 x 3\r## date ta yr\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2000-01-01 00:00:00 5.4 2000\r## 2 2000-01-02 00:00:00 5 2000\r## 3 2000-01-03 00:00:00 3.5 2000\r## 4 2000-01-04 00:00:00 4.3 2000\r## 5 2000-01-05 00:00:00 0.6 2000\r## 6 2000-01-06 00:00:00 3.8 2000\r## 7 2000-01-07 00:00:00 6.2 2000\r## 8 2000-01-08 00:00:00 5.4 2000\r## 9 2000-01-09 00:00:00 5.5 2000\r## 10 2000-01-10 00:00:00 4.8 2000\r## # ... with 356 more rows\r#joining with a new id column\rdir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;%\rmap_df(read_excel, .id = \u0026quot;city\u0026quot;)\r## # A tibble: 732 x 4\r## city date ta yr\r## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 berlin_temp.xlsx 2000-01-01 00:00:00 1.2 2000\r## 2 berlin_temp.xlsx 2000-01-02 00:00:00 3.6 2000\r## 3 berlin_temp.xlsx 2000-01-03 00:00:00 5.7 2000\r## 4 berlin_temp.xlsx 2000-01-04 00:00:00 5.1 2000\r## 5 berlin_temp.xlsx 2000-01-05 00:00:00 2.2 2000\r## 6 berlin_temp.xlsx 2000-01-06 00:00:00 1.8 2000\r## 7 berlin_temp.xlsx 2000-01-07 00:00:00 4.2 2000\r## 8 berlin_temp.xlsx 2000-01-08 00:00:00 4.2 2000\r## 9 berlin_temp.xlsx 2000-01-09 00:00:00 4.2 2000\r## 10 berlin_temp.xlsx 2000-01-10 00:00:00 1.7 2000\r## # ... with 722 more rows\rHowever, in this case we only import the first sheet of each Excel file. To solve this problem, we must create our own function. In this function we do what we previously did individually.\nread_multiple_excel \u0026lt;- function(path) {\rpath %\u0026gt;%\rexcel_sheets() %\u0026gt;% set_names() %\u0026gt;% map_df(read_excel, path = path)\r}\rWe apply our created function to import multiple sheets of several Excel files.\n#separately\rdata \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map(read_multiple_excel)\rstr(data)\r## List of 2\r## $ berlin_temp.xlsx: tibble [2,192 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## ..$ ta : num [1:2192] 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r## $ madrid_temp.xlsx: tibble [2,192 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ date: POSIXct[1:2192], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## ..$ ta : num [1:2192] 5.4 5 3.5 4.3 0.6 3.8 6.2 5.4 5.5 4.8 ...\r## ..$ yr : num [1:2192] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r#joining all data.frames\rdata_df \u0026lt;- dir_ls(regexp = \u0026quot;xlsx\u0026quot;) %\u0026gt;% map_df(read_multiple_excel,\r.id = \u0026quot;city\u0026quot;)\rstr(data_df)\r## tibble [4,384 x 4] (S3: tbl_df/tbl/data.frame)\r## $ city: chr [1:4384] \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; \u0026quot;berlin_temp.xlsx\u0026quot; ...\r## $ date: POSIXct[1:4384], format: \u0026quot;2000-01-01\u0026quot; \u0026quot;2000-01-02\u0026quot; ...\r## $ ta : num [1:4384] 1.2 3.6 5.7 5.1 2.2 1.8 4.2 4.2 4.2 1.7 ...\r## $ yr : num [1:4384] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\r\n\r","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"da7ce9b76e059ebeea549d66e1b75fd5","permalink":"https://dominicroye.github.io/en/2019/import-excel-sheets-with-r/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/en/2019/import-excel-sheets-with-r/","section":"post","summary":"We usually work with different data sources, and sometimes we can find tables distributed over several Excel sheets. In this post we are going to import the average daily temperature of Madrid and Berlin which is found in two Excel files with sheets for each year between 2000 and 2005.","tags":["excel","sheets","import"],"title":"Import Excel sheets with R","type":"post"},{"authors":["D Royé","N Lorenzo","D Rasilla","A Martí"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"a886efbb0057c84bc3ecacb4e1f008aa","permalink":"https://dominicroye.github.io/en/publication/2018-cloudiness-peninsula-ij-climatology/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/en/publication/2018-cloudiness-peninsula-ij-climatology/","section":"publication","summary":"This paper presents the first systematic study of the relationships between atmospheric circulation types (CT) and cloud fraction (CF) over the whole Iberian Peninsula, using satellite data from the MODIS (MOD09GA and MYD09GA) cloud mask for the period 2001-2017. The high level of detail, in combination with a classification for circulation patterns, provides us with relevant information about the spatio-temporal variability of cloudiness and the main mechanisms affecting the genesis of clouds. The results show that westerly CTs are the most influential, followed by cyclonic types, in cloudiness in the west of the Iberian Peninsula. Westerly flows, however, do not affect the Mediterranean coastline, which is dominated by easterly CTs, suggesting that local factors such as convective processes, orography and proximity to a body of warm water could play a major role in cloudiness processes. The Cantabrian Coast also has a particularly characteristic cloudiness dominated by northerly CTs. In general, the results found in this study are in line with the few studies that exist on cloudiness in the Iberian Peninsula. Furthermore, the results are geographically consistent, showing links to synoptic forcing in terms of atmospheric circulation patterns and the impact of the Iberian Peninsulas complex orography upon this element of the climate system.","tags":["cloudiness","circulation types","iberian peninsula","MODIS","weather","spatio-temporal patterns"],"title":"Spatio-temporal variations of cloud fraction based on circulation types in the Iberian Peninsula","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://dominicroye.github.io/en/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/en/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["gis","R","R:elementary"],"content":"\r\rThe distance to the sea is a fundamental variable in geography, especially relevant when it comes to modeling. For example, in interpolations of air temperature, the distance to the sea is usually used as a predictor variable, since there is a casual relationship between the two that explains the spatial variation. How can we estimate the (shortest) distance to the coast in R?\nPackages\rIn this post we will use the following libraries:\n\r\r\r\rLibrary\rDescription\r\r\r\rtidyverse\rCollection of packages (visualization, manipulation): ggplot2, dplyr, etc.\r\rsf\rSimple Feature: import, export and manipulate vector data\r\rraster\rImport, export and manipulate raster\r\rrnaturalearth\rSet of vector maps ‘natural earth’\r\rRColorBrewer\rColor palettes\r\r\r\r#install the libraries if necessary\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;)\rif(!require(\u0026quot;rnaturalearth\u0026quot;)) install.packages(\u0026quot;rnaturalearth\u0026quot;)\r#packages\rlibrary(rnaturalearth)\rlibrary(sf)\rlibrary(raster)\rlibrary(tidyverse)\rlibrary(RColorBrewer)\r\rThe coast of Iceland as an example\rOur example in this post will be Iceland, and, as it is an island territory it will facilitate the tutorial showing the process in a simple manner. The rnaturalearth package allows you to import the boundaries of countries (with different administrative levels) from around the world. The data comes from the platform naturalearthdata.com. I recommend exploring the package, more info here. The ne_countries( ) function imports the country boundaries. In this case we indicate with the argument scale the resolution (10, 50 or 110m), with country we indicate the specific country of interest and with returnclass we determine which class we want (sf or sp), in our case sf (simple feature).\nworld \u0026lt;- ne_countries(scale = 50) #world map with 50m resolution\rplot(world) #sp class by default\r#import the limits of Iceland\riceland \u0026lt;- ne_countries(scale = 10, country = \u0026quot;Iceland\u0026quot;, returnclass = \u0026quot;sf\u0026quot;)\r#info of our spatial vector object\riceland\r## Simple feature collection with 1 feature and 94 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: -24.53991 ymin: 63.39671 xmax: -13.50292 ymax: 66.56415\r## CRS: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\r## featurecla scalerank labelrank sovereignt sov_a3 adm0_dif level\r## 188 Admin-0 country 0 3 Iceland ISL 0 2\r## type admin adm0_a3 geou_dif geounit gu_a3 su_dif subunit\r## 188 Sovereign country Iceland ISL 0 Iceland ISL 0 Iceland\r## su_a3 brk_diff name name_long brk_a3 brk_name brk_group abbrev postal\r## 188 ISL 0 Iceland Iceland ISL Iceland \u0026lt;NA\u0026gt; Iceland IS\r## formal_en formal_fr name_ciawf note_adm0 note_brk name_sort\r## 188 Republic of Iceland \u0026lt;NA\u0026gt; Iceland \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Iceland\r## name_alt mapcolor7 mapcolor8 mapcolor9 mapcolor13 pop_est pop_rank\r## 188 \u0026lt;NA\u0026gt; 1 4 4 9 339747 10\r## gdp_md_est pop_year lastcensus gdp_year economy\r## 188 16150 2017 NA 2016 2. Developed region: nonG7\r## income_grp wikipedia fips_10_ iso_a2 iso_a3 iso_a3_eh iso_n3\r## 188 1. High income: OECD NA IC IS ISL ISL 352\r## un_a3 wb_a2 wb_a3 woe_id woe_id_eh woe_note adm0_a3_is\r## 188 352 IS ISL 23424845 23424845 Exact WOE match as country ISL\r## adm0_a3_us adm0_a3_un adm0_a3_wb continent region_un subregion\r## 188 ISL NA NA Europe Europe Northern Europe\r## region_wb name_len long_len abbrev_len tiny homepart min_zoom\r## 188 Europe \u0026amp; Central Asia 7 7 7 NA 1 0\r## min_label max_label ne_id wikidataid name_ar name_bn name_de name_en\r## 188 2 7 1159320917 Q189 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Island Iceland\r## name_es name_fr name_el name_hi name_hu name_id name_it name_ja name_ko\r## 188 Islandia Islande \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Izland Islandia Islanda \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt;\r## name_nl name_pl name_pt name_ru name_sv name_tr name_vi name_zh\r## 188 IJsland Islandia Islândia \u0026lt;NA\u0026gt; Island Izlanda Iceland \u0026lt;NA\u0026gt;\r## geometry\r## 188 MULTIPOLYGON (((-14.56363 6...\r#here Iceland\rplot(iceland)\rBy default, the plot( ) function with the class sf creates as many facets of the map as there are variables in it. To limit this behavior we can use either a variable name plot(iceland[\"admin\"]) or the limit argument plot(iceland, max.plot = 1). With the argument max.plot = 1 the function uses the first available variable of the map.\nIn addition, we see in the information of the object sf that the projection is WGS84 with decimal degrees (EPSG code: 4326). For the calculation of distances it is more convenient to use meters instead of degrees. Because of this, the first thing we do is to transform the map of Iceland to UTM Zone 27 (EPSG code: 3055). More information about EPSG and projections here. For that purpose, we use the st_transform( ) function. We simply indicate the map and the EPSG code.\n#transform to UTM\riceland \u0026lt;- st_transform(iceland, 3055)\r\rCreate a fishnet of points\rWe still need the points where we want to know the distance. In our case it will be a regular fishnet of points in Iceland with a resolution of 5km. We do this with the function st_make_grid( ), indicating the resolution in the unit of the coordinate system (meters in our case) with the argument cellsize, and what geometry we would like to create what (polygons, centers or corners).\n#create the fishnet\rgrid \u0026lt;- st_make_grid(iceland, cellsize = 5000, what = \u0026quot;centers\u0026quot;)\r#our fishnet with the extension of Iceland\rplot(grid)\r#only extract the points in the limits of Iceland\rgrid \u0026lt;- st_intersection(grid, iceland) #our fishnet now\rplot(grid)\r\rCalculating the distance\rTo estimate the distance we use the st_distance( ) function that returns a vector of distances for all our points in the fishnet. But first it is necessary to transform the map of Iceland from a polygon shape (MULTIPOLYGON) to a line (MULTILINESTRING). More details with ?st_cast.\n#transform Iceland from polygon shape to line\riceland \u0026lt;- st_cast(iceland, \u0026quot;MULTILINESTRING\u0026quot;)\r#calculation of the distance between the coast and our points\rdist \u0026lt;- st_distance(iceland, grid)\r#distance with unit in meters\rhead(dist[1,])\r## Units: [m]\r## [1] 790.7906 1151.4360 1270.7603 3128.9057 2428.5677 4197.7472\r\rPlotting the calculated distance\rOnce obtained the distance for our points, we can combine them with the coordinates and plot them in ggplot2. For this, we create a data.frame. The object dist is a matrix of one column, so we have to convert it to a vector with the function as.vector( ). In addition, we divide by 1000 to convert the distance in meters to km. The st_coordinates( ) function extracts the coordinates of our points. For the final visualization we use a vector of colors with the RdGy palette (more here).\n#create a data.frame with the distance and the coordinates of the points\rdf \u0026lt;- data.frame(dist = as.vector(dist)/1000,\rst_coordinates(grid))\r#structure\rstr(df)\r## \u0026#39;data.frame\u0026#39;: 4104 obs. of 3 variables:\r## $ dist: num 0.791 1.151 1.271 3.129 2.429 ...\r## $ X : num 608796 613796 583796 588796 593796 ...\r## $ Y : num 7033371 7033371 7038371 7038371 7038371 ...\r#colors col_dist \u0026lt;- brewer.pal(11, \u0026quot;RdGy\u0026quot;)\rggplot(df, aes(X, Y, fill = dist))+ #variables\rgeom_tile()+ #geometry\rscale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance\rlabs(fill = \u0026quot;Distance (km)\u0026quot;)+ #legend name\rtheme_void()+ #map theme\rtheme(legend.position = \u0026quot;bottom\u0026quot;) #legend position\r\rExport the distance as a raster\rTo be able to export the estimated distance to the sea of Iceland, we need to use the rasterize( ) function of the library raster.\nFirst, it is necessary to create an empty raster. In this raster we have to indicate the resolution, in our case it is of 5000m, the projection and the extension of the raster.\nWe can extract the projection from the information of the map of Iceland.\n\rThe extension can be extracted from our grid points with the function extent( ). However, this last function needs the class sp, so we pass the object grid in sf format, only for this time, to the class sp using the function as( ) and the argument “Spatial”.\n\r\rIn addition to the above, the data.frame df, that we created earlier, has to be converted into the sf class. Therefore, we apply the function st_as_sf( ) with the argument coords indicating the names of the coordinates. Additionally, we also define the coordinate system that we know.\n\r\r#get the extension\rext \u0026lt;- extent(as(grid, \u0026quot;Spatial\u0026quot;))\r#extent object\rext\r## class : Extent ## xmin : 338795.6 ## xmax : 848795.6 ## ymin : 7033371 ## ymax : 7383371\r#raster destination\rr \u0026lt;- raster(resolution = 5000, ext = ext, crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\r#convert the points to a spatial object class sf\rdist_sf \u0026lt;- st_as_sf(df, coords = c(\u0026quot;X\u0026quot;, \u0026quot;Y\u0026quot;)) %\u0026gt;%\rst_set_crs(3055)\r#create the distance raster\rdist_raster \u0026lt;- rasterize(dist_sf, r, \u0026quot;dist\u0026quot;, fun = mean)\r#raster\rdist_raster\r## class : RasterLayer ## dimensions : 70, 102, 7140 (nrow, ncol, ncell)\r## resolution : 5000, 5000 (x, y)\r## extent : 338795.6, 848795.6, 7033371, 7383371 (xmin, xmax, ymin, ymax)\r## crs : +proj=utm +zone=27 +ellps=intl +units=m +no_defs ## source : memory\r## names : layer ## values : 0.006124901, 115.1712 (min, max)\r#plot the raster\rplot(dist_raster)\r#export the raster\rwriteRaster(dist_raster, file = \u0026quot;dist_islandia.tif\u0026quot;, format = \u0026quot;GTiff\u0026quot;, overwrite = TRUE)\rThe rasterize( ) function is designed to create rasters from an irregular grid. In case we have a regular grid, like this one, we can use an easier alternative way. The rasterFromXYZ( ) function converts a data.frame with longitude, latitude and the variable Z into a raster object. It is important that the order should be longitude, latitude, variables.\nr \u0026lt;- rasterFromXYZ(df[, c(2:3, 1)], crs = \u0026quot;+proj=utm +zone=27 +ellps=intl +towgs84=-73,47,-83,0,0,0,0 +units=m +no_defs\u0026quot;)\rplot(r)\rWith the calculation of distance we can create art, as seen in the header of this post, which includes a world map only with the distance to the sea of all continents. A different perspective to our world (here more (spanish)) .\n\n\r","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"14e05fb8982521e72f61499a834295af","permalink":"https://dominicroye.github.io/en/2019/calculating-the-distance-to-the-sea-in-r/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/en/2019/calculating-the-distance-to-the-sea-in-r/","section":"post","summary":"The distance to the sea is a fundamental variable in geography, especially relevant when it comes to modeling. For example, in interpolations of air temperature, the distance to the sea is usually used as a predictor variable, since there is a casual relationship between the two that explains the spatial variation. How can we estimate the (shortest) distance to the coast in R?","tags":["distance","raster","estimation","variable"],"title":"Calculating the distance to the sea in R","type":"post"},{"authors":["F Mori-Gamarra","L Moure-Rodríguez","X Sureda","C Carbiae","D Royé","A Montes-Martínez","F Cadaveira","F Caamaño-Isorna"],"categories":null,"content":"","date":1545350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545350400,"objectID":"efc75ad0020b6bd8f49ed175c631e04b","permalink":"https://dominicroye.github.io/en/publication/2018-alcohol-galicia-gaceta/","publishdate":"2020-12-21T00:00:00Z","relpermalink":"/en/publication/2018-alcohol-galicia-gaceta/","section":"publication","summary":"Objective: To assess the influence that alcohol outlet density, off- and on-alcohol premises, and alcohol consumption wield on the consumption patterns of young pre-university students in Galicia (Spain). Method: A cross-sectional analysis of a cohort of students of the University of Santiago de Compostela (Compostela Cohort 2016) was carried out. Consumption prevalence were calculated for each of the municipalities from the first-cycle students’ home residence during the year prior to admission. The association with risky alcohol consumption (RC) and binge-drinking (BD) was assessed with a logistic model considering as independent variables the municipality population, alcohol outlet density of off- premises, density of off- and on- premises and total density of both types of premises in the municipality. Results: The prevalence of RC was 60.5% (95% confidence interval [95%CI]: 58.4-62.5) and the BD was 28.5% (95%CI: 26.7-30.2). A great variability was observed according to the municipality of provenance. The multivariate logistic model showed municipalities with a density of 8.42-9.34 of both types of premises per thousand inhabitants presented a higher risk of RC (odds ratio [OR]: 1,39; 95%CI: 1.09-1.78) and BD (OR: 1.29; 95%CI: 1.01-1.66). Conclusion: These data suggest the importance of including environmental information when studying alcohol consumption. Knowing our environment better could help plan policies that encourage healthier behaviour in the population.","tags":["alcohol outlet density","alcohol","underage drinking","adolescents"],"title":"Alcohol outlet density and alcohol consumption in Galician youth","type":"publication"},{"authors":null,"categories":["datavis","R","R:elementary"],"content":"\r\rThis year, the so-called warming stripes, which were created by the scientist Ed Hawkins of the University of Reading, became very famous all over the world. These graphs represent and communicate climate change in a very illustrative and effective way.\nVisualising global temperature change since records began in 1850. Versions for USA, central England \u0026amp; Toronto available too: https://t.co/H5Hv9YgZ7v pic.twitter.com/YMzdySrr3A\n\u0026mdash; Ed Hawkins (@ed_hawkins) May 23, 2018  From his idea, I created strips for examples of Spain, like the next one in Madrid.\n#Temperatura anual en #MadridRetiro desde 1920 a 2017. #CambioClimatico #dataviz #ggplot2 (idea de @ed_hawkins 🙏) @Divulgameteo @edupenabad @climayagua @ClimaGroupUB @4gotas_com pic.twitter.com/wmLb5uczpT\n\u0026mdash; Dr. Dominic Royé (@dr_xeo) June 2, 2018  In this post I will show how you can create these strips in R with the library ggplot2. Although I must say that there are many ways in R that can lead us to the same result or to a similar one, even within ggplot2.\nData\rIn this case we will use the annual temperatures of Lisbon\rGISS Surface Temperature Analysis, homogenized time series, comprising the period from 1880 to 2018. Monthly temperatures or other time series could also be used. The file can be downloaded here. First, we should, as long as we have not done it, install the collection of tidyverse libraries that also include ggplot2. In addition, we will need the library lubridate for the treatment of dates. Then, we import the data of Lisbon in csv format.\n#install the lubridate and tidyverse libraries\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\r#packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(RColorBrewer)\r#import the annual temperatures\rtemp_lisboa \u0026lt;- read_csv(\u0026quot;temp_lisboa.csv\u0026quot;)\rstr(temp_lisboa)\r## spec_tbl_df [139 x 18] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r## $ YEAR : num [1:139] 1880 1881 1882 1883 1884 ...\r## $ JAN : num [1:139] 9.17 11.37 10.07 10.86 11.16 ...\r## $ FEB : num [1:139] 12 11.8 11.9 11.5 10.6 ...\r## $ MAR : num [1:139] 13.6 14.1 13.5 10.5 12.4 ...\r## $ APR : num [1:139] 13.1 14.4 14 13.8 12.2 ...\r## $ MAY : num [1:139] 15.7 17.3 15.6 14.6 16.4 ...\r## $ JUN : num [1:139] 17 19.2 17.9 17.2 19.1 ...\r## $ JUL : num [1:139] 19.1 21.8 20.3 19.5 21.4 ...\r## $ AUG : num [1:139] 20.6 23.5 21 21.6 22.4 ...\r## $ SEP : num [1:139] 20.7 20 18 18.8 19.5 ...\r## $ OCT : num [1:139] 17.9 16.3 16.4 15.8 16.4 ...\r## $ NOV : num [1:139] 12.5 14.7 13.7 13.5 12.5 ...\r## $ DEC : num [1:139] 11.07 9.97 10.66 9.46 10.25 ...\r## $ D-J-F : num [1:139] 10.7 11.4 10.6 11 10.4 ...\r## $ M-A-M : num [1:139] 14.1 15.2 14.3 12.9 13.6 ...\r## $ J-J-A : num [1:139] 18.9 21.5 19.7 19.4 20.9 ...\r## $ S-O-N : num [1:139] 17 17 16 16 16.1 ...\r## $ metANN: num [1:139] 15.2 16.3 15.2 14.8 15.3 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. YEAR = col_double(),\r## .. JAN = col_double(),\r## .. FEB = col_double(),\r## .. MAR = col_double(),\r## .. APR = col_double(),\r## .. MAY = col_double(),\r## .. JUN = col_double(),\r## .. JUL = col_double(),\r## .. AUG = col_double(),\r## .. SEP = col_double(),\r## .. OCT = col_double(),\r## .. NOV = col_double(),\r## .. DEC = col_double(),\r## .. `D-J-F` = col_double(),\r## .. `M-A-M` = col_double(),\r## .. `J-J-A` = col_double(),\r## .. `S-O-N` = col_double(),\r## .. metANN = col_double()\r## .. )\r## - attr(*, \u0026quot;problems\u0026quot;)=\u0026lt;externalptr\u0026gt;\rWe see in the columns that we have monthly and seasonal values, and the annual temperature value. But before proceeding to visualize the annual temperature, we must replace the missing values 999.9 with NA, using the ifelse( ) function that evaluates a condition and perform the given argument corresponding to true and false.\n#select only the annual temperature and year column\rtemp_lisboa_yr \u0026lt;- select(temp_lisboa, YEAR, metANN)\r#rename the temperature column\rtemp_lisboa_yr \u0026lt;- rename(temp_lisboa_yr, ta = metANN)\r#missing values 999.9\rsummary(temp_lisboa_yr) \r## YEAR ta ## Min. :1880 Min. : 14.53 ## 1st Qu.:1914 1st Qu.: 15.65 ## Median :1949 Median : 16.11 ## Mean :1949 Mean : 37.38 ## 3rd Qu.:1984 3rd Qu.: 16.70 ## Max. :2018 Max. :999.90\rtemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, ta = ifelse(ta == 999.9, NA, ta))\rWhen we use the year as a variable, we do not usually convert it into a date object, however it is advisable. This allows us to use the date functions of the library lubridate and the support functions inside of ggplot2. The str_c( ) function of the library stringr, part of the collection of tidyverse, is similar to paste( ) of R Base that allows us to combine characters by specifying a separator (sep = “-”). The ymd( ) (year month day) function of the lubridate library converts a date character into a Date object. It is possible to combine several functions\rusing the pipe operator %\u0026gt;% that helps to chain without assigning the result to a new object. Its use is very extended especially with the library tidyverse. If you want to know more about its use, here you have a tutorial.\ntemp_lisboa_yr \u0026lt;- mutate(temp_lisboa_yr, date = str_c(YEAR, \u0026quot;01-01\u0026quot;, sep = \u0026quot;-\u0026quot;) %\u0026gt;% ymd())\r\rCreating the strips\rFirst, we create the style of the graph, specifying all the arguments of the theme we want to adjust. We start with the default style of theme_minimal( ). In addition, we assign\rthe colors from RColorBrewer to an object col_srip. More information about the colors used here.\ntheme_strip \u0026lt;- theme_minimal()+\rtheme(axis.text.y = element_blank(),\raxis.line.y = element_blank(),\raxis.title = element_blank(),\rpanel.grid.major = element_blank(),\rlegend.title = element_blank(),\raxis.text.x = element_text(vjust = 3),\rpanel.grid.minor = element_blank(),\rplot.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;)\r)\rcol_strip \u0026lt;- brewer.pal(11, \u0026quot;RdBu\u0026quot;)\rbrewer.pal.info\r## maxcolors category colorblind\r## BrBG 11 div TRUE\r## PiYG 11 div TRUE\r## PRGn 11 div TRUE\r## PuOr 11 div TRUE\r## RdBu 11 div TRUE\r## RdGy 11 div FALSE\r## RdYlBu 11 div TRUE\r## RdYlGn 11 div FALSE\r## Spectral 11 div FALSE\r## Accent 8 qual FALSE\r## Dark2 8 qual TRUE\r## Paired 12 qual TRUE\r## Pastel1 9 qual FALSE\r## Pastel2 8 qual FALSE\r## Set1 9 qual FALSE\r## Set2 8 qual TRUE\r## Set3 12 qual FALSE\r## Blues 9 seq TRUE\r## BuGn 9 seq TRUE\r## BuPu 9 seq TRUE\r## GnBu 9 seq TRUE\r## Greens 9 seq TRUE\r## Greys 9 seq TRUE\r## Oranges 9 seq TRUE\r## OrRd 9 seq TRUE\r## PuBu 9 seq TRUE\r## PuBuGn 9 seq TRUE\r## PuRd 9 seq TRUE\r## Purples 9 seq TRUE\r## RdPu 9 seq TRUE\r## Reds 9 seq TRUE\r## YlGn 9 seq TRUE\r## YlGnBu 9 seq TRUE\r## YlOrBr 9 seq TRUE\r## YlOrRd 9 seq TRUE\rFor the final graphic we use the geometry geom_tile( ). Since the data does not have a specific value for the Y axis, we need a dummy value, here I used 1. Also, I adjust the width of the color bar in the legend.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile()+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_continuous(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rguides(fill = guide_colorbar(barwidth = 1))+\rlabs(title = \u0026quot;LISBOA 1880-2018\u0026quot;,\rcaption = \u0026quot;Datos: GISS Surface Temperature Analysis\u0026quot;)+\rtheme_strip\rIn case we want to get only the strips, we can use theme_void( ) and the argument show.legend = FALSE in geom_tile( ) to remove all style elements. We can also change the color for the NA values, including the argument na.value = “gray70” in the scale_fill_gradientn( ) function.\n ggplot(temp_lisboa_yr,\raes(x = date, y = 1, fill = ta))+\rgeom_tile(show.legend = FALSE)+\rscale_x_date(date_breaks = \u0026quot;6 years\u0026quot;,\rdate_labels = \u0026quot;%Y\u0026quot;,\rexpand = c(0, 0))+\rscale_y_discrete(expand = c(0, 0))+\rscale_fill_gradientn(colors = rev(col_strip))+\rtheme_void()\r\n\r","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"8696d9be9ddbae7593f4c5e3da8d1408","permalink":"https://dominicroye.github.io/en/2018/how-to-create-warming-stripes-in-r/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/en/2018/how-to-create-warming-stripes-in-r/","section":"post","summary":"This year, the so-called warming stripes, which were created by the scientist Ed Hawkins of the University of Reading, became very famous all over the world. These graphs represent and communicate climate change in a very illustrative and effective way.","tags":["ggplot2","warming stripes","global warming","visualization"],"title":"How to create 'Warming Stripes' in R","type":"post"},{"authors":null,"categories":["visualization","R:elementary","R","mapping"],"content":"\r\rThe database of Open Street Maps\rRecently I created a map of the distribution of gas stations and electric charging stations in Europe.\nPopulation density through the number of gas stations in Europe. #dataviz @AGE_Oficial @mipazos @simongerman600 @openstreetmap pic.twitter.com/eIUx2yn7ej\n\u0026mdash; Dr. Dominic Royé (@dr_xeo) February 25, 2018  How can you obtain this data?\nWell, in this case I used points of interest (POIs) from the database of Open Street Maps (OSM). Obviously OSM not only contains streets and highways, but also information that can be useful when we use a map such as locations of hospitals or gas stations. To avoid downloading the entire OSM and extracting the required information, you can use an overpass API, which allows us to query the OSM database with our own criteria.\nAn easy way to access an overpass API is through overpass-turbo.eu, which even includes a wizard to build a query and display the results on a interactive map. A detailed explanation of the previous web can be found here.\rHowever, we have at our disposal a package osmdata that allows us to create and make queries directly from the R environment. Nevertheless, the use of the overpass-turbo.eu can be useful when we are not sure what we are looking for or when we have some difficulty in building the query.\n\rAccessing the overpass API from R\rThe first step is to install several packages, in case they are not installed. In almost all my scripts I use tidyverse which is a fundamental collection of different packages, including dplyr (data manipulation), ggplot2 (visualization), etc. The sf package is the new standard for working with spatial data and is compatible with ggplot2 and dplyr. Finally, ggmap makes it easier for us to create maps.\n#install the osmdata, sf, tidyverse and ggmap package\rif(!require(\u0026quot;osmdata\u0026quot;)) install.packages(\u0026quot;osmdata\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\rif(!require(\u0026quot;ggmap\u0026quot;)) install.packages(\u0026quot;ggmap\u0026quot;)\r#load packages\rlibrary(tidyverse)\rlibrary(osmdata)\rlibrary(sf)\rlibrary(ggmap)\r\rBuild a query\rBefore creating a query, we need to know what we can filter. The available_features( ) function returns a list of available OSM features that have different tags. More details are available in the OSM wiki here.\rFor example, the feature shop contains several tags among others supermarket, fishing, books, etc.\n#the first five features\rhead(available_features())\r## [1] \u0026quot;4wd_only\u0026quot; \u0026quot;abandoned\u0026quot; \u0026quot;abutters\u0026quot; \u0026quot;access\u0026quot; \u0026quot;addr\u0026quot; \u0026quot;addr:city\u0026quot;\r#amenities\rhead(available_tags(\u0026quot;amenity\u0026quot;))\r## [1] \u0026quot;animal_boarding\u0026quot; \u0026quot;animal_breeding\u0026quot; \u0026quot;animal_shelter\u0026quot; \u0026quot;arts_centre\u0026quot; ## [5] \u0026quot;atm\u0026quot; \u0026quot;baby_hatch\u0026quot;\r#shops\rhead(available_tags(\u0026quot;shop\u0026quot;))\r## [1] \u0026quot;agrarian\u0026quot; \u0026quot;alcohol\u0026quot; \u0026quot;anime\u0026quot; \u0026quot;antiques\u0026quot; \u0026quot;appliance\u0026quot; \u0026quot;art\u0026quot;\rThe first query: Where are cinemas in Madrid?\rTo build the query, we use the pipe operator %\u0026gt;%, which helps to chain several functions without assigning the result to a new object. Its use is very extended especially within the tidyverse package collection. If you want to know more about its use, you can find here a tutorial.\nIn the first part of the query we need to indicate the place where we want to extract the information. The getbb( ) function creates a boundering box for a given place, looking for the name. The main function is opq( ) which build the final query. We add our filter criteria with the add_osm_feature( ) function. In this first query we will look for cinemas in Madrid. That’s why we use as key amenity and cinema as tag. There are several formats to obtain the resulting spatial data of the query. The osmdata_*( ) function sends the query to the server and, depending on the suffix * sf/sp/xml, returns a simple feature, spatial or XML format.\n#building the query\rq \u0026lt;- getbb(\u0026quot;Madrid\u0026quot;) %\u0026gt;%\ropq() %\u0026gt;%\radd_osm_feature(\u0026quot;amenity\u0026quot;, \u0026quot;cinema\u0026quot;)\rstr(q) #query structure\r## List of 4\r## $ bbox : chr \u0026quot;40.3119774,-3.8889539,40.6437293,-3.5179163\u0026quot;\r## $ prefix : chr \u0026quot;[out:xml][timeout:25];\\n(\\n\u0026quot;\r## $ suffix : chr \u0026quot;);\\n(._;\u0026gt;;);\\nout body;\u0026quot;\r## $ features: chr \u0026quot; [\\\u0026quot;amenity\\\u0026quot;=\\\u0026quot;cinema\\\u0026quot;]\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr [1:2] \u0026quot;list\u0026quot; \u0026quot;overpass_query\u0026quot;\r## - attr(*, \u0026quot;nodes_only\u0026quot;)= logi FALSE\rcinema \u0026lt;- osmdata_sf(q)\rcinema\r## Object of class \u0026#39;osmdata\u0026#39; with:\r## $bbox : 40.3119774,-3.8889539,40.6437293,-3.5179163\r## $overpass_call : The call submitted to the overpass API\r## $meta : metadata including timestamp and version numbers\r## $osm_points : \u0026#39;sf\u0026#39; Simple Features Collection with 220 points\r## $osm_lines : NULL\r## $osm_polygons : \u0026#39;sf\u0026#39; Simple Features Collection with 12 polygons\r## $osm_multilines : NULL\r## $osm_multipolygons : NULL\rWe see that the result is a list of different spatial objects. In our case, we are only interested in osm_points.\nHow can we visulise these points?\nThe advantage of sf objects is that for ggplot2 already exists a geometry function geom_sf( ). Furthermore, we can include a background map using ggmap. The get_map( ) function downloads the map for a given place. Alternatively, it can be an address, latitude/longitude or a bounding box. The maptype argument allows us to indicate the style or type of map. You can find more details in the help of the ?get_map function.\nWhen we build a graph with ggplot we usually start with ggplot( ). In this case, we start with ggmap( ) that includes the object with our background map. Then we add with geom_sf( ) the points of the cinemas in Madrid. It is important to indicate with the argument inherit.aes = FALSE that it has to use the aesthetic mappings of the spatial object osm_points. In addition, we change the color, fill, transparency (alpha), type and size of the circles.\n#our background map\rmad_map \u0026lt;- get_map(getbb(\u0026quot;Madrid\u0026quot;), maptype = \u0026quot;toner-background\u0026quot;)\r#final map\rggmap(mad_map)+\rgeom_sf(data = cinema$osm_points,\rinherit.aes = FALSE,\rcolour = \u0026quot;#238443\u0026quot;,\rfill = \u0026quot;#004529\u0026quot;,\ralpha = .5,\rsize = 4,\rshape = 21)+\rlabs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;)\r\rWhere can we find Mercadona supermarkets?\rInstead of obtaining a bounding box with the function getbb( ) we can build our own box. To do this, we create a vector of four elements, the order has to be West/South/East/North. In the query we use two features: name and shop to filter supermarkets that are of this particular brand. Depending on the area or volume of the query, it is necessary to extend the waiting time. By default, the limit is set at 25 seconds (timeout).\nThe map, we create in this case, consists only of the supermarket points. Therefore, we use the usual grammar by adding the geometry geom_sf( ). The theme_void( ) function removes everything except for the points.\n#bounding box for the Iberian Peninsula\rm \u0026lt;- c(-10, 30, 5, 46)\r#building the query\rq \u0026lt;- m %\u0026gt;% opq (timeout = 25*100) %\u0026gt;%\radd_osm_feature(\u0026quot;name\u0026quot;, \u0026quot;Mercadona\u0026quot;) %\u0026gt;%\radd_osm_feature(\u0026quot;shop\u0026quot;, \u0026quot;supermarket\u0026quot;)\r#query\rmercadona \u0026lt;- osmdata_sf(q)\r#final map\rggplot(mercadona$osm_points)+\rgeom_sf(colour = \u0026quot;#08519c\u0026quot;,\rfill = \u0026quot;#08306b\u0026quot;,\ralpha = .5,\rsize = 1,\rshape = 21)+\rtheme_void()\r\n\r\r","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"804d0ce609e41fc12cfc6db298f38723","permalink":"https://dominicroye.github.io/en/2018/accessing-openstreetmap-data-with-r/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/en/2018/accessing-openstreetmap-data-with-r/","section":"post","summary":"Recently I created a map of the distribution of gas stations and electric charging stations in Europe. How can you obtain this data? Well, in this case I used points of interest (POIs) from the database of *Open Street Maps* (OSM). Obviously OSM not only contains the streets and highways, but also information that can be useful when we use a map such as locations of hospitals or gas stations.","tags":["database","overpass API","OSM","Point of interest"],"title":"Accessing OpenStreetMap data with R","type":"post"},{"authors":["S Mathbout","JA Lopez-Bustins","D Royé","J Martin-Vide","J Bech","FS Rodrigo"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"691f8167a730e0b2b9d64edf96fb59d9","permalink":"https://dominicroye.github.io/en/publication/2017-precipitation-eastern-mediterranean-applied-geophysics/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/en/publication/2017-precipitation-eastern-mediterranean-applied-geophysics/","section":"publication","summary":"The Eastern Mediterranean is one of the most prominent hot spots of climate change in the world and extreme climatic phenomena in this region such as drought or extreme rainfall events are expected to become more frequent and intense. In this study climate extreme indices recommended by the joint World Meteorological Organization Expert Team on Climate Change Detection and Indices are calculated for daily precipitation data in 70 weather stations during 1961–2012. Observed trends and changes in daily precipitation extremes over the EM basin were analysed using the RClimDex package, which was developed by the Climate Research Branch of the Meteorological Service of Canada. Extreme and heavy precipitation events showed globally a statistically significant decrease in the Eastern Mediterranean and, in the southern parts, a significant decrease in total precipitation. The overall analysis of extreme precipitation indices reveals that decreasing trends are generally more frequent than increasing trends. We found statistically significant decreasing trends (reaching 74% of stations for extremely wet days) and increasing trends (reaching 36% of stations for number of very heavy precipitation days). Finally, most of the extreme precipitation indices have a statistically significant positive correlation with annual precipitation, particularly the number of heavy and very heavy precipitation days.","tags":["eastern mediterranean","extreme precipitation","trend","spatial temporal distribution"],"title":"Observed Changes in Daily Precipitation Extremes at Annual Timescale Over the Eastern Mediterranean During 1961–2012","type":"publication"},{"authors":null,"categories":["R","R:intermediate"],"content":"\r\r\r1 Introduction\r2 NCEP\r\r2.1 Packages\r2.2 Data download\r2.3 Monthly average\r2.4 Visualization\r\r3 ERA-Interim\r\r3.1 Installation\r3.2 Connection and download with the ECMWF API\r3.3 Processing ncdf\r\r4 Update for accessing ERA-5\r\r\rA friend advised me to introduce R levels as categories. An idea that I now add to each blog post. There are three levels: elementary, intermediate, and advanced. I hope it will help the reader and the R user.\n1 Introduction\rIn this post, I will show how we can download and work directly with data from climatic reanalysis in R. These kind of datasets are a combination of forcast models and data assimilation systems, which allows us to create corrected global grids of recent history of the atmosphere, land surface, and oceans. The two most used reanalyses are NCEP-DO (Reanalysis II) from the NOAA/OAR/ESRL, an improved version of NCEP-NCAR (Reanalysis I), and ERA-Interim from the ECMWF. Since NCEP-DO is the first generation, it is recommended to use third-generation climate reanalysis, especially ERA-Interim. An overview of the current atmospheric reanalysis can be found here. First, let’s see how to access the NCEP data through an R library on CRAN that facilitates the download and handling of the data. Then we will do the same with the ERA-Interim, however, to access this last reanalysis dataset it is necessary to use python and the corresponding API of the ECMWF.\n\r2 NCEP\rTo access the NCEP reanalysis it is required to install the corresponding package RNCEP. The main function is NCEP.gather( ). The resolution of the NCEP reanalysis is 2.5º X 2.5º.\n2.1 Packages\r#install the RNCEP, lubridate and tidyverse packages\rif(!require(\u0026quot;RNCEP\u0026quot;)) install.packages(\u0026quot;RNCEP\u0026quot;)\rif(!require(\u0026quot;lubridate\u0026quot;)) install.packages(\u0026quot;lubridate\u0026quot;)\rif(!require(\u0026quot;tidyverse\u0026quot;)) install.packages(\u0026quot;tidyverse\u0026quot;)\rif(!require(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;)\r#load the packages\rlibrary(RNCEP)\rlibrary(lubridate) #date and time manipulation\rlibrary(tidyverse) #data manipulation and visualization\rlibrary(RColorBrewer) #color schemes\rlibrary(sf) #to import a spatial object and to work with geom_sf in ggplot2\r\r2.2 Data download\rWe will download the air temperature of the 850haPa pressure level for the year 2016. The variables and pressure levels can be found in the details of the function ?NCEP.gather. The reanalysis2 argument allows us to download both version I and version II, being by default FALSE, that is, we access reanalysis I. In all the requests we will obtain data of every 6 hours (00:00, 06:00, 12:00 and 18:00). This supposes a total of 1464 values for the year 2016.\n#define the necessary arguments\rmonth_range \u0026lt;- c(1,12) #period of months\ryear_range \u0026lt;- c(2016,2016) #period of years\rlat_range \u0026lt;- c(30,60) #latitude range\rlon_range \u0026lt;- c(-30,50) #longitude range\rdata \u0026lt;- NCEP.gather(\u0026quot;air\u0026quot;, #name of the variable\r850, #pressure level 850hPa\rmonth_range,year_range,\rlat_range,lon_range,\rreturn.units = TRUE,\rreanalysis2=TRUE)\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r## [1] Units of variable \u0026#39;air\u0026#39; are degK\r#dimensions dim(data) \r## [1] 13 33 1464\r#we find lon, lat and time with dimnames()\r#date and time\rdate_time \u0026lt;- dimnames(data)[[3]]\rdate_time \u0026lt;- ymd_h(date_time)\rhead(date_time)\r## [1] \u0026quot;2016-01-01 00:00:00 UTC\u0026quot; \u0026quot;2016-01-01 06:00:00 UTC\u0026quot;\r## [3] \u0026quot;2016-01-01 12:00:00 UTC\u0026quot; \u0026quot;2016-01-01 18:00:00 UTC\u0026quot;\r## [5] \u0026quot;2016-01-02 00:00:00 UTC\u0026quot; \u0026quot;2016-01-02 06:00:00 UTC\u0026quot;\r#longitude and latitude\rlat \u0026lt;- dimnames(data)[[1]]\rlon \u0026lt;- dimnames(data)[[2]]\rhead(lon);head(lat)\r## [1] \u0026quot;-30\u0026quot; \u0026quot;-27.5\u0026quot; \u0026quot;-25\u0026quot; \u0026quot;-22.5\u0026quot; \u0026quot;-20\u0026quot; \u0026quot;-17.5\u0026quot;\r## [1] \u0026quot;60\u0026quot; \u0026quot;57.5\u0026quot; \u0026quot;55\u0026quot; \u0026quot;52.5\u0026quot; \u0026quot;50\u0026quot; \u0026quot;47.5\u0026quot;\r\r2.3 Monthly average\rWe see that the downloaded data is an array of three dimensions with [lat, lon, time]. As above mentioned, we extracted latitude, longitude and time. The temperature is given in Kelvin. The objective in the next section will be to show two maps comparing January and July.\n#create our grouping variable\rgroup \u0026lt;- month(date_time) #estimate the average temperature by month data_month \u0026lt;- aperm(\rapply(\rdata, #our data\rc(1,2), #apply to each time series 1:row, 2:column a the mean( ) function\rby, #group by\rgroup, #months\rfunction(x)ifelse(all(is.na(x)),NA,mean(x))),\rc(2,3,1)) #reorder to get an array like the original\rdim(data_month) #850haPa temperature per month January to December\r## [1] 13 33 12\r\r2.4 Visualization\rOnce we got here, we can visualize the 850hPa temperature of January and July with ggplot2. In this example, I use geom_sf( ) from the library sf, which makes the work easier to visualize spatial objects in ggplot (in the near future I will make a post about sf and ggplot). In the dimension of latitude and longitude we saw that it only indicates a value for each row and column. But we need the coordinates of all the cells in the matrix. To create all combinations between two variables we can use the expand.grid( ) function.\n#first we create all the combinations of lon-lat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#as lonlat was a row/column name, it is character, that\u0026#39;s why we convert it into numeric\rlonlat \u0026lt;- apply(lonlat,2,as.numeric)\r#lon and lat are not in the order as we expect\r#row=lon; column=lat\rdata_month \u0026lt;- aperm(data_month,c(2,1,3))\r#subtract 273.15K to convert K to ºC.\rdf \u0026lt;- data.frame(lonlat,\rTa01=as.vector(data_month[,,1])-273.15,\rTa07=as.vector(data_month[,,7])-273.15)\rBefore we can make the map with ggplot2, we have to adapt the table. The shapefile with the countries limits can be downloaded here.\n#convert the wide table into a long one\rdf \u0026lt;- gather(df,month,Ta,Ta01:Ta07)%\u0026gt;%\rmutate(month=factor(month,unique(month),c(\u0026quot;Jan\u0026quot;,\u0026quot;Jul\u0026quot;)))\r#import the countries limits\rlimit \u0026lt;- st_read(\u0026quot;CNTR_RG_03M_2014.shp\u0026quot;)\r## Reading layer `CNTR_RG_03M_2014\u0026#39; from data source ## `E:\\GitHub\\blog_update_2021\\content\\en\\post\\2018-09-15-access-to-climate-reanalysis-data-from-r\\CNTR_RG_03M_2014.shp\u0026#39; ## using driver `ESRI Shapefile\u0026#39;\r## Simple feature collection with 256 features and 3 fields\r## Geometry type: MULTIPOLYGON\r## Dimension: XY\r## Bounding box: xmin: -180 ymin: -90 xmax: 180 ymax: 83.66068\r## Geodetic CRS: ETRS89\r#color scheme\rcolbr \u0026lt;- brewer.pal(11,\u0026quot;RdBu\u0026quot;)\rggplot(df)+\rgeom_tile(aes(lon,lat,fill=Ta))+ #temperature data\rgeom_sf(data=limit,fill=NA,size=.5)+ #limits scale_fill_gradientn(colours=rev(colbr))+\rcoord_sf(ylim=c(30,60),xlim=c(-30,50))+\rscale_x_continuous(breaks=seq(-30,50,10),expand=c(0,0))+\rscale_y_continuous(breaks=seq(30,60,5),expand=c(0,0))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Ta 850hPa (ºC)\u0026quot;)+\rfacet_grid(month~.)+ #plot panels by month\rtheme_bw()\r\r\r3 ERA-Interim\rThe ECMWF offers access to its public databases from a pyhton-API. It is required to be registered on the ECMWF website. You can register here. When dealing with another programming language, in R we have to use an interface between both which allows the library reticulate. We must also have installed a pyhton distribution (version 2.x or 3.x). In the case of Windows we can use anaconda.\n Recently a new package called ecmwfr has been published that facilitates accessing the Copernicus and ECMWF APIs. The major advantage is that it is not necessary to install python. More details here. I wrote a more updated version in 2022.   3.1 Installation\rif(!require(\u0026quot;reticulate\u0026quot;)) install.packages(\u0026quot;reticulate\u0026quot;)\rif(!require(\u0026quot;ncdf4\u0026quot;)) install.packages(\u0026quot;ncdf4\u0026quot;) #to manage netCDF format\r#load packages\rlibrary(reticulate)\rlibrary(ncdf4)\rOnce we have installed anaconda and the package reticulate, we can install the library python ecmwfapi. We can carry out the installation, or through the Windows CMD using the command conda install -c conda-forge ecmwf-api-client, or with the R function py_install( ) from the reticulate package. The same function allows us to install any python library from R.\n#install the python ECMWF API\rpy_install(\u0026quot;ecmwf-api-client\u0026quot;)\r\r3.2 Connection and download with the ECMWF API\rIn order to access the API, it is required to create a file with the user’s information.\nThe “.ecmwfapirc” file must contain the following information:\n{\r\u0026quot;url\u0026quot; : \u0026quot;https://api.ecmwf.int/v1\u0026quot;,\r\u0026quot;key\u0026quot; : \u0026quot;XXXXXXXXXXXXXXXXXXXXXX\u0026quot;,\r\u0026quot;email\u0026quot; : \u0026quot;john.smith@example.com\u0026quot;\r}\rThe key can be obtained with the user account here.\nThe file can be created with the Windows notebook.\nWe create a document “ecmwfapirc.txt”.\rRename this file to “.ecmwfapirc.”\r\rThe last point disappears automatically. Then we save this file in “C:/USERNAME/.ecmwfapirc” or “C:/USERNAME/Documents/.ecmwfapirc”.\n#import the python library ecmwfapi\recmwf \u0026lt;- import(\u0026#39;ecmwfapi\u0026#39;)\r#for this step there must exist the file .ecmwfapirc\rserver = ecmwf$ECMWFDataServer() #start the connection\rOne we get here, how do we create a query? The easiest thing is to go to the website of ECMWF, where we choose the database, in this case ERA-Interim surface, to create a script with all the necessary data. More details about the syntax can be found here. When we proceed on the website, we only have to click on “View MARS Request”. This step takes us to the script in python.\nWith the syntax of the script from the MARS Request, we can create the query in R.\n#we create the query\rquery \u0026lt;-r_to_py(list(\rclass=\u0026#39;ei\u0026#39;,\rdataset= \u0026quot;interim\u0026quot;, #dataset\rdate= \u0026quot;2017-01-01/to/2017-12-31\u0026quot;, #time period\rexpver= \u0026quot;1\u0026quot;,\rgrid= \u0026quot;0.125/0.125\u0026quot;, #resolution\rlevtype=\u0026quot;sfc\u0026quot;,\rparam= \u0026quot;167.128\u0026quot;, # air temperature (2m)\rarea=\u0026quot;45/-10/30/5\u0026quot;, #N/W/S/E\rstep= \u0026quot;0\u0026quot;,\rstream=\u0026quot;oper\u0026quot;,\rtime=\u0026quot;00:00:00/06:00:00/12:00:00/18:00:00\u0026quot;, #hours\rtype=\u0026quot;an\u0026quot;,\rformat= \u0026quot;netcdf\u0026quot;, #format\rtarget=\u0026#39;ta2017.nc\u0026#39; #file name\r))\r#query to get the ncdf\rserver$retrieve(query)\rThe result is a netCDF file that we can process with the library ncdf4.\n\r3.3 Processing ncdf\rIn the next section, the objective will be the extraction of a time serie from the closest coordinate to a given one. We will use the coordinates of Madrid (40.418889, -3.691944).\n#load packages\rlibrary(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\r#open the connection with the ncdf file\rnc \u0026lt;- nc_open(\u0026quot;ta2017.nc\u0026quot;)\r#extract lon and lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r#extract the time\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#time unit: hours since 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r#convert the hours into date + hour\r#as_datetime() function of the lubridate package needs seconds\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#import the data\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#close the conection with the ncdf file\rnc_close(nc)\rIn this next section we use the sf package, which is replacing the well known sp and rgdal packages.\n#create all the combinations of lon-lat\rlonlat \u0026lt;- expand.grid(lon=lon,lat=lat)\r#we must convert the coordinates in a spatial object sf\r#we also indicate the coordinate system in EPSG code\rcoord \u0026lt;- st_as_sf(lonlat,coords=c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;))%\u0026gt;%\rst_set_crs(4326)\r#we do the same with our coordinate of Madrid\rpsj \u0026lt;- st_point(c(-3.691944,40.418889))%\u0026gt;%\rst_sfc()%\u0026gt;%\rst_set_crs(4326)\r#plot all points\rplot(st_geometry(coord))\rplot(psj,add=TRUE,pch = 3, col = \u0026#39;red\u0026#39;)\rIn the next steps we calculate the distance of our reference point to all the grid points. Then we look for the one with less distance.\n#add the distance to the points\rcoord \u0026lt;- mutate(coord,dist=st_distance(coord,psj))\r#create a distance matrix with the same dimensions as our data\rdist_mat \u0026lt;- matrix(coord$dist,dim(data)[-3])\r#the arrayInd function is useful to obtain the row and column indexes\rmat_index \u0026lt;- as.vector(arrayInd(which.min(dist_mat), dim(dist_mat)))\r#we extract the time serie and change the unit from K to ºC\r#we convert the time in date + hour\rdf \u0026lt;- data.frame(ta=data[mat_index[1],mat_index[2],],time=timestamp)%\u0026gt;%\rmutate(ta=ta-273.15,time=ymd_hms(time))\rFinally, we visualize our time series.\nggplot(df,\raes(time,ta))+\rgeom_line()+\rlabs(y=\u0026quot;Temperature (ºC)\u0026quot;,\rx=\u0026quot;\u0026quot;)+\rtheme_bw()\r\r\r4 Update for accessing ERA-5\rRecently the new reanalysis ERA-5 with single level or pressure level was made available to users. It is the fifth generation of the European Center for Medium-Range Weather Forecasts (ECMWF) and accessible through a new Copernicus API. The ERA-5 reanalysis has a temporary coverage from 1950 to the present at a horizontal resolution of 30km worldwide, with 137 levels from the surface to a height of 80km. An important difference with respect to the previous ERA-Interim is the temporal resolution with hourly data.\nThe access changes to the Climate Data Store (CDS) infrastructure with its own API. It is possible to download directly from the web or using the Python API in a similar way to the one already presented in this post. However, there are slight differences which I will explain below.\nIt is necessary to have a Copernicus CDS account link\rAgain, you need a account key link\rThere are changes in the Python library and in some arguments of the query.\r\r#load libraries library(sf)\rlibrary(ncdf4)\rlibrary(tidyverse)\rlibrary(reticulate)\r#install the CDS API\rconda_install(\u0026quot;r-reticulate\u0026quot;,\u0026quot;cdsapi\u0026quot;, pip=TRUE)\rTo be able to access the API, a requirement is to create a file with the user’s information.\nThe “.cdsapirc” file must contain the following information:\n\rurl: https://cds.climate.copernicus.eu/api/v2\rkey: {uid}:{api-key}\r\rThe key can be obtained with the user account in the User profile.\nThe file can be created in the same way as it has been explained for ERA-Interim.\n#import python CDS-API\rcdsapi \u0026lt;- import(\u0026#39;cdsapi\u0026#39;)\r#for this step there must exist the file .cdsapirc\rserver = cdsapi$Client() #start the connection\rWith the syntax of the script from the Show API request single level, we can create the query in R.\n#we create the query\rquery \u0026lt;- r_to_py(list(\rvariable= \u0026quot;2m_temperature\u0026quot;,\rproduct_type= \u0026quot;reanalysis\u0026quot;,\ryear= \u0026quot;2018\u0026quot;,\rmonth= \u0026quot;07\u0026quot;, #formato: \u0026quot;01\u0026quot;,\u0026quot;01\u0026quot;, etc.\rday= str_pad(1:31,2,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;), time= str_c(0:23,\u0026quot;00\u0026quot;,sep=\u0026quot;:\u0026quot;)%\u0026gt;%str_pad(5,\u0026quot;left\u0026quot;,\u0026quot;0\u0026quot;),\rformat= \u0026quot;netcdf\u0026quot;,\rarea = \u0026quot;45/-20/35/5\u0026quot; # North, West, South, East\r))\r#query to get the ncdf\rserver$retrieve(\u0026quot;reanalysis-era5-single-levels\u0026quot;,\rquery,\r\u0026quot;era5_ta_2018.nc\u0026quot;)\rIt is possible that the first time an error message is received, given that the required terms and conditions have not yet been accepted. Simply, the indicated link should be followed.\nError in py_call_impl(callable, dots$args, dots$keywords) : Exception: Client has not agreed to the required terms and conditions.. To access this resource, you first need to accept the termsof \u0026#39;Licence to Use Copernicus Products\u0026#39; at https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products\rFrom here we can follow the same steps as with ERA-Interim.\n#open the connection with the file\rnc \u0026lt;- nc_open(\u0026quot;era5_ta_2018.nc\u0026quot;)\r#extract lon, lat\rlat \u0026lt;- ncvar_get(nc,\u0026#39;latitude\u0026#39;)\rlon \u0026lt;- ncvar_get(nc,\u0026#39;longitude\u0026#39;)\rdim(lat);dim(lon)\r#extract time\rt \u0026lt;- ncvar_get(nc, \u0026quot;time\u0026quot;)\r#time unit: hours from 1900-01-01\rncatt_get(nc,\u0026#39;time\u0026#39;)\r#we convert the hours into date+time #as_datetime from lubridate needs seconds\rtimestamp \u0026lt;- as_datetime(c(t*60*60),origin=\u0026quot;1900-01-01\u0026quot;)\r#temperatures in K from july 2018\rhead(timestamp)\r#import temperature data\rdata \u0026lt;- ncvar_get(nc,\u0026quot;t2m\u0026quot;)\r#plot 2018-07-01\rfilled.contour(data[,,1])\r#time serie plot for a pixel\rplot(data.frame(date=timestamp,\rta=data[1,5,]),\rtype=\u0026quot;l\u0026quot;)\r#close the conection with the ncdf file\rnc_close(nc)\r\n\r","date":1537005584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537005584,"objectID":"786e97c169dec212e503ce7288af513e","permalink":"https://dominicroye.github.io/en/2018/access-to-climate-reanalysis-data-from-r/","publishdate":"2018-09-15T10:59:44+01:00","relpermalink":"/en/2018/access-to-climate-reanalysis-data-from-r/","section":"post","summary":"In this post, I will show how we can download and work directly with data from climatic reanalysis in R. These kind of datasets are combination of forcast models and data assimilation systems, which allows us to create corrected global grids of recent history of the atmosphere, land surface, and oceans.","tags":["reanalisis","interim","NCEP/NCAR","era","download","ncdf","access","api","python","ECMWF"],"title":"Access to climate reanalysis data from R","type":"post"},{"authors":null,"categories":["datavis","R","R:elementary"],"content":"\r\rWelcome to my blog! I am Dominic Royé, researcher and lecturer of physical geography at the University of Santiago de Compostela. One of my passions is R programming to visualize and analyze any type of data. Hence, my idea of this blog has its origin in my datavis publications I have been cooking in the last year on Twitter on different topics describing the world. In addition, I would like to take advantage of the blog and publish short introductions and explanation on data visualization, management and manipulation in R. I hope you like it. Any suggestion or ideas are welcomed.\nBackground\rI have always wanted to write about the use of the pie chart. The pie chart is widely used in research, teaching, journalism or technical reports. I do not know if it is due to Excel, but even worse than the pie chart itself, is its 3D version (the same for the bar chart). About the 3D versions, I only want to say that they are not recommended, since in these cases the third dimension does not contain any information and therefore it does not help to correctly read the information of the graphic. Regarding the pie chart, among many experts its use is not advised. But why?\nAlready in a study conducted by Simkin (1987) they found that the interpretation and processing of angles is more difficult than that of linear forms. Mostly it is easier to read a bar chart than a pie chart. A problem that becomes very visible when we have; 1) too many categories 2) few differences between categories 3) a misuse of colors as legend or 4) comparisons between various pie charts.\nIn general, to decide what possible graphic representations exist for our data, I recommend using the website www.data-to-viz.com or the Financial Times Visual Vocabulary.\n\nWell, now what alternative ways can we use in R?\n\rAlternatives to the pie chart\rThe dataset we will use about the vaccination status of measles correspond to June 2018 in Europe and come from the ECDC.\n#packages\rlibrary(tidyverse)\rlibrary(scales)\rlibrary(RColorBrewer)\r#data\rmeasles \u0026lt;- data.frame(\rvacc_status=c(\u0026quot;Unvaccinated\u0026quot;,\u0026quot;1 Dose\u0026quot;,\r\u0026quot;\u0026gt;= 2 Dose\u0026quot;,\u0026quot;Unkown Dose\u0026quot;,\u0026quot;Unkown\u0026quot;),\rprop=c(0.75,0.091,0.05,0.012,0.096)\r)\r#we order from the highest to the lowest and fix it with a factor\rmeasles \u0026lt;- arrange(measles,\rdesc(prop))%\u0026gt;%\rmutate(vacc_status=factor(vacc_status,vacc_status))\r\r\rvacc_status\rprop\r\r\r\rUnvaccinated\r0.750\r\rUnkown\r0.096\r\r1 Dose\r0.091\r\r\u0026gt;= 2 Dose\r0.050\r\rUnkown Dose\r0.012\r\r\r\rBar plot or similar\rggplot(measles,aes(vacc_status,prop))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\rggplot(measles,aes(x=vacc_status,prop,ymin=0,ymax=prop))+\rgeom_pointrange()+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1))+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;)+\rtheme_minimal()\r#custom themes definitions\rtheme_singlebar \u0026lt;- theme_bw()+\rtheme(\rlegend.position = \u0026quot;bottom\u0026quot;,\raxis.title = element_blank(),\raxis.ticks.y = element_blank(),\raxis.text.y = element_blank(),\rpanel.border = element_blank(),\rpanel.grid=element_blank(),\rplot.title=element_text(size=14, face=\u0026quot;bold\u0026quot;)\r)\r#plot\rmutate(measles,\rvacc_status=factor(vacc_status, #we change the order of the categories\rrev(levels(vacc_status))))%\u0026gt;%\rggplot(aes(1,prop,fill=vacc_status))+ #we put 1 in x to create a single bar\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent,\rlimits=c(0,1),\rexpand=c(.01,.01))+\rscale_x_continuous(expand=c(0,0))+\rscale_fill_brewer(\u0026quot;\u0026quot;,palette=\u0026quot;Set1\u0026quot;)+\rcoord_flip()+\rtheme_singlebar\r#we expand our data with numbers from Italy\rmeasles2 \u0026lt;- mutate(measles,\ritaly=c(0.826,0.081,0.053,0.013,0.027),\rvacc_status=factor(vacc_status,rev(levels(vacc_status))))%\u0026gt;%\rrename(europe=\u0026quot;prop\u0026quot;)%\u0026gt;%\rgather(region,prop,europe:italy)\r#plot\rggplot(measles2,aes(region,prop,fill=vacc_status))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,position=\u0026quot;stack\u0026quot;)+ #stack bar\rscale_y_continuous(breaks=seq(0,1,.1),\rlabels=percent, #convert to %\rlimits=c(0,1),\rexpand=c(0,0))+\rscale_fill_brewer(palette = \u0026quot;Set1\u0026quot;)+\rlabs(x=\u0026quot;\u0026quot;,y=\u0026quot;\u0026quot;,fill=\u0026quot;Vaccination Status\u0026quot;)+\rtheme_minimal()\r\rWaffle plot\r#package\rlibrary(waffle)\r#the waffle function uses a vector with names\rval_measles \u0026lt;- round(measles$prop*100)\rnames(val_measles) \u0026lt;- measles$vacc_status\r#plot\rwaffle(val_measles, #data\rcolors=brewer.pal(5,\u0026quot;Set1\u0026quot;), #colors\rrows=5) #row number \rThe Waffle chart seems very interesting to me when we want to show a proportion of an individual category.\n#data\rmedida \u0026lt;- c(41,59) #data from the OECD 2015\rnames(medida) \u0026lt;- c(\u0026quot;Estudios Superiores\u0026quot;,\u0026quot;Otros estudios\u0026quot;)\r#plot\rwaffle(medida,\rcolors=c(\u0026quot;#377eb8\u0026quot;,\u0026quot;#bdbdbd\u0026quot;),\rrows=5)\r\rTreemap\r#package\rlibrary(treemap)\r#plot\rtreemap(measles,\rindex=\u0026quot;vacc_status\u0026quot;, #variable with categories\rvSize=\u0026quot;prop\u0026quot;, #values\rtype=\u0026quot;index\u0026quot;, #style more in ?treemap\rtitle=\u0026quot;\u0026quot;, palette = brewer.pal(5,\u0026quot;Set1\u0026quot;) #colors\r)\rPersonally, I think that all types of graphic representations have their advantages and disadvantages. However, we currently have a huge variety of alternatives to avoid using the pie chart. If you still want to make a pie chart, which I would not rule out either, I recommend following certain rules, which you can find very well summarized in a recent post by Lisa Charlotte Rost. For example, you should order from the highest to the lowest unless there is a natural order or use a maximum of five categories. Finally, I leave you a link to a cheat sheet from policyviz with basic rules of data visualization. A good reference on graphics using different programs from Excel to R can be found in the book Creating More Effective Graphs (Robbins 2013).\n\rReferences\r\r\r","date":1534896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534896000,"objectID":"a71031793d5a9e203fe0bd8265ebbbcc","permalink":"https://dominicroye.github.io/en/2018/the-pie-chart/","publishdate":"2018-08-22T00:00:00Z","relpermalink":"/en/2018/the-pie-chart/","section":"post","summary":"Welcome to my blog! I am Dominic Royé, researcher and lecturer of physical geography at the University of Santiago de Compostela. One of my passions is R programming to visualize and analyze any type of data.","tags":["pie chart","data","circular","proportions","first post","treemap","waffle","bar"],"title":"the pie chart","type":"post"},{"authors":["D Royé","A Figueiras","M Taracido-Trunk"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"ea6e53da0515d25a726b65d99f008856","permalink":"https://dominicroye.github.io/en/publication/2018-over-the-counter-drugs-pharmacoepidemiolgy/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/en/publication/2018-over-the-counter-drugs-pharmacoepidemiolgy/","section":"publication","summary":"The consumption of medication, especially over-the-counter (OTC) drugs, can reflect environmental exposure with a lesser degree of severity in terms of morbidity. The non-linear effects of maximum and minimum apparent temperature on respiratory drug sales in A Coruña from 2006 to 2010 were examined using a distributed lag non-linear model. In particular, low apparent temperatures proved to be associated with increased sales of respiratory drugs. The strongest consistent risk estimates were found for minimum apparent temperatures in respiratory drug sales with an increase of 33.4% (95% CI: 12.5-58.0%) when the temperature changed from 2.8 ºC to −1.4 ºC. These findings may serve to guide the planning of public health interventions in order to predict and manage the health effects of exposure to the thermal environment for lower degrees of morbidity. More precisely, significant increases in the use of measured OTC medication could be used to identify and anticipate influenza outbreaks due to a more sensitive degree of the data source.","tags":["drug sales","pharmacoepidemiology","respiratory cause","short‐term effects","Spain","thermal environment"],"title":"Short-term effects of heat and cold on respiratory drug use. A time-series epidemiological study in A Coruña, Spain","type":"publication"},{"authors":["D Royé","N Lorenzo","J Martin-Vide"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"d2783bf176c9ad719df9b5bca1010728","permalink":"https://dominicroye.github.io/en/publication/2019-lightning-galicia-natural-hazards/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/en/publication/2019-lightning-galicia-natural-hazards/","section":"publication","summary":"The spatial-temporal patterns of cloud-to-ground (CG) lightning covering the period 2010-2015 over the northwest Iberian Peninsula were investigated. The analysis conducted employed three main methods: the circulation weather types developed by Jenkinson \u0026 Collison, the fit of a generalized additive model for geographic variables and the use of a concentration index for the ratio of lightning strikes and thunderstorm days. The main activity in the summer months can be attributed to situations with eastern or anticyclonic flow due to convection by insolation. In winter, lightning proves to have a frontal origin and is mainly associated with western or cyclonic flow situations which occur with advections of air masses of maritime origin. The largest number of CG discharges occurs under eastern flow and their hybrids with anticyclonic situations. Thunderstorms with greater CG lightning activity, highlighted by a higher Concentration Index, are located in areas with a higher density of lightning strikes, above all in mountainous areas away from the sea. The modeling of lightning density with geographic variables shows the positive influence of altitude and, particularly, distance to the sea, with nonlinear relationships due to the complex orography of the region. Likewise, areas with convex topography receive more lightning strikes than concave ones, a relation which has been demonstrated for the first time from a Generalized Additive Model (GAM).","tags":["thunderstorm","iberian peninsula","concentration index","weather types","convexity index","generalized additive model","cloud-to-ground lightning"],"title":"Spatial–temporal patterns of cloud-to-ground lightning over the northwest Iberian Peninsula during the period 2010–2015","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"069710e0950378660e62ac25f43fac73","permalink":"https://dominicroye.github.io/en/publication/2017-hotnights-barcelona-ij-biometeo/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/en/publication/2017-hotnights-barcelona-ij-biometeo/","section":"publication","summary":"Heat-related effects on mortality have been widely analyzed using maximum and minimum temperatures as exposure variables. Nevertheless, the main focus is usually on the former with the minimum temperature being limited in use as far as human health effects are concerned. Therefore, new thermal indices were used in this research to describe the duration of night hours with air temperatures higher than the 95% percentile of the minimum temperature (Hot Night hours) and intensity as the summation of these air temperatures in degrees (Hot Night degrees). An exposure-response relationship between mortality due to natural, respiratory and cardiovascular causes and summer night temperatures was assessed using data from the Barcelona region between 2003 and 2013. The non-linear relationship between the exposure and response variables was modeled using a distributed lag non-linear model. The estimated associations for both exposure variables and mortality shows a relationship with high and medium values that persist significantly up to a lag of 1–2 days. In mortality due to natural causes an increase of 1.1% per 10% (CI95% 0.6–1.5) for Hot Night hours and 5.8% per each 10º (CI95% 3.5–8.2%) for Hot Night degrees is observed. The effects of Hot Night hours reach their maximum with 100% and leads to an increase by 9.2% (CI95% 5.3–13.1%). The hourly description of night heat effects reduced to a single indicator in duration and intensity is a new approach and shows a different perspective and significant heat-related effects on human health.","tags":["heat","mortality","tropical night","hot night","effects","human health","climate change"],"title":"The effects of hot nights on mortality in Barcelona, Spain","type":"publication"},{"authors":["D Royé","J Martin-Vide"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"26c02ac889210ed94598f98e7006ee4b","permalink":"https://dominicroye.github.io/en/publication/2017-ci-usa-atmospheric-research/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/en/publication/2017-ci-usa-atmospheric-research/","section":"publication","summary":"The contiguous US exhibits a wide variety of precipitation regimes, first, because of the wide range of latitudes and altitudes. The physiographic units with a basic meridional configuration contribute to the differentiation between east and west in the country while generating some large interior continental spaces. The frequency distribution of daily precipitation amounts almost anywhere conforms to a negative exponential distribution, reflecting the fact that there are many small daily totals and few large ones. Positive exponential curves, which plot the cumulative percentages of days with precipitation against the cumulative percentage of the rainfall amounts that they contribute, can be evaluated through the Concentration Index. The Concentration Index has been applied to the contiguous United States using a gridded climate dataset of daily precipitation data, at a resolution of 0.25°, provided by CPC/NOAA/OAR/Earth System Research Laboratory, for the period between 1956 and 2006. At the same time, other rainfall indices and variables such as the annual coefficient of variation, seasonal rainfall regimes and the probabilities of a day with precipitation have been presented with a view to explaining spatial CI patterns. The spatial distribution of the CI in the contiguous United States is geographically consistent, reflecting the principal physiographic and climatic units of the country. Likewise, linear correlations have been established between the CI and geographical factors such as latitude, longitude and altitude. In the latter case the Pearson correlation coefficient (r) between this factor and the CI is −0.51 (p-value ","tags":["concentration index","contiguous United States","daily precipitation","precipitation indices","spatial–temporal patterns"],"title":"Concentration of Daily Precipitation in the Contiguous United States","type":"publication"},{"authors":null,"categories":null,"content":"\n                                                                \n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"f587a6ed3c666895dc27f4decefa8d5e","permalink":"https://dominicroye.github.io/en/project/climate/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/en/project/climate/","section":"project","summary":".","tags":["climate","meteo","weather","dataviz","atmosphere","temperature"],"title":"Climate and Weather","type":"project"},{"authors":["P Fdez-Arroyabe","D Royé"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f837334b8a2b59cb162956814d1fe5cb","permalink":"https://dominicroye.github.io/en/publication/2017-health-related-climate-services-springer/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/publication/2017-health-related-climate-services-springer/","section":"publication","summary":"Co-creation of scientific knowledge based on new technologies and big data sources is one of the main challenges for the digital society in the XXI century. Data management and the analysis of patterns among datasets based on machine learning and artificial intelligence has become essential for many sectors nowadays. The development of real time health-related climate services represents an example where abundant structured and unstructured information and transdisciplinary research are needed. The study of the interactions between atmospheric processes and human health through a big data approach can reveal the hidden value of data. The Oxyalert technological platform is presented as an example of a digital biometeorological infrastructure able to forecast, at an individual level, oxygen changes impacts on human health.","tags":["co-creation","interdisciplinarity","transdisciplinarity","morbidity","climate services","digital divide","big data","health"],"title":"Co-creation and Participatory Design of Big Data Infrastructures on the Field of Human Health Related Climate Services","type":"publication"},{"authors":null,"categories":null,"content":"\n                                                                \n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2af5c55ffb6a7c9711c91ff9da440e2f","permalink":"https://dominicroye.github.io/en/project/geography/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/project/geography/","section":"project","summary":".","tags":["geography","dataviz"],"title":"Geography","type":"project"},{"authors":["D Royé","J Taboada","A Ezpeleta-Martí","N Lorenzo"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"9c734b1b35d39b36fa1e6438b90d6e59","permalink":"https://dominicroye.github.io/en/publication/2016-cwt-hospital-galicia-ij-biometeo/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/en/publication/2016-cwt-hospital-galicia-ij-biometeo/","section":"publication","summary":"The link between various pathologies and atmospheric conditions has been a constant topic of study over recent decades in many places across the world; knowing more about it enables us to pre-empt the worsening of certain diseases, thereby optimizing medical resources. This study looked specifically at the connections in winter between respiratory diseases and types of atmospheric weather conditions (Circulation Weather Types, CWT) in Galicia, a region in the north-western corner of the Iberian Peninsula. To do this, the study used hospital admission data associated with these pathologies as well as an automatic classification of weather types. The main result obtained was that weather types giving rise to an increase in admissions due to these diseases are those associated with cold, dry weather, such as those in the east and south-east, or anticyclonic types. A second peak was associated with humid, hotter weather, generally linked to south-west weather types. In the future, this result may help to forecast the increase in respiratory pathologies in the region some days in advance.","tags":["weather type","respiratory diseases","hospital admissions","human health","Spain"],"title":"Winter circulation weather types and hospital admissions for respiratory diseases in Galicia, Spain","type":"publication"},{"authors":["D Royé","A Ezpeleta-Martí"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"6cc0078c47ea592afe6336436ef9ed39","permalink":"https://dominicroye.github.io/en/publication/2015-hotnight-fachada-atlantica-bage/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/en/publication/2015-hotnight-fachada-atlantica-bage/","section":"publication","summary":"Analysis of tropical nights on the Atlantic coast of the Iberian peninsula. A proposed methodology. This paper presents a new methodology for the study of warm nights, also called «tropical», in Galicia and Portugal in order to identify those nights where people can be affected by heat stress. The use of two indicators obtained through half-hourly data has allowed us to define in more detail the thermal characteristics of the nights between May and October, thereby being able to more accurately assess the risk to the health and well-being of the population. There is a significant increase in the frequency of tropical nights and warm nights on the Atlantic coast, from the north of Galicia to the south of Portugal. The lower latitude and proximity to the coastline are associated with greater persistence of heat and thermal stress during these nights. In inland areas the persistence is less. The warmest nights are more frequent and intense in centres of the cities, due to the effect of the urban heat island.","tags":["tropical night","thermic stress","heat island","Galicia","Portugal"],"title":"Analysis of tropical nights on the atlantic coast of the Iberian Peninsula. A proposed methodology","type":"publication"},{"authors":["D Royé"],"categories":null,"content":"Used datasets are available for download here. Alternative datasets for Spain in ncdf format can be downloaded:\nAEMET\n  Gridded 20km and 50km (precipitation and temperature)\n  Gridded 5km (precipitation)\n  CSIC\n Gridded 5km (precipitation)  ","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"43da158fda5ff317611ce8996ecab175","permalink":"https://dominicroye.github.io/en/publication/2015-manual-ncdf-semata/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/en/publication/2015-manual-ncdf-semata/","section":"publication","summary":"A practical introduction in the use of netCDF in the environment of R Spatio-temporal data is currently key to many disciplines, especially to climatology and meteorology. A widespread format is netCDF allowing a multidimensional structure and an exchange of data machine independently. In this article, we introduce the use of these databases with the free software environment R. To do this, we will work with a grid of the maximum temperature of the Iberian Peninsula for the period 1971-2007. The goal is to read and visualize the netCDF format, and make some fist overall and specifi calculations. Finally the applicability is shown in a case study: the diurnal temperature variation in the Iberian Peninsula for January and August 2006. (Spanish)","tags":["netCDF","R","climatology","temperature","matrix","database"],"title":"The use of climate databases netCDF with array structure in the environment of R","type":"publication"},{"authors":null,"categories":null,"content":"\n                              \n","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"60c1fbba20235e8b3ac453c8f914ad01","permalink":"https://dominicroye.github.io/en/project/population/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/en/project/population/","section":"project","summary":".","tags":["population","dataviz"],"title":"Population","type":"project"},{"authors":null,"categories":null,"content":"\n   Urban development since 1900 for the center of Galicia, Spain. Data: HISDAC-ES.\n   Urban development since 1900 for the center of mainland Spain. Data: HISDAC-ES.\n   How cloudiness changes throughout the year in Europe. Data: METEOSAT.\n   Smoothed daily maximum temperature throughout the year in Europe. Data: ERA5-Land.\n   Smoothed daily maximum temperature throughout the year in China. Data: ERA5-Land.\n   Smoothed daily maximum temperature throughout the year in the Indian subcontinent. Data: ERA5-Land.\n   Smoothed daily precipitation throughout the year in Brazil. Data: ERA5-Land.\n   Smoothed daily maximum temperature throughout the year in Brazil. Data: ERA5-Land.\n   The average temperature of 24 hours in August 2020 for Europe. Data: ERA5-Land.\n   The average temperature of 24 hours in January 2020. Data: ERA5-Land.\n   Smoothed daily rainfall throughout the year in Australia. Data: SILO.\n   Smoothed daily maximum temperature throughout the year in Australia. Data: SILO.\n   How do the spatial patterns of daily precipitation change throughout the year in Europe? Data: E-OBS.\n   Smoothed daily maximum temperature throughout the year in the contiguous USA. Data: PRISM.\n   Smoothed daily maximum temperature throughout the year in Europe. Data: E-OBS.\n   How do the spatial patterns of daily precipitation change throughout the year in mainland Spain and the Balearic Islands? Data: SPREAD.\n   Smoothed daily sea surface temperature throughout the year for the Northeast Atlantic, the Mediterranean, North and Black Sea. Data: NOAA/NODC.\n   Probability of a summer day (maximum temperature greater than 25ºC/77ºF) through the year in Europe. Data: E-OBS.\n   Probability of a summer day (maximum temperature greater than 25ºC/77ºF) through the year in the Contiguous United States. Data: PRISM.\n   Probability of a summer day (maximum temperature greater than 25ºC) through the year in Australia. Data: SILO.\n   Probability of a frost day (minimum temperature less than 0ºC) through the year in Europe. Data: E-OBS 18e.\n   Probability of a frost day (minimum temperature less than 0ºC/32ºF) through the year in the Contiguous United States. Data: PRISM. Platform: Google Earth Engine.\n\n","date":1398556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398556800,"objectID":"46d95729e9233e3265f2537f46bb13f7","permalink":"https://dominicroye.github.io/en/project/animations/","publishdate":"2014-04-27T00:00:00Z","relpermalink":"/en/project/animations/","section":"project","summary":".","tags":["climate","meteo","maps","charts","weather","dataviz","atmosphere","temperature"],"title":"Animations","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://dominicroye.github.io/en/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]